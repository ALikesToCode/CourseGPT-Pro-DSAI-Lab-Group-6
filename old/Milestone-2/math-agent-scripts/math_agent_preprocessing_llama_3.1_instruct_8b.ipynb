{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9375ba83",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd193c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U transformers datasets accelerate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d68d81",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548d839",
   "metadata": {},
   "source": [
    "### 1. Dataset Overview\n",
    "\n",
    "* **Dataset Name:** `XenArcAI/MathX-5M`\n",
    "* **Total Size:** ~2.43 GB\n",
    "* **Number of Rows:** ~4.32 Million\n",
    "* **Format:** Parquet\n",
    "* **Task:** Text Generation, Question Answering\n",
    "* **Description:** `MathX-5M` is a large-scale, high-quality corpus of mathematical reasoning problems. It contains 5 million examples of problems with detailed, step-by-step solutions, making it ideal for instruction-based fine-tuning. The dataset covers a wide range of mathematical domains, from basic arithmetic to advanced calculus.\n",
    "\n",
    "### 2. Data Features\n",
    "\n",
    "The dataset consists of three primary columns (features):\n",
    "\n",
    "1.  **`problem`** (string):\n",
    "    * **Content:** This column contains the mathematical problem statement. The problems are expressed in natural language and often include LaTeX formatting for mathematical notation.\n",
    "    * **Example:** \"Determine how many 1000 digit numbers \\\\( a \\\\) have the property that when any digit...\"\n",
    "    * **Observations:** The problems vary significantly in length and complexity, ranging from simple calculations to complex, multi-step proofs.\n",
    "\n",
    "2.  **`expected_answer`** (string):\n",
    "    * **Content:** This column holds the final, correct answer to the problem. The answers are typically concise and may also use LaTeX.\n",
    "    * **Example:** `\\[[32]]`\n",
    "    * **Observations:** This feature provides the ground truth for evaluating the model's final output. The format is generally clean and direct.\n",
    "\n",
    "3.  **`generated_solution`** (string):\n",
    "    * **Content:** This is arguably the most valuable feature for fine-tuning. It contains a detailed, step-by-step thought process and derivation of the solution. It often starts with a `<think>` tag, outlining the reasoning path.\n",
    "    * **Example:** `<think> Okay, so I need to figure out how many 100-digit numbers ... The problem is to compute the ... </think> To find the probability...`\n",
    "    * **Observations:** This \"chain-of-thought\" or reasoning path is crucial for teaching the model *how* to solve problems, not just what the final answer is. The quality and detail of these solutions are key to the dataset's effectiveness.\n",
    "\n",
    "### 3. Initial Findings & Implications for Fine-Tuning\n",
    "\n",
    "* **Instructional Format:** The dataset's structure is perfectly suited for instruction fine-tuning. The combination of a problem, a reasoning process, and a final answer provides a clear and rich learning signal for the model.\n",
    "* **LaTeX Formatting:** The prevalence of LaTeX means the tokenizer and model must be proficient at handling mathematical notation.\n",
    "* **Chain-of-Thought:** The `generated_solution` column enables the model to learn complex reasoning. During fine-tuning, the prompt should be structured to encourage the model to generate a similar step-by-step thought process before arriving at the final answer.\n",
    "* **Data Scale:** With over 4 million rows, the dataset is substantial. Even a small fraction of this data is sufficient for effective fine-tuning, especially when using techniques like LoRA.\n",
    "* **Complexity Distribution:** The dataset claims a distribution of basic (30%), intermediate (30%), and advanced (40%) problems. This diversity is excellent for training a well-rounded model that can handle a variety of mathematical challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f2814-9662-419e-8411-cebf6ae916f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nailsonseat/anaconda3/envs/dsaiproject/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from itertools import islice\n",
    "\n",
    "# Step 1: Load the dataset in streaming mode\n",
    "streamed_dataset = load_dataset(\"XenArcAI/MathX-5M\", split=\"train\", streaming=True)\n",
    "\n",
    "# Step 2: Normalize columns on the fly\n",
    "def unify_columns(ex):\n",
    "    if \"question\" in ex:\n",
    "        ex[\"problem\"] = ex.pop(\"question\")\n",
    "    return ex\n",
    "\n",
    "streamed_dataset = streamed_dataset.map(unify_columns)\n",
    "\n",
    "# Step 3: Take first 1% (~10k examples) and materialize in memory\n",
    "subset = list(islice(streamed_dataset, 10000))\n",
    "\n",
    "dataset = Dataset.from_list(subset).select_columns(\n",
    "    [\"problem\", \"generated_solution\", \"expected_answer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['problem', 'generated_solution', 'expected_answer'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd622c-767a-4435-accb-474ec8a82834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Problem #1\n",
       "Given a group of $ N $ balls consisting of $ C $ colors, where the number of balls in each color is represented as $ n_1, n_2, \\ldots, n_C $ (with $ n_1 + n_2 + \\ldots + n_C = N $), what is the probability that when $ A $ balls are randomly picked (where $ A \\leq N $), the picked balls consist of $ a_1, a_2, \\ldots, a_C $ balls of each color, where $ a_1 + a_2 + \\ldots + a_C = A $?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Problem #1\n",
       "Given a group of $ N $ balls consisting of $ C $ colors, where the number of balls in each color is represented as $ n_1, n_2, \\ldots, n_C $ (with $ n_1 + n_2 + \\ldots + n_C = N $), what is the probability that when $ A $ balls are randomly picked (where $ A \\leq N $), the picked balls consist of $ a_1, a_2, \\ldots, a_C $ balls of each color, where $ a_1 + a_2 + \\ldots + a_C = A $?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Solution (truncated):**\n",
       "<think>\n",
       "Okay, so I need to find the probability that when I pick A balls out of N, where there are C different colors, the number of each color I pick is exactly a1, a2, ..., aC. Hmm, let's think about how to approach this.\n",
       "\n",
       "First, probability problems often involve combinations. ..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Problem #1\n",
       "Given a group of $ N $ balls consisting of $ C $ colors, where the number of balls in each color is represented as $ n_1, n_2, \\ldots, n_C $ (with $ n_1 + n_2 + \\ldots + n_C = N $), what is the probability that when $ A $ balls are randomly picked (where $ A \\leq N $), the picked balls consist of $ a_1, a_2, \\ldots, a_C $ balls of each color, where $ a_1 + a_2 + \\ldots + a_C = A $?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Solution (truncated):**\n",
       "<think>\n",
       "Okay, so I need to find the probability that when I pick A balls out of N, where there are C different colors, the number of each color I pick is exactly a1, a2, ..., aC. Hmm, let's think about how to approach this.\n",
       "\n",
       "First, probability problems often involve combinations. ..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Expected Answer:**\n",
       "$\\frac{C_{n_1}^{a_1} \\cdot C_{n_2}^{a_2} \\cdot \\ldots \\cdot C_{n_C}^{a_C}}{C_N^A}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def show_example_jupyter(dataset, index=0, max_solution_len=300):\n",
    "    example = dataset[index]\n",
    "\n",
    "    # Wrap problem in Markdown-friendly LaTeX\n",
    "    problem = example['problem']\n",
    "    solution = example['generated_solution']\n",
    "    answer = example['expected_answer']\n",
    "\n",
    "    # Truncate solution for readability\n",
    "    if len(solution) > max_solution_len:\n",
    "        solution = solution[:max_solution_len]\n",
    "        last_period = solution.rfind('.')\n",
    "        if last_period != -1:\n",
    "            solution = solution[:last_period+1] + \" ...\"\n",
    "\n",
    "    # Replace inline parentheses like ( N ) with proper LaTeX\n",
    "    problem = problem.replace(\"\\\\(\", \"$\").replace(\"\\\\)\", \"$\")\n",
    "    answer = answer.replace(\"\\\\(\", \"$\").replace(\"\\\\)\", \"$\")\n",
    "\n",
    "    display(Markdown(f\"### Problem #{index+1}\\n{problem}\"))\n",
    "    display(Markdown(f\"**Generated Solution (truncated):**\\n{solution}\"))\n",
    "    display(Markdown(f\"**Expected Answer:**\\n{answer}\"))\n",
    "    display(Markdown(\"---\"))\n",
    "\n",
    "show_example_jupyter(dataset, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42be333-d5b6-4053-b48f-3df881a134f9",
   "metadata": {},
   "source": [
    "## Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d974195-2606-4e9d-aaca-172a95091b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a41044-d255-4a45-aca3-4ad768667fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16b5d3",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:01<00:00, 5519.77 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:01<00:00, 5519.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Proper Llama 3 Chat Template\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    \"You are a helpful assistant that solves math problems step by step.<|eot_id|>\"\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n{user_message}<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{assistant_response}<|eot_id|>\"\n",
    ")\n",
    "\n",
    "def format_prompt(sample):\n",
    "    # Create the user message with problem and expected solution format\n",
    "    user_message = f\"Problem:\\n{sample['problem']}\\n\\nPlease provide a step-by-step solution.\"\n",
    "    \n",
    "    # The assistant response contains the generated solution\n",
    "    assistant_response = sample['generated_solution']\n",
    "    \n",
    "    # Apply the proper Llama 3 chat template\n",
    "    formatted_text = LLAMA_3_CHAT_TEMPLATE.format(\n",
    "        user_message=user_message,\n",
    "        assistant_response=assistant_response\n",
    "    )\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Apply to dataset\n",
    "print(\"üîÑ Formatting dataset with proper Llama 3 chat template...\")\n",
    "formatted_dataset = dataset.map(format_prompt)\n",
    "print(\"‚úÖ Dataset formatted successfully!\")\n",
    "\n",
    "# Show an example of the formatted text\n",
    "print(\"\\nüìù Example of formatted text:\")\n",
    "print(\"=\" * 80)\n",
    "print(formatted_dataset[0]['text'][:500] + \"...\" if len(formatted_dataset[0]['text']) > 500 else formatted_dataset[0]['text'])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4777a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0af5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training arguments - REDUCED FOR LOW MEMORY\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-8b-math-tuned\",\n",
    "    per_device_train_batch_size=1,      # Reduced from 4 to 1\n",
    "    gradient_accumulation_steps=16,     # Increased from 4 to 16 to maintain effective batch size\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=100,      \n",
    "    save_steps=50,\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=False,        # Reduce memory usage\n",
    "    gradient_checkpointing=True,        # Trade compute for memory\n",
    "    remove_unused_columns=False,        # Keep all columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce28a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Status:\n",
      "  Total: 5.68 GB\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Free: 5.68 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU Memory Optimization\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory and cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
    "        free = total - allocated\n",
    "        \n",
    "        print(f\"GPU Memory Status:\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\") \n",
    "        print(f\"  Free: {free:.2f} GB\")\n",
    "        \n",
    "        return free > 1.0  # Return True if we have >1GB free\n",
    "    return False\n",
    "\n",
    "# Clear memory before training\n",
    "clear_gpu_memory()\n",
    "has_memory = check_gpu_memory()\n",
    "\n",
    "if not has_memory:\n",
    "    print(\"‚ö†Ô∏è Warning: Low GPU memory detected. Consider using CPU training or smaller model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d217baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== System Check ===\n",
      "Configured download directory: /home/nailsonseat/StudioProjects/CourseGPT-Pro-DSAI-Lab-Group-6/Milestone-2/math-agent-scripts/models\n",
      "Available disk space in './models': 30.58 GB / 338.11 GB\n",
      "Hugging Face connectivity: OK (Status: 200)\n",
      "Model cache found at: ./models/models--meta-llama--Llama-3.1-8B-Instruct\n",
      "Cache size: 29.92 GB\n",
      "\n",
      "System Status:\n",
      "- Download location: /home/nailsonseat/StudioProjects/CourseGPT-Pro-DSAI-Lab-Group-6/Milestone-2/math-agent-scripts/models\n",
      "- Disk space: ‚úì (30.6 GB available)\n",
      "- Connectivity: ‚úì\n",
      "- Model cached: ‚úì\n"
     ]
    }
   ],
   "source": [
    "# Check system requirements and connectivity before loading model\n",
    "import shutil\n",
    "import subprocess\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# ===== CONFIGURE DOWNLOAD LOCATION =====\n",
    "# This should match the MODEL_DOWNLOAD_DIR in the next cell\n",
    "MODEL_DOWNLOAD_DIR = \"./models\"\n",
    "\n",
    "# Check available disk space in the specified directory\n",
    "def check_disk_space(path=\".\"):\n",
    "    total, used, free = shutil.disk_usage(path)\n",
    "    free_gb = free / (1024**3)\n",
    "    total_gb = total / (1024**3)\n",
    "    print(f\"Available disk space in '{path}': {free_gb:.2f} GB / {total_gb:.2f} GB\")\n",
    "    return free_gb\n",
    "\n",
    "# Check internet connectivity to Hugging Face\n",
    "def check_connectivity():\n",
    "    try:\n",
    "        response = requests.get(\"https://huggingface.co\", timeout=10)\n",
    "        print(f\"Hugging Face connectivity: OK (Status: {response.status_code})\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Hugging Face connectivity: FAILED - {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Check if model is already cached\n",
    "def check_model_cache(cache_dir):\n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Look for the specific model cache\n",
    "    model_cache_path = os.path.join(cache_dir, \"models--meta-llama--Llama-3.1-8B-Instruct\")\n",
    "    if os.path.exists(model_cache_path):\n",
    "        # Check cache size\n",
    "        cache_size = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
    "                        for dirpath, dirnames, filenames in os.walk(model_cache_path)\n",
    "                        for filename in filenames)\n",
    "        cache_size_gb = cache_size / (1024**3)\n",
    "        print(f\"Model cache found at: {model_cache_path}\")\n",
    "        print(f\"Cache size: {cache_size_gb:.2f} GB\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"No model cache found in: {cache_dir}\")\n",
    "        print(\"Will download from scratch\")\n",
    "        return False\n",
    "\n",
    "print(\"=== System Check ===\")\n",
    "print(f\"Configured download directory: {os.path.abspath(MODEL_DOWNLOAD_DIR)}\")\n",
    "\n",
    "# Check disk space in the download directory\n",
    "free_space = check_disk_space(MODEL_DOWNLOAD_DIR if os.path.exists(MODEL_DOWNLOAD_DIR) else \".\")\n",
    "connectivity = check_connectivity()\n",
    "cache_exists = check_model_cache(MODEL_DOWNLOAD_DIR)\n",
    "\n",
    "print(f\"\\nSystem Status:\")\n",
    "print(f\"- Download location: {os.path.abspath(MODEL_DOWNLOAD_DIR)}\")\n",
    "print(f\"- Disk space: {'‚úì' if free_space > 20 else '‚ö†Ô∏è'} ({free_space:.1f} GB available)\")\n",
    "print(f\"- Connectivity: {'‚úì' if connectivity else '‚ùå'}\")\n",
    "print(f\"- Model cached: {'‚úì' if cache_exists else '‚ùå'}\")\n",
    "\n",
    "if free_space < 20:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Less than 20GB free space. Llama-3.1-8B requires ~15GB+\")\n",
    "    print(f\"Consider changing MODEL_DOWNLOAD_DIR to a location with more space\")\n",
    "if not connectivity:\n",
    "    print(f\"\\n‚ùå No internet connectivity - cannot download model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58ab6187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be downloaded to: /home/nailsonseat/StudioProjects/CourseGPT-Pro-DSAI-Lab-Group-6/Milestone-2/math-agent-scripts/models\n",
      "Loading model with 4-bit quantization to reduce memory usage...\n",
      "Quantized loading failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying without quantization...\n",
      "Quantized loading failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying without quantization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be downloaded to: /home/nailsonseat/StudioProjects/CourseGPT-Pro-DSAI-Lab-Group-6/Milestone-2/math-agent-scripts/models\n",
      "Loading model with 4-bit quantization to reduce memory usage...\n",
      "Quantized loading failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying without quantization...\n",
      "Quantized loading failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying without quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.39it/s]\n",
      "\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be downloaded to: /home/nailsonseat/StudioProjects/CourseGPT-Pro-DSAI-Lab-Group-6/Milestone-2/math-agent-scripts/models\n",
      "Loading model with 4-bit quantization to reduce memory usage...\n",
      "Quantized loading failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying without quantization...\n",
      "Quantized loading failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying without quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.39it/s]\n",
      "\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SFTTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:01<00:00, 5518.88 examples/s]\n",
      "Tokenizing train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [01:45<00:00, 95.06 examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [01:45<00:00, 95.06 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 65161.83 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 65161.83 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be downloaded to: /home/nailsonseat/StudioProjects/CourseGPT-Pro-DSAI-Lab-Group-6/Milestone-2/math-agent-scripts/models\n",
      "Loading model with 4-bit quantization to reduce memory usage...\n",
      "Quantized loading failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying without quantization...\n",
      "Quantized loading failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying without quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.39it/s]\n",
      "\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SFTTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:01<00:00, 5518.88 examples/s]\n",
      "Tokenizing train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [01:45<00:00, 95.06 examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [01:45<00:00, 95.06 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 65161.83 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 65161.83 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SFTTrainer created successfully!\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "import time\n",
    "\n",
    "# ===== CONFIGURE DOWNLOAD LOCATION =====\n",
    "MODEL_DOWNLOAD_DIR = \"./models\"\n",
    "os.makedirs(MODEL_DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"300\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = MODEL_DOWNLOAD_DIR\n",
    "os.environ[\"HF_HOME\"] = MODEL_DOWNLOAD_DIR\n",
    "\n",
    "print(f\"Model will be downloaded to: {os.path.abspath(MODEL_DOWNLOAD_DIR)}\")\n",
    "\n",
    "# ===== MEMORY OPTIMIZATION =====\n",
    "# Clear GPU memory before loading model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Configure 4-bit quantization to reduce memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # Use 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",          # Normalized float 4-bit\n",
    "    bnb_4bit_use_double_quant=True,     # Double quantization for better accuracy\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Compute in bfloat16\n",
    ")\n",
    "\n",
    "print(\"Loading model with 4-bit quantization to reduce memory usage...\")\n",
    "\n",
    "try:\n",
    "    # Load model with quantization - should use ~2.5GB instead of ~15GB\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,    # Enable 4-bit quantization\n",
    "        device_map=\"auto\",                 # Automatically distribute layers\n",
    "        cache_dir=MODEL_DOWNLOAD_DIR,\n",
    "        force_download=False,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Model loaded with 4-bit quantization!\")\n",
    "    \n",
    "    # Check GPU memory after loading\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"GPU memory used after model loading: {allocated:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Quantized loading failed: {e}\")\n",
    "    print(\"Trying without quantization...\")\n",
    "    \n",
    "    # Fallback to regular loading\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=MODEL_DOWNLOAD_DIR,\n",
    "        force_download=False,\n",
    "    )\n",
    "\n",
    "# Create SFTTrainer\n",
    "print(\"Creating SFTTrainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "print(\"‚úì SFTTrainer created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644663bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72b1b1",
   "metadata": {},
   "source": [
    "## Export adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"./llama3-8b-math-tuned-adapters\"\n",
    "trainer.save_model(adapter_path)\n",
    "\n",
    "print(f\"LoRA adapters saved to {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa9503",
   "metadata": {},
   "source": [
    "## Merge with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63396f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# --- Reload the base model without quantization ---\n",
    "# This is important for merging and for Ollama compatibility\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# --- Load the PeftModel with the adapters ---\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# --- Merge the weights and save the new model ---\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "merged_model_path = \"./llama3-8b-math-merged\"\n",
    "model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "\n",
    "print(f\"Merged model saved to {merged_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289eb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r llama3-8b-math-merged.zip ./llama3-8b-math-merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsaiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
