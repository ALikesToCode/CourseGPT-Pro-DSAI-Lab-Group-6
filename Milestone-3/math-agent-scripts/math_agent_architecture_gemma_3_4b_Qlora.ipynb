{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c7e879",
   "metadata": {},
   "source": [
    "# Fine-Tuning Gemma-3-4B on MathX-5M Dataset\n",
    "\n",
    "This notebook demonstrates fine-tuning Google's Gemma-3-4B-IT model for mathematical reasoning using the XenArcAI/MathX-5M dataset with QLoRA (Quantized Low-Rank Adaptation).\n",
    "\n",
    "## Overview\n",
    "- **Model**: google/gemma-3-4b-it (4B parameter instruction-tuned model)\n",
    "- **Dataset**: XenArcAI/MathX-5M (~4.32M mathematical problems with solutions)\n",
    "- **Technique**: QLoRA with 4-bit quantization\n",
    "- **Goal**: Train a math reasoning agent capable of step-by-step problem solving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282f342",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ff99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a200e",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1b8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e50513",
   "metadata": {},
   "source": [
    "## 3. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefdbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/gemma-3-4b-it\n",
      "Dataset: XenArcAI/MathX-5M (train[:1%])\n",
      "Output directory: ./gemma3-4b-math-lora-adapter\n"
     ]
    }
   ],
   "source": [
    "# Model and dataset configuration\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "DATASET_NAME = \"XenArcAI/MathX-5M\"\n",
    "\n",
    "# Directory to store downloaded models and tokenizers\n",
    "MODEL_DIR = \"./models\"\n",
    "\n",
    "# Use a small fraction for demo/testing (remove [:1%] to use full dataset)\n",
    "DATASET_SPLIT = \"train[:1%]\"  # ~43K examples\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = \"./gemma3-4b-math-lora-adapter\"\n",
    "MERGED_MODEL_DIR = \"./gemma3-4b-math-merged\"\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "PER_DEVICE_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch size = 2 * 8 = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "LOGGING_STEPS = 10\n",
    "SAVE_STEPS = 100\n",
    "\n",
    "# Set your Hugging Face token if needed\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_NAME} ({DATASET_SPLIT})\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf872ca0",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Dataset\n",
    "\n",
    "The MathX-5M dataset contains:\n",
    "- **problem**: Mathematical problem statement (often with LaTeX)\n",
    "- **expected_answer**: Final correct answer\n",
    "- **generated_solution**: Step-by-step reasoning with `<think>` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b90489a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f05d49fd324fa3b60b61dcdcbba4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loaded successfully!\n",
      "Number of examples: 10000\n",
      "\n",
      "Dataset features: {'problem': Value('string'), 'generated_solution': Value('string'), 'expected_answer': Value('string')}\n",
      "\n",
      "Column names: ['problem', 'generated_solution', 'expected_answer']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from itertools import islice\n",
    "\n",
    "# Step 1: Load the dataset in streaming mode\n",
    "streamed_dataset = load_dataset(\"XenArcAI/MathX-5M\", split=\"train\", streaming=True)\n",
    "\n",
    "# Step 2: Normalize columns on the fly\n",
    "def unify_columns(ex):\n",
    "    if \"question\" in ex:\n",
    "        ex[\"problem\"] = ex.pop(\"question\")\n",
    "    return ex\n",
    "\n",
    "streamed_dataset = streamed_dataset.map(unify_columns)\n",
    "\n",
    "# Step 3: Take first 1% (~10k examples) and materialize in memory\n",
    "subset = list(islice(streamed_dataset, 10000))\n",
    "\n",
    "dataset = Dataset.from_list(subset).select_columns([\n",
    "    \"problem\", \"generated_solution\", \"expected_answer\"\n",
    "])\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Number of examples: {len(dataset)}\")\n",
    "print(f\"\\nDataset features: {dataset.features}\")\n",
    "print(f\"\\nColumn names: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c5b2eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE EXAMPLE FROM DATASET\n",
      "================================================================================\n",
      "\n",
      "PROBLEM:\n",
      "Given a group of \\( N \\) balls consisting of \\( C \\) colors, where the number of balls in each color is represented as \\( n_1, n_2, \\ldots, n_C \\) (with \\( n_1 + n_2 + \\ldots + n_C = N \\)), what is the probability that when \\( A \\) balls are randomly picked (where \\( A \\leq N \\)), the picked balls consist of \\( a_1, a_2, \\ldots, a_C \\) balls of each color, where \\( a_1 + a_2 + \\ldots + a_C = A \\)?...\n",
      "\n",
      "\n",
      "EXPECTED ANSWER:\n",
      "\\(\\frac{C_{n_1}^{a_1} \\cdot C_{n_2}^{a_2} \\cdot \\ldots \\cdot C_{n_C}^{a_C}}{C_N^A}\\)\n",
      "\n",
      "\n",
      "GENERATED SOLUTION (first 800 chars):\n",
      "<think>\n",
      "Okay, so I need to find the probability that when I pick A balls out of N, where there are C different colors, the number of each color I pick is exactly a1, a2, ..., aC. Hmm, let's think about how to approach this.\n",
      "\n",
      "First, probability problems often involve combinations. The general formula for probability is the number of favorable outcomes divided by the total number of possible outcomes. So, in this case, the favorable outcomes are the ways to pick exactly a1 of color 1, a2 of color 2, and so on up to aC of color C. The total possible outcomes would be all the ways to pick A balls regardless of color distribution.\n",
      "\n",
      "Let me break it down. The total number of ways to choose A balls from N is the combination of N choose A, which is C(N, A) = N! / (A! (N - A)!).\n",
      "\n",
      "Now, the favorable ...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample example\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE EXAMPLE FROM DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nPROBLEM:\\n{sample['problem'][:500]}...\")\n",
    "print(f\"\\n\\nEXPECTED ANSWER:\\n{sample['expected_answer']}\")\n",
    "print(f\"\\n\\nGENERATED SOLUTION (first 800 chars):\\n{sample['generated_solution'][:800]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c2201",
   "metadata": {},
   "source": [
    "## 5. Load Tokenizer\n",
    "\n",
    "Load the Gemma-3 tokenizer and configure it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744352d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: google/gemma-3-4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef45cacd2d584cb8903063df0d7a445a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DSAI-Project\\CourseGPT-Pro-DSAI-Lab-Group-6\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shubh\\.cache\\huggingface\\hub\\models--google--gemma-3-4b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a926963eaf498b8c5256dd2f3e152e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c374bd44ea842d784dd67c5964da365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2772a285b98e4040b4004acec9cf5605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c0b1373491409393d21ccaa26c2fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n",
      "Vocab size: 262145\n",
      "PAD token: <pad> (ID: 0)\n",
      "EOS token: <eos> (ID: 1)\n",
      "BOS token: <bos> (ID: 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer: {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=MODEL_DIR,\n",
    ")\n",
    "\n",
    "# Set padding token (Gemma models typically use eos_token as pad_token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer loaded successfully!\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e10f2",
   "metadata": {},
   "source": [
    "## 6. Format Dataset for Instruction Tuning\n",
    "\n",
    "Format the dataset using Gemma's chat template. We'll combine the problem and solution into a conversational format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d57172d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee6973692984fd6b64758c9eaaeb77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset formatted successfully!\n",
      "Number of examples: 10000\n"
     ]
    }
   ],
   "source": [
    "def format_math_example(example):\n",
    "    \"\"\"\n",
    "    Format a math example for instruction tuning.\n",
    "    Creates a conversation with:\n",
    "    - System: Math assistant role\n",
    "    - User: Problem statement\n",
    "    - Assistant: Step-by-step solution with final answer\n",
    "    \"\"\"\n",
    "    # Construct the prompt\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert mathematics tutor. Solve problems step-by-step, showing your reasoning clearly.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": example['problem']\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": example['generated_solution']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "print(\"Formatting dataset...\")\n",
    "formatted_dataset = dataset.map(\n",
    "    format_math_example,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset formatted successfully!\")\n",
    "print(f\"Number of examples: {len(formatted_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de4d0a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FORMATTED EXAMPLE\n",
      "================================================================================\n",
      "<bos><start_of_turn>user\n",
      "You are an expert mathematics tutor. Solve problems step-by-step, showing your reasoning clearly.\n",
      "\n",
      "Given a group of \\( N \\) balls consisting of \\( C \\) colors, where the number of balls in each color is represented as \\( n_1, n_2, \\ldots, n_C \\) (with \\( n_1 + n_2 + \\ldots + n_C = N \\)), what is the probability that when \\( A \\) balls are randomly picked (where \\( A \\leq N \\)), the picked balls consist of \\( a_1, a_2, \\ldots, a_C \\) balls of each color, where \\( a_1 + a_2 + \\ldots + a_C = A \\)?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<think>\n",
      "Okay, so I need to find the probability that when I pick A balls out of N, where there are C different colors, the number of each color I pick is exactly a1, a2, ..., aC. Hmm, let's think about how to approach this.\n",
      "\n",
      "First, probability problems often involve combinations. The general formula for probability is the number of favorable outcomes divided by the total number of possible outcomes. So, in this case, the favorable outcom\n",
      "...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Inspect a formatted example\n",
    "print(\"=\" * 80)\n",
    "print(\"FORMATTED EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(formatted_dataset[0]['text'][:1000])\n",
    "print(\"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16620806",
   "metadata": {},
   "source": [
    "## 7. Configure 4-bit Quantization (QLoRA)\n",
    "\n",
    "Set up BitsAndBytes configuration for 4-bit quantization to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0240fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute dtype: torch.float16\n",
      "\n",
      "4-bit quantization config:\n",
      "  - Quantization type: nf4\n",
      "  - Compute dtype: torch.float16\n",
      "  - Double quantization: True\n"
     ]
    }
   ],
   "source": [
    "# Determine compute dtype based on GPU capability\n",
    "COMPUTE_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "print(f\"Compute dtype: {COMPUTE_DTYPE}\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",               # NormalFloat 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=COMPUTE_DTYPE,    # Compute in BF16/FP16\n",
    "    bnb_4bit_use_double_quant=True,          # Nested quantization for extra memory savings\n",
    ")\n",
    "\n",
    "print(\"\\n4-bit quantization config:\")\n",
    "print(f\"  - Quantization type: nf4\")\n",
    "print(f\"  - Compute dtype: {COMPUTE_DTYPE}\")\n",
    "print(f\"  - Double quantization: True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba5ab2",
   "metadata": {},
   "source": [
    "## 8. Load Gemma-3 Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec92fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/gemma-3-4b-it with 4-bit quantization...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30e0d958e834070bb8feb9d0215b7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0567e7546d1148f1a0e5cef13b33ef9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67c244099ad47e0ad23cf273cab0e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18c6e89fd214cf19eabdfc6838bd0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d123eb6d51438f9f19e2c75f34cdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea1a72b4e6d464d83ed3bfa7ff02800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c71c5d8b86458b8e9a83a25a5bcb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully!\n",
      "Model dtype: torch.float32\n",
      "Device map: {'': 'cpu'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_NAME} with 4-bit quantization...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    "    cache_dir=MODEL_DIR,\n",
    ")\n",
    "\n",
    "# Disable caching for training\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Device map: {model.hf_device_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c166f6f",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA\n",
    "\n",
    "Set up Low-Rank Adaptation (LoRA) for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842091cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,                                    # LoRA attention dimension (Rank)\n",
    "    lora_alpha=32,                           # Scaling factor (usually 2*r is a good start)\n",
    "    target_modules=[                         # Target all linear layers for Gemma\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  - Rank (r): {lora_config.r}\")\n",
    "print(f\"  - Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  - Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  - Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  - Task type: {lora_config.task_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02344945",
   "metadata": {},
   "source": [
    "## 10. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e11787",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optim=\"paged_adamw_8bit\",                # Memory-efficient optimizer\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    \n",
    "    # Mixed precision training\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    \n",
    "    # Dataset settings\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,  # Set to True for better efficiency if sequences are short\n",
    "    \n",
    "    # Other settings\n",
    "    report_to=\"none\",  # Change to \"wandb\" if you want to use Weights & Biases\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  - Epochs: {NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"  - Batch size per device: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"  - Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  - Effective batch size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  - Mixed precision: {'BF16' if torch.cuda.is_bf16_supported() else 'FP16'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e085530",
   "metadata": {},
   "source": [
    "## 11. Initialize SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4858ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing SFT Trainer...\\n\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"\\nTrainable parameters:\")\n",
    "trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb0757",
   "metadata": {},
   "source": [
    "## 12. Start Training\n",
    "\n",
    "âš ï¸ **Note**: Training will take time depending on your hardware and dataset size. Monitor GPU memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis may take several hours depending on your hardware and dataset size.\")\n",
    "print(\"Monitor GPU usage and adjust batch size if needed.\\n\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed152e",
   "metadata": {},
   "source": [
    "## 13. Save LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_PATH = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
    "\n",
    "print(f\"Saving LoRA adapters to: {ADAPTER_PATH}\")\n",
    "\n",
    "trainer.model.save_pretrained(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "\n",
    "print(f\"\\nLoRA adapters saved successfully!\")\n",
    "print(f\"Location: {ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677276a",
   "metadata": {},
   "source": [
    "## 14. Clean Up Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f677e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before loading merged model\n",
    "del trainer\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27201f79",
   "metadata": {},
   "source": [
    "## 15. Merge LoRA Adapters with Base Model (Optional)\n",
    "\n",
    "Merge the LoRA adapters back into the base model for easier deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model for merging...\")\n",
    "\n",
    "# Load base model without quantization for merging\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN,\n",
    "    cache_dir=MODEL_DIR,\n",
    ")\n",
    "\n",
    "print(f\"Loading LoRA adapters from: {ADAPTER_PATH}\")\n",
    "model_with_adapter = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "\n",
    "print(\"Merging adapters with base model...\")\n",
    "merged_model = model_with_adapter.merge_and_unload()\n",
    "\n",
    "print(f\"\\nSaving merged model to: {MERGED_MODEL_DIR}\")\n",
    "merged_model.save_pretrained(MERGED_MODEL_DIR)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_DIR)\n",
    "\n",
    "print(f\"\\nMerged model saved successfully!\")\n",
    "print(f\"Location: {MERGED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f20bc4",
   "metadata": {},
   "source": [
    "## 16. Test the Fine-Tuned Model\n",
    "\n",
    "Let's test the model with a sample math problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ba645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "test_problem = \"\"\"Solve the following problem step by step:\n",
    "\n",
    "A rectangle has a length that is 3 units longer than twice its width. \n",
    "If the perimeter of the rectangle is 54 units, find the dimensions of the rectangle.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert mathematics tutor. Solve problems step-by-step, showing your reasoning clearly.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": test_problem\n",
    "    }\n",
    "]\n",
    "\n",
    "# Format prompt\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(merged_model.device)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nProblem:\\n{test_problem}\\n\")\n",
    "print(\"Generating solution...\\n\")\n",
    "\n",
    "# Generate response\n",
    "outputs = merged_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract just the assistant's response\n",
    "response_start = response.find(\"assistant\") + len(\"assistant\")\n",
    "assistant_response = response[response_start:].strip()\n",
    "\n",
    "print(\"Model Response:\")\n",
    "print(\"-\" * 80)\n",
    "print(assistant_response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a1ca7",
   "metadata": {},
   "source": [
    "## 17. Summary\n",
    "\n",
    "### Training Complete! ðŸŽ‰\n",
    "\n",
    "**What we did:**\n",
    "1. Loaded XenArcAI/MathX-5M dataset (4.32M math problems)\n",
    "2. Formatted data for instruction tuning with Gemma chat template\n",
    "3. Configured 4-bit quantization (QLoRA) for memory efficiency\n",
    "4. Applied LoRA adapters (r=16, alpha=32) to Gemma-3-4B\n",
    "5. Fine-tuned on mathematical reasoning tasks\n",
    "6. Saved adapters and merged model\n",
    "\n",
    "**Output files:**\n",
    "- LoRA adapters: `./gemma3-4b-math-lora-adapter/final_adapter/`\n",
    "- Merged model: `./gemma3-4b-math-merged/`\n",
    "\n",
    "**Next steps:**\n",
    "- Evaluate model on validation set\n",
    "- Test on various math problem types\n",
    "- Deploy as a math reasoning agent\n",
    "- Optionally: Train for more epochs or on larger dataset fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5585ce",
   "metadata": {},
   "source": [
    "## Appendix: Model Loading for Inference\n",
    "\n",
    "To use the fine-tuned model later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load merged model for inference\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"./gemma3-4b-math-merged\",\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./gemma3-4b-math-merged\")\n",
    "\n",
    "# Or load with adapters:\n",
    "# from peft import PeftModel\n",
    "#\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"google/gemma-3-4b-it\",\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     base_model,\n",
    "#     \"./gemma3-4b-math-lora-adapter/final_adapter\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CourseGPT",
   "language": "python",
   "name": "coursegpt-math"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
