{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9375ba83",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd193c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U transformers datasets accelerate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d68d81",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2fdf2",
   "metadata": {},
   "source": [
    "### 1. Dataset Overview\n",
    "\n",
    "* **Dataset Name:** `XenArcAI/MathX-5M`\n",
    "* **Total Size:** ~2.43 GB\n",
    "* **Number of Rows:** ~4.32 Million\n",
    "* **Format:** Parquet\n",
    "* **Task:** Text Generation, Question Answering\n",
    "* **Description:** `MathX-5M` is a large-scale, high-quality corpus of mathematical reasoning problems. It contains 5 million examples of problems with detailed, step-by-step solutions, making it ideal for instruction-based fine-tuning. The dataset covers a wide range of mathematical domains, from basic arithmetic to advanced calculus.\n",
    "\n",
    "### 2. Data Features\n",
    "\n",
    "The dataset consists of three primary columns (features):\n",
    "\n",
    "1.  **`problem`** (string):\n",
    "    * **Content:** This column contains the mathematical problem statement. The problems are expressed in natural language and often include LaTeX formatting for mathematical notation.\n",
    "    * **Example:** \"Determine how many 1000 digit numbers \\\\( a \\\\) have the property that when any digit...\"\n",
    "    * **Observations:** The problems vary significantly in length and complexity, ranging from simple calculations to complex, multi-step proofs.\n",
    "\n",
    "2.  **`expected_answer`** (string):\n",
    "    * **Content:** This column holds the final, correct answer to the problem. The answers are typically concise and may also use LaTeX.\n",
    "    * **Example:** `\\[[32]]`\n",
    "    * **Observations:** This feature provides the ground truth for evaluating the model's final output. The format is generally clean and direct.\n",
    "\n",
    "3.  **`generated_solution`** (string):\n",
    "    * **Content:** This is arguably the most valuable feature for fine-tuning. It contains a detailed, step-by-step thought process and derivation of the solution. It often starts with a `<think>` tag, outlining the reasoning path.\n",
    "    * **Example:** `<think> Okay, so I need to figure out how many 100-digit numbers ... The problem is to compute the ... </think> To find the probability...`\n",
    "    * **Observations:** This \"chain-of-thought\" or reasoning path is crucial for teaching the model *how* to solve problems, not just what the final answer is. The quality and detail of these solutions are key to the dataset's effectiveness.\n",
    "\n",
    "### 3. Initial Findings & Implications for Fine-Tuning\n",
    "\n",
    "* **Instructional Format:** The dataset's structure is perfectly suited for instruction fine-tuning. The combination of a problem, a reasoning process, and a final answer provides a clear and rich learning signal for the model.\n",
    "* **LaTeX Formatting:** The prevalence of LaTeX means the tokenizer and model must be proficient at handling mathematical notation.\n",
    "* **Chain-of-Thought:** The `generated_solution` column enables the model to learn complex reasoning. During fine-tuning, the prompt should be structured to encourage the model to generate a similar step-by-step thought process before arriving at the final answer.\n",
    "* **Data Scale:** With over 4 million rows, the dataset is substantial. Even a small fraction of this data is sufficient for effective fine-tuning, especially when using techniques like LoRA.\n",
    "* **Complexity Distribution:** The dataset claims a distribution of basic (30%), intermediate (30%), and advanced (40%) problems. This diversity is excellent for training a well-rounded model that can handle a variety of mathematical challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56f2814-9662-419e-8411-cebf6ae916f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nailsonseat/anaconda3/envs/dsaiproject/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from itertools import islice\n",
    "\n",
    "# Step 1: Load the dataset in streaming mode\n",
    "streamed_dataset = load_dataset(\"XenArcAI/MathX-5M\", split=\"train\", streaming=True)\n",
    "\n",
    "# Step 2: Normalize columns on the fly\n",
    "def unify_columns(ex):\n",
    "    if \"question\" in ex:\n",
    "        ex[\"problem\"] = ex.pop(\"question\")\n",
    "    return ex\n",
    "\n",
    "streamed_dataset = streamed_dataset.map(unify_columns)\n",
    "\n",
    "# Step 3: Take first 1% (~10k examples) and materialize in memory\n",
    "subset = list(islice(streamed_dataset, 10000))\n",
    "\n",
    "dataset = Dataset.from_list(subset).select_columns(\n",
    "    [\"problem\", \"generated_solution\", \"expected_answer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d317558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['problem', 'generated_solution', 'expected_answer'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bdd622c-767a-4435-accb-474ec8a82834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Problem #1\n",
       "Given a group of $ N $ balls consisting of $ C $ colors, where the number of balls in each color is represented as $ n_1, n_2, \\ldots, n_C $ (with $ n_1 + n_2 + \\ldots + n_C = N $), what is the probability that when $ A $ balls are randomly picked (where $ A \\leq N $), the picked balls consist of $ a_1, a_2, \\ldots, a_C $ balls of each color, where $ a_1 + a_2 + \\ldots + a_C = A $?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Solution (truncated):**\n",
       "<think>\n",
       "Okay, so I need to find the probability that when I pick A balls out of N, where there are C different colors, the number of each color I pick is exactly a1, a2, ..., aC. Hmm, let's think about how to approach this.\n",
       "\n",
       "First, probability problems often involve combinations. ..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Expected Answer:**\n",
       "$\\frac{C_{n_1}^{a_1} \\cdot C_{n_2}^{a_2} \\cdot \\ldots \\cdot C_{n_C}^{a_C}}{C_N^A}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def show_example_jupyter(dataset, index=0, max_solution_len=300):\n",
    "    example = dataset[index]\n",
    "\n",
    "    # Wrap problem in Markdown-friendly LaTeX\n",
    "    problem = example['problem']\n",
    "    solution = example['generated_solution']\n",
    "    answer = example['expected_answer']\n",
    "\n",
    "    # Truncate solution for readability\n",
    "    if len(solution) > max_solution_len:\n",
    "        solution = solution[:max_solution_len]\n",
    "        last_period = solution.rfind('.')\n",
    "        if last_period != -1:\n",
    "            solution = solution[:last_period+1] + \" ...\"\n",
    "\n",
    "    # Replace inline parentheses like ( N ) with proper LaTeX\n",
    "    problem = problem.replace(\"\\\\(\", \"$\").replace(\"\\\\)\", \"$\")\n",
    "    answer = answer.replace(\"\\\\(\", \"$\").replace(\"\\\\)\", \"$\")\n",
    "\n",
    "    display(Markdown(f\"### Problem #{index+1}\\n{problem}\"))\n",
    "    display(Markdown(f\"**Generated Solution (truncated):**\\n{solution}\"))\n",
    "    display(Markdown(f\"**Expected Answer:**\\n{answer}\"))\n",
    "    display(Markdown(\"---\"))\n",
    "\n",
    "show_example_jupyter(dataset, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42be333-d5b6-4053-b48f-3df881a134f9",
   "metadata": {},
   "source": [
    "## Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d974195-2606-4e9d-aaca-172a95091b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a41044-d255-4a45-aca3-4ad768667fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Selected Model: meta-llama/Llama-3.2-3B-Instruct\n",
      "üíæ Expected GPU Usage: ~4.5GB (fits comfortably in 6GB)\n",
      "‚ö° Training Speed: Medium-Fast (4-5 hours estimated)\n",
      "üéØ Quality: Excellent for math reasoning tasks\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# üöÄ LLAMA 3.2 3B MODEL - Perfect for 6GB GPU!\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16b5d3",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:02<00:00, 4940.41 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:02<00:00, 4940.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Proper Llama 3 Chat Template\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    \"You are a helpful assistant that solves math problems step by step.<|eot_id|>\"\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n{user_message}<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{assistant_response}<|eot_id|>\"\n",
    ")\n",
    "\n",
    "def format_prompt(sample):\n",
    "    # Create the user message with problem and expected solution format\n",
    "    user_message = f\"Problem:\\n{sample['problem']}\\n\\nPlease provide a step-by-step solution.\"\n",
    "    \n",
    "    # The assistant response contains the generated solution\n",
    "    assistant_response = sample['generated_solution']\n",
    "    \n",
    "    # Apply the proper Llama 3 chat template\n",
    "    formatted_text = LLAMA_3_CHAT_TEMPLATE.format(\n",
    "        user_message=user_message,\n",
    "        assistant_response=assistant_response\n",
    "    )\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Apply to dataset\n",
    "print(\"Formatting dataset with proper Llama 3 chat template...\")\n",
    "formatted_dataset = dataset.map(format_prompt)\n",
    "print(\"Dataset formatted successfully!\")\n",
    "\n",
    "# Show an example of the formatted text\n",
    "print(\"\\nExample of formatted text:\")\n",
    "print(\"=\" * 80)\n",
    "print(formatted_dataset[0]['text'][:500] + \"...\" if len(formatted_dataset[0]['text']) > 500 else formatted_dataset[0]['text'])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4777a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0af5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA configuration - Optimized for Llama 3.2 3B\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                    # Rank - good balance for 3B model\n",
    "    lora_alpha=32,           # Scaling factor\n",
    "    lora_dropout=0.05,       # Dropout for regularization  \n",
    "    bias=\"none\",             # No bias adaptation\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Target all linear layers\n",
    ")\n",
    "\n",
    "# Training arguments - Optimized for 6GB GPU with 3B model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3.2-3b-math-tuned\",\n",
    "    per_device_train_batch_size=2,      # Increased from 1 - 3B model allows this\n",
    "    gradient_accumulation_steps=8,      # Reduced since we can use larger batch\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=100,      \n",
    "    save_steps=50,\n",
    "    fp16=True,                          # Use mixed precision\n",
    "    dataloader_pin_memory=False,        # Reduce memory usage\n",
    "    gradient_checkpointing=True,        # Trade compute for memory\n",
    "    remove_unused_columns=False,        # Keep all columns\n",
    "    warmup_steps=10,                    # Add warmup for stability\n",
    "    optim=\"paged_adamw_8bit\",          # Use 8-bit optimizer to save memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce28a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Status:\n",
      "  Total: 5.68 GB\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Free: 5.68 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU Memory Optimization\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory and cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
    "        free = total - allocated\n",
    "        \n",
    "        print(f\"GPU Memory Status:\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\") \n",
    "        print(f\"  Free: {free:.2f} GB\")\n",
    "        \n",
    "        return free > 1.0  # Return True if we have >1GB free\n",
    "    return False\n",
    "\n",
    "# Clear memory before training\n",
    "clear_gpu_memory()\n",
    "has_memory = check_gpu_memory()\n",
    "\n",
    "if not has_memory:\n",
    "    print(\"‚ö†Ô∏è Warning: Low GPU memory detected. Consider using CPU training or smaller model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d217baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç === SYSTEM CHECK FOR LLAMA 3.2 3B ===\n",
      "üìÇ Download directory: /home/nailsonseat/StudioProjects/CourseGPT-Pro-DSAI-Lab-Group-6/Milestone-2/math-agent-scripts/models\n",
      "Available disk space in './models': 30.57 GB / 338.11 GB\n",
      "Hugging Face connectivity: OK (Status: 200)\n",
      "üì• No Llama 3.2 3B cache found - will download (~6GB)\n",
      "\n",
      "üìä System Status for Llama 3.2 3B:\n",
      "- üíæ Disk space: ‚úÖ (30.6 GB available)\n",
      "- üåê Connectivity: ‚úÖ\n",
      "- üì¶ Model cached: üì• Will download\n"
     ]
    }
   ],
   "source": [
    "# Check system requirements and connectivity before loading model\n",
    "import shutil\n",
    "import subprocess\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# ===== CONFIGURE DOWNLOAD LOCATION =====\n",
    "MODEL_DOWNLOAD_DIR = \"./models\"\n",
    "\n",
    "# Check available disk space in the specified directory\n",
    "def check_disk_space(path=\".\"):\n",
    "    total, used, free = shutil.disk_usage(path)\n",
    "    free_gb = free / (1024**3)\n",
    "    total_gb = total / (1024**3)\n",
    "    print(f\"Available disk space in '{path}': {free_gb:.2f} GB / {total_gb:.2f} GB\")\n",
    "    return free_gb\n",
    "\n",
    "# Check internet connectivity to Hugging Face\n",
    "def check_connectivity():\n",
    "    try:\n",
    "        response = requests.get(\"https://huggingface.co\", timeout=10)\n",
    "        print(f\"Hugging Face connectivity: OK (Status: {response.status_code})\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Hugging Face connectivity: FAILED - {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Check if model is already cached - Updated for Llama 3.2 3B\n",
    "def check_model_cache(cache_dir):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Look for the Llama 3.2 3B cache\n",
    "    model_cache_path = os.path.join(cache_dir, \"models--meta-llama--Llama-3.2-3B-Instruct\")\n",
    "    if os.path.exists(model_cache_path):\n",
    "        cache_size = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
    "                        for dirpath, dirnames, filenames in os.walk(model_cache_path)\n",
    "                        for filename in filenames)\n",
    "        cache_size_gb = cache_size / (1024**3)\n",
    "        print(f\"Llama 3.2 3B cache found at: {model_cache_path}\")\n",
    "        print(f\"Cache size: {cache_size_gb:.2f} GB\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\" No Llama 3.2 3B cache found - will download (~6GB)\")\n",
    "        return False\n",
    "\n",
    "print(\"=== SYSTEM CHECK FOR LLAMA 3.2 3B ===\")\n",
    "print(f\"Download directory: {os.path.abspath(MODEL_DOWNLOAD_DIR)}\")\n",
    "\n",
    "# Check system requirements\n",
    "free_space = check_disk_space(MODEL_DOWNLOAD_DIR if os.path.exists(MODEL_DOWNLOAD_DIR) else \".\")\n",
    "connectivity = check_connectivity()\n",
    "cache_exists = check_model_cache(MODEL_DOWNLOAD_DIR)\n",
    "\n",
    "print(f\"\\nüìä System Status for Llama 3.2 3B:\")\n",
    "print(f\"- üíæ Disk space: {'‚úÖ' if free_space > 10 else '‚ö†Ô∏è'} ({free_space:.1f} GB available)\")\n",
    "print(f\"- üåê Connectivity: {'‚úÖ' if connectivity else '‚ùå'}\")\n",
    "print(f\"- üì¶ Model cached: {'‚úÖ' if cache_exists else 'üì• Will download'}\")\n",
    "\n",
    "if free_space < 10:\n",
    "    print(f\"\\nWarning: Less than 10GB free space. Llama 3.2 3B needs ~6GB\")\n",
    "if not connectivity:\n",
    "    print(f\"\\nNo internet - cannot download model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab6187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading Llama 3.2 3B Model\n",
      "üìÇ Download location: /home/nailsonseat/StudioProjects/CourseGPT-Pro-DSAI-Lab-Group-6/Milestone-2/math-agent-scripts/models\n",
      "üíæ Expected GPU usage: ~4.5GB (perfect for 6GB GPU)\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üì• Loading Llama 3.2 3B (attempting without quantization first)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [19:17<00:00, 578.76s/it] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Standard loading failed: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
      "üîÑ Falling back to 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.20s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded with 4-bit quantization!\n",
      "\n",
      "üõ†Ô∏è Creating SFTTrainer for Llama 3.2 3B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:06<00:00, 1640.25 examples/s]\n",
      "\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [01:47<00:00, 93.24 examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [01:47<00:00, 93.24 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 36212.42 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SFTTrainer created successfully!\n",
      "üéØ Ready for training! The 3B model should train much faster than 8B.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "import time\n",
    "\n",
    "# ===== CONFIGURE DOWNLOAD LOCATION =====\n",
    "MODEL_DOWNLOAD_DIR = \"./models\"\n",
    "os.makedirs(MODEL_DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"300\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = MODEL_DOWNLOAD_DIR\n",
    "os.environ[\"HF_HOME\"] = MODEL_DOWNLOAD_DIR\n",
    "\n",
    "print(f\"Loading Llama 3.2 3B Model\")\n",
    "print(f\"Download location: {os.path.abspath(MODEL_DOWNLOAD_DIR)}\")\n",
    "print(f\"Expected GPU usage: ~4.5GB (perfect for 6GB GPU)\")\n",
    "\n",
    "# Clear GPU memory before loading\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared\")\n",
    "\n",
    "# For Llama 3.2 3B, we can try without quantization first (since it fits in 6GB)\n",
    "# But keep quantization as backup\n",
    "print(\"\\nLoading Llama 3.2 3B (attempting without quantization first)...\")\n",
    "\n",
    "try:\n",
    "    # Try loading without quantization first - should work fine with 6GB\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,        # Use bfloat16 precision\n",
    "        device_map=\"auto\",                 # Auto distribute across devices\n",
    "        cache_dir=MODEL_DOWNLOAD_DIR,\n",
    "        force_download=False,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n",
    "    )\n",
    "    \n",
    "    print(\"Llama 3.2 3B loaded successfully without quantization!\")\n",
    "    \n",
    "    # Check GPU memory after loading\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU memory used: {allocated:.2f} GB / {total:.2f} GB ({allocated/total*100:.1f}%)\")\n",
    "        \n",
    "        if allocated > 5.0:\n",
    "            print(\"High memory usage - will use gradient checkpointing during training\")\n",
    "        else:\n",
    "            print(\"Good memory usage - should train smoothly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Standard loading failed: {e}\")\n",
    "    print(\"Falling back to 4-bit quantization...\")\n",
    "    \n",
    "    # Fallback to quantization if needed\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=MODEL_DOWNLOAD_DIR,\n",
    "        force_download=False,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Model loaded with 4-bit quantization!\")\n",
    "\n",
    "# Create SFTTrainer\n",
    "print(\"\\nCreating SFTTrainer for Llama 3.2 3B...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "print(\"SFTTrainer created successfully!\")\n",
    "print(\"Ready for training! The 3B model should train much faster than 8B.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66fca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72b1b1",
   "metadata": {},
   "source": [
    "## Export adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"./llama3.2-3b-math-tuned-adapters\"\n",
    "trainer.save_model(adapter_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa9503",
   "metadata": {},
   "source": [
    "## Merge with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63396f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Merging LoRA adapters with Llama 3.2 3B base model...\")\n",
    "\n",
    "# Clear GPU memory first\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# --- Reload the base model without quantization ---\n",
    "print(\"Loading base Llama 3.2 3B model for merging...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=MODEL_DOWNLOAD_DIR\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapters...\")\n",
    "# --- Load the PeftModel with the adapters ---\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "print(\"Merging weights...\")\n",
    "# --- Merge the weights and save the new model ---\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "merged_model_path = \"./llama3.2-3b-math-merged\"\n",
    "print(f\"Saving merged model to {merged_model_path}...\")\n",
    "model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289eb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r llama3.2-3b-math-merged.zip ./llama3.2-3b-math-merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsaiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
