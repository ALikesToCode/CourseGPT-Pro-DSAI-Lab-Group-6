{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8780c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_dataset_builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9674ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"OpenCoder-LLM/opc-sft-stage2\"\n",
    "DATA_FILES_PATTERN = \"data/*.parquet\" # This pattern loads all Parquet files in the 'data' folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "68cf6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(DATASET_NAME,'educational_instruct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ea8a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ca92b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_id': 26622393304,\n",
       " 'instruction': 'Write a function to find the kth largest elements in an unsorted array using a min heap.',\n",
       " 'output': 'Here is the code to solve this problem: \\n```python\\nimport heapq\\n\\ndef kthLargestElement(nums, k):\\n    min_heap = []\\n    for num in nums:\\n        heapq.heappush(min_heap, num)\\n        if len(min_heap) > k:\\n            heapq.heappop(min_heap)\\n    return min_heap[0]\\n```',\n",
       " 'code': 'import heapq\\n\\ndef kthLargestElement(nums, k):\\n    min_heap = []\\n    for num in nums:\\n        heapq.heappush(min_heap, num)\\n        if len(min_heap) > k:\\n            heapq.heappop(min_heap)\\n    return min_heap[0]',\n",
       " 'entry_point': 'kthLargestElement',\n",
       " 'testcase': ['assert kthLargestElement([9,8,7,6,5,4,3,2,1],1)==9',\n",
       "  'assert kthLargestElement([3,2,3,1,2,4,5,5,6],4)==4',\n",
       "  'assert kthLargestElement([3,2,1,5,6,4],2)==5']}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1431]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b153b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_3_CODE_CHAT_TEMPLATE = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    \"You are an expert programming assistant.<|eot_id|>\" # Keep the system prompt simple and general\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{full_response}<|eot_id|>\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b473eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_code_example(example):\n",
    "    \"\"\"\n",
    "    Formats a single example by directly using the 'output' column\n",
    "    as the assistant's full response.\n",
    "    \"\"\"\n",
    "    LLAMA_3_CODE_CHAT_TEMPLATE = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    \"You are an expert programming assistant.<|eot_id|>\" # Keep the system prompt simple and general\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{full_response}<|eot_id|>\")\n",
    "    \n",
    "    full_text = LLAMA_3_CODE_CHAT_TEMPLATE.format(\n",
    "        instruction=example['instruction'],\n",
    "        full_response=example['output']\n",
    "    )\n",
    "    return {\"text\": full_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d4bb8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_formatted = train_dataset.map(\n",
    "    format_code_example,\n",
    "    remove_columns=train_dataset.column_names, # Only keep the newly created 'text' column\n",
    "    num_proc=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6dd26c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an expert programming assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWrite a function that takes two lists as input and returns a new list with elements from the first list that are not present in the second list. The function should maintain the order of elements in the first list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHere is the code to solve this problem: \\n```python\\ndef list_difference(list1, list2):\\n    \"\"\"\\n    Returns a new list with elements from list1 that are not present in list2, preserving the order of elements in list1.\\n    \"\"\"\\n    return [x for x in list1 if x not in list2]\\n```<|eot_id|>'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_formatted[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d0b232ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41da3f1cd7e240619ab22886f3469043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/119 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "92135568"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_FILE = \"llama31_code_finetune_simple.jsonl\"\n",
    "code_formatted.to_json(OUTPUT_FILE, orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fe502e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "DATASET_PATH = \"llama31_code_finetune_simple.jsonl\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a55ebe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from transformers import (  AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "71039152",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "DATASET_PATH = \"llama31_code_finetune_simple.jsonl\" # Your correctly formatted JSONL file\n",
    "OUTPUT_DIR = \"./llama31_code_lora_adapter\"\n",
    "MERGED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"merged_model\")\n",
    "SAFE_WINDOWS_WORKERS = 50 # Max workers for datasets.map() to avoid Windows handle error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac67f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38ee10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",               # Use NormalFloat 4-bit for better memory efficiency\n",
    "    bnb_4bit_compute_dtype=COMPUTE_DTYPE,    # Use BF16 for math operations on RTX 4080\n",
    "    bnb_4bit_use_double_quant=True,          # Nested quantization for extra memory saving\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e86ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,                                    # LoRA attention dimension (Rank)\n",
    "    lora_alpha=32,                           # Scaling factor (usually 2*r is a good start)\n",
    "    target_modules=[                         # QLoRA best practice: target all linear layers\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c9362d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_DEVICE_MAP = {\"\": 0, \"cpu\": 0} # Try to put everything on GPU 0, but allow CPU as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c70c43a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d585bebf0f47a0b82c24088a2e6405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1614: UserWarning: The following device_map keys do not match any submodules in the model: ['cpu']\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    \n",
    "    # ðŸš¨ FIX: Pass a custom device_map and the offload flag ðŸš¨\n",
    "    device_map=CUSTOM_DEVICE_MAP,\n",
    "    #llm_int8_enable_fp32_cpu_offload=True, \n",
    "    \n",
    "    trust_remote_code=True,\n",
    "    token=os.environ.get(\"HF_TOKEN\")\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0a6beb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    padding_side=\"right\", # Required for Llama models during training\n",
    "    add_eos_token=True,\n",
    "    token=os.environ.get(\"HF_TOKEN\")\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a0e6a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from llama31_code_finetune_simple.jsonl with explicit features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab0a100dae2465cb946ad054dea26f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value # <-- Ensure Features and Value are imported\n",
    "\n",
    "# --- 4. Load and Prepare Dataset (FINAL FIX) ---\n",
    "\n",
    "# Define the expected feature structure: a single column named 'text' with string values\n",
    "DATASET_FEATURES = Features({\"text\": Value(\"string\")})\n",
    "\n",
    "print(f\"Loading dataset from {DATASET_PATH} with explicit features...\")\n",
    "dataset = load_dataset(\n",
    "    'json', \n",
    "    data_files=DATASET_PATH, \n",
    "    split=\"train\",\n",
    "    # ðŸš¨ FINAL FIX: Force the dataset structure ðŸš¨\n",
    "    features=DATASET_FEATURES\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1bed223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,             # Per-GPU batch size\n",
    "    gradient_accumulation_steps=4,             # Accumulate gradients over 4 steps (4 * 4 = 16)\n",
    "    optim=\"paged_adamw_8bit\",                  # Optimized optimizer for QLoRA\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),   # Use FP16 if BF16 is not supported\n",
    "    bf16=torch.cuda.is_bf16_supported(),       # Use BF16 if supported (RTX 4080 does)\n",
    "    save_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    report_to=\"none\",\n",
    "    # Add gradient checkpointing for extra memory efficiency if needed\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a6a48335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,             \n",
    "    gradient_accumulation_steps=4,             \n",
    "    optim=\"paged_adamw_8bit\",                  \n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),   \n",
    "    bf16=torch.cuda.is_bf16_supported(),       \n",
    "    save_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=2048,                     # Renamed from max_seq_length\n",
    "    packing=True,\n",
    "    dataset_num_proc=SAFE_WINDOWS_WORKERS \n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fbe78770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c45ed5c1bc143359cbc0582596bc70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset (num_proc=50):   0%|          | 0/118278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This is your SFTConfig object\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#processing_class=tokenizer,       # Keep the tokenizer here\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# All data/processing parameters are gone from here!\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:830\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_only_loss \u001b[38;5;129;01mand\u001b[39;00m formatting_func:\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA formatting function was provided while `completion_only_loss=True`, which is incompatible. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a formatter converts the dataset to a language modeling type, conflicting with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    827\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion-only loss. To resolve this, apply your formatting function before passing the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, or disable `completion_only_loss` in `SFTConfig`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    829\u001b[0m     )\n\u001b[1;32m--> 830\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    834\u001b[0m     packing \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpacking \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_packing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_packing\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:963\u001b[0m, in \u001b[0;36m_prepare_dataset\u001b[1;34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3165\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3159\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3161\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3162\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3163\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3164\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs_per_job\u001b[49m\n\u001b[0;32m   3167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\utils\\py_utils.py:711\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing."
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,        # This is your SFTConfig object\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    #processing_class=tokenizer,       # Keep the tokenizer here\n",
    "    # All data/processing parameters are gone from here!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "\n",
    "ADAPTER_PATH = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
    "trainer.model.save_pretrained(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e773017",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596fc107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
