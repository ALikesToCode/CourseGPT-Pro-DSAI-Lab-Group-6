{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8780c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_dataset_builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9674ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"OpenCoder-LLM/opc-sft-stage2\"\n",
    "DATA_FILES_PATTERN = \"data/*.parquet\" # This pattern loads all Parquet files in the 'data' folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "68cf6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(DATASET_NAME,'educational_instruct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ea8a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ca92b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_id': 26622393304,\n",
       " 'instruction': 'Write a function to find the kth largest elements in an unsorted array using a min heap.',\n",
       " 'output': 'Here is the code to solve this problem: \\n```python\\nimport heapq\\n\\ndef kthLargestElement(nums, k):\\n    min_heap = []\\n    for num in nums:\\n        heapq.heappush(min_heap, num)\\n        if len(min_heap) > k:\\n            heapq.heappop(min_heap)\\n    return min_heap[0]\\n```',\n",
       " 'code': 'import heapq\\n\\ndef kthLargestElement(nums, k):\\n    min_heap = []\\n    for num in nums:\\n        heapq.heappush(min_heap, num)\\n        if len(min_heap) > k:\\n            heapq.heappop(min_heap)\\n    return min_heap[0]',\n",
       " 'entry_point': 'kthLargestElement',\n",
       " 'testcase': ['assert kthLargestElement([9,8,7,6,5,4,3,2,1],1)==9',\n",
       "  'assert kthLargestElement([3,2,3,1,2,4,5,5,6],4)==4',\n",
       "  'assert kthLargestElement([3,2,1,5,6,4],2)==5']}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1431]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b153b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_3_CODE_CHAT_TEMPLATE = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    \"You are an expert programming assistant.<|eot_id|>\" # Keep the system prompt simple and general\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{full_response}<|eot_id|>\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b473eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_code_example(example):\n",
    "    \"\"\"\n",
    "    Formats a single example by directly using the 'output' column\n",
    "    as the assistant's full response.\n",
    "    \"\"\"\n",
    "    LLAMA_3_CODE_CHAT_TEMPLATE = (\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    \"You are an expert programming assistant.<|eot_id|>\" # Keep the system prompt simple and general\n",
    "    \"<|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{full_response}<|eot_id|>\")\n",
    "    \n",
    "    full_text = LLAMA_3_CODE_CHAT_TEMPLATE.format(\n",
    "        instruction=example['instruction'],\n",
    "        full_response=example['output']\n",
    "    )\n",
    "    return {\"text\": full_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d4bb8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_formatted = train_dataset.map(\n",
    "    format_code_example,\n",
    "    remove_columns=train_dataset.column_names, # Only keep the newly created 'text' column\n",
    "    num_proc=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6dd26c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an expert programming assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWrite a function that takes two lists as input and returns a new list with elements from the first list that are not present in the second list. The function should maintain the order of elements in the first list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHere is the code to solve this problem: \\n```python\\ndef list_difference(list1, list2):\\n    \"\"\"\\n    Returns a new list with elements from list1 that are not present in list2, preserving the order of elements in list1.\\n    \"\"\"\\n    return [x for x in list1 if x not in list2]\\n```<|eot_id|>'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_formatted[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d0b232ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41da3f1cd7e240619ab22886f3469043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/119 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "92135568"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_FILE = \"llama31_code_finetune_simple.jsonl\"\n",
    "code_formatted.to_json(OUTPUT_FILE, orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe502e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "DATASET_PATH = \"llama31_code_finetune_simple.jsonl\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a55ebe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from transformers import (  AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71039152",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "DATASET_PATH = \"llama31_code_finetune_simple.jsonl\" # Your correctly formatted JSONL file\n",
    "OUTPUT_DIR = \"./llama31_code_lora_adapter\"\n",
    "MERGED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"merged_model\")\n",
    "SAFE_WINDOWS_WORKERS = 50 # Max workers for datasets.map() to avoid Windows handle error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "# DATASET_PATH = \"llama31_code_finetune_simple.jsonl\" # Your correctly formatted JSONL file\n",
    "# OUTPUT_DIR = \"./qwen_code_lora_adapter\"\n",
    "# MERGED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"merged_model\")\n",
    "# SAFE_WINDOWS_WORKERS = 50 # Max workers for datasets.map() to avoid Windows handle error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac67f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e9cc2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMPUTE_DTYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ee10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",               # Use NormalFloat 4-bit for better memory efficiency\n",
    "    bnb_4bit_compute_dtype=COMPUTE_DTYPE,    # Use BF16 for math operations on RTX 4080\n",
    "    bnb_4bit_use_double_quant=True,          # Nested quantization for extra memory saving\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e86ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,                                    # LoRA attention dimension (Rank)\n",
    "    lora_alpha=32,                           # Scaling factor (usually 2*r is a good start)\n",
    "    target_modules=[                         # QLoRA best practice: target all linear layers\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9362d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_DEVICE_MAP = {\"\": 0, \"cpu\": 0} # Try to put everything on GPU 0, but allow CPU as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70c43a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e575c999dab43788ef4a25d49baff9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1614: UserWarning: The following device_map keys do not match any submodules in the model: ['cpu']\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    \n",
    "    # ðŸš¨ FIX: Pass a custom device_map and the offload flag ðŸš¨\n",
    "    device_map=CUSTOM_DEVICE_MAP,\n",
    "    #llm_int8_enable_fp32_cpu_offload=True, \n",
    "    \n",
    "    trust_remote_code=True,\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6beb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    padding_side=\"right\", # Required for Llama models during training\n",
    "    add_eos_token=True,\n",
    "    token=os.environ.get(\"HF_TOKEN\")\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0e6a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from llama31_code_finetune_simple.jsonl with explicit features...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value # <-- Ensure Features and Value are imported\n",
    "\n",
    "# --- 4. Load and Prepare Dataset (FINAL FIX) ---\n",
    "\n",
    "# Define the expected feature structure: a single column named 'text' with string values\n",
    "DATASET_FEATURES = Features({\"text\": Value(\"string\")})\n",
    "\n",
    "print(f\"Loading dataset from {DATASET_PATH} with explicit features...\")\n",
    "dataset = load_dataset(\n",
    "    'json', \n",
    "    data_files=DATASET_PATH, \n",
    "    split=\"train\",\n",
    "    # ðŸš¨ FINAL FIX: Force the dataset structure ðŸš¨\n",
    "    features=DATASET_FEATURES\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "394536b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118278"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5d04bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04a44c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bed223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,             # Per-GPU batch size\n",
    "    gradient_accumulation_steps=4,             # Accumulate gradients over 4 steps (4 * 4 = 16)\n",
    "    optim=\"paged_adamw_8bit\",                  # Optimized optimizer for QLoRA\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),   # Use FP16 if BF16 is not supported\n",
    "    bf16=torch.cuda.is_bf16_supported(),       # Use BF16 if supported (RTX 4080 does)\n",
    "    save_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    report_to=\"none\",\n",
    "    # Add gradient checkpointing for extra memory efficiency if needed\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391f7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6a48335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,             \n",
    "    gradient_accumulation_steps=4,             \n",
    "    optim=\"paged_adamw_8bit\",                  \n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),   \n",
    "    bf16=torch.cuda.is_bf16_supported(),       \n",
    "    save_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=2048,                     # Renamed from max_seq_length\n",
    "    packing=True,\n",
    "    dataset_num_proc=SAFE_WINDOWS_WORKERS \n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbe78770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b38fce8fc8e449388e58c5b9942ee1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset (num_proc=50):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b0446b7b324fa8a4da927ac26a0d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=50):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be7bca8358844fbafd89e155534bcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset (num_proc=50):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,        # This is your SFTConfig object\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    #processing_class=tokenizer,       # Keep the tokenizer here\n",
    "    # All data/processing parameters are gone from here!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fedad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 1:30:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.150300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=True):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "674c8c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama31_code_lora_adapter\\\\final_adapter\\\\tokenizer_config.json',\n",
       " './llama31_code_lora_adapter\\\\final_adapter\\\\special_tokens_map.json',\n",
       " './llama31_code_lora_adapter\\\\final_adapter\\\\chat_template.jinja',\n",
       " './llama31_code_lora_adapter\\\\final_adapter\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainer.train()\n",
    "\n",
    "\n",
    "ADAPTER_PATH = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
    "trainer.model.save_pretrained(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e773017",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bbed83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4], [2, 8], [3, 6]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[[1,4],[3,6],[2,8]]\n",
    "l.sort()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06081c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
