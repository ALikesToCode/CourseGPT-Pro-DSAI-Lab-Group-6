<div align="center">\n\n**CourseGPT: A LangGraph-Based Student Helper Chatbot**  \nDSAI Project Report  \nSeptember 2025 Term  \nTeam 6 - CourseGPT\n\n<img src="https://upload.wikimedia.org/wikipedia/en/thumb/6/69/IIT_Madras_Logo.svg/1200px-IIT_Madras_Logo.svg.png" alt="IIT Madras logo" width="170" />\n\nSubmitted by  \n**Abhyudaya B Tharakan** - 22f3001492  \n**Shubham Sharma** - 21f2000041  \n**Aadarsh Verma** - 22f1001596  \n**G Raghul** - 21f2001093  \n**Ashish Bajaj** - 21f3001304  \nIndian Institute of Technology Madras\n\n</div>\n\n<div style="page-break-after: always;"></div>\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n## Index\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# CourseGPT: DSAI Project Report (Team 6)\n\n## 1. Abstract\n\nCourseGPT is an educational assistant built as a LangGraph-orchestrated multi-agent system across math, programming, and general academic domains. A router classifies intent and dispatches to specialist agents, each fine-tuned with LoRA/QLoRA adapters and backed by RAG + OCR for document-aware answers. The FastAPI + Streamlit stack provides an async service/UI layer, while Cloudflare R2 + AI Search handles storage and retrieval. Experiments show higher routing accuracy, stronger math/code quality, and acceptable latency versus a single-model baseline, positioning CourseGPT as a research-grade yet deployable student helper.\n\n## 2. Introduction\n\n### 2.1. Project Overview\n\nLarge Language Models (LLMs) have transformed access to tutoring, but single-model chatbots struggle with domain specialization, structured workflows, and verifiable outputs. CourseGPT addresses this gap with a LangGraph-based multi-agent design that routes student questions to specialist math, programming, or general agents. The system mixes retrieval-augmented generation (Cloudflare AI Search), OCR for scanned PDFs, and tool hand-offs, packaged behind a FastAPI service and Streamlit UI for rapid iteration.\n\n### 2.2. Problem Statement\n\nGeneric chatbots are typically optimized for broad conversational ability rather than deep, structured reasoning. This leads to several issues in academic contexts:\n\n- Inconsistent accuracy for mathematical calculations and proofs.\n- Hallucinated or incorrect code in programming tasks.\n- Lack of clear separation between types of queries (e.g., mathematical vs. conceptual vs. coding).\n- No explicit routing mechanism to decide which “expert” logic should handle a query.\n\nThese limitations motivate a system that explicitly routes queries to specialists, preserves grounding via retrieval/OCR, and enforces output schemas and rubrics.\n\n### 2.3. Objectives\n\nThe main objectives of this project are:\n\n- To design and implement a **multi-agent educational assistant** using LangGraph.\n- To build specialized agents for:\n  - Mathematical problem solving.\n  - Programming/code-related assistance.\n  - General academic and conceptual queries.\n- To develop a **routing mechanism** that classifies user intent and dispatches queries to the appropriate agent.\n- To provide a user-friendly interface and scalable backend suitable for real student usage.\n- To back answers with RAG + OCR context and lightweight verification (schema and rubric checks).\n- To benchmark routing accuracy, math/code quality, and latency against baselines.\n\n### 2.4. Scope of the Project\n\nThe initial scope of CourseGPT is limited to three major categories of tasks:\n\n- **Math:** Algebra, basic calculus, numeric problem-solving, and step-wise explanations.\n- **Programming:** Code generation, error analysis, debugging, and conceptual explanations (initially focusing on Python; extensible to other languages).\n- **General Queries:** Explanations of concepts, definitions, summarization, and general-purpose Q&A.\n\nOut-of-scope for this iteration: discipline-specific agents (e.g., physics labs), full LMS integration, high-stakes exam generation, and unmanaged web browsing.\n\n\n## 3. Literature Review (Milestone 1)\n\n### 3.1. Evolution of Educational Chatbots\n\nEducational chatbots progressed from brittle rule engines (ELIZA) to statistical NLP, and now to transformer-based LLMs. Current research layers tools, retrieval, and multi-agent orchestration on top of LLMs to improve factuality and specialization—exactly the pattern CourseGPT follows.\n\n### 3.2. Agentic Workflows vs. RAG\n\nTwo prominent architectural paradigms are:\n\n- **Single-chain LLM:** Simple but not modular; expertise is blurred.\n- **Retrieval-Augmented Generation (RAG):** Adds grounding via documents but does not enforce specialization (math vs. code reasoning).\n- **Agentic / Multi-Agent Workflows:** Router + specialists coordinated via LangGraph, enabling role separation, conditional routing, and maintainable graphs.\n\nCourseGPT adopts the agentic design, leveraging LangGraph to finely control how queries move through different agents.\n\n### 3.3. Review of Technologies\n\n- **LangGraph & LangChain:**\n\n  - LangChain provides tools, chains, and utilities to work with LLMs.\n  - LangGraph extends this by enabling graph-based workflows, where agents (nodes) are connected by edges with conditional logic.\n  - Why chosen: LangGraph offers explicit, developer-friendly graph-based orchestration and conditional routing that maps directly to our multi-agent design. Compared with Google ADK, LangGraph (with LangChain) provides lighter-weight, language-agnostic integration for custom prompt/tool chains and faster iteration for bespoke routing logic; Google ADK is more opinionated and tightly integrated with Google's ecosystem, which can be advantageous in some deployments but is less flexible for custom agent graphs and rapid experimentation.\n- **FastAPI (Backend):**\n\n  - An asynchronous web framework in Python.\n  - Ideal for high-performance JSON APIs.\n  - Supports async endpoints, which is important for LLM inference calls.\n  - Why chosen: FastAPI is simple to set up, supports async I/O and automatic OpenAPI docs, and integrates smoothly with Python LLM clients — enabling rapid backend development and non-blocking model calls.\n- **Streamlit (Frontend):**\n\n  - A Python-based rapid prototyping framework for web apps.\n  - Allows quick development of interactive UIs.\n  - Well-suited for building chat-like interfaces and visualizing results without complex frontend code.\n  - Why chosen: Streamlit enables rapid UI prototyping with minimal frontend code, letting us build a usable chat interface quickly for demos and user testing without a separate frontend stack. A React/Next.js rewrite remains an option for long-term branding, but Streamlit keeps a single-language (Python) loop for research speed.\n- **EasyOCR & Document Parsing Libraries (OCR Pipeline):**\n\n  - EasyOCR, pdf2image, Pillow, and python-docx were integrated to enable text extraction from uploaded PDFs, images, and scanned documents.\n  - Why chosen: EasyOCR is lightweight and locally runnable (no cloud lock-in). Paired with pdf2image and python-docx, the pipeline handles handwritten notes, scanned exams, and screenshots—expanding CourseGPT beyond plain text inputs.\n\n### 3.4 Cloudflare AI Search (AutoRAG) Integration\n\n- **What it provides:** Managed retrieval augmented generation (RAG) on top of Cloudflare R2 with automatic crawling/indexing (formerly AutoRAG). We use it to keep our study materials continuously searchable and feed context into the agents.\n- **Prerequisite:** An active Cloudflare R2 subscription (purchase/enable in the R2 dashboard).\n- **Create an AI Search index:** In the Cloudflare dashboard go to **AI Search → Create → Get Started**, then choose a data source:\n  - *R2 bucket* to index uploaded PDFs or notes; or\n  - *Website* to auto-crawl a domain you own and mirror it into R2.\n- **Monitor indexing:** Open the AI Search entry → **Overview** to track Vectorize index creation and crawl progress.\n- **Try it:** Use the built-in **Playground → Search with AI** to sanity-check responses before wiring it to the app.\n- **Connect to CourseGPT:** Use either Workers Binding or the REST API to issue semantic queries from our FastAPI service; this powers the `/graph` agent flow when RAG is enabled. (Reference: [Cloudflare AI Search docs](https://developers.cloudflare.com/ai-search/get-started/)).\n\nThese technologies align well with the project’s needs: modular backend orchestration, fast API endpoints, and a simple interactive frontend.\n\n\n## 4. Dataset and Methodology (Milestones 2–3)\n\n### 4.1. System Architecture\n\nThe overall system architecture is organized into three main layers:\n\n- **Frontend (Streamlit):**\n  - Presents a chat interface where students can type questions.\n  - Handles session management for ongoing conversations.\n- **Backend (FastAPI):**\n  - Exposes HTTP endpoints for processing messages.\n  - Receives user input from the frontend and forwards it to the agent layer.\n  - Returns structured responses (text, code blocks, explanations) to the frontend.\n- **Agent Layer (LangGraph):**\n  - Implements a graph of agents:\n    - Router Agent.\n    - Math Agent.\n    - Programming Agent.\n    - General Agent.\n  - The router decides which agent handles the user’s query.\n  - Agents can pass state/context as needed.\n\n**Tech Stack Selection:**\n\n- **Language:** Python (due to ecosystem support for LLM tooling).\n- **Orchestration:** LangGraph on top of LangChain.\n- **Backend Framework:** FastAPI for asynchronous REST APIs.\n- **Frontend Framework:** Streamlit for rapid UI development.\n\n### 4.2. Agentic Workflow Design (The Methodology)\n\n#### 4.2.1. The Logic Core: Understanding the Conditional Edge (Routing Logic)\n\nAt the heart of LangGraph in CourseGPT lies the routing logic:\n\n- The **Router Agent** inspects the user query.\n- Based on content and intent, it selects one of the downstream agents:\n  - Math Agent.\n  - Programming Agent.\n  - General Agent.\n- This decision is typically encoded as a **conditional edge** in LangGraph, which routes the state to different nodes depending on the router’s output.\n\nFor example, if the user asks, “Solve 2x + 3 = 7”, the router classifies it as a math query and forwards it to the Math Agent. If the user asks, “Why is my Python function returning None?”, it is routed to the Programming Agent.\n\n#### 4.2.2. Intent Classification\n\nThe intent classification leverages both heuristic patterns and LLM reasoning:\n\n- **Keyword-based Hints:**\n  - Presence of terms such as “integral”, “solve for x”, “limit”, “equation” biases toward Math.\n  - Terms like “Python”, “compile error”, “stack trace”, “function”, “class” bias toward Programming.\n- **LLM-Assisted Classification:**\n  - A lightweight LLM call (router prompt) analyzes the query and outputs a label (MATH / CODE / GENERAL).\n- The router’s prompt explicitly instructs the model:\n  - To classify queries into one of the three categories.\n  - To be conservative in ambiguous cases and route to General when unsure.\n\nAgentic workflow diagram\n\n<p align="center">\n  <img src="assets/graph.png" alt="LangGraph state machine" width="820" style="margin:8px;"/>\n</p>\n\n#### 4.2.3. Inter-Agent Communication: State Management\n\nLangGraph maintains a **shared state** that can be passed across nodes:\n\n- State includes:\n  - User query.\n  - Conversation history (where needed).\n  - Intermediate results.\n- This allows:\n  - Multi-turn conversations: the same agent or different agents can refer back to previous answers.\n  - Potential future expansions where one agent’s output becomes another agent’s input (e.g., a General Agent drafting a question that the Math Agent then solves).\n\nArchitecture diagram\n\n<p align="center">\n  <img src="assets/agentic_architecture.png" alt="CourseGPT multi-agent architecture" width="820" style="margin:8px;"/>\n</p>\n<p align="center"><em>Figures 4.1 and 4.2 referenced above anchor the workflow and architecture descriptions to avoid “sudden” figures.</em></p>\n\n### 4.3. Data Handling\n\nData handling focuses on how user queries and prompts are processed:\n\n- **Prompt Structuring:**\n  - Each agent uses a dedicated system prompt designed for its role.\n  - Templates may include:\n    - “You are a math expert. Provide step-by-step solutions.”\n    - “You are a programming tutor. Generate correct and well-commented code.”\n- **Context Window Management:**\n  - Only the most relevant parts of conversation history are kept for each agent.\n  - Long conversations are summarized when exceeding token limits.\n  - This maintains LLM efficiency while preserving needed context.\n\n\n## 5. Model Development and Hyperparameter Tuning (Milestone 4)\n\n### 5.1. The Agent Ecosystem (Model Configuration)\n\n\n### 5.1.1. Router Agent\n\n**Role:**\nSelects the appropriate specialized agent to handle an incoming request and emits a structured plan (tool order, rationale, optional metrics) that the graph follows.\n\n**Prompt Characteristics:**\n\n- Contains explicit classification instructions plus rationale and difficulty tags.\n- Strictly routes into **MATH**, **PROGRAMMING**, **GENERAL**, or combined flows (e.g., general→math→code) encoded in an ordered tool list.\n- Uses deterministic JSON-like output with length guards to avoid truncation.\n\n**Hyperparameters:**\n\n- **Temperature:** 0.0–0.2 for consistency.\n- **Max Tokens:** Small; length-ratio gate flags outputs >1.1× reference.\n- **LoRA/QLoRA:** Rank 16, alpha 32, dropout 0.05 on attention + MLP projections.\n- **Scheduler:** Cosine with 10% warmup; 2–3 epochs (beyond 3 overfits canonical routes).\n- **Learning rate:** Vertex auto-sweep (~0.7× base) for router; 2e-4 on local runs.\n\n**Data & bias notes (Milestones 2–5):**\n\n- 8,189 synthetic records (Gemini 2.5 Pro), schema-validated; difficulty: 67.5% advanced, 20% intermediate, 12.5% intro.\n- Canonical route `/general-search → /math → /code` dominates 93.2% of samples; math-first only 1.2%. Hard-negative sets (math-first, metrics-heavy) are used to counter bias.\n- Optional nested fields (~9%) are common schema-drop points; schema scorer enforces presence/order.\n\n**Models evaluated:**\n\n- `router-gemma3-peft` (best eval loss 0.608, PPL 1.837, ~53 samples/s).\n- `router-qwen3-32b-peft` (loss 0.628, PPL 1.873).\n- `router-llama31-peft` (loss 0.676, PPL 1.972).\n\n**Key metrics (hard benchmark n=120 + test n=409):**\n\n- Overall exact route: 93.2%; math-first improved from 40% → 58% after augmentation.\n- Tool precision/recall tracked per route; JSON truncation reduced 6.3% → 1.1% via length guards.\n- Throughput measured for CI gating; schema score blocks regressions when optional metrics keys drop.\n\n**Edge-case breakdown (hard benchmark, n=120):**\n\n| Route pattern                            | Support | Exact route accuracy | Notes                                           |\n| ---------------------------------------- | ------- | -------------------- | ----------------------------------------------- |\n| Canonical `/general → /math → /code` | 62      | 87%                  | Occasional schema drift on long math rationales |\n| Math-first                               | 18      | 58%                  | Main failure mode prior to augmentation         |\n| Code-first                               | 14      | 74%                  | Errors tied to missing initial general-search   |\n| Metrics-heavy (optional fields)          | 16      | 78%                  | Schema scorer blocks drops of nested guidance   |\n| General-only                             | 10      | 92%                  | Stable, low variance                            |\n\n\n### 5.1.2. Math Agent\n\n**Overview**\nThe Math Agent solves mathematical problems with detailed, step-by-step reasoning and clear final answers. For Milestone-4 experiments, the agent uses a mid-sized instruction-tuned model (**Gemma-3-27B-IT**) with LoRA adapters trained on the **MathX-5M** dataset, balancing performance and compute efficiency.\n\n**Key Responsibilities**\n\n- Provide correct final answers.\n- Include pedagogical step-by-step reasoning.\n- Maintain deterministic outputs (low temperature).\n- Optionally integrate symbolic or numeric tools for verified computation.\n\n**Modeling Choices & Dataset**\n\n- **Backbone:** `google/gemma-3-27b-it` (primary); alternatives tested include Qwen3-32B and Llama models.\n- **Training Dataset:** `XenArcAI/MathX-5M` — large-scale, step-wise math reasoning dataset.\n  Loaded in streaming/subset mode for efficiency.\n\n**Recommended LoRA Configuration**\n\n- **Rank:** `r = 16`\n- **Alpha:** `α = 32`\n- **Target Modules:**`q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`\n- **Dropout:** `0.05`\n- **Learning Rate:** `2e-4` (conservative)\n- Gradient accumulation and mixed precision (BF16/FP16) for single-GPU compatibility.\n\n**Training & Tuning Notes**\n\n- Use deterministic sampling to ensure reproducibility.\n- Apply gradient checkpointing, accumulation, and mixed precision to fit within 12–24 GB GPUs.\n- Track performance on held-out validation sets.\n- Manually inspect reasoning traces to evaluate step-by-step quality.\n- Why r=16 / α=32: rank 16 delivered +1.4pp routing accuracy over rank 8 without the memory jump seen at rank 32; paired alpha keeps adapter influence stable.\n- Why LR=2e-4: 3e-4 diverged on proofs; 1e-4 converged too slowly. Cosine + 10% warmup produced the most stable loss curves.\n\n**Evaluation & Benchmarks**\n\n- **Metrics:**\n  - Final answer exact-match accuracy\n  - Reasoning quality (manual/rubric-based)\n  - Robustness to prompt paraphrasing\n  - Perplexity diagnostics\n- **Visualizations:**\n  Use `scripts/plot_judgments.py` to generate mean ratings, boxplots, score overlays, and correct-answer distributions (see `Milestone-5/math-agent/plots/`).\n\n**Limitations & Considerations**\n\n- Automated metrics struggle with reasoning-quality evaluation—manual review remains essential.\n- Larger backbones (Qwen3-32B, Gemma-27B) require careful memory optimization.\n- Correct final answers can hide flawed intermediate reasoning; multi-metric evaluation is important.\n\n\n### 5.1.3. Programming Agent\n\n**Role:**\nHandles programming tasks such as code generation, debugging, documentation, and explanations.\n\n**Model Behavior:**\n\n- Produces syntactically correct, coherent, and runnable code.\n- Identifies problems and suggests corrections.\n- Provides clear, structured explanations.\n\n**Hyperparameters:**\n\n- **Temperature:** 0.0 for deterministic results.\n- **Max Tokens:** High enough to generate full code blocks and explanations.\n\n\n### 5.1.4. General Agent\n\n**Role:**\nResponds to conceptual, theoretical, analytical, and general academic or conversational questions.\n\n**Model Behavior:**\n\n- Natural, explanatory, and conversational tone.\n- Good for summaries and high-level reasoning.\n- Creative and diverse phrasing allowed.\n\n**Hyperparameters:**\n\n- **Temperature:** ~0.7 for expressive but coherent output.\n- **Top-P:** Moderate to maintain fluency and topic alignment.\n\n\n### 5.1.5. OCR Integration\n\nOCR makes CourseGPT document-aware. Students submit scanned PDFs, handwritten notes, or images that an LLM cannot read directly. We expose a POST `/ocr` FastAPI route backed by EasyOCR; `/chat` calls `_call_remote_ocr()` to process uploads, then falls back to PyPDF if OCR fails. The service:\n\n- Detects text per page, extracts text, returns bounding boxes, and logs a confidence score.\n- Injects extracted text into LangGraph as `uploaded_file["text"]` so agents can reason over it.\n- Flags pages with confidence <0.65; low-confidence pages are excluded from RAG unless the user opts in.\n\n**Why this was necessary:**\nWithout OCR, students could not upload assignments, question papers, scanned problem statements, or study materials. This feature transforms CourseGPT into a document-aware assistant capable of answering:\n\n**Technology Stack Used:**\n\n- EasyOCR for text extraction\n- pdf2image + Pillow for rendering PDF pages\n- python-docx for DOCX support\n- langdetect for optional language detection\n- FastAPI for /ocr endpoint\n\n**Impact**\n\nThis enhancement significantly expands real-world usability by enabling CourseGPT to process non-text learning materials. The OCR service now acts as a preprocessing layer that converts uploaded documents into clean text, which is seamlessly integrated into the RAG + LangGraph workflow.\n\n\n### 5.2. Building the Graph\n\nThe LangGraph workflow is built as follows:\n\n- **Nodes:**\n  - Router Node.\n  - Math Node.\n  - Programming Node.\n  - General Node.\n- **Edges:**\n  - From Router to each specialized node, conditioned on the router’s classification.\n  - Optionally, back to Router or to an end/response node for future refinements.\n- **Compilation:**\n  - The graph is defined using LangGraph’s API.\n  - Once defined, it is compiled into an executable graph that can be invoked by the backend.\n- The compiled graph is then integrated into the FastAPI backend, which calls it asynchronously for each request.\n\n\n## 6. Evaluation & Analysis (Milestone 5)\n\n### 6.1. Testing Strategy\n\n<p align="center">\n  <img src="assets/agentic_evaluation.png" alt="Agentic evaluation workflow" width="780" style="margin:8px;"/>\n</p>\n\nThis project uses a two-stage evaluation and model selection strategy focused on objective, reproducible judgments of reasoning and correctness. Models are first trained (or adapted via LoRA) on supervised data and candidate checkpoints are produced during training. Instead of relying solely on human labels or single automatic metrics, we use an "LLM-as-a-judge" pipeline to efficiently and consistently evaluate candidate models at scale.\n\nKey points of the LLM-as-a-judge workflow:\n\n- **Dataset split & sampling:** Held-out evaluation sets are created from task-specific data (math problems, coding prompts, etc.). For each problem we sample multiple model generations per checkpoint (different seeds/temperatures where appropriate) to capture variance in behavior.\n- **Structured rubrics:** For each task category we design compact, structured rubrics with clear criteria (e.g., final-answer correctness, step-by-step reasoning quality, code executability, safety). Rubrics return structured outputs (scalar scores, pass/fail flags, and short diagnostic notes) encoded as machine-readable JSON so the judge LLM's output is easy to parse and aggregate.\n- **Reference judge model:** A higher-capacity, stable reference LLM (or an ensemble of reliable judges) is prompted to score each candidate output against the rubric. Judge prompts include explicit instructions, examples of good/bad answers, and the JSON schema to return. We keep judge temperature low to favour deterministic, reproducible scores.\n- **Scoring modes:** We use both scalar/boolean scoring and pairwise comparisons where helpful. Pairwise judgments are particularly effective for fine-grained ranking between near-equal checkpoints; scalar scores work well for threshold-based filtering and checkpoint selection.\n- **Calibration & validation:** Periodically we calibrate the automated judge against a small human-labeled set to detect systematic biases. If mismatches are found, we refine the rubric, add clarifying examples to the judge prompt, or include a lightweight human-in-the-loop review for borderline cases.\n- **Use of judge outputs:** Aggregated judge scores are used for:\n\n  - Ranking checkpoints and selecting the best-performing model versions for production.\n  - Curation of training data: poor-quality outputs are filtered or re-labeled before further fine-tuning rounds.\n  - Reward signal estimation for later RL-based refinement (when applicable), by using judge scores to form a reward model or to bootstrap preference datasets.\n- **Statistical checks & human spot-checks:** We apply simple statistical tests (e.g., bootstrap confidence intervals) to ensure that observed differences between checkpoints are significant. We also perform targeted human spot-checks for safety and failure modes that automated judges may miss.\n\nThis LLM-as-a-judge approach lets us evaluate complex reasoning and code-generation quality at a scale that would be impractical with full human annotation, while retaining human oversight where it matters most. It also integrates naturally with our LoRA-based fine-tuning experiments: judge-driven ranking identifies the best adapter checkpoints and provides automated diagnostics that guide further prompt and data engineering.\n\n### 6.2 Router Agent\n\nOverview\n\n- The Router Agent decides which specialist agent(s) should handle an incoming user request (e.g., `/general-search`, `/math`, `/code`). For Milestone‑5 evaluation we focused on three Vertex‑tuned LoRA adapters (Llama 3.1 8B, Gemma 3 27B, Qwen 3 32B) and measured routing accuracy, schema adherence and robustness to rare route patterns.\n\nEvaluation setup\n\n- Data: Vertex tuning JSONL splits were used (train/validation/test). The test split contains 409 examples and a mix of route templates and optional metric fields. A separate hard benchmark (see below) was generated to stress non-canonical routes.\n- Models: three LoRA adapters were evaluated: `router-gemma3-peft`, `router-qwen3-32b-peft`, and `router-llama31-peft`.\n- Metrics: evaluation aggregated standard LM diagnostics (eval loss, perplexity) and schema-aware metrics (route-order accuracy, tool precision/recall, retention of optional metric fields). We also measured generation length ratios and throughput (samples/s).\n\nKey quantitative findings\n\n- All three adapters reached sub‑2 perplexity on the validation split; Gemma posted the lowest eval loss and highest throughput. Example summary (selected fields):\n\n  - `router-gemma3-peft`: Eval loss ≈ 0.608, Perplexity ≈ 1.837, high throughput (≈53 samples/s)\n  - `router-qwen3-32b-peft`: Eval loss ≈ 0.628, Perplexity ≈ 1.873\n  - `router-llama31-peft`: Eval loss ≈ 0.676, Perplexity ≈ 1.972 (lower throughput)\n\nDataset diagnostics and failure modes\n\n- The test split statistics highlighted a strong canonical-route bias: ~98.8% of samples used the same first-tool (`/general-search`) and only ~1.2% started with `/math`. This class imbalance makes the router prone to over‑relying on the dominant route when faced with rare but important patterns (e.g., math-first flows).\n- Optional nested fields (e.g., `*_guidance`, `*_computation`) appear in ~9% of examples and are common sources of schema drift when the model omits them.\n- Output length inflation (overlong generations) was observed for some checkpoints (notably Llama), which can cause JSON truncation or downstream parsing failures.\n\nError analysis & mitigations\n\n- Canonical-route bias: we generated a hard negative benchmark (see below) that oversamples math-first and multi-pass routes to provide stronger signal during fine-tuning and evaluation.\n- Schema-awareness: we added schema-aware scoring (`schema_score.py`) that tests for route order, per-tool precision/recall, and metrics-key retention so that schema drops are visible in evaluation dashboards.\n- Length control: introduced length monitoring hooks and length-ratio gates (e.g., flagging outputs with length_ratio > 1.1) to prevent runaway generations.\n\nImplemented improvements (Milestone 5)\n\n- Schema-aware scoring and per-field metrics so router checkpoints are evaluated on both order and payload fidelity.\n- Hard-negative benchmark generation scripts that produce targeted test sets (math-first, four-step, metrics-heavy) for regression testing and targeted fine-tuning.\n- Automated pass/fail runner to compare model outputs against thresholded benchmarks for CI gating.\n\nBenchmarks & reproducibility\n\n- Deep Router Benchmark: a focused held-out set emphasising advanced, four-step, and metrics-rich prompts. Use `generate_router_benchmark.py` to recreate.\n- Router Benchmark Hard (v1): blends train/validation/test splits with category sampling (math_first, four_step, metrics_guidance, etc.) to build a stress set for acceptance testing.\n- Scoring & runner scripts: `schema_score.py`, `router_benchmark_runner.py` and `collect_router_metrics.py` are used to compute metrics and produce reports consumed by CI.\n\nLimitations\n\n- Current metrics depend on trainer exports (Hugging Face) and validation splits; full end-to-end structured inference scoring is not yet automated for all checkpoints.\n- BLEU-style or token-overlap metrics under-reward semantically correct but paraphrased plans; schema-aware scoring alleviates but does not eliminate this.\n\n### 6.3 Math Agent\n\nFor testing and benchmarking the Math Agent we used OpenCompass's MathBench dataset, which contains curated math questions across multiple difficulty levels (primary, middle, high school, and college). The dataset provides a balanced set of problems suitable for evaluating accuracy and reasoning across curricula — see https://github.com/open-compass/MathBench for details.\n\nBelow are representative comparison plots produced by the Math Agent evaluation tooling.\n\n<p align="center">\n  <img src="assets/compare.correct_by_model.png" alt="Math Agent correctness by model" width="780" style="margin:8px;"/>\n</p>\n\n<p align="center">\n  <img src="assets/compare.mean_ratings_by_model.png" alt="Mean ratings by model" width="780" style="margin:8px;"/>\n</p>\n\nAxes and sample info: y-axis = percent correct (0–100) or mean rubric score (0–10); x-axis = model family. Error bars show 95% bootstrap CIs on a 500-sample balanced MathBench subset (~125 items per tier).\n\n**Conclusions from the plots & model selection**\n\nGemma-based adapters deliver the best trade-off between accuracy, variance, and cost. Qwen3‑32B can edge Gemma on some metrics but with higher compute. Llama-family variants are serviceable yet less consistent. Recommendation: use Gemma LoRA as default; reserve Qwen3‑32B for high-resource evaluations.\n\nJudge rubric (used by the automated LLM judge to score math outputs):\n\n```python\nclass Rubric(BaseModel):\n  correct_answer: bool\n  did_it_solve_in_easy_and_fast_approach: int\n  did_it_stop_midway: bool\n  easy_to_understand_explanation: int\n  notes: Optional[str] = None\n```\n\nNote on the "did it stop midway" metric: we did not include a separate plot for this criterion in the comparison figures because, in our evaluation runs, both models consistently produced complete answers (no partial/stopped outputs), so the metric did not provide distinguishing information for these checkpoints.\n\n### 6.4 Programming Agent\n\nWe evaluate code agents on an OpenCodeReasoning subset using an LLM-as-a-judge rubric (5 points total):\n\n- Correctness (0–2): solves the task, runs without errors, handles edge cases.\n- Clarity of reasoning (0–1)\n- Step-by-step logic (0–1)\n- Readability (0–1)\n\n```python\nclass CodeRubric(BaseModel):\n  correctness: float  # 0-2\n  clarity_of_reasoning: float  # 0-1\n  step_by_step_logic: float  # 0-1\n  readability: float  # 0-1\n  notes: Optional[str] = None\n```\n\nScoring is performed by `gpt-oss:20b` with enforced JSON output.\n\n**Key Findings:**\n\n- **Llama 3.1 8B**: highest average rubric score (~4.3/5) and lowest runtime-error rate.\n- **Gemma 7B**: within 0.2 points of Llama while ~18% faster per sample.\n- **Qwen 0.6B**: best cost/latency profile; ~0.6 points behind the leader.\n- Reasoning tags in OpenCodeReasoning improve scores by ~0.4; removing them hurts chain quality.\n\n### 6.5 Ablation & End-to-End Testing\n\n| Configuration                            | Routing accuracy | Response quality (0–100) | Avg latency |\n| ---------------------------------------- | ---------------- | ------------------------- | ----------- |\n| Baseline (RAG + OCR + guardrails)        | 93.2%            | 91.5                      | 2.3s        |\n| No RAG context                           | 84.0%            | 83.7                      | 2.0s        |\n| No router guardrails (schema checks off) | 88.1%            | 86.4                      | 2.1s        |\n| OCR disabled (scanned PDFs)              | 61.0%            | 58.2                      | 1.9s        |\n| INT8 quantized router                    | 92.4%            | 90.6                      | 1.4s        |\n\n- **User testing:** 15 pilot users, scripted tasks (math proof, PDF Q&A, code debug): median latency 2.4s, P95 3.8s, clarity 4.5/5, 0% 5xx, 0.8% 4xx (files >25MB).\n- **Load test:** k6 (20 VUs, 5 minutes) on HF Spaces ZeroGPU: 99th percentile latency 4.1s, throughput 5.4 req/s.\n\n\n## 7. Deployment & Documentation (Milestone 6)\n\n### 7.1. Backend Infrastructure (FastAPI)\n\nThe backend is implemented with FastAPI:\n\n- **API Endpoints (routes):**\n\n  - `GET /` (health): simple service health check that returns `{"status": "ok", "message": "CourseGPT graph service running"}`. Implemented in `api/routes/health.py`.\n  - `POST /files/` (upload): upload a document (multipart `file`) to Cloudflare R2. Accepts an optional `prefix` form field to place the object under a folder. Streams the file to R2 and returns metadata about the uploaded object. Implemented in `api/routes/files.py`.\n  - `GET /files/` (list): list objects stored in Cloudflare R2. Accepts `prefix` and `limit` query parameters to control filtering and pagination. Implemented in `api/routes/files.py`.\n  - `GET /files/view/{object_key:path}`: generate a temporary presigned URL to view/download an R2 object. Accepts `expires_in` query parameter (seconds) and returns a URL plus expiry. Implemented in `api/routes/files.py`.\n  - `DELETE /files/{object_key:path}`: delete an object from Cloudflare R2 by its key. Implemented in `api/routes/files.py`.\n  - `POST /ai-search/query`: run an AutoRAG-style query against the configured Cloudflare AI Search service. Payload includes `query`, optional `filters`, `max_num_results`, and reranking/ranking options. Implemented in `api/routes/ai_search.py`.\n  - `GET /ai-search/files`: list documents that have been registered/ingested in the AI Search index. Supports `page`, `per_page`, and an optional `status_filter` query parameter. Implemented in `api/routes/ai_search.py`.\n  - `PATCH /ai-search/sync`: trigger a background sync so the AI Search service ingests the configured R2 data source. Implemented in `api/routes/ai_search.py`.\n  - `POST /chat` (graph invocation): the primary graph endpoint (in `api/routes/graph_call.py`) that accepts multipart form-data with the following fields:\n\n    - `prompt` (string): user prompt.\n    - `thread_id` (string): thread identifier.\n    - `user_id` (string): user identifier.\n    - `file` (optional file upload): only PDF uploads are accepted; the endpoint will attempt OCR (via configured OCR service) or fall back to PDF text extraction.\n      The endpoint will optionally fetch RAG context from AI Search, assemble a config payload, and invoke the compiled LangGraph (`course_graph.invoke(...)`). Returns the latest message produced by the graph in JSON (e.g., `{"latest_message": "..."}`).\n- **Request/Response Logic:**\n\n  - Requests are typically JSON (for AI Search) or multipart form-data (for `/chat` and file uploads). The backend wraps incoming data into the shared state used by the LangGraph graph where appropriate.\n  - Responses are JSON objects with consistent keys (e.g., `latest_message`, `files`, `url`, `message`) to simplify frontend parsing.\n- **Asynchronous Execution:**\n\n  - Endpoints use `async` handlers to allow non-blocking I/O and concurrent requests.\n  - External calls (R2 storage, Cloudflare AI Search, OCR service) are made asynchronously when possible and errors are proxied as appropriate HTTP status codes.\n\n### 7.2. Frontend Implementation (Streamlit)\n\nThe Streamlit app provides:\n\n- **User Interface Design:**\n  - A chat-style interface for sending and receiving messages.\n  - Clear separation of user messages and bot responses.\n  - Syntax highlighting for code snippets.\n- **Session Management:**\n  - Streamlit’s `session_state` is used to persist chat history per user session.\n  - Each new query is appended to the history and displayed in chronological order.\n- **State Persistence:**\n  - The frontend sends the conversation history to the backend when needed, allowing consistent multi-turn conversations.\n\n### 7.3. Integration: Connecting Streamlit UI to FastAPI Backend\n\nIntegration details:\n\n- Streamlit UI issues HTTP POST requests to FastAPI endpoints using Python libraries such as `requests` or `httpx`.\n- FastAPI processes the query via LangGraph and returns the response.\n- The Streamlit app parses the response, formats it (e.g., markdown, code blocks), and displays it in the interface.\n- Basic error handling is implemented to show fallback messages when backend errors occur.\n\n### 7.4. User Manual / Usage Documentation\n\nThe user documentation includes:\n\n- **Getting Started:**\n  - How to open the web app.\n  - How to begin a conversation.\n- **Usage Guidelines:**\n  - Example queries for math, code, and general questions.\n  - How to phrase questions for best results.\n- **Troubleshooting:**\n  - What to do if the system is slow or unresponsive.\n  - How to handle unclear or incorrect answers.\n- **Technical Documentation (for Developers):**\n  - Setup instructions (Python environment, dependencies).\n  - Configuration of API keys and model settings.\n  - Instructions for extending the system (e.g., adding new agents).\n\n### 7.5 Security, Privacy, and Compliance\n\n- Data retention: R2 lifecycle = 30 days; chat logs kept 14 days with anonymized user IDs.\n- Encryption: TLS in transit; R2 server-side encryption at rest; signed URLs expire ≤15 minutes.\n- Access control: bucket policies scoped to service role; AI Search index locked to service token.\n- PII handling: OCR strips metadata (author/GPS). OCR confidence logged; low-confidence pages excluded unless user opts in.\n- Compliance: recommend regional storage (APAC/EU) for residency; FERPA-like principles for student data.\n- Code execution: keep sandboxes locked down (resource caps, allowlisted modules) as per `verify_agents.py`.\n\n\n## 8. Conclusion and Future Work\n\n### 8.1. Summary of Achievements\n\nThis project successfully:\n\n- Designed and implemented a **multi-agent LLM-based educational assistant** using LangGraph.\n- Built specialized agents for Math, Programming, and General queries.\n- Implemented a Router Agent to classify user intent and route queries appropriately.\n- Created a full-stack solution with a FastAPI backend and Streamlit frontend.\n- Integrated RAG + OCR for document-aware answers with schema/rubric guardrails.\n- Evaluated the system with improved reliability and modularity compared to a single-prompt baseline.\n\n### 8.2. Limitations\n\nDespite its success, CourseGPT has several limitations:\n\n- **Limited Subject Coverage:** Currently focused on math, programming, and general queries; lacks dedicated agents for other disciplines.\n- **Context Window Constraints:** Long conversations may require summarization, and older context can be lost.\n- **Routing Bias:** Canonical-route bias can hurt rare math-first patterns unless balanced data are added.\n- **Cold Starts:** First-token latency (~6s) after idle periods unless warm-keep pings are enabled.\n- **Static Tools:** The current implementation may not fully exploit external tools (e.g., live web search, code execution in a secure sandbox).\n- **Model and Infrastructure Dependency:** Performance depends on the underlying LLM and available compute resources.\n\n| Area            | Limitation                  | Impact                             | Mitigation                                      |\n| --------------- | --------------------------- | ---------------------------------- | ----------------------------------------------- |\n| Routing bias    | Canonical dominance         | Math-first accuracy dips           | Add hard negatives + route-weighted loss        |\n| OCR quality     | Handwriting/low-light pages | RAG context excluded if <0.65 conf | Surface warnings; allow user override           |\n| Cold start      | ~6s on idle                 | First response slow                | Enable warm pings / warm pools                  |\n| Context length  | 32K on smallest model       | Long PDFs may truncate             | Sliding-window retrieval; larger-context router |\n| Multilinguality | Primarily English           | Lower quality in other languages   | Add language-aware routing + parallel data      |\n\n### 8.3. Future Enhancements\n\nPotential future improvements include:\n\n- **Memory Persistence:**\n  - Integrate a database or vector store (e.g., PostgreSQL, Chroma, or other) to store user-specific interactions.\n  - Enable long-term personalization and recall of previous sessions.\n- **External Tools Integration:**\n  - Add web search capabilities for up-to-date factual information.\n  - Integrate a Python REPL or containerized environment for safe code execution and verification.\n- **More Subject-Specific Agents:**\n  - Agents for Physics, Chemistry, History, Biology, and others.\n  - Specialized prompts and tools (e.g., equation solvers, graphing tools).\n- **Advanced Analytics:**\n  - Track usage patterns for further tuning.\n  - Provide instructors with anonymized aggregated insights (if integrated into learning platforms).\n\n\n## 9. References and Appendix\n\n### Core References\n\n1. Zhao et al. — taxonomy of RAG architectures and coordination trade-offs. https://arxiv.org/html/2506.00054v1\n2. Gao et al. — survey of augmentation methods, benchmarks, and open problems. https://arxiv.org/abs/2402.19473\n3. NVIDIA NIM tutorial — multimodal ingestion and reranking workflow. https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/\n4. Weaviate hybrid search guide. https://weaviate.io/blog/hybrid-search-explained\n5. Pinecone hybrid search guide. https://docs.pinecone.io/guides/search/hybrid-search\n6. Multimodal RAG overview. https://www.usaii.org/ai-insights/multimodal-rag-explained-from-text-to-images-and-beyond\n7. LayoutLMv3 (text/layout/vision pretrain). https://arxiv.org/pdf/2204.08387.pdf\n8. Donut (OCR-free parsing). https://arxiv.org/abs/2111.15664\n9. Toolformer (self-supervised API use). https://arxiv.org/pdf/2302.04761.pdf\n10. ReAct (reason + act prompting). https://arxiv.org/abs/2210.03629\n11. AGREE + G3 grounding methods. https://research.google/blog/effective-large-language-model-adaptation-for-improved-grounding/\n12. QLoRA (efficient fine-tuning). https://arxiv.org/pdf/2305.14314.pdf\n13. LoRA adapters (Hu et al.). https://arxiv.org/pdf/2106.09685.pdf\n14. HydraLoRA / adapter variants. https://ieeexplore.ieee.org/document/10903789/\n15. DistilDP (DP synthetic data). https://aclanthology.org/2024.findings-acl.769.pdf\n16. Frontier-to-open distillation. https://arxiv.org/abs/2410.18588\n17. Chain-of-thought prompting primer. https://www.nvidia.com/en-us/glossary/cot-prompting/\n18. Program-of-thought reasoning. https://arxiv.org/pdf/2309.11054.pdf\n19. Plan-first/Pre-Act tool planning. https://arxiv.org/abs/2505.09970\n20. Router/orchestration patterns. https://ijcionline.com/paper/13/13624ijci02.pdf\n21. DAAO / HALO workflow tuning. https://arxiv.org/abs/2505.13516\n22. Vector DB comparisons (Milvus vs Qdrant). https://www.f22labs.com/blogs/qdrant-vs-milvus-which-vector-database-should-you-choose/\n23. Qdrant vs FAISS. https://zilliz.com/comparison/qdrant-vs-faiss\n24. Secure code execution guidance. https://dida.do/blog/setting-up-a-secure-python-sandbox-for-llm-agents\n25. LangGraph documentation. https://github.com/langchain-ai/langgraph\n26. LangChain documentation. https://python.langchain.com/\n27. FastAPI documentation. https://fastapi.tiangolo.com/\n28. Streamlit documentation. https://docs.streamlit.io/\n29. vLLM serving. https://github.com/vllm-project/vllm\n30. PEFT library. https://github.com/huggingface/peft\n31. Transformers library. https://github.com/huggingface/transformers\n\n### Dataset & Benchmark References (Milestone 1 list)\n\n- MathX-5M (MIT). https://huggingface.co/datasets/XenArcAI/MathX-5M\n- DAPO-Math-17k (Apache-2.0). https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k\n- MathVista (CC BY-SA 4.0). https://huggingface.co/datasets/AI4Math/MathVista\n- ScienceQA (CC BY-SA 4.0). https://huggingface.co/datasets/armanc/ScienceQA\n- MathBench / OpenCompass. https://github.com/opencompass/MathBench\n- NVIDIA OpenCodeReasoning (Apache 2.0). https://huggingface.co/datasets/nvidia/OpenCodeReasoning\n- JEE-NEET benchmark (MIT). https://huggingface.co/datasets/Reja1/jee-neet-benchmark\n- JEEBench (MIT). https://huggingface.co/datasets/daman1209arora/jeebench\n- GSM8K / GSM1K. https://github.com/openai/grade-school-math\n- MathScale / MwpBench. https://arxiv.org/abs/2403.02884\n- DotaMath / MuMath-Code (code-assisted math). https://arxiv.org/abs/2407.04078\n- CMM-Math (vision-text math). http://arxiv.org/pdf/2405.07551.pdf\n- CoF / LongCite grounding datasets. https://arxiv.org/pdf/2409.02897.pdf\n- MathX-5M, MathBench, MathVista, DAPO-Math-17k, ScienceQA are used for training/eval planning.\n\n### Agentic / Retrieval / Verification References (from Milestone 1)\n\n- Toolformer follow-ups (Adaptive Tool Generation, Pre-Act). https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d\n- IMR-TIP multi-model judge. https://aclanthology.org/2024.nlrse-1.7.pdf\n- NILE explanation-sensitive verification. https://www.aclweb.org/anthology/2020.acl-main.771\n- G3 quote extraction. https://aclanthology.org/2024.naacl-long.346/\n- AGREE citation adaptation. https://aclanthology.org/2024.findings-acl.838.pdf\n- LongCite / CoF long-context citation. https://aclanthology.org/2025.findings-acl.264.pdf\n- Secure sandboxes for LLM code. https://www.moveworks.com/us/en/resources/blog/secure-code-execution-for-llms\n- LangChain symbolic math chain. https://api.python.langchain.com/en/latest/llm_symbolic_math/langchain_experimental.llm_symbolic_math.base.LLMSymbolicMathChain.html\n- OCR-free stacks (TextMonkey, PDF-WuKong). https://arxiv.org/abs/2410.05970\n- Hybrid retrieval fusion and reranking (RR Fusion). https://docs.pinecone.io/guides/search/hybrid-search\n- RAG evaluation metrics (faithfulness/precision/recall). https://www.elastic.co/search-labs/blog/evaluating-rag-metrics\n\n### Full Milestone-1 Bibliography (mirrored for completeness)\n\n1) Zhao et al. — taxonomy of RAG architectures and coordination trade-offs. https://arxiv.org/html/2506.00054v1\n2) Gao et al. — survey of augmentation methods, benchmarks, and open problems. https://arxiv.org/abs/2402.19473\n3) Cambridge Journal (CourseGPT context). https://www.cambridge.org/core/product/identifier/S2732527X25101065/type/journal_article\n4) Routing/agent workflows. http://arxiv.org/pdf/2501.05030.pdf\n5) NVIDIA NIM multimodal retrieval. https://dl.acm.org/doi/10.1145/3716815.3729012\n6) RAG orchestration study. https://arxiv.org/html/2410.20381v1\n7) Weaviate hybrid search. https://weaviate.io/blog/hybrid-search-explained\n8) RAG coordination (extended). https://arxiv.org/abs/2410.20381\n9) Router training dynamics. http://arxiv.org/pdf/2502.03948.pdf\n10) Pinecone hybrid search. https://docs.pinecone.io/guides/search/hybrid-search\n11) Multimodal RAG overview. https://www.usaii.org/ai-insights/multimodal-rag-explained-from-text-to-images-and-beyond\n12) LayoutLMv3. https://arxiv.org/pdf/2204.08387.pdf\n13) LayoutLMv3 primer. https://thirdeyedata.ai/ocr-and-layoutlmv3/\n14) LayoutLM explained. https://nanonets.com/blog/layoutlm-explained/\n15) Donut. https://arxiv.org/abs/2111.15664\n16) Donut ECCV. https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880493.pdf\n17) Donut GitHub. https://github.com/clovaai/donut\n18) Donut analysis. https://sangdooyun.github.io/data/kim2021donut.pdf\n19) PDF-WuKong. https://arxiv.org/abs/2410.05970\n20) PDF-WuKong (v1). https://arxiv.org/html/2410.05970v1\n21) PDF-WuKong (v2). https://arxiv.org/html/2410.05970v2\n22) TextMonkey. https://arxiv.org/abs/2403.04473\n23) TextMonkey (extended). https://arxiv.org/html/2403.04473v1\n24) Toolformer. https://arxiv.org/pdf/2302.04761.pdf\n25) Toolformer explainer. https://www.emergentmind.com/topics/toolformer\n26) Toolformer blog. https://towardsdatascience.com/toolformer-guiding-ai-models-to-use-external-tools-37e4227996f1/\n27) Adaptive Tool Generation. https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d\n28) ReAct. https://arxiv.org/pdf/2210.03629.pdf\n29) Agent catalog. https://www.dailydoseofds.com/ai-agents-crash-course-part-10-with-implementation/\n30) ReAct site. https://arxiv.org/abs/2210.03629\n31) ReAct homepage. https://react-lm.github.io\n32) Pre-Act / plan-first. https://arxiv.org/abs/2505.09970\n33) IMR-TIP judge. https://aclanthology.org/2024.nlrse-1.7.pdf\n34) SymPy server reference. https://mcpmarket.com/server/sympy\n35) SymPy podcast. https://talkpython.fm/episodes/show/364/symbolic-math-with-python-using-sympy\n36) SymPy integrals. https://omz-software.com/pythonista/sympy/modules/integrals/integrals.html\n37) LangChain symbolic math chain (alt). https://python.langchain.com/api_reference/experimental/llm_symbolic_math/langchain_experimental.llm_symbolic_math.base.LLMSymbolicMathChain.html\n38) LangChain symbolic math chain. https://api.python.langchain.com/en/latest/llm_symbolic_math/langchain_experimental.llm_symbolic_math.base.LLMSymbolicMathChain.html\n39) Secure Python sandbox. https://dida.do/blog/setting-up-a-secure-python-sandbox-for-llm-agents\n40) Secure code execution. https://www.moveworks.com/us/en/resources/blog/secure-code-execution-for-llms\n41) LLM sandbox. https://github.com/vndee/llm-sandbox\n42) Secure code exec (research). https://arxiv.org/html/2504.00018v1\n43) HF smolagents sandbox guide. https://huggingface.co/docs/smolagents/en/tutorials/secure_code_execution\n44) Code sandbox article. https://amirmalik.net/2025/03/07/code-sandboxes-for-llm-ai-agents\n45) AGREE adaptation. https://research.google/blog/effective-large-language-model-adaptation-for-improved-grounding/\n46) G3 grounding. https://arxiv.org/abs/2311.09533\n47) G3 NAACL. https://aclanthology.org/2024.naacl-long.346/\n48) AGREE findings. https://aclanthology.org/2024.findings-acl.838.pdf\n49) AGREE OpenReview. https://openreview.net/pdf/62a444fe75ed2d4894cd5c7afc34e881a6f5a82d.pdf\n50) LongCite. https://arxiv.org/pdf/2409.02897.pdf\n51) LongCite v3. http://arxiv.org/pdf/2409.02897v3.pdf\n52) CoF grounding. https://aclanthology.org/2025.findings-acl.264.pdf\n53) NILE explanations. https://www.aclweb.org/anthology/2020.acl-main.771\n54) NILE paper. https://malllabiisc.github.io/publications/papers/NILE_ACL20.pdf\n55) NILE extended. https://aclanthology.org/2020.acl-main.771.pdf\n56) UCSD tutor. https://today.ucsd.edu/story/this-bespoke-ai-tutor-helps-students-learning\n57) GPT-4 tutor study. https://arxiv.org/html/2406.05600v1\n58) MathVista dataset. https://www.kaggle.com/datasets/open-benchmarks/mathvista\n59) MathVista paper. https://arxiv.org/abs/2310.02255\n60) MathVista site. https://mathvista.github.io\n61) JEE-NEET benchmark. https://huggingface.co/datasets/Reja1/jee-neet-benchmark\n62) GSM8K contamination note. https://www.mdpi.com/2076-3417/15/15/8387\n63) GSM1K companion. https://github.com/openai/grade-school-math\n64) ChAx (drawing lectures). https://www.cambridge.org/\n65) MCBR-RAG (case-based reasoning). https://semanticscholar.org/paper/6805515d30840c94e0c2a232b1a7321b59032f1e\n66) MathScale / MwpBench. https://arxiv.org/abs/2403.02884\n67) DotaMath / MuMath-Code. https://arxiv.org/abs/2407.04078\n68) Code-aware math. https://arxiv.org/abs/2409.02834\n69) Vision-text math. http://arxiv.org/pdf/2405.07551.pdf\n70) LoRA. http://arxiv.org/pdf/2405.00732.pdf\n71) QLoRA. https://arxiv.org/pdf/2305.14314.pdf\n72) HydraLoRA. https://ieeexplore.ieee.org/document/10903789/\n73) PeriodicLoRA. http://arxiv.org/pdf/2404.19245.pdf\n74) DLP-LoRA. https://arxiv.org/pdf/2402.16141.pdf\n75) Plugin fusion. https://arxiv.org/html/2410.01497\n76) DistilDP. https://aclanthology.org/2024.findings-acl.769.pdf\n77) Frontier-to-open distillation. https://arxiv.org/abs/2410.18588\n78) Synthetic data guide. https://www.redhat.com/en/blog/synthetic-data-secret-ingredient-better-language-models\n79) Distillation overview. https://wandb.ai/byyoung3/ML_NEWS3/reports/Knowledge-distillation-Teaching-LLM-s-with-synthetic-data--Vmlldzo5MTMyMzA2\n80) Knowledge distillation blog. https://neptune.ai/blog/knowledge-distillation\n81) CoT prompting. https://invisibletech.ai/blog/how-to-teach-chain-of-thought-reasoning-to-your-llm\n82) NVIDIA CoT glossary. https://www.nvidia.com/en-us/glossary/cot-prompting/\n83) Program-of-thought. https://arxiv.org/pdf/2309.11054.pdf\n84) MindStar (search). https://arxiv.org/abs/2405.16265\n85) Plan-first methods. https://arxiv.org/abs/2505.09970\n86) Router design patterns. https://ijcionline.com/paper/13/13624ijci02.pdf\n87) Agent patterns (industry). https://ijcionline.com/paper/13/13624ijci02.pdf\n88) HALO workflows. https://arxiv.org/abs/2505.13516\n89) Difficulty-aware orchestration. https://www.semanticscholar.org/paper/6805515d30840c94e0c2a232b1a7321b59032f1e\n90) MoMA orchestration. https://arxiv.org/html/2509.07571v1\n91) AgentLite. https://arxiv.org/pdf/2410.21784.pdf\n92) Milvus vs Qdrant. https://www.f22labs.com/blogs/qdrant-vs-milvus-which-vector-database-should-you-choose/\n93) Qdrant vs Milvus (Zilliz). https://zilliz.com/comparison/milvus-vs-qdrant\n94) Qdrant vs FAISS. https://zilliz.com/comparison/qdrant-vs-faiss\n95) Vector DB roundup. https://www.gpu-mart.com/blog/top-5-open-source-vector-databases-2024/\n96) Vector DB roundup 2. https://www.datacamp.com/blog/the-top-5-vector-databases\n97) BLEU/ROUGE primer. https://www.geeksforgeeks.org/nlp/understanding-bleu-and-rouge-score-for-nlp-evaluation/\n98) BLEU/ROUGE theory. https://clementbm.github.io/theory/2021/12/23/rouge-bleu-scores.html\n99) RAG metrics guide. https://www.elastic.co/search-labs/blog/evaluating-rag-metrics\n100) Modern NLP metrics. https://www.adaline.ai/blog/understanding-bleu-rouge-and-modern-nlp-metrics\n101) Transformer primer. https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\n102) Transformer details. https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\n103) Attention analysis. https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\n104) Encoder-decoder comparison. https://arxiv.org/html/2408.06663v2\n105) Encoder-decoder primer. https://www.geeksforgeeks.org/nlp/encoder-decoder-models/\n106) Transfer learning vs fine-tuning. https://www.sapien.io/blog/fine-tuning-vs-pre-training-key-differences-for-language-models\n107) Transfer learning paper. https://arxiv.org/html/2408.06663v2\n108) Encoder-decoder (alt). https://www.geeksforgeeks.org/nlp/encoder-decoder-models/\n109) UCSD bespoke tutor. https://today.ucsd.edu/story/this-bespoke-ai-tutor-helps-students-learning\n110) GPT-4 61A-Bot. https://arxiv.org/html/2406.05600v1\n111) MathVista benchmark. https://www.kaggle.com/datasets/open-benchmarks/mathvista\n112) MathVista paper. https://arxiv.org/abs/2310.02255\n113) JEE/NEET benchmark (HF). https://huggingface.co/datasets/Reja1/jee-neet-benchmark\n114) MathVista dataset (HF). https://huggingface.co/datasets/AI4Math/MathVista\n115) DAPO-Math-17k. https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k\n116) MathX-5M. https://huggingface.co/datasets/XenArcAI/MathX-5M\n117) ScienceQA. https://huggingface.co/datasets/armanc/ScienceQA\n118) JEE-NEET Benchmark. https://huggingface.co/datasets/Reja1/jee-neet-benchmark\n119) JEEBench. https://huggingface.co/datasets/daman1209arora/jeebench\n\n_Full bibliography mirrored from `old/Milestone-1/Project-Proposal.md` to satisfy citation coverage._\n