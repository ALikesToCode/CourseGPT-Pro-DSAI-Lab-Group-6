{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e56beb",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37847960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Any, Dict\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78094fa",
   "metadata": {},
   "source": [
    "## Set Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c03dc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = 'Milestone-5/math-agent/qwen3-32b-eval.balanced.jsonl'  # <- change this to your file\n",
    "OUTPUT_FILE = 'qwen3-32b.judgments.jsonl'  # judgments will be written here\n",
    "MODEL_NAME = 'gemma3:4b'  # change to your model if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61b277",
   "metadata": {},
   "source": [
    "## Set Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d13686ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rubric(BaseModel):\n",
    "    correct_answer: bool\n",
    "    did_it_solve_in_easy_and_fast_approach: int\n",
    "    did_it_stop_midway: bool\n",
    "    easy_to_understand_explanation: int\n",
    "    notes: Optional[str] = None\n",
    "    score: Optional[float] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dbdb4",
   "metadata": {},
   "source": [
    "## Define Judge Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ae05458",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"You are a careful, strict math grader.\n",
    "\n",
    "Given the QUESTION, the MODEL GENERATED ANSWER, and the EXACT ANSWER, produce a single JSON object matching this schema exactly (no extra keys):\n",
    "\n",
    "{Rubric.model_json_schema()}\n",
    "\n",
    "Return only valid JSON.\n",
    "\n",
    "QUESTION: {q}\n",
    "\n",
    "MODEL GENERATED ANSWER: {gen}\n",
    "\n",
    "Notes: decide `correct_answer` by whether the generated answer matches the exact answer (for numeric answers compare numerically when possible). Rate `did_it_solve_in_easy_and_fast_approach` from 1 (very poor) to 10 (excellent). Set `did_it_stop_midway` true if the answer is incomplete or the model says it cannot continue. Rate `easy_to_understand_explanation` from 1 to 10. Provide short `notes` if necessary and optional `score` between 0 and 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e04f41",
   "metadata": {},
   "source": [
    "## Judge Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ddbec284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entry_with_llm(entry: Dict[str, Any]) -> Rubric:\n",
    "    \"\"\"Use the exact JSONL shape you provided: `prompt` for the question and\n",
    "    `response.choices[0].message.content` for the generated answer.\n",
    "\n",
    "    This function is intentionally strict to match your examples and avoid\n",
    "    guessing other key names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Question: use the `prompt` key from your examples\n",
    "    q = entry.get('prompt') if entry.get('prompt') is not None else entry.get('question', '')\n",
    "\n",
    "    # Generated answer: expect the structure `response.choices[0].message.content`\n",
    "    gen = ''\n",
    "    resp_obj = entry.get('response')\n",
    "    if isinstance(resp_obj, dict):\n",
    "        choices = resp_obj.get('choices')\n",
    "        if isinstance(choices, list) and len(choices) > 0:\n",
    "            first_choice = choices[0]\n",
    "            if isinstance(first_choice, dict):\n",
    "                msg = first_choice.get('message')\n",
    "                if isinstance(msg, dict) and 'content' in msg:\n",
    "                    gen = msg['content']\n",
    "                elif 'text' in first_choice:\n",
    "                    gen = first_choice.get('text')\n",
    "                elif 'content' in first_choice:\n",
    "                    gen = first_choice.get('content')\n",
    "    # Minimal fallback: if structure isn't as expected, try a few small, safe fallbacks\n",
    "    if not gen:\n",
    "        gen = entry.get('response_text') or entry.get('generated_answer') or ''\n",
    "\n",
    "    # Try to get exact/gold answer if present (safe fallback)\n",
    "    exact = entry.get('exact-answer') or entry.get('exact_answer') or entry.get('reference') or entry.get('answer') or ''\n",
    "\n",
    "    # Prepare prompt template: substitute the literal Rubric model schema placeholder\n",
    "    prompt_template = JUDGE_PROMPT\n",
    "    if '{Rubric.model_json_schema()}' in prompt_template:\n",
    "        try:\n",
    "            prompt_template = prompt_template.replace('{Rubric.model_json_schema()}', Rubric.model_json_schema())\n",
    "        except Exception:\n",
    "            # If for some reason schema generation fails, remove the placeholder to avoid KeyError\n",
    "            prompt_template = prompt_template.replace('{Rubric.model_json_schema()}', '')\n",
    "\n",
    "    # Format the judge prompt using q, gen, and exact (do not change the prompt text itself)\n",
    "    try:\n",
    "        prompt = prompt_template.format(q=q or '', gen=gen or '', exact=exact or '')\n",
    "    except KeyError:\n",
    "        # As a last resort, escape braces and format only known placeholders\n",
    "        safe_template = prompt_template.replace('{', '{{').replace('}', '}}')\n",
    "        safe_template = safe_template.replace('{{q}}', '{q}').replace('{{gen}}', '{gen}').replace('{{exact}}', '{exact}')\n",
    "        prompt = safe_template.format(q=q or '', gen=gen or '', exact=exact or '')\n",
    "\n",
    "    # Call the LLM (same as before)\n",
    "    resp = chat(\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        model=MODEL_NAME,\n",
    "        format=Rubric.model_json_schema(),\n",
    "    )\n",
    "\n",
    "    # Extract response content and validate the JSON returned by the model\n",
    "    content = None\n",
    "    # Typical ollama response provides resp.message.content\n",
    "    if hasattr(resp, 'message') and getattr(resp, 'message') is not None:\n",
    "        try:\n",
    "            content = getattr(resp.message, 'content') if hasattr(resp.message, 'content') else (resp.message.get('content') if isinstance(resp.message, dict) else None)\n",
    "        except Exception:\n",
    "            content = None\n",
    "\n",
    "    if content is None:\n",
    "        if isinstance(resp, str):\n",
    "            content = resp\n",
    "        elif isinstance(resp, dict):\n",
    "            content = resp.get('content') or resp.get('message') or json.dumps(resp, ensure_ascii=False)\n",
    "        else:\n",
    "            content = str(resp)\n",
    "\n",
    "    try:\n",
    "        rubric = Rubric.model_validate_json(content)\n",
    "    except Exception:\n",
    "        try:\n",
    "            parsed = json.loads(content) if isinstance(content, str) else content\n",
    "            rubric = Rubric.model_validate(parsed)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f'Failed to parse/validate rubric from model response: {e}')\n",
    "\n",
    "    return rubric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "884ca9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def simple_fallback_evaluate(entry: Dict[str, Any]) -> Rubric:\n",
    "#     # Lightweight heuristic fallback when the LLM client is unavailable or errors occur\n",
    "#     gen = (entry.get('generated_answer') or '').strip()\n",
    "#     exact = entry.get('exact-answer')\n",
    "#     correct = False\n",
    "#     # Try numeric comparison when possible\n",
    "#     try:\n",
    "#         token = gen.split()[0] if gen.split() else ''\n",
    "#         gval = float(token)\n",
    "#         evalv = float(exact)\n",
    "#         correct = abs(gval - evalv) < 1e-6\n",
    "#     except Exception:\n",
    "#         # fallback to simple string compare (case-insensitive, stripped)\n",
    "#         if isinstance(exact, str) and gen.lower() == exact.strip().lower():\n",
    "#             correct = True\n",
    "#     stopped = gen.endswith('...') or 'cannot' in gen.lower() or \"can't\" in gen.lower()\n",
    "#     easy = 8 if len(gen.split()) <= 30 else max(1, 10 - (len(gen.split()) // 50))\n",
    "#     fast = 8 if len(gen.split()) <= 50 else 5\n",
    "#     score = 1.0 if correct else 0.0\n",
    "#     return Rubric(correct_answer=correct, did_it_solve_in_easy_and_fast_approach=(easy+fast)//2, did_it_stop_midway=stopped, easy_to_understand_explanation=easy, notes=None, score=score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981f87e",
   "metadata": {},
   "source": [
    "## Call Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c7274e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating. Output written to: /home/nailsonseat/StudioProjects/CourseGPT-Pro-DSAI-Lab-Group-6/Milestone-5/math-agent/qwen3-32b.judgments.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Main loop: read input JSONL and produce output JSONL with judgments\n",
    "in_path = Path(INPUT_FILE)\n",
    "out_path = Path(OUTPUT_FILE)\n",
    "\n",
    "if not in_path.exists():\n",
    "    raise FileNotFoundError(f'Input file not found: {in_path.resolve()}')\n",
    "\n",
    "with in_path.open('r', encoding='utf-8') as fin, out_path.open('w', encoding='utf-8') as fout:\n",
    "    for i, line in enumerate(fin, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        entry = json.loads(line)\n",
    "        try:\n",
    "            # Try structured LLM judgment first\n",
    "            rubric = evaluate_entry_with_llm(entry)\n",
    "        except Exception as e:\n",
    "            # On errors (eg. no ollama client), fall back to simple heuristics\n",
    "            print(f'Warning: evaluation failed for entry {i}: {e}; using fallback heuristic')\n",
    "            # Lightweight heuristic fallback when the LLM client is unavailable or errors occur\n",
    "            gen = ''\n",
    "            # Try to extract a best-effort generated answer\n",
    "            resp_obj = entry.get('response')\n",
    "            if isinstance(resp_obj, dict):\n",
    "                choices = resp_obj.get('choices')\n",
    "                if isinstance(choices, list) and choices:\n",
    "                    first_choice = choices[0]\n",
    "                    if isinstance(first_choice, dict):\n",
    "                        msg = first_choice.get('message')\n",
    "                        if isinstance(msg, dict) and 'content' in msg:\n",
    "                            gen = msg['content']\n",
    "                        elif 'text' in first_choice:\n",
    "                            gen = first_choice.get('text')\n",
    "                        elif 'content' in first_choice:\n",
    "                            gen = first_choice.get('content')\n",
    "            if not gen:\n",
    "                gen = entry.get('response_text') or entry.get('generated_answer') or entry.get('answer') or ''\n",
    "\n",
    "            exact = entry.get('exact-answer') or entry.get('exact_answer') or entry.get('reference') or entry.get('answer') or ''\n",
    "\n",
    "            correct = False\n",
    "            try:\n",
    "                token = gen.strip().split()[0] if gen and gen.strip().split() else ''\n",
    "                gval = float(token)\n",
    "                evalv = float(exact)\n",
    "                correct = abs(gval - evalv) < 1e-6\n",
    "            except Exception:\n",
    "                if isinstance(exact, str) and isinstance(gen, str) and gen.strip().lower() == str(exact).strip().lower():\n",
    "                    correct = True\n",
    "\n",
    "            stopped = isinstance(gen, str) and (gen.strip().endswith('...') or 'cannot' in gen.lower() or \"can't\" in gen.lower())\n",
    "            easy = 8 if isinstance(gen, str) and len(gen.split()) <= 30 else max(1, 10 - (len(str(gen).split()) // 50))\n",
    "            fast = 8 if isinstance(gen, str) and len(gen.split()) <= 50 else 5\n",
    "            score = 1.0 if correct else 0.0\n",
    "\n",
    "            rubric = Rubric(\n",
    "                correct_answer=correct,\n",
    "                did_it_solve_in_easy_and_fast_approach=(easy + fast) // 2,\n",
    "                did_it_stop_midway=stopped,\n",
    "                easy_to_understand_explanation=easy,\n",
    "                notes=f'Fallback used due to error: {e}',\n",
    "                score=score,\n",
    "            )\n",
    "\n",
    "        output = {\n",
    "            'judgment': rubric.model_dump(),\n",
    "        }\n",
    "        fout.write(json.dumps(output, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f'Finished evaluating. Output written to: {out_path.resolve()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsaiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
