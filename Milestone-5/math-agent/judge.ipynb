{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e56beb",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37847960",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chat\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Any, Dict\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Any, Dict\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78094fa",
   "metadata": {},
   "source": [
    "## Set Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03dc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = 'responses.jsonl'  # <- change this to your file\n",
    "OUTPUT_FILE = 'judgments.jsonl'  # judgments will be written here\n",
    "MODEL_NAME = 'gemma3:4b'  # change to your model if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61b277",
   "metadata": {},
   "source": [
    "## Set Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13686ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rubric(BaseModel):\n",
    "    correct_answer: bool\n",
    "    did_it_solve_in_easy_and_fast_approach: int\n",
    "    did_it_stop_midway: bool\n",
    "    easy_to_understand_explanation: int\n",
    "    notes: Optional[str] = None\n",
    "    score: Optional[float] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dbdb4",
   "metadata": {},
   "source": [
    "## Define Judge Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae05458",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"You are a careful, strict math grader.\n",
    "\n",
    "Given the QUESTION, the MODEL GENERATED ANSWER, and the EXACT ANSWER, produce a single JSON object matching this schema exactly (no extra keys):\n",
    "\n",
    "{Rubric.model_json_schema()}\n",
    "\n",
    "Return only valid JSON.\n",
    "\n",
    "QUESTION: {q}\n",
    "\n",
    "MODEL GENERATED ANSWER: {gen}\n",
    "\n",
    "EXACT ANSWER: {exact}\n",
    "\n",
    "Notes: decide `correct_answer` by whether the generated answer matches the exact answer (for numeric answers compare numerically when possible). Rate `did_it_solve_in_easy_and_fast_approach` from 1 (very poor) to 10 (excellent). Set `did_it_stop_midway` true if the answer is incomplete or the model says it cannot continue. Rate `easy_to_understand_explanation` from 1 to 10. Provide short `notes` if necessary and optional `score` between 0 and 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e04f41",
   "metadata": {},
   "source": [
    "## Judge Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbec284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entry_with_llm(entry: Dict[str, Any]) -> Rubric:\n",
    "    q = entry.get('question')\n",
    "    gen = entry.get('generated_answer')\n",
    "    exact = entry.get('exact-answer')\n",
    "    prompt = JUDGE_PROMPT.format(q=q, gen=gen, exact=exact)\n",
    "\n",
    "    resp = chat(\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        model=MODEL_NAME,\n",
    "        format=Rubric.model_json_schema(),\n",
    "    )\n",
    "\n",
    "    # Validate the returned JSON against the Rubric model\n",
    "    rubric = Rubric.model_validate_json(resp.message.content)\n",
    "    return rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ca9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def simple_fallback_evaluate(entry: Dict[str, Any]) -> Rubric:\n",
    "#     # Lightweight heuristic fallback when the LLM client is unavailable or errors occur\n",
    "#     gen = (entry.get('generated_answer') or '').strip()\n",
    "#     exact = entry.get('exact-answer')\n",
    "#     correct = False\n",
    "#     # Try numeric comparison when possible\n",
    "#     try:\n",
    "#         token = gen.split()[0] if gen.split() else ''\n",
    "#         gval = float(token)\n",
    "#         evalv = float(exact)\n",
    "#         correct = abs(gval - evalv) < 1e-6\n",
    "#     except Exception:\n",
    "#         # fallback to simple string compare (case-insensitive, stripped)\n",
    "#         if isinstance(exact, str) and gen.lower() == exact.strip().lower():\n",
    "#             correct = True\n",
    "#     stopped = gen.endswith('...') or 'cannot' in gen.lower() or \"can't\" in gen.lower()\n",
    "#     easy = 8 if len(gen.split()) <= 30 else max(1, 10 - (len(gen.split()) // 50))\n",
    "#     fast = 8 if len(gen.split()) <= 50 else 5\n",
    "#     score = 1.0 if correct else 0.0\n",
    "#     return Rubric(correct_answer=correct, did_it_solve_in_easy_and_fast_approach=(easy+fast)//2, did_it_stop_midway=stopped, easy_to_understand_explanation=easy, notes=None, score=score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981f87e",
   "metadata": {},
   "source": [
    "## Call Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7274e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: read input JSONL and produce output JSONL with judgments\n",
    "in_path = Path(INPUT_FILE)\n",
    "out_path = Path(OUTPUT_FILE)\n",
    "\n",
    "if not in_path.exists():\n",
    "    raise FileNotFoundError(f'Input file not found: {in_path.resolve()}')\n",
    "\n",
    "with in_path.open('r', encoding='utf-8') as fin, out_path.open('w', encoding='utf-8') as fout:\n",
    "    for i, line in enumerate(fin, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        entry = json.loads(line)\n",
    "        try:\n",
    "            # Try structured LLM judgment first\n",
    "            rubric = evaluate_entry_with_llm(entry)\n",
    "        except Exception as e:\n",
    "            # On errors (eg. no ollama client), fall back to simple heuristics\n",
    "            print(f'Warning: evaluation failed for entry {i}: {e}; using fallback heuristic')\n",
    "            # rubric = simple_fallback_evaluate(entry)\n",
    "\n",
    "        output = {\n",
    "            'question': entry.get('question'),\n",
    "            'generated_answer': entry.get('generated_answer'),\n",
    "            'exact-answer': entry.get('exact-answer'),\n",
    "            'judgment': rubric.model_dump(),\n",
    "        }\n",
    "        fout.write(json.dumps(output, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f'Finished evaluating. Output written to: {out_path.resolve()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsaiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
