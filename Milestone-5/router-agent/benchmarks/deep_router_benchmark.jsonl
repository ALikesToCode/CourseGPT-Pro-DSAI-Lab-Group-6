{"index": 0, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous analysis of second-order optimization in Model-Agnostic Meta-Learning (MAML). Please provide a full mathematical derivation of the MAML meta-gradient, explicitly showing the Hessian term. Then, implement this using efficient Hessian-vector products in JAX for a few-shot sinusoidal regression problem. Finally, empirically demonstrate and validate the convergence benefits of this second-order approach compared to a first-order approximation like Reptile or FOMAML, focusing on the trade-off between per-epoch complexity and final adaptation performance.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org model-agnostic meta-learning second-order optimization Hessian-vector product\\\", mode=both)\", \"/math(Derive the MAML meta-objective gradient, applying the chain rule through the inner-loop update to expose the full Hessian term. Analyze the Bayesian convergence properties and contrast with simpler first-order methods.)\", \"/code(Implement the MAML algorithm in JAX for a few-shot sinusoidal regression task, using jax.jvp and jax.vjp to create an efficient Hessian-vector product. Ensure the data pipeline supports curriculum learning by varying wave amplitude., using Python)\"], \"route_rationale\": \"The blue general-search agent is used first to gather authoritative papers on second-order MAML optimization. The math agent then formally derives the complex gradient expression from these sources. The first code agent translates this derivation into an efficient JAX implementation, leveraging its automatic differentiation capabilities for Hessian-vector products. The second code agent executes the empirical validation to check if the theoretical benefits hold in practice.\", \"expected_artifacts\": [\"A PDF document containing the step-by-step mathematical derivation of the second-order MAML gradient.\", \"A Python script (`maml_hvp.py`) implementing the MAML algorithm with Hessian-vector products and the comparative experiment.\", \"PNG plots illustrating the convergence curves (meta-loss vs. epochs) for both second-order and first-order MAML.\", \"A JSON file containing the final meta-test loss metrics for both implemented algorithms.\", \"A brief summary report citing at least two relevant arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Formulate the MAML objective function as an expectation over a distribution of tasks, defining the inner-loop and outer-loop optimization structure.\", \"2. Apply the multivariable chain rule to the meta-objective with respect to the meta-parameters, differentiating through the inner-loop gradient descent update to derive the exact meta-gradient involving the Hessian of the inner-loop loss.\", \"3. Show how the computationally expensive Hessian matrix can be avoided by implementing a Hessian-vector product, which can be calculated efficiently using automatic differentiation libraries.\", \"4. Implement the few-shot sinusoidal regression benchmark, including a data loader that samples tasks with varying amplitude, phase, and frequency.\", \"5. Validate the implementation by running experiments and checking that the second-order method exhibits faster convergence or reaches a lower final loss compared to a first-order baseline, as predicted by theory.\", \"6. Audit the final plots and metrics to ensure they are correctly labeled and statistically significant, confirming the experimental conclusion.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent's derivation must be correctly implemented by the code agent, confirmed by comparing the symbolic gradient terms to the auto-diff output on a toy problem; fallback: if empirical results contradict theory, the task is returned to /math for derivation review).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on MAML (e.g., Finn et al., 2017) and its second-order optimization.\", \"- [ ] /math: Provide a full derivation of the MAML meta-gradient, clearly showing the origin of the Hessian term.\", \"- [ ] /math: Explain the Hessian-vector product approximation and its role in making second-order MAML computationally feasible.\", \"- [ ] /code: Implement the sinusoidal regression task generator and the MAML training framework in JAX.\", \"- [ ] /code: Implement both the second-order (HVP) and first-order meta-update rules.\", \"- [ ] /math: Verify that the code agent's implementation of the HVP update correctly corresponds to the mathematical derivation.\", \"- [ ] /code: Execute the comparative experiments, log metrics, and generate convergence plots.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"few-shot-learning\", \"optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly identifies the Hessian term in the meta-gradient.\", \"- The JAX implementation of the Hessian-vector product is correct and enables the MAML algorithm to run successfully.\", \"- The final report includes plots that empirically show a performance advantage (e.g., faster convergence) for the second-order method over the first-order baseline.\", \"- The solution cites at least two relevant arXiv papers with their corresponding IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 1, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a formal proof connecting the multilinear rank of a Tucker decomposition for a neural network's weight tensor to its Vapnik-Chervonenkis (VC) dimension. Provide a rigorous analysis of the optimization landscape on the Grassmannian manifold of factor matrices, including convergence guarantees. Subsequently, implement this as a CUDA-optimized PyTorch layer and validate its parameter efficiency and generalization gap on a computer vision benchmark, referencing recent arXiv findings.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Tucker decomposition' 'VC dimension' 'representation learning' 'optimization on Grassmannian manifold' convergence\\\", mode=both)\", \"/math(Derive an upper bound on the VC dimension of a neural network layer parameterized by a Tucker tensor, analyzing the optimization convergence on the product of Stiefel manifolds for the factor matrices. Address the problem's complexity and eigenvalue spectrum.)\", \"/code(Implement a Tucker-decomposed linear layer in PyTorch with CUDA acceleration, comparing its parameter count and validation accuracy against a standard dense layer on CIFAR-10. Profile the CUDA kernel and verify gradient correctness., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature from arXiv on tensor decompositions and model complexity. This foundational research informs the /math agent's task of creating a rigorous theoretical proof. The /code agent then implements and empirically validates the mathematical concepts, providing a practical demonstration of the theory.\", \"expected_artifacts\": [\"- A formal mathematical proof in LaTeX connecting Tucker rank to an upper bound on VC dimension.\", \"- A Python script implementing the Tucker-decomposed layer in PyTorch, with CUDA optimization, and an experimental setup for CIFAR-10.\", \"- A summary report including a literature review with at least 2 arXiv citations, experimental results (plots of accuracy vs. parameters), and the final verification outputs.\", \"- A JSON file containing the primary and secondary metrics from the experiment.\", \"- A verification log detailing the check between the mathematical derivation and the code's gradient implementation.\"], \"thinking_outline\": [\"1. Survey foundational literature from arXiv to establish the theoretical links between tensor rank, parameterization, and measures of model complexity like VC dimension.\", \"2. Develop the formal mathematical proof relating the Tucker decomposition's multilinear rank to an upper bound on the model's VC dimension, clearly stating all assumptions.\", \"3. Formulate the optimization problem for learning the tensor factors, analyzing the geometry of the underlying manifold (a product of Stiefel manifolds) and its implications for gradient-based convergence.\", \"4. Verify the correctness of the derived gradients for the factor matrices before implementation.\", \"5. Implement the tensor layer in PyTorch with a custom CUDA kernel for efficiency and conduct experiments to measure performance against a dense baseline.\", \"6. Validate that the empirical results (parameter savings, generalization gap) are consistent with the theoretical bounds established in the proof.\"], \"handoff_plan\": \"/general-search provides foundational papers and equations -> /math uses this context to construct a formal proof and derives the optimization objective -> /code implements the derived formulation and runs experiments to produce metrics -> router QA (verification: check that the code's gradient computation numerically matches the analytical gradients derived by /math; fallback: if proof is intractable, pivot to an empirical study with a formal conjecture based on search results).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on Tucker decomposition for neural network compression and its relation to model complexity.\", \"- [ ] /math: Derive the proof connecting Tucker rank to VC dimension, including the convergence analysis on the specified manifold.\", \"- [ ] /code: Implement the PyTorch layer with its corresponding CUDA kernel.\", \"- [ ] /code: Run the validation experiment on CIFAR-10, collecting all specified metrics.\", \"- [ ] /math: Verify that the gradients implemented in the /code agent's work are consistent with the analytical derivation.\", \"- [ ] /code: Audit the CUDA kernel for performance and numerical stability.\", \"- [ ] /router: Perform final QA check, ensuring all acceptance criteria are met and artifacts are generated correctly.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"computational-complexity\"], \"acceptance_criteria\": [\"- The submitted proof must be mathematically sound and rigorously connect the multilinear rank of the Tucker tensor to an explicit upper bound on the layer's VC dimension.\", \"- The PyTorch implementation must achieve at least a 5x parameter reduction over a dense layer of comparable expressive capacity with less than a 2% drop in validation accuracy on CIFAR-10.\", \"- The final report must include at least two relevant citations from arXiv with their corresponding IDs.\", \"- The verification log must confirm that numerical gradients (computed via finite differences) in the code align with the analytical gradients from the mathematical derivation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 2, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive and implement a Lagrangian-based proximal policy optimization (PPO) algorithm for a robotic manipulation task framed as a Constrained Markov Decision Process (CMDP). The safety constraint involves limiting the cumulative cost associated with joint torque limits. Analyze the algorithm's convergence properties and demonstrate its performance against a baseline PPO in a simulated environment, tracking both reward maximization and constraint satisfaction.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org constrained markov decision process lagrangian relaxation proximal policy optimization robotics safety control theory\\\", mode=both)\", \"/math(Derive the constrained policy optimization objective for a CMDP using Lagrangian relaxation, defining the update rule for the Lagrange multiplier and showing how it integrates with the PPO surrogate objective function with Bayesian uncertainty.)\", \"/code(Implement the Lagrangian PPO agent in Python using PyTorch for a robotic arm environment, ensuring the code includes modules for cost evaluation, Lagrange multiplier updates, and tracks both reward and cumulative cost convergence.)\"], \"route_rationale\": \"The blue general-search agent first grounds the solution by finding authoritative literature on Lagrangian methods for safe RL. Next, the math agent formalizes the theory and derives the necessary update equations from first principles. Finally, the code agent implements this theory to produce a working simulation and empirical results, which can then be verified against the derivation.\", \"expected_artifacts\": [\"A PDF report containing the formal mathematical derivation of the Lagrangian-PPO update rules.\", \"A zipped archive of the Python source code for the agent, environment, and experiment scripts.\", \"A performance summary including plots of reward and cost convergence, comparing the safe agent to a baseline.\", \"A bibliography file citing at least two relevant arXiv papers with their IDs.\", \"A verification log confirming the alignment of the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Research state-of-the-art Lagrangian methods for safe reinforcement learning in the context of Constrained Markov Decision Processes (CMDPs).\", \"2. Formulate the constrained optimization problem mathematically and derive the primal-dual update rules for both the policy network and the Lagrange multiplier.\", \"3. Implement the derived algorithm within a standard PPO framework, paying close attention to the constrained update step and cost accumulation.\", \"4. Validate the implementation by unit testing the Lagrange multiplier update mechanism and checking for correct cost accumulation against a known trajectory.\", \"5. Run comparative experiments in a simulated robotics environment (e.g., PyBullet) to analyze the trade-off between reward maximization and constraint satisfaction.\", \"6. Audit the final results to verify that the agent consistently satisfies the safety constraint (cumulative cost < d) while achieving a competitive task reward.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Math agent audits the code's implementation of the Lagrange multiplier update against the formal derivation; fallback: If a discrepancy is found, the code agent refactors the update module and re-runs validation tests.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on Lagrangian relaxation for CMDPs and PPO.\", \"- [ ] /math: Derive the complete update equations for the policy parameters and the Lagrange multiplier, including proofs of convergence if available in literature.\", \"- [ ] /code: Implement the Lagrangian-PPO agent and the robotics simulation environment with torque-based cost functions.\", \"- [ ] /code: Run experiments to collect reward and cost data, and generate comparison plots against a vanilla PPO baseline.\", \"- [ ] /math: Verify that the implemented code logic for the Lagrange multiplier update correctly reflects the derived mathematical formula.\", \"- [ ] /code: Validate that the cost function in the simulation accurately represents the physical safety constraint (joint torque limits).\", \"- [ ] router: Review all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"robotics\", \"constrained-optimization\", \"control-theory\"], \"acceptance_criteria\": [\"- The final report must include a complete mathematical derivation of the Lagrangian PPO algorithm.\", \"- The implemented agent must achieve at least 80% of the unconstrained agent's reward while violating the safety cost constraint in less than 5% of evaluation episodes.\", \"- The solution must cite at least two relevant arXiv papers on safe reinforcement learning.\", \"- A verification log must be provided that confirms the one-to-one alignment between the math derivation and the code implementation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 3, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive and implement a safe reinforcement learning algorithm for a robotic manipulator using Control Barrier Functions (CBFs). The policy optimization should be formulated as a constrained problem using a Lagrangian relaxation approach. Provide a formal proof of convergence for the primal-dual updates under standard assumptions and validate the safety guarantees through simulation, tracking both cumulative reward and safety violation rates as primary metrics.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org safe reinforcement learning control barrier function lagrangian convergence proof\\\", mode=rag)\", \"/math(Derive the Lagrangian for the constrained policy optimization problem, formulate the primal-dual updates, and draft a proof of convergence based on control theory literature.)\", \"/code(Implement the CBF-based safe policy optimization algorithm in PyTorch. Run simulations to validate safety guarantees, tracking cumulative reward and constraint violation metrics against a Bayesian baseline., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey recent literature on Control Barrier Function-based safe RL, as the field is rapidly evolving. The findings are then passed to /math to formally derive the optimization problem and prove its convergence. Finally, /code implements and validates the derived algorithm in a simulated environment, providing empirical evidence for the theoretical claims.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Lagrangian dual and the policy/dual variable update rules.\", \"- A literature review summary with at least two cited arXiv papers.\", \"- A Python implementation of the safe RL algorithm and simulation environment.\", \"- Plots showing cumulative reward and safety constraint satisfaction over training epochs.\", \"- A verification report confirming the alignment between the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Review state-of-the-art literature on combining Control Barrier Functions (CBFs) with policy gradient methods in reinforcement learning, focusing on Lagrangian-based approaches.\", \"2. Formulate the policy optimization as a Constrained Markov Decision Process (CMDP) and derive the corresponding Lagrangian dual problem.\", \"3. Derive the primal-dual update rules for the policy parameters and the Lagrange multipliers, and construct a formal proof of convergence.\", \"4. Validate the Karush-Kuhn-Tucker (KKT) conditions for the derived optimization problem to ensure theoretical soundness.\", \"5. Implement the algorithm and a suitable robotics simulation environment (e.g., using PyBullet or MuJoCo).\", \"6. Verify that the trained agent respects the safety constraints in simulation by monitoring the CBF values and ensuring they remain non-negative, and audit the cumulative reward to ensure task performance is not sacrificed excessively.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The final code implementation of the update rule must be cross-referenced with the derivation from /math. The simulation metrics must directly correspond to the quantities analyzed in the proof. fallback: If /math cannot prove convergence, /general-search will be re-queried for alternative formulations with known guarantees.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent arXiv papers on CBF-based safe RL with Lagrangian methods.\", \"- [ ] /math: Derive the complete primal-dual update equations for the policy and Lagrange multipliers.\", \"- [ ] /math: Formulate and write down the proof of convergence for the derived algorithm.\", \"- [ ] /code: Implement the safe RL agent and the simulation environment in PyTorch.\", \"- [ ] /code: Instrument the simulation to log safety violations and cumulative reward per episode.\", \"- [ ] /math: Verify that the update rule implemented in the /code agent's script exactly matches the final derived equation.\", \"- [ ] /code: Validate that the agent's safety violation rate is below the specified threshold in the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"robotics\", \"control-theory\", \"constrained-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Lagrangian and the convergence proof must be complete, self-contained, and mathematically sound.\", \"- The Python implementation must successfully train an agent that achieves a safety violation rate below 1% over 100 evaluation episodes.\", \"- The final report must cite at least two relevant arXiv papers published after 2020, with full arXiv IDs provided.\", \"- The generated plots must clearly show the trade-off between cumulative reward and safety constraint satisfaction during training.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 4, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating differentiable neural architecture search (NAS) and need to formalize the gradient computation for the architecture parameters in a DARTS-like bilevel optimization setup. Can you derive the full gradient expression using the implicit function theorem to approximate the inner loop's solution, analyze the resulting Hessian-vector product's impact on computational complexity, and then implement this search algorithm in PyTorch to find a competitive convolutional cell on the CIFAR-10 benchmark?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org differentiable architecture search bilevel optimization Hessian approximation implicit differentiation\\\", mode=both)\", \"/math(Derive the gradient for architecture parameters in a DARTS bilevel optimization problem using the implicit function theorem, analyzing the convergence properties and the computational complexity of the associated Hessian eigenvalue spectrum.)\", \"/code(Implement the derived differentiable AutoML search strategy in PyTorch, including the supernet for CIFAR-10, bilevel optimizer, and CUDA-accelerated training loop to validate performance and search efficiency., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather the latest literature on Hessian approximations in differentiable NAS. This informs the /math agent's formal derivation of the complex bilevel optimization gradients. Finally, the /code agent implements the verified mathematical model for empirical validation.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the bilevel optimization gradient.\", \"- A Python script implementing the differentiable NAS algorithm and evaluation harness.\", \"- A JSON file containing the final discovered architecture and its test accuracy on CIFAR-10.\", \"- A summary report including convergence plots and a bibliography citing at least 3 sources.\"], \"thinking_outline\": [\"1. Research state-of-the-art literature on differentiable NAS, focusing on approximations for the bilevel optimization problem to ensure stability and computational feasibility.\", \"2. Formulate the NAS problem as a bilevel optimization objective and derive the analytical gradient for the architecture parameters using the chain rule and the implicit function theorem.\", \"3. Verify the mathematical derivation for correctness, ensuring all assumptions (e.g., smoothness, convexity) are stated and justified based on the literature review.\", \"4. Implement the complete search algorithm in PyTorch, including the mixed-operation supernet, the bilevel optimizer logic, and data loaders for CIFAR-10.\", \"5. Validate the implementation's gradient computation using numerical methods like finite differences on a reduced-scale version of the problem.\", \"6. Execute the full search process, log the validation accuracy over epochs, and finally, train the discovered architecture from scratch to report its final test performance.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to the /math agent. The /math agent produces a formal derivation that serves as a specification for the /code agent. The /code agent's implementation is then cross-referenced with the derivation for correctness. (Verification: Ensure the Hessian approximation in code matches the derived formula; Fallback: If the initial approximation is unstable, revert to /general-search to find simpler first-order methods).\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on DARTS and recent follow-ups on second-order optimization and Hessian approximation.\", \"- [ ] /math: Derive the gradient for the architecture parameters \u03b1 with respect to the validation loss, using the implicit function theorem.\", \"- [ ] /code: Implement the supernet architecture and the continuous relaxation of the search space using softmax.\", \"- [ ] /code: Implement the bilevel optimization loop for updating network weights and architecture parameters.\", \"- [ ] /math: Verify the analytical derivation logic against a symbolic differentiation tool for a simplified objective function.\", \"- [ ] /code: Add a gradient checking utility to numerically verify the backpropagation implementation against the analytical derivation.\", \"- [ ] /code: Run the full search and evaluation pipeline, generating convergence plots and final performance metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-architecture-search\", \"bilevel-optimization\", \"automl\"], \"acceptance_criteria\": [\"- The mathematical derivation is complete, correct, and directly corresponds to the algorithm implemented in the code.\", \"- The final Python code is runnable and includes a script to reproduce the reported CIFAR-10 test accuracy.\", \"- The final report cites at least 3 authoritative papers (arXiv IDs) on differentiable architecture search.\", \"- The discovered architecture achieves a test accuracy on CIFAR-10 that is competitive with published DARTS results (e.g., >96.5%).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 6, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to understand the effect of interventions in non-linear, non-Gaussian Structural Causal Models (SCMs). Specifically, for a given acyclic causal graph Z -> X -> Y where the structural equations are non-linear (e.g., Y = f(X) + N_y, X = g(Z) + N_x), please derive the analytical form of the post-interventional probability density function P(Y|do(X=x)). Then, devise and implement a Python simulation using a generative model to empirically validate this derivation by comparing the analytical PDF to the simulated post-interventional data distribution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org non-linear structural causal model intervention \\\\\\\"change of variables\\\\\\\" Bayesian networks\\\", mode=both)\", \"/math(Derive the post-interventional density P(Y|do(X=x)) for a non-linear additive noise SCM, using the change of variables formula and Jacobian determinants. Ensure the causal graph assumptions for identifiability are explicitly stated.)\", \"/code(Implement a Python simulation of the SCM using PyTorch. Generate observational data, then interventional data. Use a Bayesian neural network to model distributions and verify the derived density against the empirical results, calculating a confidence interval for the match.)\"], \"route_rationale\": \"The blue /general-search agent first gathers foundational literature on non-linear SCMs and interventional calculus. The /math agent then uses these principles to perform the symbolic derivation. Finally, the /code agent implements a simulation to numerically validate the mathematical result, creating a closed-loop verification process.\", \"expected_artifacts\": [\"- A formal mathematical proof in LaTeX deriving the post-interventional probability density function.\", \"- A Python script (`.py`) containing the SCM simulation, intervention logic, and validation code.\", \"- A plot (`.png`) overlaying the analytically derived probability density function on a histogram of the empirically generated post-interventional data.\", \"- A markdown report (`.md`) summarizing the methodology, results, and citing at least two relevant academic papers with arXiv IDs.\"], \"thinking_outline\": [\"1. Use /general-search to find key papers on causal inference in non-linear SCMs, focusing on Pearl's do-calculus and its application via the change of variables formula for probability densities.\", \"2. Formulate a specific three-variable non-linear SCM (e.g., using sigmoids or polynomials) with non-Gaussian noise to serve as the basis for both the derivation and simulation.\", \"3. Execute the mathematical derivation for P(Y|do(X=x)), carefully calculating the Jacobian of the transformation from the noise variables to the observed variables.\", \"4. Implement the SCM in Python to generate a large sample of observational data, then implement the 'do' operation by replacing the structural equation for X and generate interventional data.\", \"5. Validate the simulation by comparing the empirical distribution of the generated post-interventional data for Y against the analytical PDF derived in step 3 using the Kolmogorov-Smirnov test.\", \"6. Check the Python implementation's logic, ensuring the data generation process for both observational and interventional settings strictly follows the SCM definition.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /math agent receives foundational papers from the search agent to ground the derivation. The /code agent receives the final analytical formula from the /math agent to use as the ground truth for its simulation. The final results are passed to the router for quality assurance. Verification: The primary check is the Kolmogorov-Smirnov test p-value comparing the code's output with the math's derivation. Fallback: If the p-value is low (<0.05), the task reverts to the /math agent to re-audit the Jacobian calculation, as this is a common source of error.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on interventions in non-linear SCMs.\", \"- [ ] /math: Define a concrete non-linear SCM with additive noise (e.g., Y = X^2 + N_y, X = sin(Z) + N_x).\", \"- [ ] /math: Derive the analytical form of P(Y|do(X=x)) based on the defined SCM.\", \"- [ ] /code: Implement the defined SCM in Python and generate 10,000 samples of observational data.\", \"- [ ] /code: Implement the intervention `do(X=x)` and generate 10,000 samples of interventional data.\", \"- [ ] /code: Plot the empirical interventional distribution and overlay the analytical PDF from the /math agent.\", \"- [ ] /math: Verify that the code's plotted analytical function correctly implements the derived formula.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"structural-causal-model\", \"do-calculus\"], \"acceptance_criteria\": [\"- The final report must contain a complete, step-by-step mathematical derivation of the post-interventional density.\", \"- The Python simulation must produce a plot clearly showing strong visual alignment between the histogram of simulated data and the line plot of the derived analytical density.\", \"- The Kolmogorov-Smirnov test comparing the empirical and analytical distributions must yield a p-value greater than 0.05, indicating no statistically significant difference.\", \"- The solution must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 7, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Bayesian Neural Network using a score-based generative model defined by a Stochastic Differential Equation. The task involves deriving the evidence lower bound (ELBO) for this continuous-time model, implementing a solver for the reverse-time SDE using a predictor-corrector method like Euler-Maruyama, and validating the posterior uncertainty quantification on a high-dimensional regression task by checking the calibration of confidence intervals.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian neural network stochastic differential equation score matching ELBO\\\", mode=web)\", \"/math(Derive the continuous-time Evidence Lower Bound (ELBO) for a Bayesian neural network posterior defined by a reverse-time SDE, ensuring the derivation for the score-matching objective is consistent with the Fokker-Planck equation for Bayesian inference.)\", \"/code(Implement the predictor-corrector SDE solver in JAX for training the Bayesian network, track AUPRC for uncertainty calibration, and include CUDA-accelerated score network training with convergence checks., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find foundational papers on score-based generative models as Bayesian posteriors. The results inform the /math agent's formal derivation of the ELBO. Finally, the /code agent implements the derived model and solver, performing the required validation.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the continuous-time ELBO.\", \"A Python script implementing the SDE-based Bayesian Neural Network, including the SDE solver and training loop.\", \"A JSON file reporting final uncertainty quantification metrics (AUPRC, ECE) and training loss curves.\", \"A summary document citing at least two relevant arXiv papers with IDs.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of score-based generative models and their application to Bayesian inference via SDEs, focusing on the connection between the reverse-time SDE and the posterior distribution.\", \"2. Formulate the forward and reverse SDEs that define the prior and posterior distributions over the neural network weights, respectively.\", \"3. Derive the evidence lower bound (ELBO) objective function, explicitly showing how the score-matching loss relates to maximizing the marginal likelihood.\", \"4. Implement the numerical solver for the reverse-time SDE and the score network architecture. Validate the solver's stability and convergence properties on a simple distribution.\", \"5. Train the full model on a regression task and evaluate the quality of its uncertainty estimates using calibration metrics like Expected Calibration Error (ECE).\", \"6. Verify that the implemented loss function in the code directly corresponds to the terms derived in the mathematical proof, auditing for any discrepancies.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Cross-reference the derived ELBO from /math with the loss implementation in /code; fallback: If mismatch, flag /math to re-check the derivation against the sources from /general-search.)\", \"todo_list\": [\"- [ ] /general-search: Find 2-3 key papers from arXiv on score-based models for Bayesian posterior approximation.\", \"- [ ] /math: Derive the ELBO for the continuous-time SDE formulation of Bayesian inference, referencing the discovered papers.\", \"- [ ] /code: Implement the Euler-Maruyama predictor-corrector solver for the reverse-time SDE.\", \"- [ ] /code: Implement the score network and the training loop using the derived ELBO as the loss function.\", \"- [ ] /math: Verify that the discrete-time loss implementation in the code is a valid approximation of the continuous-time derivation.\", \"- [ ] /code: Run experiments to measure uncertainty calibration (ECE, AUPRC) and generate plots.\", \"- [ ] /router: Perform final QA check, ensuring artifacts align with acceptance criteria and metrics are correctly reported.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-differential-equations\", \"generative-models\"], \"acceptance_criteria\": [\"The mathematical derivation of the ELBO must be complete, self-contained, and consistent with cited literature.\", \"The implementation must successfully train and produce posterior samples, with the final Expected Calibration Error (ECE) below 0.05.\", \"The final report must include plots of the loss curve and calibration diagrams, and cite at least two relevant arXiv pre-prints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 8, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and analyze a Bayesian extension of Model-Agnostic Meta-Learning (MAML). Derive the evidence lower bound (ELBO) for a variational approximation of the posterior over task-specific parameters, and implement a JAX version that uses the reparameterization trick. Critically, analyze the Hessian of the meta-objective and its impact on convergence guarantees compared to standard first-order MAML, especially in a low-data regime for few-shot image classification.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Bayesian MAML' 'variational inference' meta-learning convergence analysis\\\", mode=both)\", \"/math(Derive the evidence lower bound (ELBO) for the Bayesian meta-learning objective, including the full gradient estimator. Formulate the Hessian of the meta-objective to analyze convergence properties.)\", \"/code(Implement the Bayesian MAML algorithm in JAX, using the reparameterization trick for the derived ELBO. Validate the implementation against a few-shot benchmark, reporting accuracy and Expected Calibration Error metrics., using Python)\"], \"route_rationale\": \"The blue general-search agent will first gather state-of-the-art literature on Bayesian MAML. The /math tool will then formalize the theory based on these papers, deriving the necessary objectives and gradients. Finally, /code will translate the mathematical formulation into a verifiable implementation and run empirical evaluations to test the hypotheses.\", \"expected_artifacts\": [\"- A LaTeX document containing the full mathematical derivation of the Bayesian MAML ELBO and its gradient.\", \"- A JAX implementation of the algorithm, including data loaders and an evaluation script for a benchmark like Mini-ImageNet.\", \"- A final report summarizing the theoretical analysis, empirical results (accuracy, calibration), and a bibliography with arXiv citations.\", \"- A verification log showing the results of numerical gradient checks comparing the analytical and auto-differentiated gradients.\"], \"thinking_outline\": [\"1. Survey existing literature on Bayesian extensions to MAML and variational inference in meta-learning to establish a theoretical baseline.\", \"2. Formulate the probabilistic graphical model for Bayesian MAML and correctly derive the evidence lower bound (ELBO) for the meta-learning objective.\", \"3. Derive the gradient estimators using the reparameterization trick, paying close attention to the second-order terms involving the Hessian required for the meta-update.\", \"4. Implement the complete Bayesian MAML algorithm and verify the gradient computation logic using numerical differentiation checks (e.g., `jax.test_util.check_grads`).\", \"5. Evaluate the model on a standard few-shot learning benchmark and validate that the uncertainty estimates, measured by Expected Calibration Error, are meaningful.\", \"6. Conclude with a theoretical analysis of the Hessian's role in the meta-update and its impact on convergence compared to first-order MAML.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /math agent receives arXiv papers from /general-search to formalize the derivations. The /code agent receives the final derivations from /math to implement the algorithm. The final handoff to the router involves a full QA check. Verification: The code's gradient implementation must match the math derivation. Fallback: If the derivation is intractable or unstable, revert to /general-search to find alternative formulations like implicit MAML.\", \"todo_list\": [\"- [ ] /general-search: Find at least two foundational papers on Bayesian MAML and variational meta-learning.\", \"- [ ] /math: Derive the ELBO and its gradient for the specified Bayesian MAML objective.\", \"- [ ] /math: Formulate the Hessian of the meta-objective for the convergence analysis section.\", \"- [ ] /code: Implement the Bayesian MAML algorithm in JAX, including the reparameterization-based sampler.\", \"- [ ] /code: Set up the evaluation harness for 5-way, 1-shot classification and Expected Calibration Error.\", \"- [ ] /math: Verify that the analytical gradient derivation aligns with the JAX auto-differentiation logic.\", \"- [ ] /code: Perform numerical gradient checks to validate the correctness of the backpropagation implementation.\", \"- [ ] Router QA: Review all artifacts for consistency between the theoretical derivation, implementation, and empirical results.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"bayesian-inference\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO and its gradient is complete, correct, and clearly articulated.\", \"- The JAX implementation runs without errors and includes successful numerical gradient check logs.\", \"- The final report presents a 5-way 1-shot classification accuracy on a benchmark dataset that is competitive with published results.\", \"- The bibliography cites at least two relevant research papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 10, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop and benchmark a stochastic variational inference (SVI) algorithm for a hierarchical Bayesian latent Dirichlet allocation model. The goal is to compare the convergence of the Evidence Lower Bound (ELBO) using a non-centered reparameterization trick against a standard pathwise derivative estimator. Can you derive the necessary gradients, implement this in PyTorch, and analyze its performance on a synthetic corpus, paying close attention to posterior collapse?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org structured variational inference hierarchical bayesian models reparameterization trick convergence\\\", mode=both)\", \"/math(Derive the ELBO and its stochastic gradients for the hierarchical Bayesian model using the identified reparameterization trick, ensuring the Euler-Lagrange equations for the optimal variational factors are clearly stated.)\", \"/code(Implement the SVI algorithm in Python with PyTorch, tracking the ELBO convergence and posterior variance. Use CUDA for acceleration and include assertions to check for numerical stability during optimization.)\"], \"route_rationale\": \"The plan starts with literature review via the blue /general-search agent to find state-of-the-art reparameterization techniques. /math is then used to formalize the problem by deriving the specific gradients based on the literature. Finally, /code translates the mathematical derivation into a verifiable and benchmarked implementation.\", \"expected_artifacts\": [\"- A LaTeX/PDF document with the full mathematical derivation of the ELBO and its gradients.\", \"- A Python script implementing the SVI algorithm, the data simulator, and the evaluation loop.\", \"- A summary report (.md) containing convergence plots (ELBO vs. iterations) and a table of final performance metrics.\", \"- A bibliography file with citations to at least two relevant arXiv papers.\", \"- A log file containing the output of the gradient verification checks.\"], \"thinking_outline\": [\"1. Define the probabilistic graphical model for the hierarchical Bayesian LDA, specifying priors, likelihoods, and the mean-field variational family.\", \"2. Use search to identify suitable non-centered reparameterization tricks for Gamma and Dirichlet distributions, as these are key to the model.\", \"3. Derive the full expression for the Evidence Lower Bound (ELBO) and formally derive its stochastic gradients with respect to the variational parameters using the chosen reparameterization.\", \"4. Implement the SVI optimization loop, the model, and a synthetic data generator in PyTorch.\", \"5. Validate the implementation by performing numerical gradient checks on a small data batch to ensure the analytical gradients match the auto-differentiated ones.\", \"6. Run the full experiment, generate convergence plots, and verify that the ELBO is, on average, non-decreasing and converges to a stable value.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The arXiv papers found by /general-search provide the theoretical foundation for the derivation in /math. The final mathematical expressions from /math serve as the direct specification for the implementation in /code. Verification: The router QA will cross-reference the math derivation against the code's loss function and update steps. Fallback: If the ELBO fails to converge in the code, the task reverts to /math to check for errors in the gradient derivation.\", \"todo_list\": [\"- [ ] /general-search: Find \u22652 recent arXiv papers on advanced reparameterization tricks for hierarchical Bayesian models.\", \"- [ ] /math: Derive the ELBO and its stochastic gradients for the specified model.\", \"- [ ] /code: Implement the PGM, variational family, and SVI optimizer in PyTorch.\", \"- [ ] /math: Verify that the derived gradient expressions are mathematically sound and consistent with the literature.\", \"- [ ] /code: Implement and run numerical gradient checks to validate the correctness of the backward pass.\", \"- [ ] /code: Execute the benchmark, generate ELBO convergence plots, and save metrics to a JSON file.\", \"- [ ] router: Perform final QA review, ensuring derivation in the PDF matches the code implementation and the report correctly interprets the results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO and its reparameterized gradients is complete and correct.\", \"- The Python implementation passes numerical gradient checks with a tolerance of 1e-4.\", \"- The final report shows a clear, non-decreasing trend for the ELBO over at least 1000 training iterations.\", \"- The report cites at least two relevant arXiv papers with IDs, published within the last 5 years.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 11, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating few-shot adaptation using Bayesian Model-Agnostic Meta-Learning (BMAML). I need a formal derivation of the meta-objective as an Evidence Lower Bound (ELBO) for a hierarchical Bayesian model. Also, please implement this in JAX, focusing on amortized variational inference for the task-specific posterior. The implementation should be benchmarked on the sinusoidal regression task, analyzing the convergence behavior of the inner-loop adaptation and its sensitivity to the meta-prior's variance.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Bayesian MAML' 'amortized variational inference' meta-learning convergence analysis\\\", mode=both)\", \"/math(Derive the variational meta-objective (ELBO) for Bayesian MAML under a hierarchical model. Analyze the Hessian of the inner-loop objective to discuss conditions for fast adaptation and convergence given the meta-prior.)\", \"/code(Implement the Bayesian MAML algorithm in JAX using the derived ELBO. Evaluate on the few-shot sinusoidal regression task, tracking post-adaptation MSE and Negative Log-Likelihood (NLL) as primary convergence metrics, and visualize the predictive uncertainty., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on Bayesian MAML and its theoretical underpinnings. The findings inform the /math agent's formal derivation of the meta-objective and convergence analysis. Finally, the /code agent implements the derived mathematical framework and performs empirical validation, completing the theory-to-practice workflow.\", \"expected_artifacts\": [\"* A LaTeX document containing the full derivation of the Bayesian MAML ELBO and the Hessian analysis.\", \"* A Python script with the JAX implementation of the BMAML algorithm and the sinusoidal regression benchmark.\", \"* A JSON file reporting the final post-adaptation test MSE and NLL metrics, averaged over 1000 test tasks.\", \"* A PDF report summarizing the findings, including plots of the meta-training curve and visualizations of the model's predictive uncertainty on sample regression tasks, citing at least two relevant papers.\", \"* A verification log confirming that the implemented gradient updates in the code match the derived mathematical expressions.\"], \"thinking_outline\": [\"1. Research foundational papers on Bayesian meta-learning, specifically Bayesian MAML, focusing on the formulation of the hierarchical model and the use of amortized variational inference.\", \"2. Formally derive the Evidence Lower Bound (ELBO) for the meta-objective, clearly defining the prior, variational posterior, and likelihood terms.\", \"3. Implement the derived update rules in JAX, paying close attention to the reparameterization trick for sampling and computing gradients through the stochastic inner-loop adaptation process.\", \"4. Validate the implementation's correctness by checking the gradient flow and ensuring the loss decreases during meta-training on the sinusoidal regression benchmark.\", \"5. Analyze the results, comparing the empirical convergence speed and final performance (MSE, NLL) against theoretical expectations from the Hessian analysis.\", \"6. Verify that the final report correctly cites the literature and that the reported metrics in the JSON artifact are consistent with the generated plots and code output.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers (e.g., from arXiv) to the /math agent. The /math agent produces a formal derivation which becomes the specification for the /code agent. The /code agent's implementation is then passed to the router for final quality assurance. Verification: The router will check that the code's loss function implementation exactly matches the ELBO derived by /math. Fallback: If the derivation is intractable, /math will be prompted to simplify the assumptions (e.g., use a mean-field approximation) and notify /code of the change.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Bayesian MAML, highlighting the hierarchical model and objective function.\", \"- [ ] /math: Derive the ELBO for the specified Bayesian MAML setup.\", \"- [ ] /math: Analyze the Hessian of the inner-loop loss function w.r.t task-specific parameters.\", \"- [ ] /code: Implement the BMAML algorithm and the sinusoidal regression experiment in JAX.\", \"- [ ] /code: Generate plots for meta-training loss and predictive uncertainty on test tasks.\", \"- [ ] /math: Verify that the gradient computations in the JAX code align with the analytical gradients derived from the ELBO.\", \"- [ ] /code: Verify that the reported metrics (MSE, NLL) are stable and reproducible across different random seeds.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"bayesian-neural-networks\", \"few-shot-learning\"], \"acceptance_criteria\": [\"* The mathematical derivation of the ELBO must be complete and self-contained.\", \"* The JAX implementation must successfully run and show a decreasing trend in the meta-training loss.\", \"* The final report must include at least one plot showing the model's predictive uncertainty on a test regression task, clearly visualizing the mean and +/- 2 standard deviations.\", \"* The reported post-adaptation MSE on the sinusoidal task must be below 0.1, consistent with published benchmarks.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 13, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the local curvature of the loss landscape for a pre-trained ResNet-18 model. Specifically, I want to implement the Lanczos algorithm to approximate the top eigenvalues of the Hessian matrix without forming it explicitly. The implementation should leverage Hessian-vector products and be written in JAX for CUDA acceleration. The goal is to produce a spectral density plot and relate the dominant eigenvalue to generalization bounds discussed in recent literature on sharpness-aware minimization.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian eigenspectrum generalization deep learning Lanczos algorithm convergence analysis\\\", mode=both)\", \"/math(Derive the Hessian-vector product for a cross-entropy loss function and outline the steps of the iterative Lanczos algorithm, ensuring the resulting tridiagonal matrix's eigenvalue properties are correctly stated., includes cybersecurity)\", \"/code(Implement the Lanczos algorithm and HVP in Python using JAX and Flax, targeting CUDA devices for optimal convergence. Apply it to a pre-trained ResNet-18 and validate the top eigenvalue computation against a smaller MLP.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art literature on Hessian spectral analysis and its link to Bayesian generalization. /math is then used to formalize the required derivations for the Hessian-vector product and Lanczos method. Finally, /code implements and validates this complex numerical algorithm, providing the empirical results.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Hessian-vector product and the Lanczos algorithm.\", \"A well-documented Python script (`lanczos_hvp.py`) implementing the analysis in JAX.\", \"A plot (`hessian_spectrum.png`) showing the computed spectral density of the ResNet-18 Hessian.\", \"A summary report (`report.md`) interpreting the results and citing at least two relevant arXiv papers.\", \"Verification output (`verification_log.txt`) comparing Lanczos results to direct computation on a small model.\"], \"thinking_outline\": [\"1. Research recent advancements in Hessian spectral analysis via Lanczos methods and their implications for generalization and Bayesian inference.\", \"2. Formulate the mathematical foundation: derive the Hessian-vector product (HVP) using principles of automatic differentiation and detail the Lanczos algorithm for tridiagonalization.\", \"3. Implement the HVP and Lanczos iteration in JAX, ensuring memory efficiency by avoiding explicit Hessian materialization.\", \"4. Create a small, tractable neural network model to serve as a test case; verify the JAX implementation by comparing the computed top eigenvalue against the value from a direct `jax.numpy.linalg.eigh` call.\", \"5. Execute the validated algorithm on a pre-trained ResNet-18 model to approximate the top-k eigenvalues and plot the spectral density.\", \"6. Audit the final report to ensure the interpretation of the spectral plot correctly connects high eigenvalues to poor generalization (sharp minima) and aligns with the findings from the literature search.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The mathematical derivation from /math must match the HVP implementation logic in /code, and the final spectral plot must be consistent with theoretical expectations from /general-search; fallback: If derivation and code mismatch, return to /math for correction).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on using the Lanczos algorithm for Hessian spectral analysis in deep learning.\", \"- [ ] /math: Provide a step-by-step derivation for the Hessian-vector product and the Lanczos iteration process.\", \"- [ ] /code: Write a JAX implementation of the Lanczos algorithm using `jax.jvp` and `jax.vjp` for the HVP.\", \"- [ ] /code: Create a verification suite that compares the implemented Lanczos method's top eigenvalue with a direct `eigh` computation on a small MLP.\", \"- [ ] /code: Run the verified script on a pre-trained ResNet-18 from a library like Flax/Hugging Face and generate the spectral density plot.\", \"- [ ] /math: Verify that the numerical precision and convergence criteria used in the /code implementation are consistent with the theoretical guarantees discussed in the literature from /general-search.\", \"- [ ] /router: Perform final QA check, ensuring all artifacts are present and the report correctly synthesizes the theoretical, mathematical, and empirical results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The Python script must run without errors and reproduce the reported eigenvalues within a small tolerance (1e-4).\", \"- The final report must correctly interpret the link between the dominant Hessian eigenvalue and the flatness of the loss landscape, citing at least two arXiv papers with IDs.\", \"- The verification log must show that for a small test model, the Lanczos-approximated top eigenvalue matches the exact value from direct diagonalization.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 15, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive and implement a dynamic programming algorithm on a tree decomposition to solve the minimum dominating set problem for a graph G. The solution must include a formal proof of correctness using structural induction on the tree decomposition, a complexity analysis in terms of the graph's treewidth k and number of vertices n, and an optimized C++ implementation. I'm particularly interested in the algebraic formulation of the DP states and transitions, potentially leveraging tensor representations for the join operations to analyze computational complexity bounds beyond the standard O(k^c * n) analysis.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:cs.princeton.edu dynamic programming treewidth algebraic tensor complexity dominating set\\\", mode=web)\", \"/math(Formulate the DP states for minimum dominating set on a tree decomposition, using algebraic notation. Prove correctness via structural induction and analyze time complexity, considering potential convergence speedups from tensor contractions and bootstrap methods.)\", \"/code(Implement the DP algorithm in C++ for graphs of bounded treewidth. Use a modern library for graph representation and tree decomposition. Validate the implementation's complexity and correctness against known hard instances, using a CUDA-accelerated backend for any tensor operations., using Python)\"], \"route_rationale\": \"The route begins with the blue /general-search agent to survey state-of-the-art literature on algebraic approaches to DP on tree decompositions. This research informs the /math agent, which will construct the formal model, proofs, and complexity analysis. Finally, the /code agent will translate the verified mathematical specification into a performant and empirically validated C++ implementation.\", \"expected_artifacts\": [\"A LaTeX document containing the formal problem definition, dynamic programming formulation, proof of correctness, and detailed complexity analysis.\", \"An optimized, well-documented C++ source code file implementing the algorithm.\", \"A JSON file reporting the performance (runtime, correctness) on a set of benchmark graphs with varying treewidth.\", \"A bibliography file listing citations, including at least two arXiv preprints with their IDs.\"], \"thinking_outline\": [\"1. Deconstruct the problem: Minimum Dominating Set on graphs parameterized by treewidth, requiring an advanced algebraic analysis.\", \"2. Research advanced techniques: Use /general-search to find relevant literature on algebraic or tensor-based approaches to dynamic programming on tree decompositions.\", \"3. Formalize the algorithm: Define the DP states and transition functions rigorously using mathematical notation. Prove the correctness of the recurrence using structural induction on the tree decomposition.\", \"4. Analyze complexity: Derive the time and space complexity in terms of n (vertices) and k (treewidth), paying special attention to how the algebraic formulation of join operations affects the polynomial dependency on k.\", \"5. Validate the mathematical model: Scrutinize the base cases and inductive step of the proof for logical soundness and completeness. Check the derived complexity against established results for similar fixed-parameter tractable problems.\", \"6. Implement and verify performance: Code the algorithm and test it against known benchmark instances. Validate that the empirical runtime scaling aligns with the theoretical complexity analysis.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Math agent verifies code logic matches the formal recurrence; router verifies all artifacts meet acceptance criteria; fallback: If implementation deviates, /code agent must refactor and re-run validation tests).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on DP on tree decompositions for dominating set, focusing on algebraic/tensor formulations.\", \"- [ ] /math: Define the DP states and recurrence relation based on literature survey.\", \"- [ ] /math: Construct a formal proof of correctness for the recurrence using structural induction.\", \"- [ ] /math: Derive the computational complexity, referencing the algebraic structure of the join operation.\", \"- [ ] /code: Implement the tree decomposition algorithm and the dynamic programming logic in C++.\", \"- [ ] /code: Create a test suite with benchmark graphs to validate correctness and measure performance against theoretical bounds.\", \"- [ ] /math: Verify that the implemented logic in /code correctly reflects the derived mathematical recurrence relation and state transitions.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"dynamic-programming\", \"graph-theory\", \"treewidth\", \"computational-complexity\"], \"acceptance_criteria\": [\"The mathematical proof must be rigorous, complete, and correctly apply structural induction on the tree decomposition.\", \"The C++ implementation must pass all test cases for graphs with treewidth up to k=8 and correctly compute the minimum dominating set size.\", \"The final report must include an empirical validation section where runtime measurements on benchmark graphs are shown to be consistent with the derived time complexity.\", \"At least two relevant academic papers are cited with their full arXiv IDs in the bibliography artifact.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 18, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nHow can I implement a simple Physics-Informed Neural Network (PINN) to solve the 1D Burgers' equation? I need to understand the theoretical basis of using the strong form of the PDE in the loss function and see a working Python example.\n", "completion": "{\"route_plan\": [\"/math(Derive the composite loss function for a Physics-Informed Neural Network (PINN) designed to solve the 1D Burgers' equation, treating the PDE residual as a soft constraint analogous to an Euler-Lagrange variational principle.)\", \"/code(Implement a PINN in Python using PyTorch to solve the 1D Burgers' equation, visualizing the solution and tracking the loss components to check for convergence.)\", \"/general-search(query=\\\"site:arxiv.org physics-informed neural networks Burgers' equation tutorial\\\", mode=both)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation for the PINN loss function. This theory is then implemented and tested using /code. Finally, the blue /general-search agent is used to find authoritative references to justify the methodology and compare results against established work.\", \"expected_artifacts\": [\"A formal mathematical derivation of the PINN loss function for the 1D Burgers' equation.\", \"A documented Python script implementing the PINN solution using PyTorch.\", \"A plot comparing the PINN's predicted solution u(x,t) against the exact solution.\", \"A metrics report in JSON format detailing final MSE and loss components.\", \"A bibliography with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the 1D Burgers' equation, defining the problem domain, initial conditions (e.g., a sine wave), and Dirichlet boundary conditions.\", \"2. Derive the PINN composite loss function, showing how it combines the Mean Squared Error (MSE) from the boundary/initial data and the MSE of the PDE residual evaluated at collocation points.\", \"3. Implement a feedforward neural network in Python to approximate the solution u(x,t), using automatic differentiation to compute the partial derivatives required for the PDE residual.\", \"4. Train the network by minimizing the composite loss function, and visualize the evolution of the solution over time.\", \"5. Validate the final trained model by computing the L2 error against the known analytical solution and verifying that the PDE residual is minimized across the domain.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math provides the exact loss function formula for the /code agent to implement. The /code agent produces a solution plot and metrics, which are then contextualized by canonical examples found by /general-search. The complete package is submitted for review. Verification: The implemented loss in code must match the derivation. Fallback: If the code fails to converge, re-evaluate the neural network architecture or optimizer settings based on recommendations from the searched literature.\", \"todo_list\": [\"- [ ] /math: Derive the complete PINN loss for the 1D Burgers' equation, detailing the data and physics residual terms.\", \"- [ ] /code: Write a Python script to implement and train the PINN, ensuring automatic differentiation is correctly applied to the network output.\", \"- [ ] /code: Generate a plot comparing the PINN's predicted solution against the ground truth for a given time point.\", \"- [ ] /general-search: Find at least two key papers or tutorials on PINNs for Burgers' equation to contextualize the approach.\", \"- [ ] /verification: Check that the implemented loss function in the Python code exactly matches the mathematical derivation from the /math step.\", \"- [ ] /router QA: Submit all artifacts for final approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"physics-informed-ml\", \"deep-learning\", \"differential-equations\"], \"acceptance_criteria\": [\"- The mathematical derivation of the loss function must be correct and clearly distinguish between data loss and physics loss.\", \"- The Python code must execute successfully and produce a solution plot that qualitatively shows the characteristic shock wave formation of the Burgers' equation.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\", \"- The primary metric (MSE vs ground truth) must be below 1e-3.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"index": 20, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the computational complexity of determining the Grundy value (or nim-value) for positions in games played on directed acyclic graphs, specifically focusing on variants of Kayles or octal games. I need a formal proof sketch for its PSPACE-hardness, a Python implementation of a recursive solver with memoization, and a survey of relevant literature from arXiv regarding the Sprague-Grundy theorem's limits in such adversarial scenarios.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Sprague-Grundy theorem computational complexity PSPACE octal games\\\", mode=web)\", \"/math(Formalize the reduction from a known PSPACE-complete problem like QSAT to the Grundy value computation on a dynamically constructed graph, analyzing the polynomial-time complexity of the construction., includes cybersecurity)\", \"/code(Implement a Python solver for Grundy numbers on graph representations using dynamic programming, including runtime assertions and validation against known combinatorial game theory benchmarks to check for convergence issues.)\"], \"route_rationale\": \"The blue /general-search agent first gathers state-of-the-art research on the complexity of impartial games from arXiv. /math then uses this theoretical foundation to construct a formal hardness proof. Finally, /code implements a solver to empirically validate the theoretical findings and analyze performance characteristics.\", \"expected_artifacts\": [\"A LaTeX document detailing the PSPACE-hardness proof via reduction from a canonical PSPACE-complete problem.\", \"A Python script (`grundy_solver.py`) implementing the nim-value computation with memoization.\", \"A summary report (`report.md`) synthesizing the findings, including performance benchmarks and complexity analysis.\", \"A bibliography file (`references.bib`) with citations for at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Review literature on the complexity class of impartial games, particularly those that are known to be PSPACE-complete.\", \"2. Formalize the rules for a specific octal game or a generalized game on a Directed Acyclic Graph and define the recursive calculation for its Grundy values.\", \"3. Construct a polynomial-time reduction from a known PSPACE-complete problem (e.g., Quantified Boolean Formula) to the problem of determining if a game position has a Grundy value of zero.\", \"4. Implement an algorithm to compute Grundy values and validate its correctness on a suite of small, non-trivial game graphs with known solutions.\", \"5. Verify the logical soundness of the reduction proof, ensuring all conditions for a valid polynomial-time mapping are met and explicitly documented.\", \"6. Synthesize the proof, implementation, and literature review into a coherent final report.\"], \"handoff_plan\": \"/general-search -> /math (search results provide theoretical basis for the proof) -> /code (mathematical specification dictates implementation logic) -> router QA (verification: code output on test cases must match manual calculations from the math spec; fallback: if verification fails, /math refines the recurrence relation and /code updates the implementation).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on the complexity of Sprague-Grundy calculations.\", \"- [ ] /math: Formulate the game definition and the recursive structure of Grundy numbers (nim-values).\", \"- [ ] /math: Construct the formal PSPACE-hardness reduction proof.\", \"- [ ] /code: Implement the Grundy number solver in Python with memoization and appropriate data structures for graph representation.\", \"- [ ] /code: Develop a test suite with at least 5 non-trivial graph examples and their expected nim-values for validation.\", \"- [ ] /math: Verify that the implemented logic in the /code agent's script correctly reflects the derived recurrence relation.\", \"- [ ] /router: Perform final QA check, ensuring the proof, code, and report are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"pspace-hardness\"], \"acceptance_criteria\": [\"The PSPACE-hardness proof must be logically sound and the reduction clearly explained with polynomial-time guarantees.\", \"The Python implementation must correctly compute the Grundy values for all provided test cases within the specified time limits.\", \"The final report must cite at least two relevant arXiv papers with proper formatting and IDs.\", \"The analysis of algorithmic complexity in the report must align with both the theoretical proof and empirical runtime data.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 24, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm mentoring a student on neural network optimization. Can you help me create a tutorial explaining how the Hessian matrix's eigenvalue spectrum relates to the curvature of the loss surface for a simple regression model? We need a clear mathematical derivation and a small Python demo.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org introductory Hessian eigenvalue neural network loss landscape curvature\\\", mode=both)\", \"/math(Derive the relationship between the Hessian matrix of a neural network loss function and its local curvature, focusing on the role of its largest eigenvalue in determining the learning rate for convergence.)\", \"/code(Implement a simple feed-forward neural network in Python with JAX to compute and visualize the loss surface and the Hessian's eigenvalue spectrum at different points during training on a synthetic biology dataset.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on Hessian analysis. This informs the /math agent's formal derivation of the relationship between eigenvalues and curvature. Finally, the /code agent provides an empirical demonstration by implementing and visualizing the concepts, grounding the theory in a practical example.\", \"expected_artifacts\": [\"A formal mathematical proof connecting Hessian eigenvalues to loss surface curvature, delivered as a LaTeX-formatted PDF.\", \"A Python script using JAX that visualizes the 2D loss landscape and the Hessian eigenvalue distribution for a simple model.\", \"A summary report in Markdown format synthesizing the findings, including generated plots and at least 2 citations.\", \"A verification log confirming that the computed eigenvalues qualitatively match the observed curvature in the plots.\"], \"thinking_outline\": [\"1. Define the mean squared error loss function for a simple, two-layer feed-forward neural network on a regression task.\", \"2. Formulate the Hessian matrix of this loss function with respect to the network's weights using matrix calculus.\", \"3. Use a second-order Taylor expansion of the loss function to formally show how the eigenvalues of the Hessian matrix characterize the local curvature of the loss surface.\", \"4. Implement Python code to compute the Hessian and its full eigenvalue spectrum for a trained model at the minimum.\", \"5. Verify that regions with large positive eigenvalues correspond to steep, narrow valleys in the visualized loss landscape, and small or zero eigenvalues correspond to flat plateaus.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results provide context for the math derivation. The math derivation provides the formulas for the code implementation. The code's outputs are then cross-referenced against the mathematical theory for verification. Fallback: If the code's visualization does not match the theory, the /math agent will be re-invoked to check for errors in the derivation's assumptions for this specific model architecture.\", \"todo_list\": [\"- [ ] Use /general-search to gather foundational materials on Hessian analysis in deep learning.\", \"- [ ] Use /math to produce the formal derivation of the Hessian-curvature relationship.\", \"- [ ] Use /code to create the Python/JAX implementation and generate visualizations.\", \"- [ ] Verify that the largest eigenvalue computed by the code correctly predicts the steepest direction of curvature in the corresponding plot.\", \"- [ ] Assemble the final report, including all artifacts, and submit for router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"matrix-calculus\", \"spectral-analysis\", \"optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly links the Hessian's eigenvalues to the second-order term in the Taylor expansion of the loss function.\", \"- The Python code runs without errors and produces both a 2D/3D plot of the loss surface and a histogram of the Hessian eigenvalues.\", \"- The final report includes \u22652 citations to authoritative sources (e.g., arXiv papers, university course notes) with stable URLs or IDs.\", \"- The verification step confirms a qualitative match between the largest eigenvalue's magnitude and the sharpness of the loss landscape curvature.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Qualitative assessment of the correlation between the magnitude of the largest Hessian eigenvalue and the visually inspected sharpness of the loss landscape plots. Scored on a 1-5 scale by a human grader.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Boolean pass/fail based on whether the provided code runs successfully and generates the specified artifacts using the provided seed.\"}}", "categories": ["metrics_guidance"]}
{"index": 25, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a differentiable architecture search (DARTS) pipeline using JAX. The focus should be on second-order optimization to improve convergence. Can you derive and implement the unrolled gradient approximation using Hessian-vector products, and also incorporate a regularization technique like scheduled drop-path to mitigate the performance collapse often seen in DARTS where skip-connections dominate the final architecture? The final output should validate the gradient computation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org DARTS second-order optimization Hessian-vector product convergence collapse\\\", mode=both)\", \"/math(Derive the bilevel optimization objective for this AutoML problem, detailing the implicit differentiation for the architecture gradient via Hessian-vector products and incorporating a Bayesian perspective on regularization.)\", \"/code(Implement the DARTS supernet and bilevel optimizer in JAX, using jax.jvp and jax.vjp. The implementation must be CUDA-compatible and track validation AUPRC and architecture parameter entropy., using Python)\", \"/math(Formulate a numerical verification procedure for the implemented gradient. Compare the analytical Hessian-vector product output against a finite-difference approximation to check for correctness and analyze the convergence criteria.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on second-order DARTS. This informs the /math agent's formal derivation of the complex gradient approximation. The derived math is then passed to the /code agent for a high-performance JAX implementation. A final loop-back to the /math agent provides a crucial verification step, ensuring the code correctly implements the complex mathematical theory.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the second-order DARTS update rule in a markdown report.\", \"- A Python script containing the full JAX implementation of the DARTS training and search pipeline.\", \"- A verification report comparing analytical and numerical gradients, including error metrics.\", \"- A JSON file containing final validation metrics and the discovered architecture genotype.\", \"- A bibliography citing at least two relevant research papers with their arXiv IDs.\"], \"thinking_outline\": [\"1. Formalize the bilevel optimization problem for DARTS, defining the inner loop for weight optimization and the outer loop for architecture optimization.\", \"2. Derive the second-order approximation for the architecture gradient using the implicit function theorem, resulting in a formulation that can be efficiently computed with Hessian-vector products.\", \"3. Implement the complete DARTS training loop in JAX, including the supernet, architecture parameter updates via Gumbel-Softmax, and the derived second-order gradient computation.\", \"4. Validate the implemented gradient computation by designing and running a test that compares the analytical Hessian-vector product results against a finite-difference approximation, ensuring the relative error is below a small tolerance (e.g., 1e-4).\", \"5. Integrate and tune a regularization strategy (e.g., scheduled drop-path) to discourage the over-selection of parameter-free operations like skip-connections.\", \"6. Audit the final discovered architecture by training it from scratch and verifying that its performance on a held-out test set is competitive and does not suffer from performance collapse.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent. The math agent's derivation is the formal spec for the code agent. The code agent's implementation is then passed back to the math agent for gradient verification. Verification failure triggers a fallback: the /code agent must debug the implementation against the /math spec. Success leads to a final review by the router.\", \"todo_list\": [\"- [ ] /general-search: Find at least two key papers on second-order DARTS and regularization.\", \"- [ ] /math: Produce the full derivation for the bilevel optimization gradient using the implicit function theorem.\", \"- [ ] /code: Implement the DARTS cell and supernet structure in JAX/Flax.\", \"- [ ] /code: Implement the bilevel optimizer using Hessian-vector products for the architecture update step.\", \"- [ ] /math: Create the finite-difference approximation logic for gradient verification.\", \"- [ ] /code: Integrate the math agent's verification logic into a unit test for the optimizer.\", \"- [ ] /code: Run the full search process on a benchmark dataset like CIFAR-10 and log metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-architecture-search\", \"automl\", \"differentiable-programming\"], \"acceptance_criteria\": [\"- The gradient verification report must show a relative error between the analytical and numerical gradients of less than 1e-4.\", \"- The final discovered architecture, when trained from scratch, must achieve at least 96% accuracy on the CIFAR-10 validation set.\", \"- The final architecture genotype must not be dominated by skip-connections (i.e., less than 50% of normal/reduce cell operations are skip-connects).\", \"- The report must cite at least two relevant arXiv papers with correct IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 26, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for a score-based diffusion model starting from the forward process Fokker-Planck equation. Then, show how the corresponding probability flow ordinary differential equation (ODE) can be derived as a deterministic limit. Implement both samplers in PyTorch and compare their generated sample quality using the Frechet Inception Distance (FID), analyzing the convergence properties and computational trade-offs.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse time SDE probability flow ODE\\\", mode=web)\", \"/math(Derive the reverse-time SDE from the forward Fokker-Planck equation, then derive the corresponding probability flow ODE as a deterministic counterpart, ensuring clear justification for each step based on variational principles and stochastic calculus.)\", \"/code(Implement both the SDE sampler using an Euler-Maruyama scheme and the ODE sampler using a Runge-Kutta integrator in PyTorch; evaluate both on a 2D dataset, reporting FID scores for convergence analysis and wall-clock time., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find authoritative arXiv papers, providing a theoretical foundation. This research is handed to the /math agent for a rigorous, formal derivation of both the SDE and ODE. Finally, the verified equations are passed to the /code agent for a faithful implementation and empirical performance analysis, ensuring the code directly reflects the proven theory.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the reverse-time SDE and probability flow ODE.\", \"A Python script (`sampler_comparison.py`) containing implementations of both samplers and the FID evaluation logic.\", \"A summary report in Markdown (`report.md`) analyzing the FID scores and wall-clock times to explain the sampler trade-offs.\", \"A JSON file (`metrics.json`) containing the final FID scores and execution times for each sampler.\", \"A `citations.bib` file with at least two entries for the foundational papers discovered.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation by identifying the forward process SDE and its corresponding Fokker-Planck equation from authoritative literature.\", \"2. Formally derive the reverse-time SDE by applying the principles of time-reversal for diffusion processes, explicitly showing the emergence of the score function, \u2207_x log p_t(x).\", \"3. Derive the probability flow ODE, demonstrating how it represents a deterministic transport of probability mass that shares the same marginals p_t(x) as the stochastic process.\", \"4. Implement both the SDE sampler (e.g., predictor-corrector) and a standard ODE integrator (e.g., RK45) to generate samples from a simple, known distribution using a pre-trained score network.\", \"5. Validate the implementation by computing Frechet Inception Distance (FID) to quantitatively measure sample quality and record wall-clock time to assess computational cost.\", \"6. Audit the final metrics and generated sample plots to write a conclusive analysis on the trade-off between the stochastic and deterministic samplers regarding sample fidelity and generation speed.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent passes arXiv PDFs/links to /math. /math provides the final, typeset SDE and ODE equations to /code. /code implements these and produces a metrics JSON. Verification: The router QA will cross-reference the implemented discrete update rules in the code against the continuous-time equations from /math. Fallback: If FID scores are anomalous or do not converge, the task is returned to /code for debugging the numerical integrator's stability and step-sizing.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two foundational papers on score-based SDEs and probability flow ODEs (e.g., from Song et al., Karras et al.).\", \"- [ ] /math: Produce a full derivation of the reverse-time SDE from the Fokker-Planck equation.\", \"- [ ] /math: Derive the probability flow ODE from the reverse-time SDE.\", \"- [ ] /code: Implement the SDE-based sampler using an appropriate discretization scheme like Euler-Maruyama.\", \"- [ ] /code: Implement the ODE-based sampler using a standard numerical solver like RK45 from scipy.integrate.\", \"- [ ] /code: Instrument the samplers to measure and report both FID score and wall-clock time.\", \"- [ ] /math: Verify that the discrete-time update formulas planned for the code implementation are correct approximations of the derived continuous-time equations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-model\", \"stochastic-calculus\", \"generative-modeling\"], \"acceptance_criteria\": [\"- The mathematical derivation is self-contained, rigorous, and correctly links the forward Fokker-Planck equation to both the reverse-time SDE and probability flow ODE.\", \"- The Python code successfully generates samples using both methods and produces a valid `metrics.json` file with FID scores and timings.\", \"- The final report accurately interprets the empirical results, correctly identifying the trade-offs between stochastic and deterministic sampling approaches.\", \"- At least two relevant arXiv papers (e.g., by Yang Song, Tero Karras) are cited with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 27, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the connection between the flatness of the loss landscape and generalization in deep neural networks. Please derive the theoretical relationship between the Hessian matrix's spectral norm and the generalization gap, particularly focusing on overparameterized models. Then, empirically validate this theory by tracking the top eigenvalue of the Hessian for a small vision transformer during training on CIFAR-10, comparing a baseline model against one with explicit spectral regularization.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian eigenvalue generalization bound PAC-Bayesian deep learning\\\", mode=web)\", \"/math(Derive the PAC-Bayesian bound relating the Hessian's maximum eigenvalue to the generalization gap, ensuring the proof's convergence criteria for the stochastic optimizer are clearly defined.)\", \"/code(Implement a PyTorch experiment to track the Hessian's top eigenvalue via power iteration for a vision transformer, validating convergence and comparing test accuracy against a spectrally regularized variant., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey recent arXiv literature for state-of-the-art theoretical bounds. The findings are handed to the math agent to produce a formal, self-contained derivation. This theoretical result is then passed to the code agent for empirical validation, which is a critical step to bridge theory and practice.\", \"expected_artifacts\": [\"A formal mathematical derivation in LaTeX connecting the Hessian's spectral norm to generalization bounds.\", \"A well-documented Python script (PyTorch) for reproducing the experiment.\", \"A plot visualizing the evolution of the maximum Hessian eigenvalue and the train/test loss gap over training epochs for both models.\", \"A summary report (PDF) integrating the theory, experimental results, and a bibliography with at least two arXiv citations.\", \"A verification log confirming the mathematical derivation aligns with the numerical implementation's logic.\"], \"thinking_outline\": [\"1. Survey recent literature to identify relevant theoretical frameworks (e.g., PAC-Bayes, random matrix theory) that connect the loss landscape curvature (Hessian) to generalization.\", \"2. Formulate a precise theorem and provide a step-by-step derivation for the relationship between the Hessian's maximum eigenvalue and a formal generalization bound.\", \"3. Verify the mathematical derivation for logical soundness and check that all assumptions are explicitly stated and justified.\", \"4. Implement a numerical experiment in PyTorch to estimate the top eigenvalue of the Hessian during training and measure the generalization gap.\", \"5. Validate the empirical results by cross-referencing with published benchmarks and checking for numerical stability in the eigenvalue estimation algorithm.\", \"6. Synthesize the theoretical and empirical findings into a cohesive report, analyzing any discrepancies between the derived bound and observed behavior.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: router will check if the derivation from /math matches the logic implemented by /code; fallback: if a mismatch occurs, /math will be asked to clarify the derivation's assumptions for the specific model architecture).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from the last 3 years on Hessian spectral analysis and generalization.\", \"- [ ] /math: Produce a formal, step-by-step derivation based on the literature review.\", \"- [ ] /code: Implement the baseline vision transformer experiment, tracking metrics.\", \"- [ ] /code: Add the spectrally regularized model variant and generate comparison plots.\", \"- [ ] /math: Verify that the assumptions in the derivation (e.g., smoothness) are reasonable for the ReLU-based network used in the code.\", \"- [ ] /code: Validate the correctness of the power iteration method for eigenvalue estimation with a known matrix.\", \"- [ ] /router: Perform final QA, ensuring the theoretical claims are supported by the empirical plots and all acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation must be self-contained and logically sound.\", \"- The Python code must be runnable and reproduce the plots and metrics presented in the final report.\", \"- The primary metric (correlation coefficient) must be correctly calculated and reported.\", \"- The final report must cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 29, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a robust causal inference pipeline. Given a complex Structural Causal Model (SCM) with unobserved confounders, derive the causal effect of a treatment variable on an outcome, P(Y|do(X=x)), using Judea Pearl's do-calculus. The model involves non-linear relationships and requires applying the front-door criterion. Please provide a formal mathematical derivation of the estimand and then implement a corresponding Bayesian estimator in Python to verify its convergence and statistical power on simulated data. The simulation must track estimator bias and variance as a function of sample size.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'front-door criterion' 'Bayesian causal inference' computational implementation do-calculus\\\", mode=both)\", \"/math(Derive the causal estimand for P(Y|do(X=x)) by applying the three rules of do-calculus to the specified SCM, proving identifiability under the front-door criterion for a Bayesian framework.)\", \"/code(Implement the derived Bayesian estimator in Python using pymc, simulate data from the SCM, and perform a convergence analysis by tracking the Mean Absolute Error (MAE) and posterior credible intervals for the causal effect.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on computational approaches for the front-door criterion. This informs the /math agent's formal derivation of the causal estimand. Finally, the /code agent implements this derivation and runs a simulation to validate the estimator's statistical properties, providing empirical evidence for the theoretical work.\", \"expected_artifacts\": [\"- A formal mathematical proof deriving the causal estimand using do-calculus and the front-door criterion.\", \"- A documented Python script implementing the data simulation and the Bayesian causal estimator.\", \"- A report summarizing the simulation results, including plots of estimator bias, variance, and MAE against sample size.\", \"- A bibliography with at least two cited arXiv papers on modern causal inference techniques.\", \"- A JSON file containing the final computed metrics (MAE, bias, variance).\"], \"thinking_outline\": [\"1. Review the theoretical underpinnings of the front-door criterion and its relationship to Pearl's do-calculus rules for causal identifiability.\", \"2. Formulate the specific Structural Causal Model (SCM) and formally apply the do-calculus rules step-by-step to derive the interventional distribution P(Y|do(X=x)) as an expression involving only observational probabilities (the estimand).\", \"3. Verify that the derived estimand is correct and that all assumptions for the front-door criterion are satisfied by the proposed SCM.\", \"4. Design and implement a data-generating process in Python that conforms to the SCM, including the unobserved confounder.\", \"5. Implement the Bayesian estimator for the derived estimand, for example, using a library like `pymc`.\", \"6. Validate the entire pipeline by running simulations to check if the estimator recovers the true causal effect parameter, analyzing its convergence, bias, and variance.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal derivation) -> /code (implementation & validation) -> router QA (verification: code implementation matches math derivation, metrics meet criteria; fallback: if verification fails, return to /math for derivation check or /code for debugging).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize two recent papers (last 5 years) from arXiv on implementing do-calculus or the front-door criterion.\", \"- [ ] /math: Produce a LaTeX-formatted derivation of the front-door estimand for the specified causal graph.\", \"- [ ] /code: Create a Python class for the data-generating process based on the SCM.\", \"- [ ] /code: Implement the Bayesian estimator for the causal effect.\", \"- [ ] /math: Verify that the implemented statistical model in the code correctly represents the derived mathematical estimand.\", \"- [ ] /code: Run simulation experiments for N={100, 500, 1000, 5000} samples, compute metrics, and generate convergence plots.\", \"- [ ] /code: Audit the simulation code for correctness and statistical validity.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"bayesian-statistics\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies the three rules of do-calculus to arrive at the standard front-door formula.\", \"- The Python implementation of the estimator recovers the true causal effect in the simulation with a Mean Absolute Error (MAE) below 0.05 for the largest sample size.\", \"- The final report includes convergence plots (MAE vs. sample size) and properly cites at least two relevant peer-reviewed sources with arXiv IDs.\", \"- The code is well-documented and includes unit tests for the data-generating process.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 30, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the local convergence dynamics of adversarial training for a simple GAN. Can you help me analyze the saddle point structure of the minimax objective? Specifically, I need to derive the Hessian of the Lagrangian for a toy GAN problem, identify the conditions under which its eigenvalues indicate a strict saddle point versus a local minimum, and then implement a numerical optimization in JAX or PyTorch to locate these critical points and visualize the loss landscape. The analysis should be grounded in recent literature on non-convex optimization, particularly relating spectral norm of the Hessian to convergence guarantees.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org GAN convergence Hessian eigenvalues 'strict saddle' adversarial optimization\\\", mode=rag)\", \"/math(Derive the Lagrangian, gradient, and Hessian for a simplified GAN objective. Analyze the Hessian's eigenvalue spectrum to establish conditions for identifying local minima, maxima, and strict saddle points, considering theoretical convergence properties from the literature.)\", \"/code(Implement a Newton-Raphson or gradient-based optimizer in Python/JAX to find critical points of the GAN objective. Numerically compute the Hessian's eigenvalues at these points to classify them and generate a 2D contour plot of the adversarial loss landscape showing the saddle point structure.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art literature on GAN convergence and saddle point analysis. This theoretical foundation is then passed to the /math agent to perform the formal symbolic derivations of the Hessian and its properties. Finally, the /code agent implements the theory numerically to validate the derivations, find the critical points empirically, and visualize the results.\", \"expected_artifacts\": [\"- A PDF document containing the full mathematical derivation of the Lagrangian, gradient, and Hessian, along with the eigenvalue analysis.\", \"- A Python script (`main.py`) implementing the numerical optimization and saddle point classification.\", \"- A final report (`report.md`) summarizing the findings, including visualizations of the loss landscape and a table of classified critical points.\", \"- A bibliography file (`references.bib`) with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Survey foundational papers on non-convex optimization in GANs to understand the role of the Hessian and strict saddle properties for ensuring convergence.\", \"2. Formulate the minimax objective for a simplified GAN (e.g., generator and discriminator are linear models on Gaussian data).\", \"3. Analytically derive the gradient and the Hessian of the Lagrangian for this system.\", \"4. Implement a numerical method in Python to find points where the gradient is zero and then compute the Hessian matrix at these points.\", \"5. Verify that the eigenvalues of the numerically computed Hessian at the critical points correctly classify them as local minima, local maxima, or saddle points, consistent with the mathematical derivation.\", \"6. Validate that the algorithm's observed convergence behavior near these points aligns with theoretical predictions (e.g., fast convergence away from saddles).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: search results provide the basis for the mathematical model derived in /math; /math derivations provide the equations implemented in /code; /code's numerical output must match theoretical predictions. fallback: if derivations are intractable, /math agent should simplify the model; if code fails to converge, /code agent should implement a more robust first-order method and report failure to find critical points).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on GAN convergence and Hessian analysis.\", \"- [ ] /math: Derive the Hessian of the Lagrangian for the specified toy GAN problem.\", \"- [ ] /math: Define the conditions on the Hessian eigenvalues for a point to be a strict saddle.\", \"- [ ] /code: Implement an optimizer to find the critical points of the GAN loss function.\", \"- [ ] /code: Create visualizations of the loss landscape and the locations of the classified critical points.\", \"- [ ] /math: Verify that the symbolic Hessian derivation is correctly transcribed into the numerical implementation in the /code step.\", \"- [ ] /code: Validate the classification of critical points by comparing the numerical eigenvalues against the conditions derived by the /math agent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"adversarial-training\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Hessian must be complete and correct.\", \"- The Python code must successfully identify and classify at least one non-trivial saddle point.\", \"- The final report must contain a 2D visualization of the loss landscape with critical points clearly marked and classified.\", \"- The report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 31, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Variational Autoencoder where the latent space is modeled by a Stochastic Differential Equation. Can you help me derive the Evidence Lower Bound for this continuous-time model, specifically using Girsanov's theorem to handle the change of measure, and then implement it in PyTorch using the torchsde library for Bayesian uncertainty quantification on a time-series regression benchmark?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference stochastic differential equation Girsanov theorem ELBO\\\", mode=both)\", \"/math(Derive the continuous-time Evidence Lower Bound (ELBO) for a latent SDE, applying Girsanov's theorem for the change of measure between the prior and posterior diffusion processes, ensuring the derivation handles the Radon-Nikodym derivative correctly.)\", \"/code(Implement the SDE-based Bayesian neural net in PyTorch with torchsde, using the derived ELBO as the loss. Implement the Euler-Maruyama solver and validate uncertainty estimates via calibration error metrics on a finance dataset., using Python)\"], \"route_rationale\": \"The task requires deriving a complex mathematical objective and then implementing it. The blue /general-search agent will first find foundational papers on SDE-based variational inference. /math will then formalize the derivation of the ELBO. Finally, /code will implement the model based on the mathematical specification, requiring a specialized library like torchsde.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the continuous-time ELBO using Girsanov's theorem.\", \"Python source code for the SDE-based Bayesian Neural Network, including data loaders and training scripts.\", \"A report summarizing the model's performance, including plots of predictive uncertainty and calibration error metrics.\", \"A bibliography file citing at least two relevant arXiv papers with their IDs.\", \"A verification log confirming the consistency between the mathematical derivation and the code implementation of the ELBO.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of variational inference for continuous-time models governed by SDEs, focusing on the role of Girsanov's theorem.\", \"2. Formally derive the ELBO, paying close attention to the drift and diffusion terms and the Radon-Nikodym derivative that arises from the change of measure.\", \"3. Validate the mathematical derivation by cross-referencing it with established literature and checking for dimensional consistency in the resulting loss function.\", \"4. Implement the neural SDE architecture and the custom loss function in code, ensuring the SDE solver is correctly integrated.\", \"5. Design and run experiments to evaluate the model's predictive accuracy and uncertainty quantification, and verify that the calibration error metric is computed correctly.\"], \"handoff_plan\": \"/general-search -> /math (verification: check if found papers contain the necessary theorems for the derivation) -> /code (verification: audit code implementation against the derived ELBO formula from /math; fallback: if discrepancy, return to /math for correction) -> router QA (verification: ensure final report metrics align with acceptance criteria).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Neural SDEs and continuous-time variational inference.\", \"- [ ] /math: Derive the ELBO for the latent SDE model, explicitly using Girsanov's theorem.\", \"- [ ] /math: Verify the final ELBO expression against the sources found by /general-search.\", \"- [ ] /code: Implement the Bayesian Neural Network with a Neural SDE latent model in PyTorch/torchsde.\", \"- [ ] /code: Implement uncertainty evaluation metrics like Expected Calibration Error (ECE).\", \"- [ ] /code: Verify the coded loss function term-by-term against the derivation from /math.\", \"- [ ] /router: Perform final QA review of all artifacts against acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-deep-learning\", \"stochastic-differential-equations\", \"variational-inference\"], \"acceptance_criteria\": [\"The LaTeX derivation of the ELBO must be mathematically sound and correctly apply Girsanov's theorem.\", \"The Python implementation must successfully train and produce predictions with uncertainty estimates on a synthetic or real-world dataset.\", \"The final report must show an Expected Calibration Error (ECE) below 0.1 on the test set.\", \"The code must be well-documented and include a `requirements.txt` file for reproducibility.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 32, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive derivation and implementation of mean-field variational inference for Latent Dirichlet Allocation (LDA). Please derive the coordinate ascent variational inference (CAVI) update equations for the variational parameters, starting from the Evidence Lower Bound (ELBO). Then, implement this algorithm and validate its correctness by monitoring the ELBO's monotonic convergence on a synthetic corpus.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:jmlr.org site:arxiv.org 'Latent Dirichlet Allocation' 'coordinate ascent variational inference' derivation Blei Jordan\\\", mode=both)\", \"/math(Derive the CAVI updates for the per-document topic proportions (gamma) and per-word topic assignments (phi) by optimizing the Evidence Lower Bound (ELBO) within a mean-field Bayesian variational inference framework, assuming Dirichlet conjugate priors.)\", \"/code(Implement the Coordinate Ascent Variational Inference (CAVI) algorithm for Latent Dirichlet Allocation in Python with NumPy, ensuring the ELBO convergence check is implemented for model diagnostics and validation against a synthetic biology dataset.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find canonical literature on LDA and CAVI, which is essential for grounding the subsequent theoretical work. Next, /math is used to perform the core, rigorous derivation of the update equations. Finally, /code implements the derived algorithm and provides empirical verification of the theory, a critical step in math_plus_code tasks.\", \"expected_artifacts\": [\"- A PDF document containing the full mathematical derivation of the CAVI update equations for LDA and the ELBO formulation.\", \"- A standalone Python script (`cavi_lda.py`) implementing the algorithm and a data synthesizer.\", \"- A plot (`elbo_convergence.png`) showing the ELBO value at each iteration, demonstrating monotonic increase.\", \"- A summary report in Markdown (`report.md`) explaining the results and citing the gathered literature.\"], \"thinking_outline\": [\"1. Formulate the complete generative process for Latent Dirichlet Allocation (LDA), specifying the random variables, parameters, and their corresponding distributions (Dirichlet, Multinomial).\", \"2. Define the mean-field variational family for the latent variables and write down the Evidence Lower Bound (ELBO) objective function.\", \"3. Derive the detailed, closed-form coordinate ascent update equations for each variational parameter by taking functional derivatives of the ELBO with respect to each parameter while holding others fixed.\", \"4. Implement the CAVI algorithm in Python, including a function to compute the ELBO at each step.\", \"5. Validate the implementation by generating a synthetic corpus from a known LDA model and confirming that the ELBO is non-decreasing over iterations.\", \"6. Verify that the final inferred topic-word distributions (beta) from the model qualitatively align with the ground-truth distributions used to generate the synthetic data.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: math derivation must match code logic exactly; ELBO must be non-decreasing; fallback: if ELBO decreases, flag for review by /math agent to check for sign errors in derivation)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two canonical papers on LDA and Variational Inference (e.g., Blei et al. 2003).\", \"- [ ] /math: Produce a full derivation of the CAVI update equations for LDA's variational parameters (gamma and phi).\", \"- [ ] /code: Implement a synthetic data generator based on the LDA generative process.\", \"- [ ] /code: Implement the CAVI algorithm using the equations from the /math agent.\", \"- [ ] /math: Verify that the update equations implemented in the Python script are algebraically identical to the derived forms.\", \"- [ ] /code: Run the CAVI implementation on synthetic data, compute the ELBO at each step, and generate a convergence plot.\", \"- [ ] /general-search: Finalize the list of citations with arXiv IDs or DOIs for the final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"topic-modeling\"], \"acceptance_criteria\": [\"- The provided mathematical derivation of the CAVI update equations for LDA must be complete and correct.\", \"- The Python implementation must execute without errors and demonstrate a monotonically non-decreasing ELBO on a generated synthetic dataset.\", \"- The final report must cite at least two authoritative papers on the topic, including their JMLR or arXiv identifiers.\", \"- The ELBO convergence plot must be clearly labeled and included as a PNG artifact.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 33, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Physics-Informed Neural Network (PINN) in JAX to solve the 2D incompressible Navier-Stokes equations for lid-driven cavity flow. The core requirement is to enforce exact energy conservation by incorporating a divergence-free velocity field constraint directly into the network architecture using a scalar potential formulation. Please derive the corresponding loss function, including the PDE residual and momentum equations, and provide an optimized CUDA-enabled implementation that tracks the convergence of both the PDE loss and the kinetic energy dissipation over time.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org energy-conserving PINN Navier-Stokes scalar potential divergence-free\\\", mode=rag)\", \"/math(Derive the strong form of the Navier-Stokes PDE residuals and the kinetic energy dissipation integral, formulating the composite loss function for a PINN that uses a scalar potential to ensure a divergence-free velocity field. Focus on the energy budget.)\", \"/code(Implement the energy-conserving PINN in JAX, using automatic differentiation for the PDE residuals. Profile the CUDA performance and validate the divergence-free constraint by numerically computing the divergence of the output velocity field., using Python)\"], \"route_rationale\": \"The blue general-search agent will first gather state-of-the-art architectures for energy-conserving PINNs. The math agent will then formalize the specific loss function and constraints based on this research. Finally, the code agent will implement this mathematical specification, with a focus on performance and numerical validation using CUDA.\", \"expected_artifacts\": [\"A PDF document detailing the mathematical derivation of the scalar potential formulation and the final loss function.\", \"A Python script (`pinn_navier_stokes.py`) containing the JAX implementation of the PINN model and training loop.\", \"A set of convergence plots (`loss_convergence.png`, `energy_dissipation.png`) showing the training dynamics.\", \"A metrics file (`results.json`) containing the final PDE residual MSE and kinetic energy dissipation rate.\", \"A `references.md` file with citations for at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research existing literature on divergence-free neural network architectures for fluid dynamics simulations.\", \"2. Formulate the Navier-Stokes equations in terms of a scalar potential (stream function) to guarantee a divergence-free velocity field by construction.\", \"3. Derive the total loss function, combining the MSE of the momentum equation residuals and boundary conditions.\", \"4. Implement the PINN model, loss function, and training loop in JAX, leveraging `jax.grad` for all differential operators.\", \"5. Validate the implementation by numerically checking that the divergence of the learned velocity field is close to zero across the entire domain.\", \"6. Verify that the kinetic energy dissipation rate computed from the model output converges and matches theoretical expectations for the given Reynolds number.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code agent numerically validates divergence-free property derived by math agent; fallback: if validation fails, math agent re-evaluates the scalar potential formulation and its derivatives).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 recent arXiv papers on energy-conserving PINNs for incompressible flows.\", \"- [ ] /math: Derive the loss function for the Navier-Stokes equations using a stream-function formulation.\", \"- [ ] /math: Formulate the integral for total kinetic energy dissipation as a diagnostic metric.\", \"- [ ] /code: Implement the PINN architecture in JAX, defining the velocity field from the scalar potential's gradients.\", \"- [ ] /code: Write the training loop, using automatic differentiation to compute PDE residuals for the loss function.\", \"- [ ] /code: Validate that the implemented velocity field is numerically divergence-free to within machine precision.\", \"- [ ] /math: Verify that the code's implementation of the loss function's differential operators matches the mathematical derivation.\", \"- [ ] router QA: Review convergence plots, final PDE residual, and energy dissipation metric against acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"partial-differential-equations\", \"computational-fluid-dynamics\"], \"acceptance_criteria\": [\"- The final PDE residual MSE, evaluated on a validation set of collocation points, must be below 1e-5.\", \"- The numerically computed divergence of the velocity field, averaged over the domain, must be less than 1e-6.\", \"- The generated plots must clearly show the convergence of the loss and the stabilization of the kinetic energy dissipation rate.\", \"- The report must cite at least two relevant arXiv papers with valid IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 34, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working with a non-parametric structural causal model (SCM) with a binary treatment and continuous outcome. I need to derive the efficient influence function for the Average Causal Effect (ACE) under an intervention via the backdoor criterion. Can you derive the semiparametric efficiency bound, and then construct a simulation to validate that a doubly robust estimator, like AIPW, achieves this bound asymptotically?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:stat.columbia.edu semiparametric efficiency theory doubly robust estimator influence function causal inference\\\", mode=both)\", \"/math(Derive the efficient influence function for the Average Causal Effect estimand based on the causal graph and backdoor criterion, assuming Bernoulli treatment assignment and incorporating econometrics literature on nuisance function estimation.)\", \"/code(Implement a Monte Carlo simulation in Python with a known SCM to validate the theoretical asymptotic variance. Use a bootstrap method for confidence interval estimation and ensure the convergence tolerance is below 1e-4.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on semiparametric theory for causal inference. This informs the /math agent, which performs the core theoretical derivation of the influence function and efficiency bound. Finally, the /code agent implements a simulation to empirically validate the mathematical results, providing a robust check on the derivation's correctness.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the efficient influence function and the resulting asymptotic variance formula.\", \"A Python script implementing the data-generating SCM, the AIPW doubly robust estimator, and the Monte Carlo simulation loop.\", \"A summary report in Markdown comparing the derived theoretical variance with the simulated empirical variance, including convergence plots.\", \"A BibTeX file with citations for at least two relevant academic papers on semiparametric causal inference.\", \"A verification log confirming the mathematical formula for variance matches the one implemented in the Python code.\"], \"thinking_outline\": [\"1. Define the statistical model, the Structural Causal Model (SCM), and the target estimand (Average Causal Effect) using potential outcomes framework.\", \"2. Apply the backdoor criterion to identify the causal effect from observational data, yielding the g-formula.\", \"3. Derive the efficient influence function for the target estimand within the specified non-parametric model, and use its expected squared value to establish the semiparametric efficiency bound (asymptotic variance).\", \"4. Verify that the derived influence function is doubly robust by checking the orthogonality properties with respect to the nuisance models (propensity score and outcome regression).\", \"5. Implement a Monte Carlo simulation to compute the empirical variance of an Augmented Inverse Propensity Weighted (AIPW) estimator and validate that it converges to the derived theoretical bound as the sample size increases.\", \"6. Audit the simulation results by checking the width of bootstrap confidence intervals for the variance estimate, ensuring stability and sufficient simulation runs.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: router will cross-validate the final formula in the /math agent's derivation against the implementation in the /code agent's simulation script; fallback: if simulation variance diverges from theory by >5%, return to /math for re-derivation with revised nuisance model assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on semiparametric efficiency and doubly robust estimators (e.g., Robins, Rotnitzky, Zhao; Tsiatis).\", \"- [ ] /math: Define the SCM and derive the efficient influence function for the ACE under backdoor adjustment.\", \"- [ ] /math: From the influence function, derive the formula for the semiparametric efficiency bound (asymptotic variance).\", \"- [ ] /code: Implement the data-generating process from a known SCM and the AIPW estimator.\", \"- [ ] /code: Execute a Monte Carlo simulation to compute the empirical variance and its bootstrap confidence interval.\", \"- [ ] /math: Verify that the Python implementation of the variance calculation in the simulation aligns with the derived mathematical formula.\", \"- [ ] /code: Validate that the simulation's empirical variance converges to the theoretical bound as N increases.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"semiparametric-statistics\", \"econometrics\"], \"acceptance_criteria\": [\"- The derived formula for the asymptotic variance must be mathematically sound and explicitly reference the efficient influence function.\", \"- The final `relative_variance_error` metric must be less than 0.05, indicating close agreement between theory and simulation.\", \"- The final report must include at least 2 citations to peer-reviewed articles or arXiv preprints on causal inference or semiparametric theory.\", \"- The provided code must be runnable and reproduce the reported simulation results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 35, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the spectral properties of the Hessian of the loss function for a deep neural network. Specifically, derive the analytical form of the Hessian matrix for a feedforward network with ReLU activations, and then implement a power iteration method in Python using JAX to find its dominant eigenvalue. Investigate the relationship between this dominant eigenvalue and the maximum stable learning rate, referencing seminal papers on the topic. The analysis should validate the theoretical convergence properties.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org hessian eigenspectrum deep learning ReLU 'power iteration' convergence theory\\\", mode=web)\", \"/math(Derive the Hessian matrix for a multi-layer perceptron with ReLU activations, focusing on the distributional nature of the second derivatives and the expected eigenvalue properties., includes cybersecurity)\", \"/code(Implement the power iteration algorithm in JAX to find the dominant eigenvalue of the Hessian for a sample network, validating convergence and ensuring CUDA compatibility., using Python)\"], \"route_rationale\": \"The blue /general-search agent is used first to gather foundational literature on Hessian spectral analysis in NNs. /math is then used to perform the formal matrix calculus derivation based on the literature. Finally, /code implements the derived numerical method (power iteration) to empirically validate the theoretical findings.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the Hessian for a ReLU MLP.\", \"A Python script using JAX to implement the power iteration method for Hessian dominant eigenvalue estimation.\", \"A summary report citing at least two relevant arXiv papers connecting the Hessian's dominant eigenvalue to learning rate stability.\", \"A log file containing the output of the JAX code, showing eigenvalue convergence over iterations.\"], \"thinking_outline\": [\"1. Survey existing literature to understand the theoretical basis for analyzing the Hessian eigenspectrum in deep learning and its implications for optimization.\", \"2. Formally derive the Hessian matrix for a feedforward neural network with ReLU activation functions, paying close attention to the non-differentiability at zero.\", \"3. Implement the power iteration algorithm, a computationally efficient method for finding the largest eigenvalue, using JAX for automatic differentiation and Hessian-vector products.\", \"4. Validate the implementation by comparing the computed eigenvalue against theoretical bounds or known results for simpler models.\", \"5. Analyze and document the relationship between the computed dominant eigenvalue and the practical choice of a stable learning rate for stochastic gradient descent.\", \"6. Check the final report to ensure the mathematical derivation, code implementation, and empirical analysis are consistent and well-cited.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers (e.g., arXiv IDs) to the math agent for the derivation. The math agent's final derivation is the specification for the code agent's implementation. The code agent's output (eigenvalue estimate) is then used to verify the theoretical claims from the math/search phase. Verification: router will cross-reference the derivation in the LaTeX doc with the logic in the JAX code. Fallback: If derivation is flawed, return to /math with corrections. If implementation fails convergence checks, return to /code with debugging instructions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Hessian spectral analysis in NNs and learning rate tuning.\", \"- [ ] /math: Derive the Hessian for a 2-layer ReLU MLP, clearly stating all assumptions.\", \"- [ ] /code: Implement power iteration using JAX's Hessian-vector product functionality.\", \"- [ ] /code: Add logging to track the convergence of the eigenvalue estimate across iterations.\", \"- [ ] /math: Verify that the JAX implementation correctly reflects the derived mathematical form of the Hessian-vector product.\", \"- [ ] /code: Run the script on a test network and generate a plot of the results.\", \"- [ ] router: QA the final report, checking for consistency between derivation, code, and cited literature.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"matrix-calculus\", \"spectral-analysis\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Hessian must be self-contained and correct.\", \"- The JAX code must execute without errors and demonstrate convergence of the power iteration method.\", \"- The final report must correctly cite at least two relevant papers and connect the dominant eigenvalue to the concept of the maximum stable learning rate (e.g., L <= 2/\u03bb_max).\", \"- The primary metric (Dominant Eigenvalue \u03bb_max) must be reported with at least 4 significant figures.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Computed as the final estimate from the power iteration algorithm after a fixed number of iterations or until convergence tolerance is met.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measured as the relative change between eigenvalue estimates in successive iterations, e.g., |\u03bb_k - \u03bb_{k-1}| / |\u03bb_k|.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 36, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the relationship between the Hessian eigenvalue spectrum and generalization in deep neural networks. Please derive the connection between the sharpness of the loss landscape, measured by the largest eigenvalue (\u03bb_max) of the Hessian, and generalization error bounds. Then, implement a procedure in PyTorch to compute an approximation of the Hessian spectral density for a pre-trained ResNet-18 on CIFAR-10 using the Lanczos algorithm. Finally, analyze how different optimizers (e.g., SGD vs. Adam) and weight decay affect the resulting spectral distribution and correlate this with validation accuracy.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian eigenvalue spectrum generalization bound PAC-Bayes lanczos algorithm deep learning\\\", mode=web)\", \"/math(Derive a generalization error bound for neural networks that explicitly depends on the spectral norm of the Hessian, referencing the foundational PAC-Bayes framework and its connection to model complexity.)\", \"/code(Implement the Lanczos algorithm in PyTorch to approximate the Hessian eigenvalue distribution for a ResNet-18, tracking convergence and ensuring numerical stability for large-scale biomedical image analysis., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather authoritative arXiv papers on Hessian spectral analysis and generalization theory. This provides the theoretical foundation for the /math agent to construct a formal proof. Finally, the /code agent implements the numerical approximation described in the literature and specified by the user, grounding the theory in a practical experiment.\", \"expected_artifacts\": [\"A LaTeX-formatted mathematical derivation of the generalization bound.\", \"A Python script implementing the Lanczos algorithm for Hessian spectral density approximation in PyTorch.\", \"A JSON file containing the computed eigenvalues and corresponding validation accuracies for models trained with SGD and Adam.\", \"A PDF report summarizing the findings, including plots of the spectral densities and a discussion of the results, citing at least two relevant research papers.\", \"A verification log confirming the alignment between the mathematical derivation and the code's algorithmic assumptions.\"], \"thinking_outline\": [\"1. Survey foundational literature connecting Hessian spectral properties to generalization bounds, focusing on PAC-Bayes analysis and sharpness measures.\", \"2. Formulate the mathematical derivation linking the dominant eigenvalue or trace of the Hessian/FIM to a formal generalization error bound.\", \"3. Implement a numerically stable version of the Lanczos algorithm to approximate the Hessian's spectral density for a given network and dataset.\", \"4. Verify the mathematical derivation against established results found in the literature search to ensure correctness and appropriate assumptions.\", \"5. Execute the implementation on a standard benchmark (e.g., ResNet-18 on CIFAR-10), comparing spectral densities from models trained with different optimizers and regularization strengths.\", \"6. Validate the codebase by comparing the computed top eigenvalue against a simple power iteration method and checking for convergence of the Lanczos process.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers (e.g., on PAC-Bayes bounds) to the math agent. The math agent provides a formal derivation that guides the code agent's implementation of the Lanczos algorithm. The code agent produces plots and metrics. Router QA performs a final review. Verification: The code's output (spectral plots) must qualitatively match the theoretical predictions from the math agent and the literature. Fallback: If the Lanczos implementation is unstable, the code agent will revert to a simpler power iteration method to estimate only the top eigenvalue (\u03bb_max) and notify the router.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on Hessian spectrum and generalization.\", \"- [ ] /math: Produce a formal derivation of a generalization bound dependent on the Hessian's spectral norm.\", \"- [ ] /code: Implement the Lanczos algorithm in PyTorch for a pre-trained ResNet-18.\", \"- [ ] /code: Run experiments comparing SGD and Adam optimizers, logging the resulting spectral data.\", \"- [ ] /math: Verify that the assumptions made in the derivation are consistent with the experimental setup in the code.\", \"- [ ] /code: Validate the Lanczos implementation's output for the top eigenvalue against a baseline power iteration method.\", \"- [ ] /router: Perform final QA, ensuring the report's conclusions are supported by both the derivation and the experimental results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The final report must include a clear mathematical derivation relating the Hessian's spectral norm to a generalization bound.\", \"- The submitted code must successfully execute and generate spectral density plots for ResNet-18 models trained with both SGD and Adam.\", \"- The analysis must correctly identify that the optimizer promoting flatter minima (smaller Hessian eigenvalues) corresponds to better generalization, as measured by validation accuracy.\", \"- The report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 38, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the coordinate-ascent variational inference (CAVI) update rules for the latent variables in a Bayesian Latent Dirichlet Allocation (LDA) model. Subsequently, implement this in Python using NumPy, validating convergence by tracking the Evidence Lower Bound (ELBO) on a synthetic corpus. Analyze the theoretical gap between the true posterior and the mean-field approximation, referencing foundational literature on the properties of Kullback-Leibler divergence in this context.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:jmlr.org site:arxiv.org 'Latent Dirichlet Allocation' Blei 'Variational Inference' ELBO convergence\\\", mode=both)\", \"/math(Derive the complete coordinate-ascent update equations for the variational parameters gamma and phi in a Bayesian LDA model, formally defining the Evidence Lower Bound (ELBO) using Jensen's inequality to establish the convergence objective.)\", \"/code(Implement the derived CAVI for LDA in Python with NumPy, tracking the ELBO to verify monotonic convergence on a synthetic corpus. The implementation must include perplexity calculation and clear links to the Bayesian theory.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational papers on LDA and Variational Inference, ensuring a solid theoretical basis. Next, the /math agent performs the core theoretical work by deriving the update equations and the ELBO. Finally, the /code agent implements the derived algorithm, providing empirical validation and performance metrics, directly connecting the theory to a practical result.\", \"expected_artifacts\": [\"* A PDF document with the complete mathematical derivation of the CAVI update equations and the ELBO for LDA.\", \"* A Python script (`lda_cavi.py`) implementing the algorithm and generating convergence plots for the ELBO.\", \"* A `references.json` file citing at least two foundational papers on the topic with their arXiv IDs or DOIs.\", \"* A verification report (`verification_log.txt`) confirming the monotonic non-decreasing behavior of the ELBO during execution.\"], \"thinking_outline\": [\"1. Research and consolidate the probabilistic graphical model for Latent Dirichlet Allocation and the core principles of mean-field variational inference from authoritative sources.\", \"2. Formally derive the CAVI update equations for the per-document topic proportions (gamma) and per-word topic assignments (phi) by taking derivatives of the ELBO with respect to each variational parameter.\", \"3. Implement the derived update equations and the ELBO computation in Python, ensuring the code structure directly maps to the mathematical formulation for auditability.\", \"4. Generate a synthetic document corpus with known ground-truth parameters to serve as a controlled environment for testing the implementation.\", \"5. Execute the implementation on the synthetic data, plotting the ELBO over iterations to validate its monotonic non-decreasing property, which is a necessary condition for correct convergence.\", \"6. Verify that the final topic distributions recovered by the model qualitatively align with the ground-truth distributions used to generate the synthetic data.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses this literature to produce a formal derivation -> /code implements the derivation and runs validation tests. The final report, code, and plots are handed off to router QA. Verification: The code's ELBO plot must show a monotonic, non-decreasing curve. Fallback: If the ELBO decreases, the /math derivation must be audited for sign errors or incorrect expectation calculations, triggering a re-derivation and code patch.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on LDA and Variational Bayes (e.g., Blei et al., 2003).\", \"- [ ] /math: Derive the CAVI update for the gamma parameter (Dirichlet posterior).\", \"- [ ] /math: Derive the CAVI update for the phi parameter (Multinomial posterior).\", \"- [ ] /math: Formulate the full ELBO objective function for the LDA model.\", \"- [ ] /code: Implement the CAVI algorithm, including the main loop and update functions.\", \"- [ ] /code: Implement the ELBO calculation and a synthetic data generator.\", \"- [ ] /math: Verify that the implemented ELBO calculation in the code matches the formal derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-statistics\", \"graphical-models\"], \"acceptance_criteria\": [\"* The submitted PDF contains a complete and mathematically correct derivation of the CAVI updates and the ELBO.\", \"* The Python script executes without errors and produces a plot showing a monotonically non-decreasing ELBO value over iterations.\", \"* The final analysis correctly references at least two relevant academic papers and discusses the implications of the mean-field approximation.\", \"* The calculated perplexity on a held-out portion of the synthetic data is reported and is a finite, reasonable value.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 39, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the training dynamics of Generative Adversarial Networks and need to analyze the local geometry around saddle points. Please derive the update rule for a Hessian-based saddle-point escape algorithm, like a regularized Newton method, for a canonical GAN objective. Then, implement this algorithm and demonstrate its ability to escape a sharp saddle by tracking the minimum eigenvalue of the generator's loss Hessian on a synthetic dataset, ensuring convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org GAN training second-order optimization saddle point escape Hessian eigenvalue analysis\\\", mode=rag)\", \"/math(Derive the gradient and Hessian for a simple GAN objective. Formalize a regularized Newton-Raphson update rule that handles indefinite Hessians and analyze its local convergence properties near a critical point with a negative eigenvalue.)\", \"/code(Implement the derived algorithm in PyTorch for a toy GAN on a 2D Gaussian mixture. Track the minimum Hessian eigenvalue and gradient norm during training to validate the adversarial escape dynamics and ensure convergence., using Python)\"], \"route_rationale\": \"The task requires blending theory with implementation. The 'blue' /general-search agent first gathers state-of-the-art techniques for saddle-point analysis in GANs. /math then provides the formal, rigorous derivation of the chosen algorithm's update rule and convergence theory. Finally, /code implements this theory, providing empirical validation and visualization of the escape dynamics.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the regularized Newton method for the GAN objective.\", \"A Python script implementing the GAN and the custom optimizer, instrumented to log metrics.\", \"A Jupyter notebook visualizing the optimization trajectory and plotting the minimum Hessian eigenvalue and gradient norm over iterations.\", \"A summary report citing at least two relevant arXiv papers discussing saddle-point escape methods.\", \"A JSON file containing the final convergence metrics and eigenvalue statistics.\"], \"thinking_outline\": [\"1. Research modern second-order and saddle-point escape algorithms used in deep learning, particularly for adversarial training, to select a promising candidate algorithm.\", \"2. Formulate a simple yet representative GAN objective function (e.g., minimax on a simple data distribution) and derive its gradient and Hessian with respect to the generator's parameters.\", \"3. Derive the specific update rule for the chosen algorithm, including the regularization term necessary to handle the negative curvature at saddle points.\", \"4. Verify the mathematical derivation of the Hessian by comparing the analytical form to a numerical approximation obtained using automatic differentiation in a framework like PyTorch.\", \"5. Implement the algorithm and a simple GAN, then run experiments to demonstrate that the optimizer successfully escapes a saddle point, unlike standard gradient descent.\", \"6. Validate the results by plotting the minimum eigenvalue of the Hessian, confirming it transitions from negative to non-negative as the optimizer escapes the saddle region.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides candidate algorithms and theoretical grounding to the math agent. The math agent produces a formal derivation and update rule, which is the precise specification for the code agent. The code agent implements this specification and produces empirical results (plots, logs). Router QA performs final verification by checking if the code's empirical eigenvalue behavior matches the theory from the math agent. Fallback: If code fails to converge, the task is returned to /math to re-audit the derivation for potential instability.\", \"todo_list\": [\"- [ ] /general-search: Identify at least two prominent papers on saddle-point escape algorithms for GANs or general non-convex problems.\", \"- [ ] /math: Derive the Hessian matrix of the generator's loss function for a standard GAN objective.\", \"- [ ] /math: Formulate the update rule for a damped or trust-region Newton method suitable for non-convex optimization.\", \"- [ ] /code: Implement the derived optimizer and a simple GAN architecture in PyTorch.\", \"- [ ] /code: Instrument the training loop to compute and log the minimum eigenvalue of the Hessian at each step using `torch.linalg.eigvalsh`.\", \"- [ ] /math: Verify that the numerical Hessian computed in the code aligns with the analytical derivation.\", \"- [ ] /code: Generate plots showing the minimum eigenvalue and gradient norm trajectories over training iterations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-finding\", \"generative-adversarial-networks\"], \"acceptance_criteria\": [\"- The provided Python code must successfully train a GAN on a synthetic dataset, demonstrably escaping a saddle point.\", \"- The final report must include a plot showing the minimum Hessian eigenvalue transitioning from a negative to a non-negative value during training.\", \"- The mathematical derivation must be complete, correct, and directly correspond to the algorithm implemented in the code.\", \"- The solution must cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 41, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the strong and weak forms of the incompressible Navier-Stokes equations for fluid dynamics. Then, implement a Physics-Informed Neural Network (PINN) in JAX to solve the 2D lid-driven cavity flow problem, focusing on how automatic differentiation enforces the PDE residual. Validate the model's convergence against established benchmarks and analyze the spectral bias by examining the Fourier features of the solution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org physics-informed neural networks Navier-Stokes convergence spectral bias Fourier features\\\", mode=web)\", \"/math(Derive the full PDE residual loss for the 2D incompressible Navier-Stokes equations, including continuity and momentum components. Analyze the eigenvalue spectrum of the linearized operator and its implications for convergence guarantees on the solution's Fourier series representation.)\", \"/code(Implement the PINN in JAX for the lid-driven cavity flow, using automatic differentiation to compute the PDE residual from the math derivation. Track convergence metrics and generate plots comparing velocity fields against a known benchmark solution., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on PINNs for fluid dynamics. The /math tool is then used to perform the rigorous derivation of the governing equations and loss function, which is a prerequisite for implementation. Finally, the /code tool implements the derived model and performs the numerical simulation and validation.\", \"expected_artifacts\": [\"A LaTeX document detailing the derivation of the Navier-Stokes PDE residual loss.\", \"A Python script (JAX) implementing the PINN for the lid-driven cavity problem.\", \"A JSON file containing the final PDE residual loss, L2 error metric, and training history.\", \"Plots comparing the PINN's velocity field (u, v) and pressure (p) to a benchmark solution.\", \"A summary report with citations of at least 2 relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research current PINN architectures and training strategies for solving the Navier-Stokes equations, specifically focusing on handling boundary conditions and mitigating spectral bias.\", \"2. Formulate the mathematical problem by deriving the strong form of the 2D incompressible Navier-Stokes equations and constructing the composite loss function, which includes the PDE residual for momentum and continuity, plus boundary condition terms.\", \"3. Implement the neural network architecture and the physics-informed loss function in Python using JAX for efficient automatic differentiation.\", \"4. Train the PINN model on a collocation grid for the lid-driven cavity problem and monitor the convergence of the loss components.\", \"5. Validate the trained model by computing the L2 relative error of the velocity fields against a well-established benchmark result (e.g., Ghia et al., 1982).\", \"6. Check the implementation by verifying that the gradients computed via automatic differentiation in the code align with the manually derived Jacobians from the mathematical formulation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: search results provide a basis for the mathematical derivation; math derivation of the PDE loss is correctly implemented in the code; fallback: if derivation is flawed, /math re-evaluates based on literature; if code fails to converge, /code adjusts hyperparameters or network architecture based on /math's stability analysis).\", \"todo_list\": [\"- [ ] /general-search: Find 2-3 key papers on PINNs for Navier-Stokes, focusing on convergence and implementation details.\", \"- [ ] /math: Derive the complete loss function for the 2D lid-driven cavity problem, clearly defining all terms.\", \"- [ ] /code: Set up the JAX environment and implement the MLP architecture for the PINN.\", \"- [ ] /code: Implement the physics-informed loss function based on the mathematical derivation.\", \"- [ ] /math: Verify that the gradients in the JAX implementation correspond to the derived analytical gradients.\", \"- [ ] /code: Run the training loop, save the model, and generate validation plots and metrics.\", \"- [ ] /code: Verify that the final L2 error is below the target threshold defined in the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"navier-stokes\"], \"acceptance_criteria\": [\"- The LaTeX derivation of the Navier-Stokes loss function must be complete and mathematically sound.\", \"- The implemented PINN must achieve a final mean squared PDE residual below 1e-5.\", \"- The L2 relative error of the predicted velocity field against the Ghia et al. benchmark must be less than 5%.\", \"- The final report must include at least two citations to relevant arXiv preprints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 42, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am working with a semi-Markovian structural causal model (SCM) that includes unobserved confounders. I need to derive the expression for the interventional distribution P(Y | do(X)=x) using do-calculus, but standard rules seem insufficient. Can you help me find the latest identifiability results from recent literature, provide a formal proof for the adjustment formula under these specific graphical conditions, and then implement a corresponding estimation algorithm in Python to verify its correctness on synthetic data?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:jmlr.org causal discovery identifiability semi-markovian models unobserved confounders do-calculus extensions\\\", mode=rag)\", \"/math(Derive the expression for the interventional distribution P(Y|do(X)) based on the causal graph structure and theorems from the sourced arXiv papers. Provide a formal, step-by-step proof of identifiability using the rules of do-calculus and Bayesian probability theory.)\", \"/code(Implement a Python algorithm using numpy and networkx to estimate the Average Causal Effect based on the derived mathematical formula. Validate the implementation against a synthetic SCM, ensuring convergence and tracking estimation error metrics.)\"], \"route_rationale\": \"The task requires cutting-edge knowledge, so the 'blue' /general-search agent is used first to find the latest theorems on causal identifiability from authoritative sources like arXiv. The /math agent is then tasked with the formal derivation and proof, which is a prerequisite for implementation. Finally, the /code agent implements the derived formula and provides empirical validation, which is a critical verification step.\", \"expected_artifacts\": [\"A LaTeX document detailing the formal derivation and proof of the adjustment formula.\", \"A commented Python script implementing the causal effect estimation algorithm.\", \"A JSON file containing the estimated causal effect and the ground truth from the synthetic data for verification.\", \"A bibliography file with citations to at least two relevant arXiv preprints.\", \"A verification report comparing the mathematical derivation with the code's output.\"], \"thinking_outline\": [\"1. Systematically review the literature on causal identifiability, focusing on extensions to do-calculus for semi-Markovian models with latent variables.\", \"2. Identify the appropriate adjustment set or mediating pathway using the graphical criteria established in the sourced papers.\", \"3. Construct a formal proof for the interventional distribution P(Y | do(X)=x) by applying the relevant rules of do-calculus sequentially.\", \"4. Verify each step of the mathematical derivation against the axiomatic foundations of do-calculus and the assumptions of the cited theorems.\", \"5. Implement the derived formula as a Python function and generate a synthetic dataset from a known SCM that matches the problem's graphical structure.\", \"6. Validate the code's output by comparing the estimated Average Causal Effect (ACE) against the true ACE computed directly from the synthetic SCM, ensuring the error is within a small epsilon.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal proof) -> /code (implementation & validation) -> router QA (verification: code output matches theoretical derivation within tolerance; fallback: if discrepancy, return to /math to re-check proof assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent (last 5 years) papers on causal identifiability in semi-Markovian models.\", \"- [ ] /math: Formulate the problem statement with formal definitions for the SCM, graph, and interventional distribution.\", \"- [ ] /math: Provide a complete, step-by-step derivation of the adjustment formula for P(Y | do(X)).\", \"- [ ] /code: Create a synthetic data-generating process based on a non-trivial SCM with confounding.\", \"- [ ] /code: Implement the estimation algorithm derived by the /math agent.\", \"- [ ] /math: Verify that the code's implementation logic correctly reflects the derived mathematical expression.\", \"- [ ] /code: Validate the algorithm's output against the ground-truth causal effect from the synthetic data and report metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-discovery\", \"structural-causal-models\", \"do-calculus\"], \"acceptance_criteria\": [\"- The provided mathematical proof must be rigorous, with each step justified by a rule from do-calculus or a cited theorem.\", \"- The Python implementation must correctly estimate the average causal effect on a synthetic dataset, with an MSE below 0.01 relative to the true effect.\", \"- The final report must cite at least two relevant papers from arXiv or a top-tier conference (e.g., UAI, NeurIPS) with full identifiers.\", \"- The verification report must explicitly confirm the consistency between the theoretical derivation and the empirical results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 45, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the stability of persistent homology for manifold learning. Specifically, derive the stability theorem for persistence diagrams under Gromov-Hausdorff perturbations and implement a Vietoris-Rips filtration to compute Betti numbers for a noisy point cloud sampled from a Clifford torus. Compare the topological features extracted via persistence to those from the Mapper algorithm, focusing on convergence properties and providing a CUDA-accelerated implementation for performance analysis.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org persistent homology stability theorem Gromov-Hausdorff mapper algorithm bioinformatics\\\", mode=both)\", \"/math(Derive the stability proof for persistence diagrams with respect to bottleneck distance under point cloud perturbations, and formalize the construction of the Mapper algorithm, including filter function and cover convergence criteria.)\", \"/code(Implement a CUDA-accelerated Vietoris-Rips filtration in Python using ripser.py or giotto-tda to compute persistence diagrams for a noisy Clifford torus. Validate Betti numbers and measure bottleneck distance to assess bootstrap stability.)\"], \"route_rationale\": \"/general-search (blue agent) will curate foundational papers on TDA stability and the Mapper algorithm. /math will then formalize the theoretical underpinnings. Finally, /code will implement and empirically validate the theory, comparing the two methods on a concrete manifold learning problem.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the persistence stability theorem.\", \"- A documented Python script implementing Vietoris-Rips filtration and the Mapper algorithm.\", \"- A report summarizing the comparison of the two methods, including plots of persistence diagrams and Mapper graphs.\", \"- A JSON file containing computed metrics (bottleneck distance, runtime).\", \"- Citations for at least two foundational papers on TDA stability from arXiv.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of persistent homology stability and the Mapper algorithm using authoritative sources.\", \"2. Formulate a rigorous mathematical proof of the stability theorem, clearly defining all terms like Gromov-Hausdorff distance and bottleneck distance.\", \"3. Implement the algorithms in Python, starting with generating a point cloud from a known manifold (Clifford torus) and adding controlled noise.\", \"4. Verify the implementation by testing on simple cases (e.g., a circle) to ensure the correct Betti numbers are recovered.\", \"5. Execute the comparative analysis, compute the specified metrics, and validate that the empirical results align with the stability theorem's predictions.\", \"6. Synthesize all findings into a comprehensive report with derivations, code, results, and citations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results provide foundational papers for the /math agent's derivation. The formal derivation from /math provides the exact specification for the /code agent. Verification: The code's output on simple manifolds must match known topological invariants. Fallback: If the derivation is ambiguous, /math will request clarification from /general-search with a more specific query.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on TDA stability (Cohen-Steiner, Edelsbrunner, Harer) and the Mapper algorithm (Singh, Memoli, Carlsson).\", \"- [ ] /math: Formally write out the proof for the stability of persistence diagrams under the bottleneck distance.\", \"- [ ] /math: Define the Mapper algorithm construction (filter function, cover, nerve of the pullback).\", \"- [ ] /code: Implement data generation for a noisy Clifford torus point cloud.\", \"- [ ] /code: Implement Vietoris-Rips filtration to compute persistence diagrams and the Mapper algorithm.\", \"- [ ] /math: Verify that the implemented bottleneck distance calculation in the code aligns with its mathematical definition.\", \"- [ ] /code: Validate the implementation by checking for correct Betti numbers on a simple annulus dataset.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"computational-geometry\"], \"acceptance_criteria\": [\"- The provided mathematical proof of the stability theorem is complete and logically sound.\", \"- The Python code successfully runs and generates persistence diagrams and Mapper graphs for the specified manifold.\", \"- The final report correctly interprets the bottleneck distance as a measure of stability and compares it to the Mapper output.\", \"- At least two relevant arXiv papers are cited correctly in the report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 46, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the connection between the Hessian eigenspectrum and generalization in deep neural networks. Please derive the implicit spectral regularization effect of Sharpness-Aware Minimization (SAM). Specifically, show how the SAM objective's gradient ascent step influences the Hessian's dominant eigenvalue. Then, create a PyTorch simulation to empirically validate this by tracking the Hessian's spectral norm and the generalization gap for a simple CNN trained on CIFAR-10 with both SGD and SAM, demonstrating convergence to a flatter minimum.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Sharpness-Aware Minimization Hessian eigenvalue generalization gap analysis\\\", mode=both)\", \"/math(Derive the update rule for SAM and its first-order effect on the Hessian's spectral norm, framing the problem as a minimax optimization and discussing convergence criteria using concepts from control theory.)\", \"/code(Implement a PyTorch simulation comparing SGD and SAM on CIFAR-10. Use a power iteration or Lanczos method to estimate the top Hessian eigenvalue during training, track the generalization gap, and ensure convergence via bootstrap validation., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational papers on SAM and Hessian analysis. This literature informs the /math agent's formal derivation of the spectral regularization effect. Finally, the /code agent implements the derived concepts to provide empirical validation, creating a closed loop of theory and practice.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF containing the full mathematical derivation of SAM's effect on the Hessian spectrum.\", \"- A Python script (`main.py`) and a Jupyter Notebook (`analysis.ipynb`) for the simulation and visualization.\", \"- A `results.json` file containing the final training/test loss, accuracy, generalization gap, and maximum Hessian eigenvalue for both SGD and SAM.\", \"- A `report.md` summarizing the findings, including plots of the Hessian eigenvalue evolution and generalization gap over training epochs, with citations to at least 2 relevant arXiv papers.\", \"- Logs from the eigenvalue estimation routine validating its convergence.\"], \"thinking_outline\": [\"1. Research foundational literature on Sharpness-Aware Minimization (SAM) and the link between Hessian eigenvalues (flatness) and the generalization gap.\", \"2. Formally derive the gradient update for the SAM objective using matrix calculus, treating it as a minimax problem.\", \"3. Analyze the derived update to prove its regularizing effect on the Hessian's spectral norm, connecting it to the geometry of the loss landscape.\", \"4. Implement the training loop and Hessian eigenvalue estimation; validate the eigenvalue estimator's convergence and stability on a known matrix (e.g., a random symmetric matrix).\", \"5. Run the comparative simulation (SGD vs. SAM) and collect metrics on loss, accuracy, and the dominant eigenvalue at regular intervals.\", \"6. Verify that the empirical results from the simulation quantitatively align with the theoretical predictions from the derivation, explaining any discrepancies in the final report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: does the code's eigenvalue plot quantitatively support the math agent's claim about spectral regularization?; fallback: if results diverge, instruct /math to re-check derivation assumptions or /code to debug the eigenvalue estimation algorithm or hyperparameters).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on SAM and Hessian spectrum analysis.\", \"- [ ] /math: Produce a formal derivation of the SAM update and its impact on the Hessian's top eigenvalue.\", \"- [ ] /code: Implement the CIFAR-10 training script for both SGD and SAM optimizers.\", \"- [ ] /code: Implement and test a numerically stable Lanczos or power iteration method for Hessian eigenvalue estimation.\", \"- [ ] /math: Verify that the implemented algorithm in the code correctly reflects the mathematical derivation.\", \"- [ ] /code: Run the full simulation, generate all plots, and save the final metrics to `results.json`.\", \"- [ ] /code: Verify that the simulation's empirical results support the theoretical claims.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"hessian-spectrum\", \"generalization-gap\", \"matrix-calculus\"], \"acceptance_criteria\": [\"- The mathematical derivation must clearly show, step-by-step, how the SAM update implicitly penalizes large Hessian eigenvalues.\", \"- The final simulation results must show a statistically significant lower maximum Hessian eigenvalue for the SAM-trained model compared to the SGD-trained model.\", \"- The SAM-trained model must exhibit a smaller final generalization gap (test loss - train loss) than the SGD model.\", \"- The report must cite at least two relevant arXiv pre-prints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 49, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update steps for a consensus Alternating Direction Method of Multipliers (ADMM) algorithm tailored for a federated Lasso problem. Specifically, formulate the global problem with L1 regularization, decompose it for N agents, derive the x, z, and u updates, and then implement this in Python to validate its convergence behavior against a synthetic dataset. The analysis should focus on the impact of the penalty parameter rho on convergence speed.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:stanford.edu/~boyd site:arxiv.org consensus ADMM federated learning lasso formulation\\\", mode=rag)\", \"/math(Derive the specific x, z, and dual variable update steps for the federated Lasso problem using the consensus ADMM framework, ensuring the Lagrangian is correctly formulated and the proximal operator for L1 norm is correctly applied for guaranteed convergence.)\", \"/code(Implement the derived consensus ADMM updates in Python using NumPy, simulate a federated learning scenario with a synthetic sparse dataset, and plot the objective function value and primal/dual residuals to verify convergence.)\"], \"route_rationale\": \"The blue /general-search agent first grounds the task in established literature from authoritative sources. Next, /math performs the formal derivation based on these sources. Finally, /code translates the mathematical derivation into a concrete Python implementation for empirical validation and analysis of convergence.\", \"expected_artifacts\": [\"- A complete LaTeX derivation of the consensus ADMM updates for federated Lasso.\", \"- A commented Python script implementing the algorithm and generating convergence plots.\", \"- A summary report analyzing the convergence behavior and the effect of the rho parameter.\", \"- A bibliography with at least two cited arXiv papers on ADMM.\"], \"thinking_outline\": [\"1. Formulate the global Lasso problem and its equivalent consensus optimization form suitable for a federated setting.\", \"2. Construct the augmented Lagrangian for the consensus problem and derive the iterative update rules for the primal variables (x_i), the consensus variable (z), and the dual variables (u_i).\", \"3. Verify the correctness of the derived update steps, particularly the application of the soft-thresholding operator for the L1 norm's proximal operator.\", \"4. Implement the derived ADMM algorithm in Python, creating a synthetic dataset that aligns with the assumptions of the Lasso model (sparse ground truth).\", \"5. Validate the implementation by checking if the primal and dual residuals converge to zero, which is a necessary condition for optimality in ADMM.\", \"6. Analyze and plot the convergence rate of the objective function for different values of the penalty parameter rho.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Math derivation is audited against the code's update loop logic and convergence plots must show decreasing residuals; fallback: If residuals diverge, /code agent flags the issue and hands back to /math to re-check the derivation for errors.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on consensus ADMM for statistical learning problems.\", \"- [ ] /math: Formulate the federated Lasso problem and derive the complete ADMM update equations.\", \"- [ ] /math: Write out the explicit proximal operator for the L1-regularized subproblem.\", \"- [ ] /code: Implement the ADMM algorithm in Python, including functions for data simulation.\", \"- [ ] /code: Generate plots for the objective value, primal residual, and dual residual against iteration number.\", \"- [ ] /math: Verify that the implemented code logic in the update loop exactly matches the derived equations.\", \"- [ ] /code: Validate that the numerical results (residual convergence) align with ADMM theory.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"convex-optimization\", \"admm\", \"federated-learning\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ADMM updates is complete and rigorously correct.\", \"- The Python implementation successfully runs and demonstrates convergence on a synthetic dataset, with primal and dual residuals approaching zero.\", \"- The final report includes plots and analysis, and correctly cites at least two relevant academic papers with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 50, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the loss landscape of a particular non-convex function, f(x, y) = (x^2 - 1)^2 + (y - x^2)^2, known as the Rosenbrock function, generalized to higher dimensions. I need a formal analysis of its second-order stationary points. Specifically, derive the Hessian matrix, find its eigenvalues to characterize the saddle points versus local minima, and then computationally verify these findings by simulating the escape dynamics using a second-order optimization method like Newton's method with trust regions. The goal is to understand the geometry of the manifold surrounding these saddle points.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org saddle point escape dynamics trust region Newton's method non-convex optimization\\\", mode=both)\", \"/math(Derive the symbolic Hessian matrix for the generalized Rosenbrock function. Analytically solve for its eigenvalue spectrum to characterize the geometry of all second-order stationary points, proving conditions for convergence.)\", \"/code(Implement a trust-region Newton method in Python with JAX/PyTorch to locate the stationary points. Numerically compute the Hessian at these points and validate its eigenvalue spectrum against the analytical derivation, ensuring CUDA compatibility and analyzing time complexity.)\"], \"route_rationale\": \"/general-search (blue agent) will gather state-of-the-art literature on saddle-point analysis and escape algorithms. /math will perform the formal theoretical derivation of the Hessian's spectral properties. /code will then provide numerical implementation and empirical validation of the theoretical findings.\", \"expected_artifacts\": [\"- A formal proof in LaTeX detailing the Hessian derivation and eigenvalue analysis for the generalized Rosenbrock function's saddle points.\", \"- A Python script implementing the trust-region Newton method to find and classify stationary points.\", \"- A set of plots visualizing the function landscape, the identified saddle points, and simulated escape trajectories.\", \"- A JSON file containing the computed eigenvalues and escape times for each identified saddle point.\", \"- A summary report citing at least two relevant academic papers on non-convex optimization.\"], \"thinking_outline\": [\"1. Formulate the generalized Rosenbrock function and derive its gradient and Hessian matrix symbolically.\", \"2. Solve for the stationary points where the gradient is zero and analytically compute the Hessian's eigenvalues at these points to classify them as local minima, maxima, or saddle points.\", \"3. Implement a numerical optimization algorithm (e.g., trust-region Newton) capable of converging to second-order stationary points.\", \"4. Verify that the numerically located stationary points and their computed Hessian eigenvalues match the analytical results from step 2.\", \"5. Validate the escape dynamics by initializing the optimizer near a saddle point with a small perturbation and plotting the trajectory.\", \"6. Synthesize all findings into a cohesive report, including proofs, code, visualizations, and citations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The blue agent's search results provide theoretical context for the /math agent's formal derivations. The /math agent's symbolic results (Hessian, eigenvalues) serve as the ground truth for the /code agent's numerical implementation. The /code agent's output is then passed to router QA for final review. Verification: The numerical eigenvalues from /code must match the symbolic ones from /math within a small tolerance. Fallback: If a mismatch occurs, the /math agent will re-audit the derivation and the /code agent will check for numerical stability issues.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 seminal papers on optimization dynamics near saddle points.\", \"- [ ] /math: Produce a formal derivation of the Hessian and its eigenvalues for the generalized Rosenbrock function.\", \"- [ ] /code: Implement the trust-region Newton algorithm and a function to compute the numerical Hessian.\", \"- [ ] /code: Execute the algorithm to find stationary points and compute their numerical eigenvalues.\", \"- [ ] /math: Verify that the numerical eigenvalues from the /code agent align with the symbolic derivation.\", \"- [ ] /code: Add functionality to simulate and plot escape trajectories from identified saddle points.\", \"- [ ] /router: Perform final QA, checking alignment between the proof, code, plots, and acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-matrix\"], \"acceptance_criteria\": [\"- The analytical derivation of the Hessian's eigenvalues is mathematically sound and correctly classifies all stationary points.\", \"- The Python implementation correctly identifies the primary saddle point and its corresponding eigenvalues match the analytical result to at least 5 decimal places.\", \"- The final report includes visualizations of escape trajectories that clearly show movement away from the saddle point along directions of negative curvature.\", \"- The report cites at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 51, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project on quantum computing and need to understand the basics of fault-tolerance. Can you help me derive the conditions for the 3-qubit bit-flip code and then simulate its error correction process for single-qubit errors?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org introductory quantum error correction 3-qubit bit-flip code stabilizer formalism\\\", mode=both)\", \"/math(Derive the stabilizer generators S1=Z_1 Z_2, S2=Z_2 Z_3 for the 3-qubit bit-flip code. Calculate the error syndromes for single-qubit X errors on each qubit and show how they uniquely identify the error location. Reference the eigenvalue properties of Pauli operators.)\", \"/code(Implement the 3-qubit bit-flip code in Python using Qiskit. Simulate the encoding, the introduction of a single X error, syndrome measurement, and the recovery operation. Validate that the final state has high fidelity with the initial logical state.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational theory and mathematical formalism from authoritative sources. This information is then used by the /math tool to formally derive the core error detection mechanics. Finally, the /code tool implements a practical simulation based on the mathematical derivation to provide a verifiable demonstration of the concept.\", \"expected_artifacts\": [\"A summary report in markdown explaining the 3-qubit bit-flip code, its stabilizers, and the error correction logic.\", \"A formal mathematical derivation of the stabilizer generators and the complete error syndrome table for single bit-flip errors.\", \"A commented Python script that simulates the entire error correction process.\", \"A list of cited sources, including at least two arXiv pre-prints with their IDs.\", \"A metrics.json file containing the final state fidelity after correction.\"], \"thinking_outline\": [\"1. Research the stabilizer formalism as it applies to the 3-qubit bit-flip code, focusing on the definition of the code space and stabilizer generators.\", \"2. Define the logical qubits |0\u27e9_L and |1\u27e9_L and the quantum circuit that performs the encoding from a single logical qubit to three physical qubits.\", \"3. Mathematically derive the stabilizer generators and confirm that the logical states are +1 eigenstates of these operators.\", \"4. Calculate the syndrome measurement outcomes for single-qubit X (bit-flip) errors on each of the three physical qubits.\", \"5. Verify that each of the three possible single-qubit errors corresponds to a unique, non-trivial syndrome, enabling unambiguous correction.\", \"6. Implement the full encode-error-detect-correct cycle in a simulation and confirm the final state fidelity is close to 1, validating the theoretical model.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results provide the theoretical basis for the mathematical derivation. The derivation from /math provides the specific equations and syndrome table needed for the /code simulation. The code's output (fidelity metric) verifies the correctness of the derived theory. Fallback: If the math derivation is inconsistent with search results, revert to /general-search for alternative explanations. If the code simulation fails to achieve high fidelity, the mathematical logic must be re-audited.\", \"todo_list\": [\"- [ ] /general-search: Find authoritative papers or lecture notes on the 3-qubit bit-flip code and stabilizer formalism.\", \"- [ ] /math: Derive the stabilizer generators and the error syndrome table for all single bit-flip errors.\", \"- [ ] /code: Implement and simulate the 3-qubit code, including encoding, error injection, and the full correction procedure.\", \"- [ ] Verification: Check that the simulated fidelity of the corrected quantum state is > 0.999.\", \"- [ ] Router QA: Submit all artifacts (report, derivation, code, citations, metrics) for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"quantum-computing\", \"error-correction\", \"linear-algebra\"], \"acceptance_criteria\": [\"The mathematical derivation correctly identifies the stabilizer generators (Z\u2081Z\u2082, Z\u2082Z\u2083) and provides a syndrome table that uniquely maps each single-qubit X error to a distinct measurement outcome.\", \"The Python simulation runs without errors and demonstrates successful correction of a single bit-flip error applied to any of the three physical qubits.\", \"The primary metric `state_fidelity` reported by the simulation is greater than 0.999 after correction.\", \"The final report cites at least two relevant arXiv papers by their ID.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as F = |\u27e8\u03c8_initial|\u03c8_final\u27e9|\u00b2, the squared inner product between the initial logical state and the final state after the correction process.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"A boolean flag indicating if the measured syndrome in the simulation correctly matches the introduced error location based on the theoretical derivation.\"}}", "categories": ["metrics_guidance"]}
{"index": 52, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the dual update rule for a Lagrangian-based Constrained Markov Decision Process (CMDP) solver that ensures near-zero constraint violation with high probability. Then, implement a PPO agent for a robotic manipulator that uses this method, validating its safety against a baseline using control barrier functions and ensuring Lyapunov stability criteria are met for the system's energy function.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Lagrangian relaxation constrained MDP PPO control barrier function Lyapunov stability robotics\\\", mode=web)\", \"/math(Derive the primal-dual optimization updates for a PPO agent within a CMDP framework using Lagrangian relaxation. Formulate the Lyapunov stability condition for the robot's energy function as a safety constraint and prove convergence guarantees for the dual variable.)\", \"/code(Implement the derived Lagrangian PPO algorithm in Python with JAX for a simulated robotic arm. Validate safety by tracking constraint violation metrics, comparing against a baseline using control barrier functions, and plotting the system's energy over time to verify Lyapunov stability and control theory principles.)\"], \"route_rationale\": \"The blue /general-search agent will first gather seminal papers on Lagrangian methods in CMDPs and control barrier functions to ground the theoretical work. Next, the /math agent will perform the core theoretical derivation of the update rules and stability proofs. Finally, the /code agent will implement this novel algorithm and run simulations to rigorously validate the mathematical theory against empirical performance and safety metrics.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Lagrangian PPO update rules and the Lyapunov stability proof in LaTeX format.\", \"A well-documented Python implementation of the algorithm and the simulation environment.\", \"A final report summarizing the methodology, results, and including plots of reward, constraint violation, and system energy, with citations to at least 2 relevant arXiv papers.\", \"A JSON file containing the final evaluation metrics, including average constraint violation and convergence rate.\"], \"thinking_outline\": [\"1. Review foundational literature on Constrained Markov Decision Processes (CMDPs) and the use of Lagrangian relaxation for solving them, focusing on convergence properties.\", \"2. Formulate the specific CMDP for the robotic manipulator problem, defining state, action, reward, and the safety constraint related to a control barrier function or system energy.\", \"3. Derive the primal-dual update equations for the policy and the Lagrange multiplier within a Proximal Policy Optimization (PPO) framework.\", \"4. Prove that the derived update rule leads to Lyapunov stability for the closed-loop system under the learned policy.\", \"5. Implement the algorithm and validate the implementation by checking if the empirical constraint violation rate matches the theoretical bounds derived in the math step.\", \"6. Analyze simulation results to verify that the agent respects safety constraints while maximizing rewards, and audit the code for numerical stability and correctness.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The final code implementation must correctly reflect the update rules from the math derivation, and empirical results must align with theoretical safety guarantees. fallback: If empirical results diverge significantly from theory, /math will re-examine the convergence proof assumptions, potentially requiring a new search query from /general-search for alternative formulations.)\", \"todo_list\": [\"- [ ] /general-search: Find 2-3 key papers on Lagrangian methods for PPO in CMDPs and control barrier functions for safety.\", \"- [ ] /math: Derive the dual update rule for the Lagrange multiplier and the policy gradient for the CMDP.\", \"- [ ] /math: Formulate and prove the Lyapunov stability condition for the robotic system under the derived policy.\", \"- [ ] /code: Implement the Lagrangian PPO agent and the robotic arm simulation environment.\", \"- [ ] /code: Implement the safety validation logic using control barrier functions and track constraint violation metrics during training.\", \"- [ ] /math: Verify that the code's implementation of the update rules mathematically corresponds to the derived equations.\", \"- [ ] /code: Validate the stability proof by plotting the system's energy function over episode rollouts and checking for decay.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-mdp\", \"lagrangian-relaxation\", \"safe-reinforcement-learning\"], \"acceptance_criteria\": [\"The final report must contain a complete mathematical derivation of the primal-dual update rules with a valid Lyapunov stability proof.\", \"The implemented agent must achieve an average constraint violation rate below a prespecified threshold (e.g., 1e-3) over 100 evaluation episodes.\", \"The generated plots must empirically demonstrate the satisfaction of the Lyapunov stability condition (i.e., a non-increasing energy function) for the majority of test runs.\", \"The solution must cite at least two relevant arXiv pre-prints on safe RL or CMDPs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 53, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the probability flow ordinary differential equation (ODE) from the reverse-time stochastic differential equation (SDE) used in score-based generative models. Detail the connection via the Fokker-Planck equation. Then, implement a simple diffusion model in PyTorch that utilizes this ODE for deterministic sampling on the MNIST dataset and evaluate its sample quality using the Fr\u00e9chet Inception Distance (FID).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models SDE probability flow ODE Fokker-Planck\\\", mode=rag)\", \"/math(Derive the probability flow ODE from the reverse-time diffusion SDE. Start from the forward process, define the score function, formulate the reverse SDE via Andersen's theorem, and show how its deterministic part corresponds to the probability flow ODE, justifying via the Fokker-Planck equation.)\", \"/code(Implement a variance-preserving SDE diffusion model in PyTorch with a U-Net score network. Write a deterministic sampler based on the derived probability flow ODE using a standard numerical solver like RK45. Train on MNIST and compute the final FID score to measure convergence., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on score-based SDEs. This theoretical grounding is passed to /math for rigorous derivation. Finally, /code implements and validates the derived mathematical principles, providing empirical evidence.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation from the forward SDE to the probability flow ODE.\", \"A Python script implementing the score-based diffusion model and the deterministic ODE sampler.\", \"A final report (.md) summarizing the theory, implementation details, training convergence, and final FID score.\", \"A plot showing generated image samples from the trained model.\", \"A JSON file containing the final training metrics, including the FID score.\"], \"thinking_outline\": [\"1. Research and synthesize the foundational theory of score-based generative modeling using SDEs, focusing on the forward and reverse processes.\", \"2. Formally derive the probability flow ODE as the deterministic counterpart to the reverse-time SDE, leveraging the relationship given by the Fokker-Planck equation.\", \"3. Validate the derived ODE by cross-referencing it with established results from the papers found during the research phase.\", \"4. Implement the score network and the ODE-based sampler in Python, ensuring numerical stability of the solver.\", \"5. Train the model and generate samples, then verify the implementation's correctness by evaluating the FID score against expected benchmarks for simple datasets.\", \"6. Consolidate all derivations, code, results, and citations into a comprehensive final report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The mathematical derivation in the final report must exactly match the logic implemented in the ODE sampler; fallback: If a discrepancy is found, /math must revise the derivation and /code must update the implementation.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers (e.g., by Song, Ermon) on score-based SDEs and the probability flow ODE.\", \"- [ ] /math: Produce a step-by-step derivation of the probability flow ODE from the reverse-time SDE.\", \"- [ ] /code: Implement the U-Net architecture for the score model and the ODE solver for sampling.\", \"- [ ] /code: Train the model on MNIST until convergence and calculate the FID score.\", \"- [ ] /math: Verify that the symbolic ODE derived matches the discretized form used in the /code implementation.\", \"- [ ] /code: Validate that the FID score is within a reasonable range for this model architecture and dataset.\", \"- [ ] /router: Review all artifacts for consistency, clarity, and correctness before final submission.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"generative-modeling\", \"stochastic-calculus\"], \"acceptance_criteria\": [\"The mathematical derivation must be complete, correct, and properly reference the Fokker-Planck equation.\", \"The Python code must be executable, train the model, and generate image samples using the ODE sampler.\", \"The final reported FID score on MNIST must be below 40, indicating successful model training.\", \"The report must correctly cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 54, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on a project about differentiable AutoML and need to extend the DARTS (Differentiable Architecture Search) framework. Can you help me formulate and implement a Bayesian variant of DARTS to quantify epistemic uncertainty over the architectural parameters? The goal is to move beyond a point estimate for the optimal architecture by maintaining a variational posterior. The final implementation should be in JAX and validated on the CIFAR-10 search space, reporting both predictive performance and architectural uncertainty.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Bayesian neural architecture search DARTS variational inference AutoML\\\", mode=web)\", \"/math(Derive the evidence lower bound (ELBO) for the bi-level optimization problem in Bayesian DARTS. Formulate the reparameterization trick for the categorical architectural parameters and specify the required gradients for the variational posterior and supernet weights, considering Hessian-vector products for the inner loop.)\", \"/code(Implement the Bayesian DARTS search process in Python using JAX/Flax. The implementation must include the supernet, the bi-level optimizer with variational parameters, and CUDA-accelerated training loops. Validate the convergence of the ELBO and architectural uncertainty metrics.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on Bayesian extensions to NAS and DARTS. This informs the /math agent, which will perform the core theoretical work of deriving the variational objective and its gradients. Finally, the /code agent will translate the verified mathematical formulation into an efficient, hardware-accelerated JAX implementation for empirical validation.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the Bayesian DARTS objective function and its gradients.\", \"- A well-documented JAX/Flax implementation of the complete search pipeline, including data loaders and the bi-level optimizer.\", \"- A final report (PDF) summarizing the method, experiment results, and an analysis of the discovered architecture's uncertainty.\", \"- A JSON file containing final test accuracy, predictive log-likelihood, and architectural entropy metrics.\", \"- Citations for at least 2 relevant research papers in BibTeX format.\"], \"thinking_outline\": [\"1. Research existing literature on Bayesian methods for Neural Architecture Search (NAS) and uncertainty quantification in AutoML to establish a theoretical baseline.\", \"2. Formulate the bi-level optimization problem of DARTS within a probabilistic framework, defining a variational posterior over the architectural parameters and deriving the corresponding Evidence Lower Bound (ELBO) objective.\", \"3. Derive the analytical gradients for both the variational parameters (outer loop) and the network weights (inner loop) using the reparameterization trick and implicit differentiation.\", \"4. Implement the complete search algorithm in JAX, paying close attention to the efficiency of the bi-level optimization and the numerical stability of the gradient estimators.\", \"5. Verify the correctness of the gradient implementation using numerical checks (e.g., finite differences) on a smaller-scale problem before running the full search.\", \"6. Validate the final model's performance on the CIFAR-10 benchmark, analyzing both the predictive accuracy and the learned architectural uncertainty to confirm the method's efficacy.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation) -> router QA. The mathematical derivation from /math must be directly implemented by /code. Verification: The /code agent will implement numerical gradient checks to validate the analytical gradients derived by the /math agent. Fallback: If numerical checks fail, the task reverts to the /math agent to re-derive and simplify the gradient expressions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Bayesian or probabilistic differentiable architecture search.\", \"- [ ] /math: Produce a complete derivation of the ELBO and gradients for the architectural variational posterior.\", \"- [ ] /code: Implement the DARTS search space and supernet structure in JAX/Flax.\", \"- [ ] /code: Implement the bi-level variational optimizer, handling the inner and outer optimization loops.\", \"- [ ] /math: Verify that the analytical gradients derived for the ELBO are mathematically sound and consistent with the specified model.\", \"- [ ] /code: Add unit tests to verify the gradient computations in the JAX implementation against numerical approximations.\", \"- [ ] /code: Execute the search process on CIFAR-10, log metrics, and generate summary plots.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-architecture-search\", \"bayesian-deep-learning\", \"automl\"], \"acceptance_criteria\": [\"- The provided JAX code must be runnable and reproduce the reported metrics within a 5% tolerance.\", \"- The mathematical derivation in the LaTeX document must be self-contained, correct, and directly correspond to the Python implementation.\", \"- The final report must clearly demonstrate that the model quantifies architectural uncertainty (e.g., via plots of the variational distribution's entropy over time).\", \"- The test accuracy on CIFAR-10 for the discovered architecture must be competitive with established DARTS baselines (e.g., >95%).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 57, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for a Bayesian hierarchical model with a non-conjugate prior, specifically a Student's-t likelihood and a Gaussian prior. Compare the convergence properties of the mean-field variational inference using the standard Kullback-Leibler divergence against the alpha-divergence family for alpha=0.5. Provide a formal proof of the ELBO's gradient with respect to the variational parameters, leveraging the reparameterization trick for low-variance estimation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference alpha-divergence non-conjugate reparameterization trick\\\", mode=web)\", \"/math(Derive the ELBO for the specified Bayesian hierarchical model. Prove the gradient expression for the variational parameters using the reparameterization trick and tensor calculus, ensuring the derivation accounts for the alpha-divergence formulation.)\", \"/code(Implement the variational inference optimization in Python with PyTorch. Compare the ELBO convergence rate and final posterior approximation quality for both the standard KL divergence and the alpha-divergence (alpha=0.5) on a synthetic dataset. Track AUPRC for model fit.)\"], \"route_rationale\": \"The plan starts with the 'blue' /general-search agent to gather state-of-the-art literature on alpha-divergence VI, which is crucial for this advanced topic. This informs the /math agent's formal derivation and proof of the ELBO gradient. Finally, the /code agent implements the derived mathematical framework to conduct an empirical comparison, providing a concrete validation of the theoretical findings.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the alpha-divergence ELBO and its gradient.\", \"A Python script implementing the variational inference algorithm for both divergence measures.\", \"A plot comparing the ELBO convergence curves and posterior distributions for the two methods.\", \"A bibliography with at least two cited arXiv papers on variational inference.\", \"A verification report confirming the consistency between the mathematical derivation and the code's gradient computation.\"], \"thinking_outline\": [\"1. Formulate the probabilistic graphical model for the described Bayesian hierarchical structure.\", \"2. Derive the general form of the Evidence Lower Bound (ELBO) using the alpha-divergence family, showing how it reduces to the standard KL-divergence ELBO for alpha -> 1.\", \"3. Derive the gradient of the alpha-ELBO with respect to the variational parameters, applying the reparameterization trick to handle the non-conjugate likelihood and create a low-variance estimator.\", \"4. Implement the stochastic optimization algorithm in code, ensuring correct computation of the gradients for both divergence measures.\", \"5. Validate the implementation by comparing numerical gradients (finite differences) against the analytical gradients derived in step 3 for a small-scale problem.\", \"6. Run the experiment on synthetic data, analyze the convergence plots, and verify that the posterior approximations are reasonable.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Math agent's derived gradient formula is checked against the code agent's numerical gradient implementation. Fallback: If gradients mismatch, flag for review by the math agent to check the derivation and reparameterization logic.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on alpha-divergence VI and non-conjugate models.\", \"- [ ] /math: Produce the formal LaTeX derivation of the alpha-divergence ELBO and its gradient.\", \"- [ ] /code: Develop a Python script to generate synthetic data based on the specified model.\", \"- [ ] /code: Implement the VI optimization loop using PyTorch, supporting both KL and alpha-divergences.\", \"- [ ] /math: Verify that the gradient expression derived is consistent with the literature found by /general-search.\", \"- [ ] /code: Implement a numerical gradient checker and verify that the analytical gradients implemented in code match the numerical ones.\", \"- [ ] /code: Generate comparison plots and a summary metrics table.\", \"- [ ] router QA: Review all artifacts for consistency, correctness, and adherence to the acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-statistics\", \"information-geometry\"], \"acceptance_criteria\": [\"The mathematical derivation of the alpha-divergence ELBO gradient must be complete and formally correct.\", \"The Python implementation must correctly perform variational inference for both divergence measures and reproduce the expected convergence behavior.\", \"The final report must include a plot clearly showing the difference in ELBO convergence between the KL and alpha-divergence methods.\", \"The code must pass the gradient verification check, comparing analytical and numerical gradients with a tolerance of less than 1e-4.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 59, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the analytical gradient for the Variational Quantum Eigensolver (VQE) cost function using a Unitary Coupled Cluster Singles and Doubles (UCCSD) ansatz for the H2 molecule. Subsequently, implement this VQE algorithm in Python using Qiskit, compare the analytical gradient descent with a parameter-shift rule implementation, and simulate the effect of depolarizing noise on the convergence to the ground state energy eigenvalue.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational quantum eigensolver analytical gradient UCCSD ansatz chemistry\\\", mode=rag)\", \"/math(Derive the analytical gradient of the VQE energy functional for a UCCSD ansatz, expressing the update rule in terms of Pauli operators and ensuring the derivation for the molecular Hamiltonian eigenvalue is explicitly shown.)\", \"/code(Implement the VQE algorithm for the H2 molecule in Python/Qiskit, comparing analytical gradient descent against the parameter-shift rule to validate convergence speed and final ground state energy. Add a depolarizing noise model.)\"], \"route_rationale\": \"The blue /general-search agent is used first to gather authoritative literature on VQE analytical gradients. The /math agent then performs the complex symbolic derivation. Finally, the /code agent implements the derived algorithm and validates its performance against a standard method and noise, providing empirical verification.\", \"expected_artifacts\": [\"- A complete LaTeX derivation of the analytical gradient for the UCCSD-VQE cost function.\", \"- A documented Python script implementing the VQE simulation, comparing gradient methods, and modeling noise.\", \"- A summary report including plots of energy convergence vs. iterations for all methods and a final table comparing ground state energy accuracy.\", \"- A list of at least 2 cited arXiv papers detailing VQE theory and implementation.\"], \"thinking_outline\": [\"1. Formulate the molecular Hamiltonian for H2 in the second quantization representation and map it to a qubit Hamiltonian using a suitable transformation (e.g., Jordan-Wigner).\", \"2. Define the Unitary Coupled Cluster Singles and Doubles (UCCSD) ansatz mathematically and construct its corresponding quantum circuit.\", \"3. Derive the analytical gradient of the expectation value <\u03c8(\u03b8)|H|\u03c8(\u03b8)> with respect to the ansatz parameters \u03b8, expressing it in a form suitable for computation.\", \"4. Implement the VQE algorithm using both the derived analytical gradient and the numerical parameter-shift rule for comparison. Validate the correctness of the analytical gradient implementation by checking its output against the parameter-shift rule on a few test points.\", \"5. Simulate the VQE convergence for the H2 molecule, plot the energy landscape, and check the final ground state energy against known FCI (Full Configuration Interaction) results.\", \"6. Introduce a depolarizing noise model into the simulation and analyze its impact on the convergence and accuracy of the final eigenvalue.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The analytical gradient derivation from /math must match the formula implemented in /code, and the simulated ground state energy must be within 1e-3 Hartree of the known FCI value for H2. fallback: If derivation is flawed, /math must revisit the literature from /general-search. If implementation fails verification, /code must debug against the /math derivation.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on analytical gradients in VQE for UCCSD ansatz.\", \"- [ ] /math: Derive the gradient of the VQE energy functional <H> w.r.t. cluster amplitudes.\", \"- [ ] /code: Implement the H2 molecular Hamiltonian and the UCCSD circuit ansatz using Qiskit.\", \"- [ ] /code: Implement optimizers using both the analytical gradient and the parameter-shift rule.\", \"- [ ] /math: Verify that the symbolic form of the derived gradient is consistent with the literature found by /general-search.\", \"- [ ] /code: Validate the analytical gradient code by comparing its output to the parameter-shift rule for several parameter sets.\", \"- [ ] /code: Run simulations, generate convergence plots, and summarize final energy eigenvalue results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-machine-learning\", \"variational-quantum-eigensolver\", \"quantum-chemistry\"], \"acceptance_criteria\": [\"- The LaTeX derivation of the analytical gradient must be mathematically sound and complete.\", \"- The Python implementation must successfully converge to the ground state energy of H2 with an error less than 0.01 Hartree compared to the FCI value.\", \"- The final report must contain plots clearly showing the convergence comparison between the analytical and parameter-shift gradient methods under both noiseless and noisy conditions.\", \"- The solution cites at least two relevant arXiv preprints on VQE or quantum chemistry.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 61, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the performance of a distance-3 surface code under a depolarizing noise model. Can you formally derive its code distance using the stabilizer formalism, analyze the logical operators, and then write a simulation to estimate the logical error rate versus the physical error rate to approximate its threshold? The analysis should consider the computational complexity of the decoding process.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org surface code threshold theorem depolarizing noise stabilizer formalism review\\\", mode=both, includes cybersecurity)\", \"/math(Formally derive the distance and stabilizer group properties for a d=3 surface code, analyzing the generator eigenvalue structure and the computational complexity of syndrome decoding.)\", \"/code(Implement a Monte Carlo simulation in Python for the d=3 surface code with a depolarizing noise model and a minimum-weight perfect matching decoder, validating the logical error rate against the physical error rate.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art theoretical background and established threshold values from authoritative sources like arXiv. This context is handed to /math to perform a formal, first-principles derivation of the code's properties. Finally, /code implements a numerical simulation to validate the theoretical analysis from /math and produce an empirical estimate of the error threshold.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the surface code's distance, stabilizer group, and logical operators.\", \"A well-commented Python script implementing the Monte Carlo simulation of the error correction cycle.\", \"A plot visualizing the logical error rate as a function of the physical error rate, with the estimated threshold highlighted.\", \"A summary report (PDF) synthesizing the theoretical and simulation results, including at least 2 arXiv citations.\", \"A JSON file containing the raw data points for the logical vs. physical error rate plot.\"], \"thinking_outline\": [\"1. Use search to find foundational papers and recent reviews on the surface code threshold theorem and its analysis under depolarizing noise.\", \"2. Formally define the stabilizer generators for a distance-3 surface code on a toric lattice and derive the code distance by identifying the minimum weight of a non-trivial logical operator.\", \"3. Validate the derived stabilizer properties and code distance against the established results found in the literature search.\", \"4. Implement a simulation that applies random Pauli errors based on a physical error rate, calculates the error syndrome, and uses a decoder (e.g., MWPM) to infer a correction.\", \"5. Check the simulation's correctness by verifying that the logical error rate collapses as expected for very low physical error rates and that the syndrome extraction logic is sound.\", \"6. Aggregate simulation data across a range of physical error rates to plot the logical error rate curve and estimate the pseudo-threshold where it intersects the p_logical = p_physical line.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The formal derivation from /math must align with the parameters and model implemented in the /code simulation; fallback: If the simulation's threshold estimate significantly deviates from the literature values gathered by /general-search, re-evaluate the decoding algorithm's implementation in /code.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on surface code thresholds, noting the noise models used.\", \"- [ ] /math: Derive the [[9,1,3]] surface code's stabilizer generators and logical operators.\", \"- [ ] /math: Prove the code distance is 3 and analyze the eigenvalue properties of the stabilizers.\", \"- [ ] /code: Implement the simulation framework, including lattice creation, noise model, and syndrome measurement.\", \"- [ ] /code: Implement the Minimum-Weight Perfect Matching (MWPM) decoder for the generated syndromes.\", \"- [ ] /math: Verify that the logical operators derived in the math step are correctly implemented and checked in the simulation.\", \"- [ ] /code: Validate the simulation's output for a trivial case (e.g., zero physical error) to ensure correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"computational-physics\"], \"acceptance_criteria\": [\"- The mathematical derivation of the d=3 code distance must be correct and complete.\", \"- The Python simulation must successfully run and generate a plot showing a clear threshold phenomenon.\", \"- The estimated error threshold from the simulation must be within a reasonable range (e.g., 10-11%) for the surface code with MWPM decoding, as cited in the literature.\", \"- The final report must cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 62, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the loss landscape of a deep generative model used for adversarial training. I need to formally characterize the saddle point structures using tensor calculus. Can you derive the second-order necessary and sufficient conditions for identifying these points based on the Hessian's eigenvalue spectrum, and then develop a proof-of-concept algorithm that leverages directions of negative curvature to guarantee escape, ensuring convergence to a local minimum?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org hessian-free optimization saddle point escape negative curvature adversarial training\\\", mode=both)\", \"/math(Derive the second-order conditions for saddle points using the Hessian's eigenvalue spectrum. Formalize a trust-region or cubic regularization method to escape saddles, proving convergence guarantees under standard assumptions for adversarial optimization.)\", \"/code(Implement the saddle point escape algorithm in Python with JAX, using its automatic differentiation capabilities to compute Hessian-vector products. Validate the implementation on a canonical non-convex test function with known saddle points, tracking convergence metrics.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey modern saddle point escape techniques from authoritative sources like arXiv. This informs the /math agent's formal derivation of the conditions and escape algorithm. The /code agent then provides an empirical implementation and validation of the theory. This sequence ensures the solution is both theoretically sound and practically verified.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivations for saddle point conditions and the convergence proof for the escape algorithm.\", \"- A Python script implementing the saddle point detection and escape algorithm.\", \"- A plot visualizing the optimization trajectory escaping a saddle point on a 2D test function.\", \"- A summary of the literature review with at least two cited arXiv papers.\", \"- A verification report confirming the alignment between the theoretical convergence rate and empirical observations.\"], \"thinking_outline\": [\"1. Define the class of non-convex functions and formalize the definition of a strict saddle point using the gradient and the Hessian matrix.\", \"2. Derive the second-order necessary and sufficient conditions for a critical point to be a saddle point by analyzing the eigenvalue spectrum of the Hessian.\", \"3. Formulate an algorithm (e.g., Newton-CG with negative curvature direction) that can provably escape these saddle points and converge to a second-order stationary point.\", \"4. Validate the theoretical convergence proof for the proposed algorithm, ensuring it accounts for computational constraints like inexact Hessian-vector products.\", \"5. Implement the algorithm and test its performance on a well-known non-convex function like the Rastrigin or Ackley function, initialized at a saddle point.\", \"6. Check the empirical results against the theoretical guarantees, verifying that the escape mechanism performs as predicted by the proof.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent provides a formal algorithm specification and proofs to the code agent. The code agent returns an implementation and empirical results for final verification against the theory. Fallback: If the math agent's proof is non-constructive, it must refine the algorithm with guidance from the initial literature search before handoff to code.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 key papers on saddle point escape methods in deep learning.\", \"- [ ] /math: Produce a formal derivation of saddle point conditions based on Hessian eigenvalues.\", \"- [ ] /math: Develop the pseudo-code and convergence proof for an escape algorithm.\", \"- [ ] /code: Implement the algorithm and a test harness for a non-convex function.\", \"- [ ] /code: Generate plots of the optimization trajectory and the minimum Hessian eigenvalue over time.\", \"- [ ] /math: Verify that the algorithm implemented in code correctly reflects the derived mathematical principles.\", \"- [ ] /code: Audit the JAX implementation for numerical stability and correctness of the Hessian-vector product calculation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-eigenvalue-decomposition\"], \"acceptance_criteria\": [\"- The mathematical proof of convergence for the escape algorithm must be complete and logically sound.\", \"- The Python implementation must successfully escape a pre-defined saddle point on a benchmark function within a specified iteration budget.\", \"- The final report must correctly cite at least two relevant arXiv pre-prints with their IDs.\", \"- The minimum eigenvalue of the Hessian, as tracked during the run, must become and remain non-negative after the escape phase, confirming arrival at a local minimum.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 63, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to formulate the dictionary learning problem for sparse signal representation as a constrained optimization problem, specifically using the Alternating Direction Method of Multipliers (ADMM). Derive the closed-form update steps for both the dictionary and the sparse codes, and analyze the convergence criteria. Then, implement this ADMM-based dictionary learning algorithm and validate its reconstruction performance on synthetic signals, ensuring the implementation's convergence behavior aligns with the theoretical derivation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org admm dictionary learning for sparse coding convergence analysis\\\", mode=both)\", \"/math(Derive the augmented Lagrangian and the sequential update rules for the sparse code and dictionary matrices using the ADMM framework, ensuring to analyze the theoretical convergence properties and eigenvalue constraints.)\", \"/code(Implement the ADMM algorithm in Python using JAX for differentiability and potential CUDA acceleration. The implementation must track convergence metrics and include Fourier analysis tools for synthetic signal generation.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on ADMM for dictionary learning. This research informs the theoretical work in /math, where the core algorithm is derived. Finally, /code translates the mathematical derivation into a verifiable implementation, allowing for empirical validation of the theory.\", \"expected_artifacts\": [\"- A formal mathematical proof detailing the derivation of the ADMM update equations for dictionary learning.\", \"- A Python script implementing the algorithm, complete with functions for data synthesis and performance evaluation.\", \"- A summary report in Markdown format, including convergence plots (objective value vs. iteration) and reconstruction error metrics.\", \"- A bibliography file with citations for at least two relevant academic papers, including arXiv IDs.\"], \"thinking_outline\": [\"1. Formulate the dictionary learning objective function (e.g., L2 data fidelity term + L1 sparsity term) and rewrite it in the ADMM consensus form with appropriate constraints.\", \"2. Derive the explicit, closed-form update rules for the sparse code subproblem, the dictionary update subproblem, and the dual variable update by minimizing the augmented Lagrangian.\", \"3. Analyze the convergence conditions of the algorithm, referencing foundational literature on ADMM and properties like the spectral norm of the dictionary atoms.\", \"4. Implement the derived update rules in code, paying close attention to numerical stability and efficient matrix operations.\", \"5. Validate the implementation's correctness by comparing its outputs on a small, known problem against a manual calculation or a trusted reference.\", \"6. Design and execute experiments on synthetic sparse signals to empirically check the algorithm's convergence rate and reconstruction accuracy, ensuring results align with the theoretical analysis.\"], \"handoff_plan\": \"/general-search (literature) -> /math (derivation) -> /code (implementation) -> router QA. Verification: /math will verify that the problem formulation from /general-search is appropriate. /code will include unit tests to validate the mathematical update steps derived by /math. Fallback: If the /code implementation fails to converge, the task will be routed back to /math to re-check the derivation and its convergence assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two seminal papers on ADMM for sparse dictionary learning, focusing on the problem formulation.\", \"- [ ] /math: Formulate the augmented Lagrangian for the dictionary learning problem.\", \"- [ ] /math: Derive the iterative update equations for the sparse codes, the dictionary, and the dual variables.\", \"- [ ] /code: Implement the complete ADMM algorithm in Python, including a function to generate synthetic test data.\", \"- [ ] /code: Instrument the implementation to log primary and secondary metrics (MSE, sparsity) at each iteration.\", \"- [ ] /math: Verify that the proximal operators and matrix inversions implemented in the /code step are mathematically equivalent to the derived equations.\", \"- [ ] /code: Generate plots visualizing the convergence of the objective function and the reconstruction error over iterations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"admm-optimization\"], \"acceptance_criteria\": [\"- The provided mathematical derivation of the ADMM update rules is complete, step-by-step, and formally correct.\", \"- The Python implementation successfully executes and demonstrates convergence (i.e., decreasing objective value) on a synthetic dataset.\", \"- The final report includes plots showing reconstruction Mean Squared Error (MSE) below a threshold of 1e-4 after 100 iterations on a non-trivial synthetic problem.\", \"- The report cites at least two relevant papers from arXiv or peer-reviewed journals.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"Calculate as `||Y - DX||_F^2 / (m*n)`, where Y is the matrix of original signals, D is the learned dictionary, X is the matrix of sparse codes, and m,n are the dimensions of Y.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"Calculate as the average number of non-zero elements in the columns of the sparse code matrix X, `mean(sum(abs(X) > 1e-6, axis=0))`.\"}}", "categories": ["advanced", "metrics_computation"]}
{"index": 64, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the mathematical formulation for computing persistent homology using a Vietoris-Rips filtration on a point cloud sampled from a synthetic manifold, such as the noisy Swiss roll. Subsequently, implement this in Python using libraries like Ripser or Gudhi, and validate the computed Betti numbers (\u03b2\u2080, \u03b2\u2081) against the known topology. Analyze the stability of the persistence diagram under perturbations of the input data and discuss the computational complexity, referencing relevant convergence guarantees from an authoritative arxiv source.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org persistent homology Vietoris-Rips filtration stability computational complexity\\\", mode=both)\", \"/math(Derive the algorithm for constructing the Vietoris-Rips complex and its boundary matrices. Formulate the persistence algorithm via matrix reduction to compute Betti numbers, incorporating Bayesian stability analysis and discussing the convergence of related graph Laplacian eigenvalues.)\", \"/code(Implement the Vietoris-Rips filtration and persistent homology calculation in Python for a noisy Swiss roll dataset. Visualize the persistence diagram and write unit tests to verify Betti number stability and track the algorithm's complexity.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational academic papers on persistent homology and its stability. This literature informs the /math agent's formal derivation of the filtration and persistence algorithm. Finally, the /code agent implements the derived mathematics, providing an empirical validation of the theory on a concrete example, which is crucial for a math_plus_code variant.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Vietoris-Rips filtration and the persistent homology algorithm.\", \"A Python script implementing the analysis on a synthetic noisy Swiss roll dataset.\", \"Generated artifacts including a persistence diagram plot and a JSON file with computed Betti numbers.\", \"A summary report linking the mathematical theory to the empirical results, including runtime complexity analysis and at least 2 citations.\", \"Verification output confirming the computed Betti numbers match the known topology of the manifold.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of persistent homology, focusing on Vietoris-Rips filtrations, boundary operators, and stability theorems.\", \"2. Formulate the mathematical steps for constructing the filtration's simplicial complexes and computing persistence via the reduction of boundary matrices.\", \"3. Implement the algorithm in Python, generating a synthetic manifold (e.g., a noisy Swiss roll) as the input point cloud.\", \"4. Validate the implementation by checking if the computed Betti numbers (\u03b2\u2080 for connected components, \u03b2\u2081 for holes) match the ground truth topology of the synthetic manifold across different noise levels.\", \"5. Empirically verify the stability theorem by adding varying levels of noise to the input point cloud and quantifying the bottleneck distance between the resulting persistence diagrams.\", \"6. Synthesize all findings into a cohesive report that connects the mathematical theory, implementation results, and stability analysis, complete with citations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code output, specifically the Betti numbers and persistence diagram, must align with the mathematical derivations and theoretical stability guarantees; fallback: if Betti numbers are incorrect, router flags the code agent to debug the filtration construction or boundary matrix reduction, potentially requesting a simplified derivation from the math agent for a toy example like a simple circle).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Vietoris-Rips persistent homology and its stability.\", \"- [ ] /math: Provide a step-by-step derivation of the Vietoris-Rips filtration construction and the persistence algorithm for computing Betti numbers.\", \"- [ ] /code: Implement the algorithm in Python, using a library like `ripser` or `gudhi`.\", \"- [ ] /code: Generate a noisy Swiss roll dataset and compute its persistence diagram and Betti numbers.\", \"- [ ] /math: Verify that the code's output (Betti numbers) for a low-noise case aligns with the known topology of the Swiss roll manifold (\u03b2\u2080=1, \u03b2\u2081=0).\", \"- [ ] /code: Conduct an experiment to demonstrate stability by varying noise and plotting the resulting persistence diagrams.\", \"- [ ] /code: Generate a final report summarizing the methods, results, and including all plots and citations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"persistent-homology\"], \"acceptance_criteria\": [\"- The persistence diagram generated by the code correctly identifies one significant feature in dimension 0 (one connected component) and no significant features in dimension 1 (no holes), corresponding to the topology of the Swiss roll.\", \"- The mathematical derivation is clear, correct, and directly corresponds to the algorithm implemented in the Python script.\", \"- The final report includes at least two citations to relevant academic papers with arXiv IDs or DOIs.\", \"- The stability analysis shows that small perturbations in the input point cloud result in small changes in the persistence diagram, as measured by bottleneck distance.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 65, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for a diffusion process, starting from the forward SDE and its corresponding Fokker-Planck equation. Show explicitly how the score function, \u2207x log p(x,t), emerges. Then, implement a PyTorch-based solver using the Euler-Maruyama method to sample from a 2D 'Swiss roll' distribution, verifying that the empirical distribution after T steps matches the target.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time SDE Fokker-Planck\\\", mode=both)\", \"/math(Derive the reverse-time SDE for a forward diffusion process, starting from the Kolmogorov backward equation or Fokker-Planck equation, and explicitly show the score function's role in the drift term.)\", \"/code(Implement a PyTorch sampler for the reverse SDE using the Euler-Maruyama method. Train a score network on a 2D Swiss roll dataset and evaluate convergence against ground truth using FID-like metrics., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find foundational literature. This informs the /math agent's formal derivation of the core SDE theory. Finally, the /code agent translates the derived mathematical equations into a practical, verifiable PyTorch implementation, ensuring the theoretical work is grounded in a functional artifact.\", \"expected_artifacts\": [\"* A PDF report containing the full mathematical derivation of the reverse-time SDE.\", \"* A Python script (`main.py`) with a PyTorch implementation of the score-based sampler.\", \"* A JSON file (`metrics.json`) reporting the final Sliced-Wasserstein distance and KL divergence.\", \"* A `references.bib` file with citations to at least two relevant arXiv papers.\", \"* A plot (`samples.png`) visualizing the generated 2D point cloud against the target distribution.\"], \"thinking_outline\": [\"1. Research the connection between the forward SDE, the Fokker-Planck partial differential equation, and the reverse-time SDE as described by Anderson (1982) and later in score-based generative modeling literature.\", \"2. Formally derive the drift term of the reverse-time SDE, showing its dependence on the score of the marginal probability density, \u2207x log p_t(x).\", \"3. Validate the derivation by checking for consistency with established literature found in the search phase and ensuring the final SDE form is correct.\", \"4. Implement a time-discretized solver (e.g., Euler-Maruyama) in PyTorch for the reverse SDE, including a trainable neural network to approximate the score function.\", \"5. Train the score network using denoising score matching on data generated from the forward SDE process.\", \"6. Verify the implementation's correctness by sampling from the trained reverse process and quantitatively comparing the final empirical distribution to the target distribution using Sliced-Wasserstein distance.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Cross-check the derived drift term in the /math report against its discretized implementation in the /code solver; fallback: if a mismatch is found, return to /math for correction based on the literature from /general-search).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on score-based generative models and the reverse-time SDE derivation via the Fokker-Planck equation.\", \"- [ ] /math: Provide a step-by-step derivation of the reverse-time SDE drift term, highlighting the emergence of the score function.\", \"- [ ] /math: Formulate the time-discretized update rule (Euler-Maruyama) for the reverse process SDE.\", \"- [ ] /code: Implement the PyTorch solver for the reverse SDE, including a trainable score network and data generation for the 2D Swiss roll.\", \"- [ ] /code: Train the model and generate final samples, saving the visualization plot.\", \"- [ ] /math: Verify the mathematical derivation in the final report aligns with the discretized update rule implemented in the code.\", \"- [ ] /code: Validate the sampler's output by computing and reporting the Sliced-Wasserstein distance in `metrics.json`.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-modeling\"], \"acceptance_criteria\": [\"* The final report contains a complete, step-by-step mathematical derivation of the reverse-time SDE from the Fokker-Planck equation.\", \"* The submitted Python script successfully runs without errors and generates a `samples.png` file that visually approximates the target 2D Swiss roll distribution.\", \"* The Sliced-Wasserstein distance reported in `metrics.json` between generated samples and the true distribution is below 0.05.\", \"* The solution correctly cites at least two relevant arXiv papers on score-based generative modeling.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 67, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the noise calibration mechanism for achieving (epsilon, delta)-differential privacy in a federated learning setting using secure aggregation. Specifically, analyze the L2 sensitivity of the sum of clipped model updates and implement a Python prototype using a Paillier homomorphic encryption proxy to demonstrate the privacy-utility trade-off. Compare the theoretical privacy loss against empirical model convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning 'secure aggregation' differential privacy L2 sensitivity analysis\\\", mode=web)\", \"/math(Derive the L2 sensitivity for the sum of clipped gradients in a federated learning context, then formulate the Gaussian mechanism for achieving (epsilon, delta)-differential privacy, ensuring the noise variance calculation is consistent with homomorphic encryption constraints.)\", \"/code(Implement a Python simulation of federated averaging with client-side gradient clipping and server-side Gaussian noise addition, using a library to simulate Paillier homomorphic encryption for aggregation. Track model convergence and the privacy budget.)\"], \"route_rationale\": \"The blue general-search agent will first gather seminal papers on secure aggregation and differential privacy in federated learning. /math will then formalize the privacy mechanism by deriving the sensitivity and noise calibration. Finally, /code will implement a simulation to empirically validate the theoretical privacy-utility trade-off derived by /math.\", \"expected_artifacts\": [\"A formal mathematical derivation of the L2 sensitivity and the corresponding Gaussian noise variance for (epsilon, delta)-differential privacy.\", \"A Python script implementing the federated learning simulation with secure aggregation and differential privacy.\", \"A report comparing the theoretical privacy loss with empirical model accuracy, including relevant plots and analysis.\", \"A bibliography citing at least two key arXiv papers on the topic.\"], \"thinking_outline\": [\"1. Review literature on differential privacy in federated learning, focusing on composition theorems and sensitivity analysis under secure aggregation.\", \"2. Formally derive the L2 sensitivity of the aggregated, clipped model updates from all clients in a single communication round.\", \"3. Based on the sensitivity, derive the required noise variance for the Gaussian mechanism to satisfy the target (epsilon, delta)-DP guarantee.\", \"4. Implement the federated training loop with per-client gradient clipping and server-side noising, simulating the secure aggregation process.\", \"5. Validate that the implemented noise level in the code precisely matches the mathematical derivation from step 3 for a given set of parameters.\", \"6. Audit the empirical results by running simulations across different privacy budgets (epsilon values) and checking if the accuracy-privacy trade-off behaves as predicted by theory.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent verifies the L2 sensitivity proof against sourced literature; /code agent verifies implementation logic against the mathematical derivation; fallback: if derivation is flawed, revert to /math with corrected assumptions from literature review.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize key papers on (epsilon, delta)-DP for federated learning, focusing on sensitivity analysis for secure aggregation.\", \"- [ ] /math: Derive the L2 sensitivity of the sum of clipped model updates per the federated learning context.\", \"- [ ] /math: Formulate the Gaussian mechanism and derive the noise standard deviation required for the target privacy budget.\", \"- [ ] /code: Implement the federated learning simulation with client-side clipping and a server-side aggregator.\", \"- [ ] /code: Integrate the derived Gaussian noise mechanism into the server's update step to ensure privacy.\", \"- [ ] /math: Verify that the noise implementation in the Python code correctly matches the derived mathematical formula.\", \"- [ ] /code: Run experiments to plot model accuracy against varying epsilon values to illustrate the privacy-utility trade-off.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"differential-privacy\", \"secure-aggregation\"], \"acceptance_criteria\": [\"- The mathematical derivation of L2 sensitivity and noise variance must be complete, rigorous, and self-contained.\", \"- The Python implementation must correctly apply gradient clipping and add Gaussian noise according to the derivation.\", \"- The final report must include a plot demonstrating the trade-off between model accuracy and the privacy parameter epsilon, supported by data from the simulation.\", \"- At least two relevant arXiv papers on secure aggregation or differential privacy in FL must be cited with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 68, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the theoretical conditions for convergence of Stochastic Variational Inference (SVI) for a non-conjugate Bayesian hierarchical model. Specifically, analyze the impact of the Robbins-Monro conditions on the learning rate schedule for the natural gradient updates. Provide a formal proof demonstrating that the expected KL divergence to the true posterior diminishes under these conditions, and implement a simulation in PyTorch to empirically validate the convergence rate against the theoretical bounds for a Bayesian logistic regression model.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Stochastic Variational Inference natural gradient convergence Robbins-Monro conditions Bayesian models\\\", mode=rag)\", \"/math(Derive the natural gradient update for a Bayesian logistic regression model. Formally prove the convergence of the expected KL divergence using the established Robbins-Monro conditions, ensuring the proof accounts for non-conjugacy and bounded variance assumptions.)\", \"/code(Implement the SVI algorithm in PyTorch for the specified Bayesian model. Empirically verify the theoretical convergence rate by tracking the ELBO and KL divergence over iterations. Use CUDA for acceleration and bootstrap methods for confidence interval estimation on the convergence rate., using Python)\"], \"route_rationale\": \"The blue /general-search agent first gathers foundational literature on SVI convergence. This is passed to /math to construct the formal proof and derivations. Finally, /code implements the algorithm to provide empirical validation of the theoretical results derived by /math.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation and proof of convergence.\", \"A Python script implementing the Bayesian model and SVI algorithm.\", \"A PDF report summarizing the findings, including plots of ELBO convergence and a comparison of empirical vs. theoretical rates, citing at least two arXiv papers.\", \"A metrics file (`results.json`) containing the final ELBO, estimated KL divergence, and computed convergence rate.\"], \"thinking_outline\": [\"1. Research the theoretical underpinnings of Stochastic Variational Inference (SVI), focusing on natural gradient methods and the Robbins-Monro conditions for stochastic approximation.\", \"2. Formulate the Bayesian hierarchical model (e.g., logistic regression) and its corresponding non-conjugate posterior distribution.\", \"3. Derive the natural gradient update rule for the variational parameters and construct a formal proof of convergence for the Evidence Lower Bound (ELBO), explicitly showing how the learning rate schedule satisfies the necessary conditions.\", \"4. Implement the model and the SVI optimization loop in PyTorch, ensuring correct calculation of the ELBO and its gradients.\", \"5. Validate the implementation by comparing the empirical convergence rate from the simulation against the derived theoretical bounds. Check for stability and sources of variance.\", \"6. Synthesize all findings into a cohesive report, verifying that the mathematical proof in the LaTeX document is consistent with the behavior observed in the code's output.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The formal proof from /math must directly inform the learning rate schedule and stopping criteria in the /code implementation. The empirical results from /code must be checked against the theoretical bounds from /math; fallback: If rates diverge, /math must re-audit the proof's assumptions, such as bounded variance.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on SVI convergence and natural gradients (e.g., Hoffman et al., 2013).\", \"- [ ] /math: Derive the natural gradient update equations for the specified model.\", \"- [ ] /math: Construct the formal proof of ELBO convergence under Robbins-Monro conditions.\", \"- [ ] /code: Implement the Bayesian logistic regression model and SVI update loop in PyTorch.\", \"- [ ] /code: Instrument the code to log ELBO and estimate KL divergence per epoch.\", \"- [ ] /math: Verify that the assumptions made in the proof (e.g., bounded gradients) are reasonable for the chosen model.\", \"- [ ] /code: Verify that the implemented learning rate schedule matches the one specified in the mathematical proof.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"convergence-theory\", \"bayesian-methods\"], \"acceptance_criteria\": [\"The formal proof must correctly apply the Robbins-Monro conditions to the natural gradient updates and demonstrate convergence.\", \"The PyTorch implementation must successfully run and show a monotonically increasing ELBO trend, converging to a stable value.\", \"The final report must contain a plot comparing the empirical convergence rate to the theoretical bounds, with a discrepancy of less than 15%.\", \"The report must cite at least two relevant arXiv preprints on variational inference.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 69, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nCan you explain the mathematical foundations of using a Wiener process as a prior in Bayesian models and show how to implement a simple Bayesian linear regression with it?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:stats.stackexchange.com Wiener process prior Bayesian regression non-parametric\\\", mode=rag)\", \"/math(Derive the posterior distribution for a Bayesian linear regression model assuming a Gaussian likelihood and a Wiener process prior for the weights, focusing on the resulting covariance structure.)\", \"/code(Implement a Python simulation of Bayesian linear regression with a discretized Wiener process prior using NumPy and Matplotlib, visualizing the posterior predictive distribution and its uncertainty.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on using stochastic processes like the Wiener process as priors. This informs the /math agent, which will perform the formal derivation of the posterior distribution. Finally, the /code agent will implement the derived model to provide a concrete, verifiable simulation and visualization.\", \"expected_artifacts\": [\"A PDF document containing the step-by-step mathematical derivation of the posterior distribution.\", \"A self-contained Python script (`simulation.py`) that implements the Bayesian regression model and generates the visualization.\", \"A PNG image (`posterior_predictive.png`) showing the true function, data points, posterior predictive mean, and 95% credible intervals.\", \"A `metrics.json` file containing the calculated MSE and average credible interval width.\", \"A `citations.txt` file with at least 2 relevant citations from authoritative sources.\"], \"thinking_outline\": [\"1. Research the formal definition of a Wiener process and its properties, particularly its covariance function, and how it's used as a function-space prior in Bayesian models.\", \"2. Formulate the Bayesian linear regression model, specifying the Gaussian likelihood for the observed data and the Wiener process prior over the regression weights.\", \"3. Derive the analytical form of the posterior distribution, which should also be a Gaussian process, by applying Bayes' theorem to the specified prior and likelihood.\", \"4. Implement a discrete-time simulation of the model in Python, generating synthetic data from a known function and then computing the posterior based on the derived equations.\", \"5. Validate the implementation by checking that the empirical mean and covariance of the posterior samples from the simulation align with the analytical values derived in the math step.\", \"6. Generate a plot to visualize the results, comparing the posterior predictive mean and its uncertainty bands against the true function and the noisy data points.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The blue agent's search results provide theoretical grounding for the /math agent's derivation. The resulting closed-form equations from /math are the direct inputs for the /code agent's implementation. The final code, plots, and derivation are passed to router QA for verification. Verification: Cross-check the analytical posterior mean from /math against the empirical mean from the /code output. Fallback: If the analytical derivation is incorrect or intractable, /math agent will flag it and suggest simplifying to a standard Gaussian Process prior with an RBF kernel.\", \"todo_list\": [\"- [ ] Use /general-search to gather foundational papers on Wiener process priors for Bayesian models (verify results)\", \"- [ ] Use /math to derive the posterior distribution for the specified regression model.\", \"- [ ] Use /code to implement the simulation and generate the required plot and metrics data.\", \"- [ ] Perform a verification step: Compare the analytical posterior mean from the derivation with the empirical estimates from the simulation.\", \"- [ ] Submit all artifacts for final router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"stochastic-processes\", \"bayesian-inference\", \"gaussian-processes\"], \"acceptance_criteria\": [\"- The mathematical derivation of the posterior is correct, complete, and clearly articulated.\", \"- The Python script runs without errors, is well-commented, and correctly implements the model based on the derivation.\", \"- The generated plot clearly visualizes the posterior predictive mean and 95% credible intervals, which plausibly cover the true function.\", \"- The `metrics.json` file is present and correctly formatted with the specified primary and secondary metrics.\", \"- At least two relevant academic papers are cited correctly in `citations.txt`.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as the average squared difference between the ground-truth function values and the mean of the posterior predictive distribution at a set of held-out test points.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Compute as the average width of the 95% credible intervals across the test points to quantify model uncertainty.\"}}", "categories": ["metrics_guidance"]}
{"index": 73, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a comprehensive Monte Carlo simulation of the planar surface code under a circuit-level depolarizing noise model. The goal is to numerically determine the fault-tolerance threshold by analyzing the logical error rate as a function of physical error probability for various code distances (d=3, 5, 7). The simulation must use a Minimum-Weight Perfect Matching (MWPM) decoder for syndrome processing.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org surface code threshold 'depolarizing noise' minimum weight perfect matching decoder performance\\\", mode=rag, includes cybersecurity)\", \"/math(Derive the stabilizer group generators for the planar code, define the logical X and Z operators, and formulate the mapping of syndrome differences to the MWPM graph problem, referencing the arxiv sources and stabilizer eigenvalue properties.)\", \"/code(Implement a Python simulation of the surface code using NetworkX for MWPM decoding; the simulation must track logical error rate convergence and measure its computational complexity as code distance increases.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on surface code thresholds and MWPM decoding. This informs the /math agent's formal derivation of the code's structure and decoding problem. Finally, the /code agent implements and runs the simulation based on the mathematical specification, providing the numerical evidence required.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the surface code stabilizers, logical operators, and the reduction of syndrome decoding to a graph matching problem, delivered as a LaTeX-formatted PDF.\", \"- A well-documented Python script implementing the Monte Carlo simulation, including the noise model, syndrome extraction, and MWPM decoder integration.\", \"- A JSON file containing the raw and processed data: logical error rates for each physical error rate and code distance.\", \"- A plot visualizing the logical error rate versus physical error rate, showing the threshold crossing point for different code distances.\", \"- A bibliography file in BibTeX format with citations for at least two key arXiv papers.\"], \"thinking_outline\": [\"1. Research the theoretical basis of the surface code, focusing on stabilizer formalism, logical operator definitions, and the principles of MWPM decoding under depolarizing noise.\", \"2. Formulate the precise mathematical model, including the stabilizer operators for a d x d lattice, the syndrome extraction circuit, and the construction of the matching graph from error syndromes.\", \"3. Implement the simulation framework in Python, creating data structures for the qubit lattice and methods for applying gate-level depolarizing noise and measuring stabilizers.\", \"4. Integrate a MWPM decoder (e.g., using NetworkX or a specialized library) to process the syndrome data and infer the most likely error chain.\", \"5. Validate the implementation by checking that the stabilizer operators commute and have +/-1 eigenvalues, and verify that the logical operators commute with stabilizers but anti-commute with each other.\", \"6. Run the simulation across a range of physical error rates and code distances, collect statistics on logical error rates, and verify the convergence of the Monte Carlo estimates.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The /code agent's implementation of stabilizer measurements must match the group generators derived by the /math agent. The final logical error rates must be statistically significant and show the expected threshold behavior from the literature found by /general-search; fallback: If the decoder implementation is too complex, the /code agent will revert to a simpler lookup-table decoder for a d=3 code as a baseline and flag the complexity issue to the router.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers and recent preprints on surface code thresholds and MWPM decoding.\", \"- [ ] /math: Formulate the stabilizer group, logical operators, and the syndrome-to-graph mapping for the d x d planar code.\", \"- [ ] /code: Implement the surface code lattice, the circuit-level depolarizing noise model, and syndrome measurement.\", \"- [ ] /code: Integrate the MWPM decoder and the logic for determining correction success or failure.\", \"- [ ] /math: Verify that the implemented stabilizer checks in the code align with the derived mathematical formalism.\", \"- [ ] /code: Run simulation sweeps, collect data, and generate the threshold plot and results summary.\", \"- [ ] /router: Perform final QA review of all artifacts, checking for consistency between the report, code, and plotted results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"monte-carlo-simulation\"], \"acceptance_criteria\": [\"- The submitted code must be executable and reproduce the claimed logical error rate plot within a defined statistical margin.\", \"- The final report must clearly show a threshold crossing point for logical error rates between d=3, d=5, and d=7 codes, consistent with established literature values (~10^-2 for this model).\", \"- The mathematical derivation must correctly define the X and Z stabilizers and logical operators for an arbitrary-d planar code.\", \"- The BibTeX file must contain valid entries for at least two peer-reviewed papers or arXiv preprints on the topic.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 74, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous derivation of the likelihood ratio policy gradient estimator (REINFORCE) and its extension to the Generalized Advantage Estimator (GAE). Please provide a formal proof demonstrating how GAE reduces variance without introducing bias. Subsequently, implement both algorithms in Python to empirically validate this variance reduction property on a classic control theory benchmark like `InvertedPendulum-v4`, analyzing the impact on policy entropy and convergence speed.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'High-Dimensional Continuous Control Using Generalized Advantage Estimation' 'policy gradient theorem' control theory\\\", mode=both)\", \"/math(Derive the full GAE formulation starting from the policy gradient theorem. Provide a step-by-step proof that the GAE is an unbiased estimator of the advantage function and analyze its theoretical impact on policy update convergence.)\", \"/code(Write a PyTorch implementation of a policy gradient agent with both a simple REINFORCE estimator and a GAE module. Target the `InvertedPendulum-v4` environment from Gymnasium and ensure the code tracks metrics for reward convergence and advantage estimate variance., using Python)\"], \"route_rationale\": \"The 'blue' /general-search agent first gathers foundational arXiv papers on GAE to ground the derivation. The /math agent then uses these to construct a formal derivation and proof. Finally, the /code agent implements the algorithm for empirical validation, allowing a direct comparison between the theoretical claims and observed performance.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full derivation of the REINFORCE and GAE estimators and the proof of GAE's unbiased variance reduction property.\", \"A Python script implementing both REINFORCE and GAE agents, with comments linking code to the mathematical derivation.\", \"A `results.json` file containing metrics like mean episode reward, advantage estimate variance, and policy entropy over training epochs.\", \"A summary plot (`.png`) comparing the convergence and reward variance of the two estimators.\"], \"thinking_outline\": [\"1. Begin with the fundamental policy gradient theorem, deriving the basic REINFORCE estimator and identifying its characteristic high variance.\", \"2. Introduce a state-value function as a baseline to formulate the standard advantage function, A(s, a) = Q(s, a) - V(s).\", \"3. Generalize the single-step advantage to the multi-step Generalized Advantage Estimator (GAE) using a weighted average of TD residuals, controlled by the lambda parameter.\", \"4. Formally prove that the GAE, despite its complexity, is an unbiased estimator of the true advantage function.\", \"5. Validate the implementation by instrumenting the code to capture the empirical variance of the advantage estimates per batch for both REINFORCE and GAE, expecting a significant reduction for the latter.\", \"6. Verify that the agent trained with GAE demonstrates statistically significant faster convergence and higher final rewards on the control theory benchmark, confirming the practical utility of the variance reduction.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: empirical variance reduction from code must align with theoretical claims from the math proof; fallback: if code performance doesn't improve, /math re-checks the proof for subtle assumptions and /code audits for implementation bugs like incorrect TD residual calculations or discounted reward logic).\", \"todo_list\": [\"- [ ] /general-search: Find the original GAE paper (Schulman et al., arXiv:1506.02438) and a canonical reference for the policy gradient theorem.\", \"- [ ] /math: Derive the likelihood ratio policy gradient and the GAE formulation, explicitly showing all steps for the unbiased variance reduction proof.\", \"- [ ] /code: Implement REINFORCE and GAE-based agents in Python using PyTorch, targeting the `InvertedPendulum-v4` environment.\", \"- [ ] /code: Instrument the code to log advantage estimate variance and mean episode rewards for both agents across at least 5 random seeds.\", \"- [ ] /math: Verify that the derivation correctly cancels out all bias-introducing terms when using the value function baseline.\", \"- [ ] /code: Verify implementation correctness by checking the shapes and magnitudes of tensors at each step of the GAE calculation.\", \"- [ ] /router: Final QA review of the derivation, code, and plotted results to ensure all acceptance criteria are met and the empirical results support the theory.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"policy-gradient\", \"variance-reduction\", \"control-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation must be complete, rigorous, and free of logical errors, correctly proving GAE is unbiased.\", \"- The Python implementation must correctly implement both REINFORCE and GAE, and the GAE agent must successfully solve `InvertedPendulum-v4` (achieve an average reward > 950 over 100 episodes).\", \"- The final report must show, via plots and statistical metrics from `results.json`, that the GAE implementation achieves statistically significant lower advantage variance and faster reward convergence than the baseline REINFORCE implementation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Average of total rewards over the last 100 evaluation episodes. Higher is better. The goal is to maximize this.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Variance of the advantage estimates calculated per batch during training. Lower indicates more stable policy updates and is the direct target of GAE.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 75, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDevelop a theoretical framework and a proof-of-concept implementation for a private watermarking scheme for LLMs based on asymmetric cryptography and rate-distortion theory. The goal is to embed a robust, verifiable signature into the model's output distribution with minimal KL divergence from the original, while ensuring the watermarking key's security against adversarial extraction attacks. Analyze the trade-off between watermark bitrate and model fidelity using mutual information.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org private watermarking large language models asymmetric cryptography rate-distortion theory\\\", mode=web, includes cybersecurity)\", \"/math(Derive the rate-distortion function for embedding an encrypted watermark, analyzing the information-theoretic security bounds against a Bayesian adversarial model and proving convergence properties.)\", \"/code(Implement a JAX-based logits processor for a pre-trained causal model that embeds an encrypted payload, verifying minimal KL divergence and robust watermark extraction under adversarial noise., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey existing literature on cryptographic watermarking in generative models. This informs the /math agent's task of creating a formal, novel derivation of the security and performance bounds. Finally, the /code agent implements this mathematical framework to provide an empirical proof-of-concept and validate the theoretical findings.\", \"expected_artifacts\": [\"- A formal proof in LaTeX detailing the rate-distortion-security trade-off for the proposed watermarking scheme.\", \"- A Python script implementing the watermarking encoder and decoder for a pre-trained transformer model.\", \"- A results summary (report.md) with plots showing the empirical relationship between KL divergence and watermark bit error rate.\", \"- A bibliography file with at least three citations to relevant academic papers.\", \"- A verification log documenting the consistency check between theoretical bounds and empirical results.\"], \"thinking_outline\": [\"1. Review literature on cryptographic and information-theoretic watermarking for generative models to establish a baseline.\", \"2. Formulate the watermarking problem mathematically, defining the encoder, decoder, adversarial threat model, and key metrics like KL divergence and watermark bit error rate (BER).\", \"3. Derive the theoretical trade-off between watermark bitrate, imperceptibility (KL divergence), and robustness using principles from rate-distortion theory.\", \"4. Implement the proposed watermarking scheme by modifying the output sampling logic of a small-scale generative model (e.g., GPT-2).\", \"5. Validate the implementation by empirically measuring the KL divergence and BER against a set of simulated adversarial attacks, and check that results are consistent with the theoretical bounds derived in step 3.\", \"6. Audit the code for cryptographic weaknesses and potential information leakage channels that could compromise the private key.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /math agent will formalize concepts found by /general-search. The /code agent will implement the formal specification from /math. Verification: The code's empirical results must align with the mathematical bounds. Fallback: If theoretical derivation is intractable, /math will propose a heuristic model for /code to implement and test.\", \"todo_list\": [\"- [ ] /general-search: Survey recent arXiv papers on asymmetric cryptographic watermarking and rate-distortion theory for LLMs.\", \"- [ ] /math: Formalize the watermarking scheme, defining the encoder/decoder and adversarial model based on search results.\", \"- [ ] /math: Derive the information-theoretic bounds for the bitrate-distortion-security trade-off.\", \"- [ ] /code: Implement the watermark embedding as a JAX logits processor for a pre-trained model like GPT-2.\", \"- [ ] /code: Implement the watermark extraction and BER/KL divergence evaluation metrics.\", \"- [ ] /math: Verify that the empirical results from the code align with the theoretical bounds derived earlier.\", \"- [ ] /code: Audit the implementation for potential security flaws and information leakage vectors.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"cryptography\", \"generative-ai-safety\"], \"acceptance_criteria\": [\"- A formal mathematical derivation of the rate-distortion-security trade-off is provided in a well-formatted LaTeX document.\", \"- The Python implementation correctly embeds and extracts watermarks with a bit error rate below 5% under specified noise conditions.\", \"- The mean KL divergence between watermarked and original text distributions is below a threshold of 0.05 on a held-out test set.\", \"- The final report cites at least 3 relevant papers from top-tier security or ML conferences/journals with valid arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 76, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive and implement a linearized Alternating Direction Method of Multipliers (ADMM) algorithm for the fused Lasso signal approximation problem. Can you provide the full primal-dual formulation, derive the iterative updates, analyze the theoretical convergence conditions, and then write a Python script to validate these theoretical bounds empirically on a synthetic piecewise-constant signal?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org linearized ADMM convergence fused lasso composite optimization\\\", mode=both)\", \"/math(Derive the augmented Lagrangian and linearized ADMM updates for the fused Lasso problem, analyzing the theoretical convergence rate and its computational complexity.)\", \"/code(Implement the ADMM solver in Python, validating the primal_residual_norm metric against the theoretical convergence bounds from the derivation with bootstrap confidence intervals on synthetic data.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather current literature on linearized ADMM convergence for composite objective functions. This informs the /math agent's formal derivation of the algorithm and its theoretical properties. Finally, the /code agent implements the algorithm to provide rigorous empirical validation of the mathematical findings.\", \"expected_artifacts\": [\"A formal mathematical derivation in LaTeX of the linearized ADMM updates for the fused Lasso and its convergence analysis.\", \"A documented Python script implementing the solver and a simulation environment with synthetic data generation.\", \"A final report (PDF) summarizing the theory and presenting plots that compare the empirical convergence rate against the theoretical bounds.\", \"A bibliography citing at least two relevant arXiv papers on ADMM and its variants.\", \"A JSON file containing the final primal and dual residual norms after convergence.\"], \"thinking_outline\": [\"1. Formulate the fused Lasso signal denoising problem as a consensus optimization problem with a composite objective, identifying the smooth and non-smooth parts.\", \"2. Construct the augmented Lagrangian for this formulation and derive the iterative update steps for each variable (primal and dual) under a linearized ADMM scheme.\", \"3. Analyze the conditions for convergence, deriving a theoretical rate based on the Lipschitz constant of the gradient and the penalty parameter \u03c1.\", \"4. Implement the complete algorithm in Python, including the specific proximal operators for the L1-norm and the total variation norm.\", \"5. Validate that the Python implementation of the proximal operators is numerically correct and matches the derived mathematical forms.\", \"6. Check that the empirical convergence rate observed on synthetic data falls within the theoretically predicted bounds.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. Verification: The /math agent uses findings from /general-search to ground the derivation. The /code agent implements the exact algorithm from /math and validates its theoretical convergence claims. Fallback: If the empirical convergence in /code deviates significantly from the theory, the task is passed back to /math to re-evaluate the assumptions in the convergence proof.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on linearized ADMM and its convergence analysis for problems like fused Lasso from arXiv.\", \"- [ ] /math: Derive the full primal-dual formulation, augmented Lagrangian, and linearized ADMM updates.\", \"- [ ] /math: Formulate and prove the theoretical convergence rate based on the problem parameters.\", \"- [ ] /code: Implement the derived algorithm in Python, focusing on an efficient implementation of the proximal operators.\", \"- [ ] /code: Generate synthetic piecewise-constant signals and run the solver, logging primal and dual residuals per iteration.\", \"- [ ] /math: Verify that the proximal operator implementations in the Python code are correct translations of the mathematical derivation.\", \"- [ ] /code: Validate that the plotted empirical convergence rate aligns with the theoretical bounds from the /math agent's analysis.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"convex-optimization\", \"admm\", \"signal-processing\"], \"acceptance_criteria\": [\"The mathematical derivation of the linearized ADMM updates and convergence proof must be complete and self-contained.\", \"The Python implementation must successfully converge on a synthetic fused Lasso problem, reducing the primal and dual residuals below a tolerance of 1e-6.\", \"The final report must include a semilogy plot showing the primal residual norm versus iteration, and this empirical rate must be consistent with the derived theoretical rate.\", \"The solution must cite at least two relevant arXiv pre-prints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 77, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm new to computational chemistry and want to understand how a Graph Neural Network can learn an interatomic potential energy surface. Can you derive the basic energy-conserving message passing update rules and show a simple Python simulation of argon atoms to verify it?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org graph neural network interatomic potential energy chemistry\\\", mode=web)\", \"/math(Derive the force F_i = -\u2207_i U(R) for a simple message-passing GNN, treating the graph as a representation of molecular structure and ensuring the formulation is equivariant to translation and rotation., includes cybersecurity)\", \"/code(Implement a basic GNN potential in Python for argon atoms using a simple Velocity Verlet integrator; verify energy conservation and check the system's stability, which relates to the Hessian eigenvalue properties.)\"], \"route_rationale\": \"The blue general-search agent first finds foundational literature on GNN potentials. Then, /math is used to derive the core physics principle: calculating forces as the negative gradient of the GNN's potential energy output. Finally, /code implements this derivation in a practical molecular dynamics simulation to provide empirical validation of the theory.\", \"expected_artifacts\": [\"A markdown document with the mathematical derivation of forces from the GNN potential.\", \"A Python script (`simulation.py`) containing the GNN model and the molecular dynamics simulation loop.\", \"A plot (`energy_conservation.png`) showing the total, kinetic, and potential energy over time, demonstrating conservation.\", \"A list of at least two cited arXiv papers with their IDs.\", \"A `metrics.json` file reporting the total energy drift.\"], \"thinking_outline\": [\"1. Research common, introductory Graph Neural Network architectures used for learning interatomic potentials, such as SchNet or PaiNN.\", \"2. Formulate the GNN's output as a total potential energy function U(R), where R is the set of all atomic coordinates in the system.\", \"3. Mathematically derive the force F_i on each atom i as the negative gradient of the potential energy with respect to its coordinates, F_i = -\u2207_{r_i} U(R).\", \"4. Implement this GNN potential and its analytical force calculation in Python using an automatic differentiation library like PyTorch.\", \"5. Use the implemented forces to run a microcanonical (NVE) ensemble simulation with a Velocity Verlet integrator and validate that the total energy of the system is conserved over time.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses these papers to formulate the potential and derive the force equations -> /code implements the mathematical model and runs a simulation. The final results are passed to router QA. Verification: The energy conservation plot from /code must show minimal drift, validating the /math derivation. Fallback: If energy is not conserved, revisit the /math derivation for errors or debug the integrator implementation in the /code step.\", \"todo_list\": [\"- [ ] Use /general-search to find introductory papers on GNNs for interatomic potentials like SchNet.\", \"- [ ] Use /math to derive the force calculation as the negative gradient of the GNN potential energy function.\", \"- [ ] Use /code to implement the GNN potential, the force calculation, and a simple Velocity Verlet integrator to simulate a small system of argon atoms.\", \"- [ ] In the /code step, generate a plot of total energy vs. simulation time to verify energy conservation.\", \"- [ ] Await router QA for final approval on the correctness of the derivation and the energy conservation results.\"], \"difficulty\": \"introductory\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"computational-chemistry\"], \"acceptance_criteria\": [\"The mathematical derivation correctly shows force as the negative gradient of the GNN potential energy.\", \"The Python code successfully implements a GNN potential and a Velocity Verlet integrator.\", \"The simulation output plot demonstrates total energy conservation with a numerical drift of less than 0.1% over the simulation period.\", \"At least 2 relevant papers from arXiv are cited in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as `100 * |E_final - E_initial| / |E_initial|`. This measures the stability and correctness of the physics integration.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the wall-clock time required to compute forces for all atoms in a single simulation step. This is a key performance indicator for MD simulations.\"}}", "categories": ["metrics_guidance"]}
{"index": 78, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of dictionary learning algorithms for sparse representation in compressed sensing. Specifically, compare the convergence properties and computational complexity of K-SVD and the Method of Optimal Directions (MOD). Derive the update rules for an ADMM-based solver for the LASSO problem with an L1-norm penalty, and then implement this solver in Python to reconstruct a sparse signal from noisy, undersampled measurements. The analysis must include a literature review on recent convergence guarantees for non-convex dictionary learning problems, particularly those involving incoherence properties of the learned dictionary and its relation to the Restricted Isometry Property (RIP).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org convergence guarantees non-convex dictionary learning ADMM K-SVD complexity\\\", mode=rag)\", \"/math(Derive the update rules for an Alternating Direction Method of Multipliers (ADMM) solver for the L1-regularized sparse coding problem, analyzing the convergence criteria and referencing the Lagrange multiplier updates.)\", \"/code(Implement the derived ADMM solver in Python using JAX to reconstruct a sparse signal. Evaluate performance using Signal-to-Noise Ratio (SNR) and verify the sparsity level of the solution against the L1-norm penalty.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather state-of-the-art literature on convergence guarantees for dictionary learning, which is critical for the theoretical analysis. Next, /math is used for the formal derivation of the ADMM optimization algorithm. Finally, /code implements the derived algorithm for empirical validation and performance measurement, directly translating the mathematical formulation into a working model.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the ADMM update rules for the dictionary learning problem.\", \"A Python script implementing the ADMM solver, the synthetic data generation process, and the signal reconstruction experiment.\", \"A summary report in Markdown comparing K-SVD and ADMM, integrating theoretical findings from the literature review with empirical results, including at least two arXiv citations.\", \"A JSON file containing the final reconstruction SNR, dictionary coherence, and iteration-level convergence metrics.\", \"A verification log confirming numerical stability and alignment between the mathematical derivation and the code's behavior.\"], \"thinking_outline\": [\"1. Review literature on dictionary learning (K-SVD, MOD) and ADMM for sparse coding to understand the state-of-the-art convergence guarantees and complexity.\", \"2. Formulate the dictionary learning problem as a constrained optimization problem (LASSO objective) and derive the scaled-form ADMM update equations for the sparse code and the dictionary atoms.\", \"3. Implement the ADMM algorithm and a synthetic data generation process for a sparse signal and a sensing matrix that satisfies the Restricted Isometry Property (RIP).\", \"4. Validate the implementation by checking that the primal and dual residuals converge to zero and verifying that the final sparse code satisfies the KKT conditions of the original problem.\", \"5. Execute experiments to measure the reconstruction SNR and compare the empirical convergence rate against theoretical bounds found in the literature.\", \"6. Audit the final report to ensure the mathematical derivations, literature citations, and empirical results are consistent and correctly presented.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation & evaluation) -> router QA (final review). Verification: /math checks if the derivation from /general-search papers is sound; /code's outputs are checked against /math's theoretical predictions. Fallback: If convergence guarantees cannot be found for the exact formulation, the plan will revert to analyzing standard LASSO convergence and note the discrepancy.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on K-SVD and recent (post-2020) arXiv preprints on ADMM convergence for non-convex dictionary learning.\", \"- [ ] /math: Derive the complete ADMM update steps for the L1-regularized sparse coding problem, including the primal and dual variable updates.\", \"- [ ] /code: Implement the ADMM solver in Python, including a test case with a known ground-truth sparse signal and dictionary.\", \"- [ ] /code: Add metric tracking for reconstruction SNR and primal/dual residuals per iteration.\", \"- [ ] /math: Verify that the implemented ADMM residuals converge to a value close to zero, confirming the implementation aligns with the derived convergence criteria.\", \"- [ ] /code: Generate plots comparing reconstruction quality versus the sparsity penalty parameter lambda.\", \"- [ ] router: QA review of the final report, checking for consistency between the derivation, code, and cited literature.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"compressed-sensing\", \"dictionary-learning\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ADMM updates must be complete, correct, and self-contained.\", \"- The Python implementation must successfully reconstruct a synthetic K-sparse signal with a Signal-to-Noise Ratio (SNR) exceeding 20 dB.\", \"- The final report must cite at least two relevant academic papers with arXiv IDs, comparing theoretical convergence with empirical results.\", \"- The code must include unit tests for the core solver components and pass static analysis checks.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 79, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on a problem from an IOI training camp that involves finding a minimum-weight k-dominating set on a rooted tree, but with an additional constraint: no two nodes in the dominating set can be on a path of length greater than L. I need a full solution, including the formal dynamic programming recurrence, a proof of its optimality and time complexity, and a highly optimized Python implementation that can pass test cases with N up to 2*10^5. Can you also research related techniques like tree decomposition to see if this approach generalizes to graphs with low treewidth, providing relevant arXiv citations?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org OR site:cs.princeton.edu tree dynamic programming k-dominating set path constraints treewidth complexity\\\", mode=web)\", \"/math(Formulate the DP states and recurrence for the constrained k-dominating set, proving optimality and deriving the time complexity. The formulation should be robust enough to consider a potential Bayesian prior on node weights.)\", \"/code(Implement the derived tree DP recurrence in Python using memoization and an adjacency list representation. The code must include runtime validation for tree properties and complexity constraints, ensuring it passes large-scale test cases.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey existing academic literature on advanced tree DP algorithms and their generalization via tree decomposition. The findings inform the /math agent, which will construct the formal proof and DP recurrence. Finally, the /code agent translates the verified mathematical model into an efficient, production-quality implementation.\", \"expected_artifacts\": [\"- A formal PDF document detailing the dynamic programming states, recurrence relations, and proofs of correctness and complexity.\", \"- A well-commented Python script implementing the tree DP solution.\", \"- A markdown file with benchmark results on various tree structures and sizes up to N=2*10^5.\", \"- A bibliography with citations to at least two relevant arXiv preprints or peer-reviewed papers.\"], \"thinking_outline\": [\"1. Deconstruct the problem: Identify the core subproblems at each node of the tree, considering its state (dominator, dominated, etc.) and the state of its children.\", \"2. Research precedents: Use search to find canonical solutions for k-dominating set on trees and related problems to inform the structure of the DP states.\", \"3. Formulate the DP State: Define the DP state tuple, which must capture not only the domination status but also information about path length constraints relative to the nearest dominator ancestor.\", \"4. Derive Recurrence and Prove Correctness: Formulate the transitions for combining solutions from children subtrees and provide a proof by induction on the height of the tree.\", \"5. Validate Complexity: Analyze the time and space complexity of the DP formulation to verify it meets the stringent requirements (e.g., O(N * L^2) or similar) for competitive programming.\", \"6. Audit Implementation: Cross-reference the Python code against the mathematical recurrence to ensure a faithful and bug-free implementation, particularly for the base cases and state transitions.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /math agent will verify that the surveyed literature from /general-search is applicable. The /code agent's implementation will be audited against the formal derivation from /math. Fallback: If the derived complexity is too high, the /math agent will request a new search from /general-search for approximation algorithms or more advanced techniques.\", \"todo_list\": [\"- [ ] /general-search: Find relevant literature on k-dominating set on trees and tree decomposition.\", \"- [ ] /math: Define the DP states, incorporating node status and path length constraints.\", \"- [ ] /math: Formulate the recurrence relations for combining solutions from child subtrees.\", \"- [ ] /math: Provide a formal proof of optimality and derive the time/space complexity.\", \"- [ ] /code: Implement the tree DP algorithm in Python, using recursion with memoization.\", \"- [ ] /code: Develop a comprehensive test suite with diverse tree structures (lines, stars, random) to validate correctness.\", \"- [ ] /math: Verify that the code's logic is a direct and correct translation of the mathematical recurrence.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"dynamic-programming\", \"graph-theory\", \"tree-algorithms\"], \"acceptance_criteria\": [\"- The provided mathematical proof of the DP recurrence and its complexity analysis must be rigorous and correct.\", \"- The Python implementation must pass all test cases, including edge cases and large inputs (N=2*10^5), within a 2-second time limit per case.\", \"- The final report must cite at least two relevant academic papers or authoritative blog posts (e.g., from Codeforces) with proper attribution.\", \"- The analysis of generalizations to low-treewidth graphs must be sound and supported by the cited literature.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 80, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating posterior inference for continuous-time generative models parameterized by Neural Stochastic Differential Equations (SDEs). Specifically, I need to derive and implement a variational inference scheme using a pathwise SDE solver like Euler-Maruyama. The goal is to approximate the posterior over latent paths. Can you formalize the Evidence Lower Bound (ELBO) for this setup, detail the reparameterization trick for SDEs to get low-variance gradients, and implement a proof-of-concept in JAX that demonstrates convergence on a synthetic dataset, like the double-well potential?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference neural stochastic differential equations bayesian posterior reparameterization\\\", mode=both)\", \"/math(Derive the ELBO for a Bayesian Neural SDE, specifying the prior, variational posterior, and likelihood terms. Detail the stochastic adjoint method for computing gradients and ensuring convergence.)\", \"/code(Implement the Bayesian Neural SDE in JAX with Equinox, using the Euler-Maruyama solver. Train the model via variational inference on a double-well potential dataset, tracking ELBO convergence and posterior uncertainty., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey recent arXiv literature on variational inference for Neural SDEs, as the field is rapidly evolving. The findings inform the /math agent, which will perform the formal derivation of the ELBO and the gradient estimation strategy. Finally, the /code agent implements this specific mathematical framework, ensuring the theory is correctly translated into a verifiable, convergent model.\", \"expected_artifacts\": [\"- A LaTeX/PDF document containing the full mathematical derivation of the variational objective for Neural SDEs.\", \"- A well-commented JAX/Equinox Python script implementing the model, training loop, and data generation.\", \"- A summary report with plots showing the learned vector field, ELBO convergence over epochs, and samples from the generative model.\", \"- A bibliography with at least two cited arXiv papers on Neural SDEs or related variational inference techniques.\"], \"thinking_outline\": [\"1. Research current state-of-the-art methods for variational inference in continuous-time Bayesian models, focusing on Neural SDEs and the reparameterization of stochastic processes.\", \"2. Formulate the specific Neural SDE model, defining the drift and diffusion neural networks, the prior SDE, and the variational posterior SDE.\", \"3. Derive the Evidence Lower Bound (ELBO) objective, carefully handling the change of measure via Girsanov's theorem and the path-integral nature of the KL divergence.\", \"4. Verify the derived ELBO and its gradients by cross-referencing with established literature found in step 1 to ensure correctness before implementation.\", \"5. Implement the model and training algorithm, paying close attention to numerical stability of the SDE solver and gradient variance.\", \"6. Validate the implementation by checking for ELBO convergence on a synthetic problem and visually inspecting the quality of generated samples against the true data distribution.\"], \"handoff_plan\": \"/general-search (arXiv papers) -> /math (derivation) -> /code (implementation) -> router QA (verification: ELBO derivation aligns with implementation's loss function, generated samples match target distribution; fallback: if ELBO diverges, /code agent must revisit solver stability or gradient estimation and consult /math agent).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 key arXiv papers on variational inference for Neural SDEs.\", \"- [ ] /math: Derive the ELBO for the specified Bayesian Neural SDE generative model.\", \"- [ ] /math: Detail the application of the stochastic adjoint method or reparameterization trick for gradient computation.\", \"- [ ] /code: Implement the SDE solver, neural networks for drift/diffusion, and the variational training loop in JAX.\", \"- [ ] /code: Generate a synthetic dataset (e.g., double-well potential) and train the model.\", \"- [ ] /math: Verify that the loss function implemented by /code correctly represents the derived ELBO.\", \"- [ ] /code: Generate plots for ELBO convergence and samples from the trained model.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-differential-equations\", \"generative-modeling\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO must be complete and consistent with Girsanov's theorem for SDEs.\", \"- The implemented code must successfully train, demonstrated by a consistently non-decreasing ELBO curve on the synthetic dataset.\", \"- The final report must include plots showing that the generative model can produce samples that qualitatively match the target distribution's modes.\", \"- The solution must cite at least two relevant academic papers from arXiv.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 81, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the information bottleneck objective for a single self-attention head in a transformer, framing it as a constrained optimization problem. Specifically, prove that minimizing the mutual information I(X; Z) while maximizing I(Z; Y) \u2014 where Z is the attention output \u2014 is related to the spectral decay of the value-key covariance matrix. Assume a Bayesian framework for the input and output embeddings and analyze the convergence properties under these assumptions.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org information bottleneck transformer attention spectral decay bayesian deep learning\\\", mode=rag)\", \"/math(Derive the information bottleneck Lagrangian for a self-attention head, incorporating spectral norm regularization on the value-key covariance matrix. Analyze the Bayesian convergence conditions and establish the theoretical link to spectral decay.)\", \"/code(Implement a symbolic verification of the derived Karush-Kuhn-Tucker conditions in Python using SymPy. Numerically validate the spectral decay hypothesis, tracking the eigenvalue distribution as a primary metric.)\"], \"route_rationale\": \"The plan starts with the 'blue' /general-search agent to gather foundational literature on information bottleneck theory in transformers, ensuring the subsequent theoretical work is well-grounded. The /math agent is then tasked with the core theoretical derivation and proof, which is the central part of the user's query. Finally, the /code agent provides a crucial verification step by implementing the derived conditions symbolically and running a numerical experiment to validate the theoretical claims, ensuring correctness and providing empirical support.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation and proof.\", \"A Python script (Jupyter notebook) with the SymPy symbolic verification and the numerical validation experiment.\", \"A summary report in Markdown citing at least two relevant arXiv papers.\", \"A JSON file containing the eigenvalue distributions from the numerical validation.\", \"A verification log detailing the alignment between the mathematical proof and the symbolic code.\"], \"thinking_outline\": [\"1. Formulate the self-attention mechanism within an information-theoretic framework, defining the random variables for input (X), attention output (Z), and target (Y).\", \"2. Construct the Information Bottleneck Lagrangian L = I(X; Z) - \u03b2 * I(Z; Y) for the attention head.\", \"3. Express the mutual information terms using KL-divergence and relate the attention output's conditional entropy to the spectral properties of the value-key covariance matrix.\", \"4. Verify the derivation by checking for consistency with established information theory principles and spectral analysis results from the literature search.\", \"5. Derive the optimality conditions (e.g., gradients of the Lagrangian) and prove the relationship between the \u03b2 parameter and the effective rank or spectral decay of the covariance matrix.\", \"6. Validate the proof's key steps and final conclusion using a symbolic computation library like SymPy to audit the algebraic manipulations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent will use the arXiv papers from search to ground its derivation. The code agent will implement the exact equations derived by the math agent. The final code output and LaTeX proof will be reviewed for consistency. fallback: If the derivation is inconsistent with the literature, the math agent will flag it and request a refined query for /general-search.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on information bottleneck in transformers and spectral analysis of attention matrices from arXiv.\", \"- [ ] /math: Formulate the attention mechanism in probabilistic terms and derive the information bottleneck Lagrangian.\", \"- [ ] /math: Prove the connection between the Lagrangian's minimizer and the spectral properties of the value-key covariance matrix.\", \"- [ ] /code: Implement the derived optimality conditions using SymPy for symbolic verification.\", \"- [ ] /code: Develop a numerical simulation in PyTorch/JAX to demonstrate the spectral decay phenomenon predicted by the theory.\", \"- [ ] /math: Verify that the symbolic implementation in the code artifact correctly represents the derived mathematical expressions.\", \"- [ ] /code: Generate plots of eigenvalue distributions and document the validation results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"attention-mechanisms\", \"self-supervised-learning\"], \"acceptance_criteria\": [\"The provided LaTeX document must contain a complete, step-by-step mathematical proof connecting the information bottleneck objective to the spectral decay of the value-key matrix.\", \"The Python script must successfully run and reproduce the symbolic verification and numerical validation plots.\", \"The final report must cite at least two relevant arXiv papers with IDs, and the claims must be supported by these citations.\", \"The results of the symbolic verification must match the manually derived equations in the proof.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 83, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working with high-dimensional point clouds sampled from non-isometric Riemannian manifolds. I need to develop a computationally tractable and provably stable metric to quantify their dissimilarity. How can I leverage persistent homology, specifically the stability of persistence diagrams under the bottleneck distance, to formulate a metric that is robust to sampling noise and related to the manifold's intrinsic geometry, such as its spectral properties or Gromov-Hausdorff distance?\n", "completion": "{\"route_plan\": [\"/general-search(Search for foundational arXiv papers on persistent homology stability theorems, focusing on their connections to Gromov-Hausdorff distance and spectral convergence properties of the connection Laplacian., mode=both)\", \"/math(Formally derive a stable metric for manifold comparison using the bottleneck distance on persistence diagrams, proving its Lipschitz continuity with respect to the underlying Gromov-Hausdorff distance and analyzing its computational complexity and eigenvalue dependencies.)\", \"/code(Implement the Vietoris-Rips filtration and the derived persistent homology metric in Python using JAX for potential CUDA acceleration; include bootstrap methods for confidence interval estimation on the computed distances.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art theoretical foundations. These papers are then handed to /math for rigorous derivation and proof construction. Finally, /code is used to empirically validate the theoretical findings through a concrete implementation and numerical experiments.\", \"expected_artifacts\": [\"- A formal mathematical proof of the proposed metric's stability, delivered as a LaTeX document.\", \"- A Python implementation capable of computing persistent homology and the derived metric for point cloud data.\", \"- A summary report including citations to at least 3 relevant academic papers with their arXiv IDs.\", \"- A verification document comparing empirical convergence results from the code with the theoretical complexity analysis.\"], \"thinking_outline\": [\"1. Conduct a literature survey to identify key theorems relating the stability of persistence diagrams (bottleneck distance) to the stability of the underlying metric spaces (Gromov-Hausdorff distance).\", \"2. Formulate a metric for manifold dissimilarity based on the persistence diagrams of Vietoris-Rips filtrations built on point cloud samples.\", \"3. Formally prove the stability of this metric, establishing explicit bounds that depend on geometric and sampling parameters.\", \"4. Validate the theoretical proof by implementing the metric and testing its robustness to noise and varying sample densities on synthetic manifolds (e.g., tori, spheres).\", \"5. Analyze the computational complexity of the algorithm and verify that the implementation's performance scaling matches the theoretical prediction.\", \"6. Audit the final report to ensure the mathematical derivations, code implementation, and empirical results are consistent and correctly interpreted.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /math agent will verify that theorems found by /general-search are applicable to the problem's constraints; fallback is to broaden the search query. The /code agent will test its output against simple, known topological spaces (e.g., a circle) to verify correctness with the /math derivation; fallback is to consult /math to debug the logic. Router QA performs a final consistency check across all artifacts.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on TDA stability theorems and their relation to Gromov-Hausdorff distance.\", \"- [ ] /math: Formulate the metric and derive its stability proof with explicit Lipschitz constants.\", \"- [ ] /math: Analyze the computational complexity of the proposed algorithm.\", \"- [ ] /code: Implement the Vietoris-Rips complex construction and bottleneck distance calculation.\", \"- [ ] /code: Generate synthetic manifold data and run numerical experiments to test stability and convergence.\", \"- [ ] /math: Verify that the empirical convergence rates from the /code agent align with the derived complexity bounds.\", \"- [ ] /code: Validate the implementation by ensuring the calculated Betti numbers for simple shapes (e.g., sphere, torus) are correct.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"computational-topology\"], \"acceptance_criteria\": [\"- A formal proof of the metric's stability is provided and is mathematically rigorous.\", \"- The Python implementation correctly computes the persistent homology and the derived metric for at least two distinct manifold types (e.g., sphere S^2, torus T^2).\", \"- The final report includes a plot demonstrating the metric's convergence as sample size increases, with results matching theoretical expectations within a 5% tolerance.\", \"- All claims are supported by citations to at least 3 relevant arXiv preprints or peer-reviewed papers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 84, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the second-order Taylor approximation for the bilevel optimization objective in Differentiable Architecture Search (DARTS). Please analyze the Hessian of the validation loss with respect to the architecture parameters (alphas) and connect its eigenvalue spectrum to the performance collapse problem. Can we formulate a regularization term based on tensor calculus to mitigate this, ensuring a smoother loss landscape and better AutoML convergence?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org differentiable architecture search DARTS Hessian eigenvalue performance collapse bilevel optimization AutoML\\\", mode=web)\", \"/math(Derive the Hessian of the validation loss with respect to architecture parameters in a DARTS-like bilevel AutoML problem, incorporating tensor calculus notation and analyzing the conditions for convergence.)\", \"/code(Implement a JAX model to compute the Hessian eigenvalue spectrum for a toy DARTS search space, validating the AutoML convergence properties and tracking the Frobenius norm of the Hessian., using Python)\"], \"route_rationale\": \"The blue general-search agent will first gather foundational papers on DARTS and Hessian analysis. The /math agent will then perform the formal derivation required. Finally, the /code agent will implement a numerical experiment to validate the theoretical findings from /math, providing empirical evidence.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the Hessian for the DARTS objective.\", \"A Python script (JAX/PyTorch) that numerically computes and plots the Hessian eigenvalue distribution for a toy problem.\", \"A summary report linking the theoretical derivation to the numerical results, with citations to at least two relevant arXiv papers.\", \"A verification log confirming that the code's output aligns with the mathematical derivation's predictions.\"], \"thinking_outline\": [\"1. Formulate the bilevel optimization problem of DARTS, clearly defining the inner (weight training) and outer (architecture search) loops.\", \"2. Apply the implicit function theorem to derive the gradient of the validation loss with respect to the architecture parameters, leading to the Hessian-vector product calculation.\", \"3. Derive the full Hessian matrix using tensor calculus, expressing it in terms of second-order derivatives of both training and validation losses.\", \"4. Analyze the derived Hessian, hypothesizing how dominant positive eigenvalues could lead to sharp minima and performance collapse.\", \"5. Validate the mathematical derivation by implementing a numerical approximation in code and comparing the eigenvalue spectrum against theoretical predictions for a simplified search space.\", \"6. Propose and check a regularization term based on the Hessian's trace or spectral norm to promote smoother loss landscapes.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent verifies the code's numerical Hessian matches the symbolic form; /code agent validates math derivation on a toy problem; fallback: if derivation and code conflict, revert to /math for re-derivation with simplified assumptions.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on DARTS, specifically focusing on its optimization instability and Hessian analysis (e.g., from ICLR, NeurIPS).\", \"- [ ] /math: Formulate the bilevel optimization problem and derive the exact Hessian of the validation loss w.r.t. architecture parameters.\", \"- [ ] /math: Analyze the structure of the Hessian and its eigenvalue properties, connecting them to the landscape geometry and convergence issues.\", \"- [ ] /code: Implement a toy search space in JAX to compute the Hessian and its eigenvalue distribution numerically.\", \"- [ ] /math: Verify that the symbolic derivation from the math agent is consistent with the numerical results from the code agent for a base case.\", \"- [ ] /code: Validate the implementation by checking gradient and Hessian computations using finite differences or automatic differentiation tools.\", \"- [ ] /router: Perform final QA check, ensuring all artifacts are consistent and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-architecture-search\", \"autoML\", \"optimization-theory\", \"hessian-analysis\"], \"acceptance_criteria\": [\"The mathematical derivation of the Hessian must be complete and self-contained.\", \"The code must successfully compute and visualize the eigenvalue spectrum of the Hessian for the defined toy problem.\", \"The final report must correctly link the dominant eigenvalues in the numerical experiment to the performance collapse phenomenon described in the cited literature.\", \"All derivations and code must be accompanied by explanatory comments and references.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 85, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on a GPU-accelerated fluid dynamics simulation that requires solving large, sparse, symmetric positive-definite linear systems. I need to implement a Preconditioned Conjugate Gradient (PCG) solver using CUDA. Please provide a detailed analysis and implementation comparing the convergence properties of a simple Jacobi preconditioner versus an Incomplete Cholesky factorization (IC0) preconditioner. The analysis should focus on how each preconditioner affects the spectral properties (eigenvalue distribution) of the system matrix and, consequently, the convergence rate of the Krylov subspace method.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org preconditioned conjugate gradient CUDA incomplete cholesky vs jacobi convergence analysis\\\", mode=web)\", \"/math(Derive the Preconditioned Conjugate Gradient algorithm and formally analyze how Jacobi and Incomplete Cholesky preconditioners are expected to cluster the eigenvalue spectrum of the system matrix, thereby accelerating convergence.)\", \"/code(Implement the PCG solver in Python using CuPy for CUDA acceleration, including modules for both Jacobi and IC0 preconditioning. The implementation must track residual norm per iteration to measure convergence.)\", \"/math(Verify that the empirical convergence rates observed in the code for a representative sparse SPD matrix align with the theoretical improvements predicted by the eigenvalue analysis of the preconditioned systems.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to ground the work in authoritative literature on GPU-based iterative solvers. This informs the theoretical work in /math, where the core algorithms and convergence theory are established. The /code tool then translates theory into a practical, GPU-accelerated implementation. A final /math step is crucial for verification, ensuring the empirical results from the code quantitatively match the mathematical predictions.\", \"expected_artifacts\": [\"A formal mathematical derivation of the PCG algorithm and an analysis of the preconditioners' effects on the system's condition number and eigenvalue distribution.\", \"A documented Python source file implementing the PCG solver and both preconditioners using the CuPy library for GPU execution.\", \"A performance report with plots comparing the convergence (residual norm vs. iteration) of standard CG, Jacobi-PCG, and IC0-PCG.\", \"A list of at least two cited arXiv pre-prints or peer-reviewed papers on the topic.\", \"A verification summary confirming the correspondence between theoretical convergence rate predictions and empirical results.\"], \"thinking_outline\": [\"1. Research established literature on GPU implementations of the Preconditioned Conjugate Gradient method, focusing on analyses of Jacobi and Incomplete Cholesky preconditioners for sparse systems.\", \"2. Formulate the complete mathematical derivation for the PCG algorithm and provide a theoretical analysis of how each preconditioner should improve the condition number and cluster the eigenvalues of the system matrix.\", \"3. Implement the solver and preconditioners using Python with CuPy, ensuring the code is vectorized for efficient execution on CUDA-enabled hardware.\", \"4. Design and execute a numerical experiment using a canonical sparse SPD matrix (e.g., from a 2D Poisson equation discretization) to benchmark the convergence of each solver variant.\", \"5. Validate that the observed reduction in iterations for the preconditioned methods is consistent with the theoretical speedup predicted by the spectral analysis.\", \"6. Audit the numerical accuracy of the final solution vector against a known ground truth or a high-precision reference solver for a small-scale problem.\"], \"handoff_plan\": \"/general-search -> /math (formulate theory) -> /code (implement & test) -> /math (verify results against theory) -> router QA (final review). Verification: The final /math step checks if the empirical convergence from /code matches the spectral analysis from the first /math step. Fallback: If verification fails, /code will be tasked to debug the preconditioner implementation, and /math will re-check the theoretical analysis for potential discrepancies.\", \"todo_list\": [\"- [ ] /general-search: Curate 2-3 seminal papers or tutorials on GPU-accelerated PCG with a focus on sparse matrix preconditioning.\", \"- [ ] /math: Derive the PCG algorithm and write the theoretical analysis of how Jacobi and IC0 preconditioners affect the eigenvalue spectrum.\", \"- [ ] /code: Implement the PCG solver in Python with CuPy, including functions for applying the Jacobi and Incomplete Cholesky preconditioners.\", \"- [ ] /code: Create a test harness that generates a sparse SPD matrix from a finite difference discretization and runs all three solver variants (CG, J-PCG, IC0-PCG).\", \"- [ ] /math: Analyze the output from the code to verify that the empirical convergence rates align with the theoretical predictions based on condition number improvement.\", \"- [ ] /code: Add a unit test to verify the solver's correctness against SciPy's sparse solver for a small matrix on the CPU.\", \"- [ ] /router: Perform a final QA review, ensuring all artifacts are consistent and the acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"hpc\"], \"acceptance_criteria\": [\"- The submitted code must implement the PCG solver with both Jacobi and IC0 preconditioners and run successfully on a CUDA-enabled GPU.\", \"- The final report must contain a plot clearly demonstrating that IC0-PCG converges in significantly fewer iterations than Jacobi-PCG, which in turn is faster than standard CG.\", \"- The mathematical analysis must correctly link the observed convergence improvements to the theoretical effect of each preconditioner on the system's eigenvalue distribution.\", \"- The solution must cite at least two relevant academic sources with their arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 86, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a high-performance solver for large-scale, sparse symmetric positive-definite (SPD) linear systems arising from finite element method (FEM) simulations. Please implement a Preconditioned Conjugate Gradient (PCG) algorithm using a GPU-accelerated Incomplete Cholesky (IC) factorization as the preconditioner. Provide a JAX implementation that leverages CUDA, benchmark its convergence and wall-clock time against the standard Conjugate Gradient method, and analyze the impact of the preconditioner on the system's eigenvalue distribution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org GPU parallel incomplete cholesky factorization conjugate gradient CUDA\\\", mode=both)\", \"/math(Derive the Preconditioned Conjugate Gradient algorithm for a sparse SPD matrix, analyzing the preconditioner's effect on the Hessian's eigenvalue spectrum and proving convergence rate improvements.)\", \"/code(Implement the PCG solver in JAX with CUDA acceleration, using CSR format for sparse matrices. Profile the sparse matrix-vector product (SpMV) kernel and track convergence against a residual norm tolerance., using Python)\"], \"route_rationale\": \"First, the blue /general-search agent will find state-of-the-art literature on parallel preconditioning techniques suitable for GPUs. Next, /math will use this research to formalize the algorithm and its theoretical convergence properties. Finally, /code will implement and benchmark the derived algorithm, providing empirical validation.\", \"expected_artifacts\": [\"A PDF report containing the mathematical derivation of the PCG algorithm and analysis of the preconditioner.\", \"A Python script (`pcg_solver.py`) with the JAX implementation of the solver.\", \"A performance plot (`convergence_benchmark.png`) comparing PCG and standard CG in terms of iterations and wall-clock time.\", \"A `metrics.json` file containing the final performance numbers (wall time, GFLOPS).\", \"A `citations.bib` file with at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research parallel preconditioning techniques, such as level-scheduling or graph coloring approaches for Incomplete Cholesky factorization, suitable for GPU architectures.\", \"2. Formally derive the Preconditioned Conjugate Gradient algorithm, detailing each step and analyzing how the preconditioner transforms the eigenvalue distribution of the system matrix A to improve the condition number.\", \"3. Implement the solver in JAX, paying close attention to efficient sparse matrix representations (e.g., CSR/COO) and GPU-accelerated linear algebra operations (SpMV, dot products).\", \"4. Verify the implementation's correctness by testing it on a smaller, well-conditioned matrix with a known solution and comparing the result to a trusted library like SciPy's sparse solvers.\", \"5. Benchmark the solver's performance on large-scale synthetic SPD matrices, measuring wall-clock time to convergence and achieved GFLOPS.\", \"6. Validate that the observed convergence rate improvement aligns with the theoretical expectations based on the mathematical analysis from step 2.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. Search results on parallel IC factorization inform the mathematical derivation. The formal derivation from /math provides the precise algorithmic steps for the /code implementation. The empirical benchmarks from /code are then compared against the theoretical guarantees derived by /math. Verification: The router will check that the implemented algorithm in the code matches the derivation. Fallback: If a parallel Incomplete Cholesky proves too difficult to implement, the plan will revert to a simpler but still parallelizable preconditioner like Jacobi (diagonal scaling) and re-evaluate.\", \"todo_list\": [\"- [ ] /general-search: Find at least two key papers on parallel Incomplete Cholesky (IC0) for GPU architectures.\", \"- [ ] /math: Derive the Preconditioned Conjugate Gradient (PCG) algorithm with the selected IC preconditioner.\", \"- [ ] /math: Analyze the theoretical reduction in the condition number and its effect on the convergence bound.\", \"- [ ] /code: Implement the PCG solver in JAX, including a sparse CSR matrix-vector product and the preconditioner application.\", \"- [ ] /code: Create a test suite to verify correctness on a small problem with a known solution.\", \"- [ ] /math: Verify that the empirical convergence rate from the code's benchmark output is consistent with the derived theoretical bound.\", \"- [ ] /code: Generate performance plots comparing PCG vs. standard CG for wall-clock time and iterations to convergence.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"sparse-solvers\"], \"acceptance_criteria\": [\"- The implemented solver must successfully converge to a solution for a sparse SPD system of at least size 1,000,000 x 1,000,000 within the allocated compute budget.\", \"- The final report must clearly show, via plots and metrics, that the PCG solver converges in significantly fewer iterations and less wall-clock time than the standard CG solver.\", \"- The mathematical derivation must be correct and properly cited with at least two academic sources from the literature search.\", \"- The code must be well-commented and include a reproducibility script to regenerate the key results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 88, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for a variance-preserving diffusion process, starting from the forward process dxt = f(xt, t)dt + g(t)dwt. Demonstrate how the score function, \u2207xt log pt(xt), replaces the unknown drift term in the reverse SDE via the Fokker-Planck equation. Finally, implement a basic Euler-Maruyama solver to sample from this reverse SDE for a 2D Gaussian mixture model, and validate the convergence of the learned score to the true score.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time SDE stochastic calculus\\\", mode=both)\", \"/math(Derive the reverse-time SDE corresponding to a forward variance-preserving diffusion process, explicitly showing how the score function \u2207x log p(x) relates to the drift term, citing foundational stochastic calculus theorems like It\u00f4's lemma.)\", \"/code(Write PyTorch code to implement a score network and an Euler-Maruyama solver for the reverse-time diffusion SDE. Validate the sampler's convergence on a 2D Gaussian mixture, tracking KL divergence from the true distribution., using Python)\"], \"route_rationale\": \"The blue general-search agent is first used to gather foundational literature and key papers on score-based SDEs from authoritative sources like arXiv. The /math tool is then used for the rigorous theoretical derivation of the reverse-time SDE. Finally, /code implements the derived theory and provides empirical validation of the model.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the reverse-time SDE.\", \"A Python script implementing the score-matching neural network and the SDE sampler.\", \"A PNG image plotting the final generated samples against the target 2D Gaussian mixture density.\", \"A JSON file reporting the final Kullback-Leibler (KL) divergence metric.\", \"A bibliography file with citations to at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Use /general-search to find the seminal papers on score-based generative models via SDEs (e.g., by Song et al.) to establish the theoretical framework.\", \"2. Formally derive the reverse-time SDE using principles from stochastic calculus, likely involving the Fokker-Planck equation and time-reversal arguments for diffusion processes.\", \"3. Implement a neural network in PyTorch to approximate the score function, \u2207x log p(x, t), and an Euler-Maruyama solver to numerically integrate the derived reverse-time SDE.\", \"4. Train the score network on samples from a known target distribution (a 2D Gaussian mixture) using a denoising score matching objective.\", \"5. Validate the correctness of the derivation by comparing the analytical score of a single Gaussian to the network's output for a simple case before full training.\", \"6. Verify the final model's performance by generating samples and quantitatively measuring their KL divergence from the true distribution, ensuring it falls below a target threshold.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides papers to the math agent for derivation. The resulting equations from the math agent are handed to the code agent for implementation. The code agent provides the final script, plots, and metrics to the router for final verification. Verification: The router confirms the SDE update rule in the code matches the derived equation. Fallback: If verification fails, the task is routed back to /math with feedback on the discrepancy.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal arXiv papers on score-based generative modeling via SDEs.\", \"- [ ] /math: Produce a step-by-step derivation of the reverse-time SDE, explicitly defining all terms.\", \"- [ ] /code: Implement the score network architecture and the Euler-Maruyama SDE sampler in PyTorch.\", \"- [ ] /code: Implement the training loop using the denoising score matching objective.\", \"- [ ] /math: Verify that the implemented SDE update rule in the code correctly reflects the derived mathematical formula.\", \"- [ ] /code: Generate final samples, plot them against the true distribution, and compute the KL divergence.\", \"- [ ] Router: Perform final QA, checking that the derivation is sound, the code runs, and the acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-modeling\"], \"acceptance_criteria\": [\"The mathematical derivation of the reverse-time SDE from the forward process is complete, rigorous, and correct.\", \"The Python implementation successfully generates samples that visually approximate all modes of the target 2D Gaussian mixture distribution.\", \"The final reported KL divergence between the generated and target distributions is less than 0.1.\", \"Citations for at least two relevant arXiv papers are provided in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Estimate the KL divergence between the empirical distribution of 5000 generated samples and the true target distribution. A lower value indicates a better fit.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Qualitatively assess a 2D scatter plot of generated samples overlaid on the true density contours. The generated points should clearly cover all modes of the Gaussian mixture.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 89, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm exploring program synthesis for theorems in cubical type theory. Can you develop a strategy to synthesize a certified program that proves a specific path induction principle? This requires formalizing the theorem in a language like Agda or Lean, then using a neural-guided search, possibly based on a Transformer architecture, to generate the proof term. I need the formal derivation, the synthesis algorithm implementation, and a benchmark against existing neuro-symbolic provers, ensuring the generated proof term passes the type checker.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'cubical type theory' 'neural program synthesis' 'proof assistant' transformer\\\", mode=web)\", \"/math(Formalize the path induction principle for cubical type theory, referencing the discovered arXiv literature and establishing the convergence criteria for the proof search, ensuring it aligns with the Euler-Lagrange framework for optimization.)\", \"/code(Implement the neural-guided proof synthesis in Python with CUDA support, using an adversarial training loop to refine the search policy and tracking convergence metrics against a baseline.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find state-of-the-art literature on neural theorem proving for cubical type theory. This research informs the /math agent, which will create the formal mathematical specification of the theorem. Finally, the /code agent implements the synthesis algorithm based on the formal spec.\", \"expected_artifacts\": [\"A formal mathematical derivation of the path induction principle in cubical type theory, presented in LaTeX.\", \"A Python script implementing the core logic of a neural-guided search for proof synthesis.\", \"A summary report in Markdown citing at least two recent arXiv papers on neuro-symbolic methods.\", \"A log file showing the synthesized proof term and the output of a mock type-checker verification.\"], \"thinking_outline\": [\"1. Research current neural-symbolic approaches for theorem proving in dependent type theories, focusing on cubical or homotopy type theory.\", \"2. Formulate the target theorem (path induction principle) using precise mathematical notation derived from the research.\", \"3. Design a simplified neural-guided search algorithm (e.g., beam search or MCTS) to generate proof terms.\", \"4. Implement the search algorithm in Python, using a mock neural network oracle to guide the synthesis.\", \"5. Validate the generated proof term by outlining how it would be checked by a proof assistant's type checker (e.g., Lean or Agda).\", \"6. Audit the entire process, ensuring the formal derivation in step 2 correctly maps to the synthesis target in step 4.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: synthesized proof term structure matches the formal derivation and passes a mock type check; fallback: if synthesis fails, /code will refine the search heuristics and retry, reporting convergence issues).\", \"todo_list\": [\"- [ ] /general-search: Find SOTA research on neuro-symbolic provers for cubical type theory and path induction.\", \"- [ ] /math: Formalize the path induction principle based on literature review findings.\", \"- [ ] /code: Implement a neural-guided search algorithm in Python for synthesizing the proof term.\", \"- [ ] /math: Verify that the structure of the formal proof goal aligns with the synthesis target for the /code agent.\", \"- [ ] /code: Implement a mock type-checker to validate the syntactic correctness of the synthesized proof term.\", \"- [ ] /general-search: Collect citation details (arXiv IDs) for the final report.\", \"- [ ] Router: Perform final QA review, checking consistency between the formal proof, the synthesized code, and the literature review.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic-methods\", \"program-synthesis\", \"type-theory\"], \"acceptance_criteria\": [\"The synthesized program (proof term) must be syntactically correct and pass a mock type-checking validation.\", \"The final report must cite at least two relevant arXiv papers with IDs on neural theorem proving or type theory published in the last 3 years.\", \"The formal derivation in the math artifact must correctly define the types and context for the path induction principle.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 90, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to formulate and implement a Physics-Informed Neural Network (PINN) to solve the 1D non-linear Schr\u00f6dinger equation. Please derive the complex-valued loss function using tensor calculus principles for the spatio-temporal partial derivatives. Then, implement this in JAX, validate the model against a known soliton propagation solution, and analyze the model's ability to conserve physical quantities like energy to ensure the convergence properties are met.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org complex-valued neural networks PINN non-linear schrodinger equation Fourier features\\\", mode=web)\", \"/math(Derive the PINN loss function for the Schr\u00f6dinger equation, detailing the complex partial derivatives using Wirtinger calculus to handle the non-analytic nature and ensure proper convergence analysis. Consider the eigenvalue spectrum of the Hamiltonian.)\", \"/code(Implement the PINN in JAX with complex number support, applying the derived loss function. Validate against a soliton case, track energy conservation as a control theory metric, and use CUDA for acceleration., using Python)\"], \"route_rationale\": \"The blue /general-search agent first gathers state-of-the-art literature on complex-valued PINNs for this specific PDE. The /math agent then uses these insights to perform the rigorous derivation of the loss function. Finally, the /code agent implements this mathematical formulation and performs empirical validation, which requires a robust coding environment.\", \"expected_artifacts\": [\"A LaTeX document providing the full mathematical derivation of the complex-valued PINN loss function for the non-linear Schr\u00f6dinger equation.\", \"A self-contained Python script using JAX to implement and train the PINN, including data generation for the analytical solution.\", \"A verification report comparing the JAX automatic differentiation output with numerical finite-difference approximations for the complex partial derivatives.\", \"A results summary including plots of the loss convergence, a comparison of the predicted vs. analytical solution, and a plot showing the conservation of energy over time.\", \"A bibliography citing at least two relevant arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Research existing literature on PINNs applied to complex-valued, non-linear PDEs like the Schr\u00f6dinger equation to identify best practices for network architecture and loss formulation.\", \"2. Formally derive the total loss function, which includes terms for the PDE residual, initial conditions, and boundary conditions, using Wirtinger calculus for the complex derivatives.\", \"3. Implement the neural network, loss function, and training loop in JAX, paying careful attention to the handling of complex numbers and their gradients.\", \"4. Verify the correctness of the implemented partial derivative calculations within the PDE residual by comparing the output of JAX's automatic differentiation against a manual finite-difference scheme on a known complex function.\", \"5. Train the model and analyze its convergence behavior, monitoring the individual components of the loss function.\", \"6. Validate the final trained model by computing the L2 relative error against the known analytical soliton solution and check that the discrete energy norm is conserved within a small tolerance over the simulation domain.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: router will cross-reference the derived equations in the LaTeX doc from /math with the loss implementation in the JAX script from /code; fallback: if a discrepancy is found, the task is returned to /code with a detailed error report for correction).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on PINNs for solving the non-linear Schr\u00f6dinger equation.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the complex-valued loss function using Wirtinger calculus.\", \"- [ ] /code: Implement the PINN architecture, loss function, and training loop in JAX, ensuring support for complex arithmetic.\", \"- [ ] /math: Verify that the implemented JAX automatic differentiation correctly computes the complex partial derivatives as specified in the formal derivation.\", \"- [ ] /code: Train the model and generate plots for loss convergence and solution accuracy against the analytical soliton case.\", \"- [ ] /code: Generate a diagnostic plot to validate the conservation of the energy norm throughout the simulation time.\", \"- [ ] router: Perform a final QA check, ensuring all artifacts are present, metrics meet acceptance criteria, and the code is well-documented.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"pinn\", \"pde-solvers\", \"tensor-calculus\"], \"acceptance_criteria\": [\"The mathematical derivation of the loss function must be complete, mathematically sound, and use appropriate notation for complex derivatives.\", \"The JAX implementation must execute successfully and reproduce the results (loss curves, final error) described in the summary report.\", \"The final L2 relative error of the predicted solution against the analytical solution must be below 1e-3.\", \"The total energy of the system, as computed from the model's prediction, must be conserved with less than a 1% deviation over the entire time domain.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 91, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the continuous-time It\u00f4 diffusion process whose stationary distribution is the posterior of a Bayesian neural network. Subsequently, apply an Euler-Maruyama discretization to obtain the update rule for Stochastic Gradient Langevin Dynamics (SGLD). Implement this SGLD sampler in JAX to approximate the posterior for a BNN on a regression task, analyzing its convergence by tracking the posterior predictive distribution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Stochastic Gradient Langevin Dynamics convergence Bayesian deep learning\\\", mode=web)\", \"/math(Derive the SGLD update rule from the continuous-time Langevin diffusion process for a Bayesian neural network posterior, ensuring the derivation details the role of the learning rate schedule for convergence.)\", \"/code(Implement the derived Bayesian SGLD sampler in JAX for a regression task, tracking posterior predictive variance and negative log-likelihood to evaluate convergence and uncertainty quantification., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on SGLD theory and application from arXiv. This informs the /math agent's formal derivation of the algorithm. Finally, the /code agent implements the mathematically specified algorithm and runs experiments to validate its properties.\", \"expected_artifacts\": [\"A LaTeX document providing the full mathematical derivation of the SGLD update rule from the continuous-time Langevin diffusion.\", \"A commented JAX/Flax Python script implementing the SGLD sampler for a Bayesian Neural Network.\", \"A report summarizing the experimental results, including plots of convergence diagnostics and posterior predictive uncertainty on both in-distribution and out-of-distribution data.\", \"A bibliography citing at least two relevant arXiv preprints with their IDs.\"], \"thinking_outline\": [\"1. Formulate the posterior distribution p(\u03b8|D) for a Bayesian Neural Network's parameters \u03b8 given data D, using Bayes' theorem to combine the likelihood and a prior.\", \"2. Define the corresponding Langevin It\u00f4 diffusion process that has this posterior as its unique stationary distribution, specifying the drift and diffusion terms.\", \"3. Apply the Euler-Maruyama method to discretize the continuous-time SDE, yielding the discrete-time update rule for Stochastic Gradient Langevin Dynamics (SGLD).\", \"4. Implement the SGLD sampler and a BNN architecture in JAX, ensuring the stochastic gradient of the log-posterior is estimated correctly using mini-batches.\", \"5. Validate the implementation by verifying that the posterior predictive uncertainty (variance) is significantly higher for out-of-distribution data points compared to in-distribution data.\", \"6. Audit the theoretical convergence conditions (e.g., Robbins-Monro conditions on the step size) against the empirical performance observed in the experiment.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: the mathematical derivation in the final report must exactly match the JAX implementation's update step; fallback: if a discrepancy is found, /math must revise the derivation and /code must update the implementation accordingly)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers (e.g., Welling & Teh, 2011) and recent follow-ups on SGLD and its convergence guarantees.\", \"- [ ] /math: Derive the SGLD update equations from first principles, clearly stating all assumptions.\", \"- [ ] /code: Implement the BNN architecture and the SGLD sampler in JAX.\", \"- [ ] /code: Add instrumentation to track posterior predictive mean and variance on a held-out test set.\", \"- [ ] /math: Verify that the discrete update rule is a valid first-order approximation of the continuous diffusion process.\", \"- [ ] /code: Validate the sampler's uncertainty quantification on a toy regression problem with OOD data.\", \"- [ ] router: Perform final QA, ensuring the LaTeX derivation, JAX code, and final report are consistent and correct.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-variational-inference\", \"bayesian-deep-learning\", \"langevin-dynamics\"], \"acceptance_criteria\": [\"The final report must contain a complete, step-by-step mathematical derivation of SGLD from the Langevin SDE.\", \"The JAX implementation must successfully execute and show convergence of the Negative Log-Likelihood metric on a synthetic regression dataset.\", \"The experimental analysis must demonstrate correct uncertainty quantification, with reported predictive variance on OOD data being at least 2x higher than on in-distribution test data.\", \"The report must cite at least two relevant papers with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 93, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nCan you help me understand the REINFORCE algorithm for policy gradients? I want to see the mathematical derivation for the policy gradient theorem and then implement it in Python for the CartPole environment, including a simple value function baseline to reduce variance.\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient theorem using the likelihood ratio trick, starting from the objective function of maximizing expected return for a Bernoulli action distribution.)\", \"/code(Implement the REINFORCE algorithm with a simple value function baseline in Python for the CartPole-v1 environment, and plot the reward convergence curve.)\", \"/general-search(query=\\\"site:arxiv.org REINFORCE algorithm variance reduction policy gradient baseline\\\", mode=rag)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation of the policy gradient theorem. This is followed by /code to translate the theory into a practical, working implementation and validate the learning dynamics. Finally, the blue /general-search agent is used to find foundational literature that justifies the use of a baseline for variance reduction, providing academic context.\", \"expected_artifacts\": [\"A markdown file with the step-by-step mathematical derivation of the policy gradient theorem.\", \"A Python script implementing the REINFORCE algorithm with a baseline for CartPole-v1.\", \"A plot showing the episodic rewards over training, demonstrating learning convergence.\", \"A summary report explaining the role of the baseline in variance reduction, with citations.\", \"A JSON file containing the final evaluation metrics.\"], \"thinking_outline\": [\"1. Formulate the reinforcement learning objective function as the expected total reward.\", \"2. Use the log-derivative trick (likelihood ratio) to derive the policy gradient theorem, showing how to estimate the gradient without knowing the environment dynamics.\", \"3. Introduce a state-dependent baseline to the policy gradient update rule to reduce the variance of the gradient estimator.\", \"4. Implement the REINFORCE algorithm in Python, including the policy network and the baseline network, and train it on the CartPole environment.\", \"5. Verify that the implementation leads to performance improvement by plotting the moving average of episodic rewards and checking for a clear upward trend.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math provides the exact gradient formula needed for the /code implementation. The working code provides practical context for the /general-search agent to find relevant papers on performance improvements like baselines. The router performs a final verification to ensure the code's gradient calculation matches the math derivation and is supported by the cited literature. Fallback: if convergence fails in /code, revisit the /math derivation for potential errors.\", \"todo_list\": [\"- [ ] /math: Derive the policy gradient theorem, clearly showing the application of the log-derivative trick.\", \"- [ ] /code: Implement the REINFORCE algorithm with a value function baseline in Python for CartPole-v1.\", \"- [ ] /general-search: Find and summarize at least two foundational papers on policy gradients and variance reduction techniques.\", \"- [ ] /code: Generate a plot of episodic rewards versus training episodes.\", \"- [ ] Verification: Check that the implemented gradient update in the code correctly matches the derived mathematical formula.\", \"- [ ] Router QA: Submit all artifacts for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"policy-gradient\", \"reinforcement-learning\", \"variance-reduction\"], \"acceptance_criteria\": [\"The mathematical derivation of the policy gradient theorem is correct and self-contained.\", \"The Python implementation successfully trains an agent that solves CartPole-v1 (achieves an average reward of >= 195 over 100 consecutive episodes).\", \"The final report correctly explains the purpose of a baseline and cites at least two relevant academic papers with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"index": 95, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the formulation for a Constrained Policy Optimization (CPO) algorithm that incorporates high-order Control Barrier Functions (CBFs) to ensure forward invariance in a safety-critical robotic manipulation task. The system dynamics are governed by nonlinear differential equations. Implement a prototype in PyTorch to validate the approach on a simulated pendulum swing-up task, ensuring the Lagrangian relaxation for the trust region step respects both performance and safety constraint satisfaction under stochastic control.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org high-order control barrier function deep reinforcement learning constrained policy optimization convergence\\\", mode=web)\", \"/math(Derive the Lagrangian for the Constrained Policy Optimization problem incorporating the high-order Control Barrier Function constraint. Analyze the Karush-Kuhn-Tucker conditions for optimality and formalize the trust region update based on Lyapunov stability arguments for control theory guarantees.)\", \"/code(Implement the CPO agent in PyTorch, using automatic differentiation for the policy and value function gradients. Validate the CBF constraint satisfaction at each step of the pendulum simulation, ensuring numerical stability and tracking convergence metrics., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey the latest arXiv literature on Control Barrier Functions in deep RL. This informs the /math agent's formal derivation of the constrained optimization problem. Finally, the /code agent implements and validates the derived theory, providing an empirical test of the formulation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the CBF-constrained CPO update rule in a PDF document.\", \"A Python script implementing the algorithm in PyTorch for the simulated environment.\", \"A CSV file containing training logs: episode rewards, constraint violation rates, and trust region convergence diagnostics.\", \"A bibliography with citations to at least two relevant arXiv preprints on safe RL.\", \"Verification output log confirming that the implemented KKT solver aligns with the mathematical derivation.\"], \"thinking_outline\": [\"1. Review state-of-the-art literature on integrating Control Barrier Functions (CBFs) with deep policy gradient methods like CPO or TRPO.\", \"2. Formulate the constrained optimization problem, defining the objective function (expected return) and the safety constraint derived from the high-order CBF.\", \"3. Derive the dual problem and the Karush-Kuhn-Tucker (KKT) conditions for the policy update step, ensuring theoretical guarantees of safety.\", \"4. Implement the algorithm, paying close attention to the numerical stability of the conjugate gradient solver for the trust region and the CBF constraint projection.\", \"5. Validate the implementation by checking if the safety constraints are satisfied across training episodes and verify that the agent's behavior respects the defined safe set.\", \"6. Audit the final performance metrics (reward vs. constraint violations) to confirm the trade-off is managed effectively as per the CPO formulation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent cross-checks the code's gradient implementation against its derivation; fallback: if a discrepancy is found, /code agent must refactor and rerun validation tests).\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 key papers on combining high-order CBFs with CPO/TRPO from arXiv.\", \"- [ ] /math: Formulate the Lagrangian dual for the CBF-constrained policy optimization problem.\", \"- [ ] /math: Derive the KKT conditions and the final policy update rule.\", \"- [ ] /code: Implement the CPO agent and the pendulum environment with the CBF safety constraint in PyTorch.\", \"- [ ] /code: Run the simulation, log metrics (reward, constraint violations), and generate plots.\", \"- [ ] /math: Verify that the implemented gradient updates in the code correspond directly to the derived KKT conditions.\", \"- [ ] /code: Validate that the simulation's constraint violation rate remains below the specified threshold.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"control-theory\", \"constrained-optimization\"], \"acceptance_criteria\": [\"The mathematical derivation of the CBF-constrained update rule is correct and self-contained.\", \"The Python implementation successfully trains an agent that improves its reward while maintaining a constraint violation rate below 1%.\", \"The final report includes plots showing reward convergence and constraint satisfaction over time, supported by the logged metrics.\", \"At least two relevant arXiv preprints are cited correctly.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 97, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating tensor decomposition for representation learning. I need a detailed derivation of the Alternating Least Squares (ALS) algorithm for CANDECOMP/PARAFAC (CP) decomposition. Please derive the update rules using matricization and the Khatri-Rao product. Then, implement this in Python to validate the algorithm's convergence on a synthetic low-rank tensor corrupted by Gaussian noise, and measure the recovery accuracy against the ground truth factors.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org canonical polyadic decomposition alternating least squares convergence Khatri-Rao product\\\", mode=web)\", \"/math(Derive the full Alternating Least Squares (ALS) update rules for Canonical Polyadic (CP) decomposition by minimizing the Frobenius norm of the residual. Detail the use of matricization and the Khatri-Rao product to formulate the per-factor subproblems as linear least squares, ensuring theoretical convergence guarantees are discussed., includes cybersecurity)\", \"/code(Implement the derived ALS algorithm in Python to recover factor matrices from a synthetic tensor. Track the reconstruction error to verify convergence and compute the Factor Match Score against ground truth factors, providing a basis for future Bayesian extensions or federated learning applications.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on CP-ALS. This informs the /math agent's formal derivation of the update rules. Finally, the /code agent implements the derived algorithm for empirical verification, which is a critical step in math_plus_code scenarios.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the CP-ALS update rules in LaTeX format.\", \"- A Python script implementing the CP-ALS algorithm and a synthetic data testbed.\", \"- A plot visualizing the reconstruction error versus iteration number, demonstrating convergence.\", \"- A summary report (.md) including the derivation, plot, Factor Match Score, and at least two citations.\", \"- A JSON file containing the final reconstruction error and Factor Match Score metrics.\"], \"thinking_outline\": [\"1. Formulate the CP decomposition as an optimization problem: minimizing the Frobenius norm of the difference between the original tensor and its low-rank approximation.\", \"2. For a single factor matrix (e.g., A), hold the others (B, C) fixed. Rewrite the optimization problem using tensor matricization and the Khatri-Rao product.\", \"3. Show that this subproblem is a standard linear least squares problem and derive its closed-form solution, which becomes the ALS update rule for that factor.\", \"4. Implement the full ALS algorithm, iterating through the update rule for each factor matrix until convergence.\", \"5. Validate the implementation by generating a known low-rank tensor, adding noise, and running the algorithm. Check that the reconstruction error decreases monotonically.\", \"6. Verify the correctness of the recovered factors by comparing them to the ground-truth factors using the Factor Match Score (FMS), accounting for permutation and scaling ambiguities.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation that the code agent must implement. The code agent's output (convergence plots, metrics) is used to verify the correctness of the derivation and implementation. Verification: The empirical convergence curve from /code must align with the theoretical monotonic decrease expected by the /math derivation. Fallback: If convergence fails or is unstable, the task is routed back to /math to audit the derivation for potential errors or unstated assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers or book chapters on CP-ALS.\", \"- [ ] /math: Derive the ALS update rules, clearly showing the matricization and Khatri-Rao product steps.\", \"- [ ] /code: Implement the CP-ALS algorithm using NumPy/TensorLy.\", \"- [ ] /code: Create a synthetic tensor generator and a function to compute the Factor Match Score.\", \"- [ ] /code: Run the algorithm on synthetic data and plot the reconstruction error per iteration.\", \"- [ ] /math: Verify that the core logic in the Python implementation correctly matches the derived least squares subproblem.\", \"- [ ] /code: Verify that the final recovered factors achieve a high Factor Match Score (>0.95) against the ground truth.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"numerical-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ALS update rule must be correct and complete.\", \"- The Python implementation must demonstrate a monotonically decreasing reconstruction error on a synthetic low-rank tensor.\", \"- The final report must include the derivation, the convergence plot, and cite at least two authoritative sources.\", \"- The recovered factors from the synthetic experiment must achieve a Factor Match Score of at least 0.95.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 98, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm exploring self-supervised learning and want to understand the core math behind scaled dot-product attention. Can you first derive the gradient of the attention mechanism's softmax output with respect to the query-key dot product, then help me code a simple SimCLR-style contrastive learning pretext task in PyTorch using this mechanism for a toy bioinformatics dataset?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org scaled dot-product attention gradient derivation 'contrastive learning' bioinformatics\\\", mode=both)\", \"/math(Derive the Jacobian matrix for the softmax function within the scaled dot-product attention mechanism, providing a step-by-step proof suitable for backpropagation. Briefly touch upon any relevant Bayesian concepts.)\", \"/code(Implement a simple SimCLR-style contrastive learning pipeline in PyTorch with an attention module. Validate the implementation by checking for convergence of the NT-Xent loss on a toy dataset, and include basic CUDA device management., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational papers and tutorials on attention and contrastive learning. Next, /math is used to handle the formal mathematical derivation requested by the user. Finally, /code translates the theory into a practical PyTorch implementation with built-in validation, creating a complete theory-to-practice workflow.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF (`derivation.pdf`) with the full, step-by-step derivation of the softmax gradient within the attention mechanism.\", \"A runnable Python script (`attention_simclr.py`) implementing the SimCLR-style contrastive learning task using a PyTorch attention module.\", \"A summary report (`report.md`) explaining the process, showing the training loss plot, and citing at least 2 relevant arXiv papers with their IDs.\", \"A verification log (`verification.log`) capturing the output of the numerical gradient check, comparing the analytical and autograd results.\"], \"thinking_outline\": [\"1. Review foundational literature on scaled dot-product attention ('Attention Is All You Need') and contrastive self-supervised learning (e.g., SimCLR) to establish context and find authoritative formulas.\", \"2. Formulate the mathematical expression for scaled dot-product attention, clearly defining the Query (Q), Key (K), and Value (V) matrices and the scaling factor.\", \"3. Execute the step-by-step derivation of the Jacobian for the softmax output with respect to the pre-softmax (QK^T) matrix.\", \"4. Implement the attention mechanism and the NT-Xent loss function in PyTorch, setting up a basic data augmentation pipeline for the contrastive task.\", \"5. Validate the implementation by running it on a toy dataset and verifying that the training loss decreases over several epochs, which confirms the gradient flow is correct.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The derived analytical gradient in the /math PDF must match the numerical gradients computed by the PyTorch autograd engine in the /code script to within a small tolerance; fallback: If they mismatch, re-run /math with a note about the discrepancy for re-derivation.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers/tutorials on attention mechanisms and SimCLR for citation and reference.\", \"- [ ] /math: Complete the formal derivation of the attention softmax gradient.\", \"- [ ] /code: Implement the contrastive learning pipeline in PyTorch.\", \"- [ ] Verification: Write a small script to numerically check the analytical gradient from /math against the autograd gradient from /code.\", \"- [ ] Router QA: Package all artifacts (report, code, derivation, log) for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"self-supervised-learning\", \"attention-mechanism\", \"deep-learning\"], \"acceptance_criteria\": [\"The mathematical derivation of the gradient is clearly presented, step-by-step, and is mathematically correct.\", \"The Python code is well-commented, runs without errors, and demonstrates a consistently decreasing loss curve on a sample dataset.\", \"The final report cites at least two relevant papers (e.g., 'Attention Is All You Need', 'A Simple Framework for Contrastive Learning of Visual Representations') with full arXiv IDs.\", \"The numerical gradient check comparing the analytical and autograd results passes, with a mean absolute error below 1e-6.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"primary_guidance\": \"Boolean indicating if the training loss shows a consistent downward trend over at least 20 epochs.\", \"secondary_guidance\": \"Mean Absolute Error between the analytical gradient (from the math derivation) and the numerical gradient (from PyTorch's autograd) on a test input tensor.\"}}", "categories": ["metrics_guidance"]}
{"index": 99, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the alternating least squares (ALS) algorithm for Canonical Polyadic (CP) decomposition with non-negativity and orthogonality constraints on the factor matrices. I need a formal proof of convergence under these constraints, particularly analyzing the block coordinate descent nature of ALS. Also, implement this constrained CP-ALS in Python using TensorLy/JAX, and validate its ability to recover latent factors from a synthetic tensor, measuring the Factor Match Score (FMS) and reconstruction error.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org \\\\\\\"canonical polyadic decomposition\\\\\\\" \\\\\\\"alternating least squares\\\\\\\" convergence proof non-negativity orthogonality constraints\\\", mode=both)\", \"/math(Derive the block coordinate descent update rules for CP-ALS with non-negativity and orthogonality constraints. Then, construct a formal convergence proof referencing Zangwill's theorem and analyzing the properties of the Khatri-Rao product., includes numpy)\", \"/code(Implement the constrained CP-ALS algorithm in Python using JAX, including projections for non-negativity and orthogonality. Validate on a synthetic rank-R tensor, tracking convergence via reconstruction error and assessing factor recovery using the Factor Match Score, enabling optional CUDA acceleration.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather authoritative literature on constrained CP-ALS convergence, which is a non-trivial theoretical topic. This research grounds the formal derivation and proof construction by the /math agent. Finally, the /code agent implements the derived algorithm to provide empirical validation, turning theory into a practical, verifiable result.\", \"expected_artifacts\": [\"- A LaTeX document detailing the derivation of the constrained CP-ALS update rules and the complete convergence proof.\", \"- A Python script (`constrained_cp_als.py`) implementing the algorithm, synthetic data generation, and validation experiment.\", \"- A JSON file (`metrics.json`) reporting the final reconstruction error and Factor Match Score.\", \"- A BibTeX file (`references.bib`) containing citations for at least two relevant academic papers.\", \"- A verification report (`verification.md`) summarizing the cross-check between the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Conduct a literature search to find seminal papers on constrained tensor decomposition and the convergence properties of Alternating Least Squares (ALS) for CP models.\", \"2. Formally derive the update equations for each factor matrix in the CP decomposition, incorporating non-negativity (e.g., via projection) and orthogonality constraints (e.g., via SVD-based projection).\", \"3. Construct a rigorous proof of convergence for the constrained CP-ALS algorithm, likely by leveraging Zangwill's Global Convergence Theorem for block coordinate descent methods.\", \"4. Implement the derived algorithm in Python, including a function to generate a synthetic ground-truth tensor and a loop for the iterative ALS updates.\", \"5. Validate the implementation's correctness by checking intermediate tensor shapes and ensuring the cost function is non-increasing at each iteration.\", \"6. Verify that the final experimental results, such as the convergence curve and the high Factor Match Score, align with the theoretical properties established in the proof.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers (e.g., arXiv preprints) to the /math agent. The /math agent produces a formal derivation and proof, which serves as the blueprint for the /code agent. The /code agent implements this blueprint and produces empirical results and metrics. The final artifacts are reviewed by the router QA. Verification: The math derivation is cross-referenced against the code logic. Fallback: If the convergence proof is invalid, the task reverts to /general-search for more suitable theoretical frameworks.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on constrained CP-ALS convergence.\", \"- [ ] /math: Derive the update rules for CP-ALS with non-negativity and orthogonality constraints.\", \"- [ ] /math: Write a formal proof of convergence for the derived algorithm.\", \"- [ ] /code: Implement the constrained CP-ALS algorithm and the experimental setup with a synthetic tensor.\", \"- [ ] /code: Compute and log the reconstruction error per iteration and the final Factor Match Score.\", \"- [ ] /math: Verify that the Python code's update and projection steps are a correct implementation of the mathematical derivation.\", \"- [ ] /code: Validate that the empirical reconstruction error is monotonically non-increasing, as predicted by the convergence theory.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"numerical-optimization\"], \"acceptance_criteria\": [\"- The convergence proof must be mathematically sound, explicitly state its assumptions, and correctly apply a relevant optimization theorem (e.g., Zangwill's).\", \"- The Python implementation must successfully recover the latent factors from a synthetic tensor, achieving a Factor Match Score > 0.95.\", \"- The final reconstruction error (relative Frobenius norm) must be below a tolerance of 1e-5.\", \"- The report must clearly link the theoretical derivation to the implementation details and experimental outcomes, citing at least two arXiv papers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 101, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDevise a neural-symbolic framework to synthesize a Python program that computes the Betti numbers for a given finite simplicial complex. The process must start by formalizing the definition of chain groups, boundary operators, and homology groups in first-order logic, then prove the correctness of the resulting algorithm based on the Mayer-Vietoris sequence, and finally synthesize the code with CUDA acceleration for the boundary matrix operations.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural-symbolic program synthesis algebraic topology homology CUDA\\\", mode=rag)\", \"/math(Formalize the definitions of simplicial homology, chain groups, and boundary operators. Structure a proof of correctness for the algorithm computing Betti numbers, referencing the Euler-Lagrange principle for potential optimization and ensuring convergence for iterative solvers.)\", \"/code(Implement a Python class for simplicial complexes and a JAX/CUDA kernel for sparse boundary matrix operations to compute homology group ranks. Validate the implementation against known examples like the torus, ensuring convergence and tracking computational complexity.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art literature on neural-symbolic methods for computational topology. This informs the /math agent's formalization of the problem and proof construction. The verified mathematical specification is then handed to the /code agent for an efficient and correct implementation, leveraging CUDA for performance.\", \"expected_artifacts\": [\"A LaTeX document containing the formal mathematical proof of the algorithm's correctness, referencing key theorems.\", \"A Python script with a JAX/CUDA implementation for computing Betti numbers from a simplicial complex representation.\", \"A performance analysis report in Markdown, comparing CPU vs. GPU computation times for boundary matrices.\", \"A verification log confirming the consistency between the formal proof and the code's output on canonical test cases (e.g., torus, sphere).\", \"A bibliography file citing at least two relevant research papers with arXiv IDs.\"], \"thinking_outline\": [\"1. Deconstruct the user request into three core components: mathematical formalization, algorithmic proof, and efficient implementation.\", \"2. Formulate the mathematical objects (chain groups, boundary operators, homology groups) in a symbolic representation suitable for theorem proving.\", \"3. Develop the algorithm for computing Betti numbers (e.g., via Smith Normal Form of the boundary matrix) and structure a formal proof of its correctness.\", \"4. Translate the verified algorithm into an efficient Python implementation using JAX and CUDA for matrix operations.\", \"5. Validate the implementation's numerical stability and correctness by testing it on well-known topological spaces (e.g., sphere, torus) and comparing results against known Betti numbers.\", \"6. Audit the formal proof against the implemented code logic to ensure there are no discrepancies in the translation from theory to practice.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code output on a torus must match theoretical Betti numbers {b0=1, b1=2, b2=1}; fallback: if mismatch, return to /math to re-check the boundary operator definitions and proof steps)\", \"todo_list\": [\"- [ ] /general-search: Find recent papers on neural-symbolic methods for computational topology and program synthesis.\", \"- [ ] /math: Formalize the definitions of chain complexes and boundary operators in a symbolic language.\", \"- [ ] /math: Construct the formal proof of correctness for the Betti number computation algorithm.\", \"- [ ] /code: Implement the simplicial complex data structure and the sparse boundary matrix construction.\", \"- [ ] /code: Develop the JAX/CUDA kernel for computing the rank of the homology group and validate its performance.\", \"- [ ] /math: Verify that the implemented algorithm for rank computation correctly reflects the formal proof and the definition of homology.\", \"- [ ] /code: Test the final program on canonical examples (sphere, torus, Klein bottle) and document the results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic-synthesis\", \"algebraic-topology\", \"theorem-proving\"], \"acceptance_criteria\": [\"- The synthesized Python code must correctly compute the Betti numbers for a torus (b\u2080=1, b\u2081=2, b\u2082=1).\", \"- The provided LaTeX document must contain a complete and logically sound proof of the algorithm's correctness.\", \"- The CUDA-accelerated implementation must demonstrate at least a 5x speedup over a CPU-only baseline for a simplicial complex with >10,000 simplices.\", \"- At least two relevant arXiv papers must be cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 102, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the convergence properties of relaxed Alternating Direction Method of Multipliers (ADMM) for solving the basis pursuit denoising problem. Please derive the primal-dual update steps for both standard ADMM and an over-relaxed variant (alpha > 1.0). Then, implement both algorithms in Python using JAX, and empirically compare their convergence rates on a synthetic sparse signal recovery task by plotting the primal and dual residuals. The analysis must connect the empirical results to the theoretical convergence guarantees from recent literature, particularly focusing on the role of the relaxation parameter and the problem's conditioning.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org over-relaxed ADMM convergence guarantees basis pursuit\\\", mode=web)\", \"/math(Derive the augmented Lagrangian for basis pursuit denoising and formulate the primal-dual update equations for standard ADMM and its over-relaxed variant, ensuring correctness of the proximal operator for the L1-norm.)\", \"/code(Implement both ADMM variants in JAX, including a synthetic data generator for sparse signals. The implementation must track primal/dual residuals per iteration and check for convergence criteria based on tolerance thresholds., using Python)\", \"/math(Analyze the convergence plots generated by the JAX code. Compare the empirical convergence rate against the theoretical bounds discovered via the literature search, paying close attention to the influence of the relaxation parameter and eigenvalue distribution of the sensing matrix.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art theoretical foundations. /math is then used to perform the formal derivations required for the algorithm. /code translates the mathematical formulation into an empirical testbed. A final /math call closes the loop by verifying that the empirical results align with the theory, providing a robust analysis.\", \"expected_artifacts\": [\"A PDF report containing the full mathematical derivation of the ADMM update equations.\", \"A Python script (`admm_comparison.py`) implementing both algorithms and the experiment.\", \"A JSON file (`convergence_metrics.json`) with primal/dual residuals per iteration for both variants.\", \"Plots (`convergence.png`) visualizing the convergence rate comparison.\", \"A summary text file (`analysis.txt`) interpreting the results with citations to at least two arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the basis pursuit denoising problem as a constrained convex optimization problem.\", \"2. Construct the augmented Lagrangian and derive the iterative primal-dual update rules for standard ADMM.\", \"3. Modify the standard update rules to incorporate an over-relaxation parameter, following literature guidance.\", \"4. Validate the JAX implementation by checking the correctness of the proximal operator and comparing results on a small, known problem against a trusted library like CVXPY.\", \"5. Execute the comparative experiment, generating plots of primal and dual residuals versus iteration count.\", \"6. Audit the final report to verify that the empirical convergence analysis is correctly justified by the cited theoretical guarantees.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results provide theoretical papers for the /math agent to derive the algorithms. The derived equations are the specification for the /code agent. The code's output (plots, metrics) is then passed back to the /math agent for formal analysis. The final package is audited by the router. Fallback: If empirical convergence contradicts theory, the /code agent must re-verify its implementation against the /math derivation.\", \"todo_list\": [\"- [ ] /general-search: Find at least two key papers on relaxed ADMM convergence.\", \"- [ ] /math: Derive the update equations for both ADMM variants.\", \"- [ ] /code: Implement the algorithms and the experimental setup in JAX.\", \"- [ ] /code: Generate plots and metrics, saving them to the specified files.\", \"- [ ] /math: Verify the correctness of the derivation against established literature.\", \"- [ ] /math: Analyze the output plots and connect empirical rates to theory.\", \"- [ ] /code: Verify the implementation's residual calculations match the mathematical definitions.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"convex-optimization\", \"admm\", \"primal-dual\"], \"acceptance_criteria\": [\"- The provided code must be executable and reproduce the plots/metrics in the report.\", \"- The mathematical derivation in the PDF must be complete and free of errors.\", \"- The final analysis must correctly link the observed faster convergence of the relaxed ADMM variant to theoretical conditions discussed in the cited papers.\", \"- The report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 103, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to synthesize a Python function for computing the Gr\u00f6bner basis of a polynomial ideal using a neural-symbolic approach. The synthesizer should leverage a pre-trained language model guided by a symbolic solver that verifies intermediate program steps against formal specifications in Coq. The final program must be provably correct and satisfy complexity constraints related to Buchberger's algorithm.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural program synthesis theorem prover 'Gr\u00f6bner basis' Buchberger algorithm complexity\\\", mode=web)\", \"/math(Formalize the synthesis problem for Buchberger's algorithm using higher-order logic, defining correctness invariants and complexity bounds. The formulation must consider Bayesian posterior updates for the program search space.)\", \"/code(Implement a Python synthesizer interfacing a transformer LLM with a Coq verifier via SerAPI. The synthesis loop must use Monte Carlo Tree Search guided by prover feedback, and track synthesis success rate and computational complexity.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to survey the state-of-the-art literature on neural-symbolic synthesis for mathematical algorithms. This informs the /math agent's task of creating a rigorous formal specification. Finally, the /code agent implements the complex synthesizer based on the formal groundwork, ensuring a provably correct output.\", \"expected_artifacts\": [\"A literature review summary as a markdown file with at least 3 arXiv citations.\", \"A formal specification document in LaTeX/PDF detailing the logic and invariants for the synthesis problem.\", \"Python source code for the neural-symbolic synthesizer, including interfaces to the LLM and Coq.\", \"A verification report in JSON format containing generated proofs and synthesis success/failure logs for a benchmark suite.\", \"A final metrics report summarizing performance against primary and secondary objectives.\"], \"thinking_outline\": [\"1. Research existing frameworks for neural-symbolic program synthesis, focusing on those integrating LLMs with formal verifiers like Coq or Lean.\", \"2. Formulate the mathematical specification of Buchberger's algorithm in a formal language suitable for the chosen theorem prover. Define the correctness properties and termination conditions.\", \"3. Design the architecture of the synthesizer, including the prompt engineering strategy for the LLM, the communication protocol with the verifier, and the search algorithm (e.g., MCTS) to guide the synthesis.\", \"4. Implement the prototype in Python, ensuring robust error handling for interactions with the external verifier process.\", \"5. Validate the synthesizer by testing it on a benchmark suite of polynomial ideals. Verify that the synthesized programs are correct by checking the generated proofs.\", \"6. Audit the performance metrics, including synthesis success rate, average synthesis time, and the complexity of the generated programs compared to a reference implementation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The /math agent's formal spec is used as the ground truth for the /code agent's verification loop. The /code agent's output must include formal proofs from the verifier; fallback: If the synthesizer's success rate is below threshold, the /code agent will refine the LLM prompting strategy based on failure patterns and re-run.)\", \"todo_list\": [\"- [ ] /general-search: Find SOTA papers on LLM-guided synthesis with formal verifiers for mathematical algorithms.\", \"- [ ] /math: Develop the formal specification for Buchberger's algorithm in Coq's logic (Gallina).\", \"- [ ] /code: Implement the Python synthesizer scaffolding, including the LLM API and Coq verifier subprocess communication via SerAPI.\", \"- [ ] /code: Implement the core Monte Carlo Tree Search synthesis loop guided by prover feedback.\", \"- [ ] /math: Verify that the logical invariants defined in the formal spec are correctly checked by the verifier during synthesis.\", \"- [ ] /code: Validate the entire system on a test suite of polynomial ideals and log the success rate and proofs.\", \"- [ ] /router: Perform final QA, checking artifacts against acceptance criteria and metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic\", \"program-synthesis\", \"theorem-proving\"], \"acceptance_criteria\": [\"The synthesizer must successfully generate a correct Gr\u00f6bner basis function for at least 80% of the test cases in a standard benchmark set (e.g., from the 'SymbolicData' project).\", \"Each successfully synthesized function must be accompanied by a formal proof of correctness generated by the integrated Coq prover.\", \"The final report must include a literature review citing at least 3 relevant arXiv papers and a performance analysis comparing synthesis time against problem complexity.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 104, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a SE(3)-equivariant graph neural network for a molecular dynamics simulation that explicitly conserves the system's Hamiltonian. Can you derive the continuous-time dynamics using Hamiltonian mechanics, formulate the GNN architecture to learn the potential energy function, and then implement a symplectic integrator, like Velocity Verlet, to ensure long-term energy stability? I want to validate this on a simple Lennard-Jones system of Argon atoms and benchmark the energy drift against a non-conservative baseline.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org equivariant graph neural networks Hamiltonian dynamics energy conservation symplectic integrators chemistry\\\", mode=rag)\", \"/math(Derive the GNN update rules as a discrete, symplectic map preserving the system Hamiltonian. Define the potential energy function using equivariant message passing and derive its force field via automatic differentiation, ensuring compliance with Euler-Lagrange equations.)\", \"/code(Implement the derived SE(3)-equivariant GNN and Velocity Verlet integrator in PyTorch Geometric. Simulate a Lennard-Jones fluid, track the total energy drift over a long trajectory, and use CUDA for acceleration to meet the convergence criteria., using Python)\", \"/math(Formally verify that the discrete update step implemented in the Python code is a valid discretization of the continuous-time Hamiltonian flow derived earlier. Audit the code's force calculation against the analytical gradient of the learned potential energy.)\"], \"route_rationale\": \"The blue general-search agent first gathers foundational literature on Hamiltonian GNNs. /math is then used for the core theoretical derivation of the symplectic integrator. /code provides the empirical implementation and validation by simulating the system. A final /math call is crucial for verifying the theoretical consistency between the derivation and the implemented code, ensuring correctness.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the Hamiltonian-conserving GNN and the symplectic integration scheme.\", \"A Python script implementing the GNN and the molecular dynamics simulation.\", \"A PNG plot showing the total system energy over time for both the conservative and a baseline non-conservative model, demonstrating stability.\", \"A final report in Markdown summarizing the methodology, results, and citing at least two relevant academic papers.\", \"A JSON file containing the primary and secondary metrics, including final energy drift statistics.\"], \"thinking_outline\": [\"1. Research state-of-the-art SE(3)-equivariant GNN architectures suitable for learning potential energy surfaces and their relationship to Hamiltonian mechanics.\", \"2. Formulate the system's Hamiltonian and derive the equations of motion. Design a GNN architecture whose output can be interpreted as a potential energy function, from which forces can be derived as the negative gradient.\", \"3. Derive the update rules for a symplectic integrator (e.g., Velocity Verlet) that uses the GNN-derived forces to propagate the system in time.\", \"4. Implement this model and simulation environment in Python, setting up a simple test case like a box of particles interacting via the Lennard-Jones potential.\", \"5. Validate the implementation by running a long simulation and checking that the total energy (kinetic + potential) remains constant up to a small, non-drifting numerical error.\", \"6. Formally verify the correspondence between the implemented discrete update steps in the code and the continuous-time symplectic dynamics derived in the math stage.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation & simulation) -> /math (verification). The final math agent verifies that the code's force computation and integrator step correctly implement the initial derivation. Verification: Check that energy drift in the simulation is below the specified tolerance. Fallback: If energy drift is high, re-evaluate the math derivation for non-symplectic terms or audit the code for implementation errors, then re-run.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Hamiltonian-preserving and equivariant GNNs for physical simulations.\", \"- [ ] /math: Produce the full derivation for the GNN-based potential and the symplectic integrator.\", \"- [ ] /code: Implement the simulation environment, the GNN model, and the integrator.\", \"- [ ] /code: Run the simulation and generate the energy conservation plot and metrics file.\", \"- [ ] /math: Verify that the force calculation in the code is the correct negative gradient of the potential energy derived from the GNN.\", \"- [ ] /math: Verify that the integrator implementation in the code correctly matches the derived symplectic update rules.\", \"- [ ] /code: Document the code with comments explaining the correspondence to the mathematical derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"computational-physics\"], \"acceptance_criteria\": [\"- The final simulation must demonstrate energy conservation, with the energy drift metric below 1e-5 over 10,000 simulation steps.\", \"- The mathematical derivation must be complete, self-contained, and rigorously justify the symplectic nature of the integrator.\", \"- The final report must cite at least two relevant arXiv preprints on equivariant GNNs or neural ODEs.\", \"- The implemented code must be well-commented and directly correspond to the variables and equations in the mathematical derivation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 105, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nConsider a Structural Causal Model (SCM) with variables {Z1, X, M, Y, Z2}, where Z1 and Z2 are unobserved confounders. The causal DAG contains edges Z1->X, Z1->M, X->M, M->Y, and Z2->M, Z2->Y. The core task is to derive a closed-form identification formula for the causal effect P(y|do(x)) using Pearl's do-calculus, as simple backdoor adjustment is blocked. Subsequently, implement a non-parametric estimator for this quantity and validate its asymptotic consistency via simulation, analyzing its convergence properties.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:ucla.edu/~pearl/ causal inference identification algorithm do-calculus front-door criterion\\\", mode=both)\", \"/math(Formally derive the identification formula for P(y|do(x)) in the specified causal Bayesian network with unobserved confounders. Sequentially apply the three rules of do-calculus to eliminate the do-operator, expressing the causal quantity purely in terms of observational probabilities.)\", \"/code(Write Python code to simulate data from the specified SCM. Implement the non-parametric estimator derived from the do-calculus formula. Validate estimator convergence by comparing its output against the true Average Treatment Effect (ATE) obtained by simulating a direct intervention in the SCM.)\"], \"route_rationale\": \"The blue general-search agent first gathers foundational papers on causal identification. The math agent then uses this theory to perform the core symbolic derivation. Finally, the code agent implements the derived formula and runs a simulation to validate its correctness and analyze its statistical properties.\", \"expected_artifacts\": [\"- A formal mathematical proof detailing the step-by-step derivation of the identification formula for P(y|do(x)) using do-calculus.\", \"- A Python script (`.py`) containing the data-generating SCM, the implementation of the derived estimator, and the simulation logic.\", \"- A simulation report (Jupyter Notebook or PDF) with plots showing the estimator's convergence towards the true causal effect as sample size increases.\", \"- A list of at least two cited academic papers on causal identification with their arXiv IDs or DOIs.\"], \"thinking_outline\": [\"1. Formalize the problem by representing the specified system as a Structural Causal Model and its corresponding Directed Acyclic Graph (DAG).\", \"2. Use /general-search to find and review the formal rules of do-calculus and seminal papers on identifiability, such as those by Pearl or Tian.\", \"3. Apply the rules of do-calculus sequentially in /math to manipulate the expression P(y|do(x)) until the do-operator is removed and the expression is solely in terms of observational distributions.\", \"4. Verify the derivation by ensuring each step is a valid application of one of the three rules and that all assumptions (e.g., positivity) are explicitly stated.\", \"5. Implement both the data generation process (SCM) and the derived estimator in /code. Validate the estimator's implementation by checking its performance on simulated data where the ground truth causal effect is known.\", \"6. Analyze the estimator's statistical properties, such as consistency and rate of convergence, presenting the results with appropriate plots and metrics.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Simulation results from /code must align with the theoretical derivation from /math. The final estimand must be expressed only in terms of P(v|pa(v)). fallback: If derivation fails, /math agent should request /general-search for literature on similar non-identifiable models.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal literature on do-calculus and the identification algorithm.\", \"- [ ] /math: Translate the user query into a formal SCM and DAG representation.\", \"- [ ] /math: Execute the step-by-step derivation for P(y|do(x)) using the rules of do-calculus.\", \"- [ ] /code: Implement a data generator in Python for the specified SCM.\", \"- [ ] /code: Implement the statistical estimator based on the final formula from the /math agent.\", \"- [ ] /math: Verify that the derived formula from the first /math step corresponds to a known identification pattern like the front-door criterion or a more general case.\", \"- [ ] /code: Run simulations to generate convergence plots and compute MSE against the true ATE.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"structural-causal-model\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly and completely removes the do-operator using valid rules.\", \"- The Python code correctly implements the derived observational formula and simulates the SCM as described.\", \"- The simulation report clearly demonstrates the estimator's convergence to the true causal effect with increasing sample size.\", \"- At least two relevant academic papers (e.g., from Pearl, Tian, Shpitser) are cited with proper identifiers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 107, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the impartial game 'Dawson's Kay'. I require a formal analysis of its computational complexity, specifically its relation to PSPACE. Please derive the recurrence relation for its nim-sequence (Grundy values) using the Sprague-Grundy theorem. Additionally, develop and benchmark a Python implementation to empirically validate the theoretical complexity by measuring runtime performance against increasing board sizes.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:oeis.org \\\\\\\"Dawson's Kay\\\\\\\" \\\\\\\"nim-sequence\\\\\\\" complexity analysis\\\", mode=rag)\", \"/math(Derive the Sprague-Grundy recurrence for Dawson's Kay and analyze its adversarial game tree complexity, proving its computational hardness.)\", \"/code(Implement a Python solver with memoization, bootstrap performance analysis by timing convergence for increasing N, and verify sequence against database entries.)\"], \"route_rationale\": \"The blue /general-search agent will first gather foundational literature and known results for Dawson's Kay. This context is then passed to /math to construct a formal, rigorous derivation of the game's properties and complexity. Finally, /code will translate the mathematical theory into a concrete implementation for empirical validation of the performance claims, creating a closed verification loop.\", \"expected_artifacts\": [\"A formal mathematical proof (PDF/LaTeX) detailing the derivation of the Grundy number recurrence and the game's computational complexity.\", \"A well-commented Python script (`dawson_kay_solver.py`) that computes the nim-sequence using memoization.\", \"A plot (`runtime_analysis.png`) visualizing the empirical runtime versus board size against the theoretical complexity curve.\", \"A final summary report (`report.md`) that synthesizes the findings and provides citations to at least two relevant academic sources.\", \"A JSON file (`metrics.json`) containing the computed primary and secondary metrics.\"], \"thinking_outline\": [\"1. Research Dawson's Kay and related impartial games to understand the rules and locate existing theoretical results (G-sequence, known complexity) from authoritative sources.\", \"2. Formally derive the recurrence relation for the Grundy numbers (nim-values) of Dawson's Kay using the Sprague-Grundy theorem, defining the set of next-state game positions.\", \"3. Analyze the algorithmic complexity of computing these Grundy numbers and rigorously argue for the game's placement within a specific complexity class (e.g., PSPACE).\", \"4. Implement a dynamic programming or memoized recursion algorithm in Python to compute the nim-sequence efficiently.\", \"5. Validate the implementation's correctness by comparing the first 50-100 computed nim-values against an established sequence from a source like the OEIS (A002187).\", \"6. Verify the theoretical complexity by plotting the algorithm's execution time against increasing board size and checking if the empirical growth rate matches the Big-O analysis.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: search results provide basis for math proof; math recurrence provides spec for code; code output validates math theory; fallback: if code results contradict math derivation, re-evaluate the recurrence derivation in /math with corrected assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers and OEIS entries for Dawson's Kay nim-sequence and complexity.\", \"- [ ] /math: Derive the recurrence for the Grundy values using the Sprague-Grundy theorem's mex rule.\", \"- [ ] /math: Formalize the argument for the game's computational complexity class.\", \"- [ ] /code: Implement the recursive solver with memoization in Python.\", \"- [ ] /code: Generate a plot of runtime vs. board size for n up to at least 1000.\", \"- [ ] /math: Verify that the first 50 values computed by the Python script match the OEIS sequence A002187.\", \"- [ ] /code: Validate that the empirical runtime plot's trend aligns with the theoretical complexity derived by /math.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"computational-complexity\", \"sprague-grundy-theorem\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Grundy number recurrence must be correct, self-contained, and rigorously justified.\", \"- The Python implementation must correctly compute the nim-sequence for Dawson's Kay for n <= 50, perfectly matching the known OEIS sequence A002187.\", \"- The empirical runtime analysis plot must demonstrate a clear trend consistent with the proven theoretical time complexity, with a goodness-of-fit R^2 > 0.95.\", \"- The final report must cite at least two relevant academic papers with valid DOIs or arXiv identifiers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 108, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to construct a Bayesian Neural Network where the weight dynamics are governed by a Stochastic Differential Equation (SDE). Please derive the variational inference objective, specifically the Evidence Lower Bound (ELBO), for this model. Then, implement this Bayesian neural SDE using a numerical solver like the Euler-Maruyama scheme and demonstrate its ability to quantify epistemic uncertainty on a regression task with out-of-distribution data.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference for bayesian neural stochastic differential equations ELBO\\\", mode=rag)\", \"/math(Derive the Evidence Lower Bound for a Bayesian neural SDE, specifying the SDE prior, the variational posterior distribution, and the resulting KL divergence term under an Euler-Maruyama discretization scheme.)\", \"/code(Implement the Bayesian neural SDE in PyTorch, including the SDE solver for weight evolution and the derived variational loss. Validate model convergence and epistemic uncertainty estimation on a synthetic dataset., using Python)\"], \"route_rationale\": \"The blue general-search agent first gathers foundational literature on Bayesian neural SDEs. Next, the math agent performs the formal derivation of the ELBO objective, which is the core theoretical component. Finally, the code agent implements the derived mathematical model and performs empirical validation, checking for convergence and correct uncertainty behavior.\", \"expected_artifacts\": [\"- A LaTeX/PDF document detailing the full mathematical derivation of the ELBO.\", \"- A commented Python script (`.py`) containing the PyTorch implementation of the Bayesian Neural SDE and the training loop.\", \"- A set of plots (`.png`) visualizing the model's predictive mean and uncertainty bounds against ground truth data.\", \"- A JSON file (`metrics.json`) reporting the final Negative Log-Likelihood and ELBO values.\", \"- A `CITATIONS.md` file listing at least two relevant arXiv papers with IDs.\"], \"thinking_outline\": [\"1. Define the formal mathematical model of a neural network whose weights evolve according to a pre-specified SDE, which acts as the prior.\", \"2. Derive the Evidence Lower Bound (ELBO) as the objective function for variational inference, clearly separating the expected log-likelihood term and the Kullback-Leibler (KL) divergence between the approximate posterior and the SDE prior.\", \"3. Discretize the continuous-time SDE using the Euler-Maruyama method to make the model computationally tractable for the code implementation.\", \"4. Implement the model in a deep learning framework, using the reparameterization trick to get low-variance gradients for the variational parameters.\", \"5. Verify that the implemented loss function in code perfectly matches the discretized ELBO derived in the mathematical analysis.\", \"6. Validate the model's performance on a synthetic regression problem, checking that the model's predictive uncertainty (epistemic) is significantly higher in regions where no training data was provided.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent provides the final, discretized ELBO equation to the code agent. The code agent implements this equation and produces empirical results (plots, metrics) for final review. Verification: The code agent's loss implementation must be audited against the math agent's derivation. Fallback: If convergence fails, the code agent will re-check gradient computation and hyperparameter settings before requesting a review of the ELBO derivation from the math agent.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on Bayesian Neural SDEs and their training via variational inference.\", \"- [ ] /math: Derive the full ELBO, showing the explicit form of the KL divergence term between the variational posterior and the SDE prior.\", \"- [ ] /math: Detail the Euler-Maruyama discretization of the weight SDE and the resulting discretized objective function.\", \"- [ ] /code: Implement the Bayesian neural SDE model and the custom training loop in PyTorch.\", \"- [ ] /math: Verify that the loss function implemented in the Python code is a correct representation of the derived ELBO.\", \"- [ ] /code: Train the model on a synthetic dataset with a gap and generate plots showing predictive mean and uncertainty bands.\", \"- [ ] /code: Validate that uncertainty is visibly higher in the data gap, confirming correct epistemic uncertainty modeling.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-deep-learning\", \"stochastic-differential-equations\", \"variational-inference\"], \"acceptance_criteria\": [\"- The provided mathematical derivation of the ELBO is rigorous and complete.\", \"- The Python code runs without errors and the model training converges (i.e., the ELBO consistently improves over epochs).\", \"- The output plots clearly demonstrate that the model's predictive uncertainty is qualitatively larger in out-of-distribution regions of the input space.\", \"- The report cites at least two relevant academic papers from arXiv.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 110, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the computational complexity of determining the Grundy value (nim-value) for the octal game .137. I need a formal proof sketch for its PSPACE-completeness by reduction from a known PSPACE-complete problem, likely related to PARTIZAN GEOGRAPHY. Additionally, implement an efficient algorithm to compute its nim-sequence and empirically analyze its periodicity, comparing its convergence properties against other octal games discussed in Winning Ways.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:cs.cmu.edu computational complexity octal games sprague-grundy PSPACE-completeness\\\", mode=both)\", \"/math(Formalize the recurrence for the Grundy values of octal game .137 and construct a proof sketch for its PSPACE-completeness, assessing the asymptotic complexity and causal structure of the game states.)\", \"/code(Implement in Python a memoized recursive algorithm to compute the nim-sequence for .137, including runtime validation checks and bootstrap analysis on the sequence's convergence to periodicity.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on the complexity of octal games, which is essential for the formal proof. This research informs the /math agent's formal derivation and proof construction. Finally, the /code agent implements and validates the theoretical findings, providing empirical evidence.\", \"expected_artifacts\": [\"A LaTeX document containing the formal definition of the octal game .137, the derivation of its Grundy value recurrence, and the PSPACE-completeness proof sketch.\", \"A Python script implementing the nim-sequence generation algorithm with memoization and performance profiling.\", \"A JSON file reporting the computed nim-sequence up to n=10000, the primary/secondary metrics, and any detected period.\", \"A summary report citing at least two relevant academic papers with their arXiv IDs.\"], \"thinking_outline\": [\"1. Research the existing literature on the computational complexity of impartial games, focusing on octal games and the Sprague-Grundy theorem, to identify common reduction techniques for PSPACE-completeness proofs.\", \"2. Formally define the move set for the octal game .137 and derive the mathematical recurrence relation for its Grundy values, g(n), based on the mex (minimum excluded value) of the g-values of reachable states.\", \"3. Verify the derived recurrence by manually calculating the first few terms of the nim-sequence (e.g., g(0) to g(10)) and comparing them against established patterns for simple impartial games.\", \"4. Develop a proof sketch for the PSPACE-completeness of determining g(n) for this game, likely by constructing a reduction from a problem like Generalized Geography or QSAT.\", \"5. Implement an efficient, memoized algorithm in Python to compute the nim-sequence and validate its correctness by comparing outputs for a known simple octal game (e.g., .1) against a trusted source.\", \"6. Execute the algorithm for .137, collect performance data, and analyze the resulting sequence for periodicity, which is a key structural property of such games.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: research findings from /general-search must be cited in the /math proof; the recurrence from /math must be correctly implemented in /code; fallback: if PSPACE proof is intractable, pivot to proving membership in PSPACE and analyzing algorithmic complexity).\", \"todo_list\": [\"- [ ] /general-search: Find at least two key papers on octal game complexity and PSPACE-completeness proofs.\", \"- [ ] /math: Derive the Grundy value recurrence relation for octal game .137.\", \"- [ ] /math: Construct the PSPACE-completeness proof sketch, clearly defining the reduction.\", \"- [ ] /code: Implement the memoized algorithm for computing the nim-sequence.\", \"- [ ] /code: Add validation logic to test the implementation against a known game sequence.\", \"- [ ] /math: Verify that the logic in the /code implementation correctly reflects the derived mathematical recurrence.\", \"- [ ] /code: Generate the final artifacts, including the nim-sequence JSON and performance metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"sprague-grundy-theorem\"], \"acceptance_criteria\": [\"- Pass: The PSPACE-completeness proof sketch is logically sound and correctly uses a reduction from a known PSPACE-complete problem.\", \"- Pass: The Python code correctly computes the nim-sequence for a known validation case and runs without error for the target game .137.\", \"- Pass: The final report includes a computed nim-sequence of at least 10,000 values and correctly identifies its period, if found within that range.\", \"- Pass: At least two relevant arXiv preprints or peer-reviewed articles are cited.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 111, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the InfoNCE loss used in contrastive self-supervised learning from first principles, specifically through the lens of mutual information maximization. Then, analyze the spectral properties of the self-attention matrix in a Vision Transformer trained with this objective. How does the principal eigenvalue relate to the convergence of the Bayesian learning objective and the emergence of semantic features? Implement a PyTorch simulation to validate your theoretical claims by tracking the spectral norm and loss.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org InfoNCE mutual information maximization bayesian interpretation contrastive learning spectral analysis attention\\\", mode=both)\", \"/math(Derive the InfoNCE loss as a lower bound on mutual information, then connect the principal eigenvalue of the attention matrix to the model's convergence rate under a Bayesian framework.)\", \"/code(Implement a PyTorch ViT using SimCLR/MoCo with CUDA acceleration to validate the derived hypothesis, tracking the top eigenvalue and InfoNCE loss convergence, and reporting confidence intervals on the spectral norm., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature from arXiv, ensuring the theoretical work is grounded in current research. This context is handed to the math agent for the formal derivation. Finally, the code agent implements a simulation to empirically verify the mathematical theory, providing a rigorous check on the derivation's validity.\", \"expected_artifacts\": [\"A LaTeX document with the full mathematical derivation of InfoNCE and the eigenvalue hypothesis.\", \"A Python script implementing the Vision Transformer and contrastive learning experiment.\", \"A JSON file containing the logged metrics (loss, principal eigenvalue, spectral norm) over training epochs.\", \"A summary report (.md) synthesizing theoretical and empirical results, including plots and citations of at least 2 relevant arXiv papers.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation by reviewing literature on contrastive learning (e.g., CPC, SimCLR) and its connection to mutual information estimation.\", \"2. Formally derive the Noise Contrastive Estimation (NCE) and InfoNCE loss functions, proving that InfoNCE maximizes a lower bound on the mutual information between two augmented views of an image.\", \"3. Formulate a hypothesis linking the spectral properties (specifically, the principal eigenvalue and spectral norm) of the self-attention matrices to the stability and convergence of the training process.\", \"4. Implement a lightweight Vision Transformer and a contrastive learning framework (like SimCLR) in PyTorch to empirically test the hypothesis.\", \"5. Validate the mathematical derivation by comparing the theoretical predictions about eigenvalue behavior with the empirical measurements from the simulation, checking for correlation with loss reduction.\", \"6. Synthesize the findings into a report that includes the mathematical proof, experimental results with visualizations, and a discussion on the implications for understanding self-supervised representation learning.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Math agent's derivation of the InfoNCE-eigenvalue relationship is checked against the empirical results from the Code agent's simulation. The loss curve must align with the spectral norm evolution. fallback: If simulation contradicts theory, the Math agent must revisit the derivation's assumptions, possibly relaxing constraints.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on InfoNCE, SimCLR, and spectral properties of Transformers (e.g., from arXiv).\", \"- [ ] /math: Derive the InfoNCE loss from the perspective of mutual information maximization.\", \"- [ ] /math: Formulate and prove the connection between the attention matrix's principal eigenvalue and the Bayesian learning convergence.\", \"- [ ] /code: Implement a Vision Transformer with a SimCLR-style contrastive loss in PyTorch.\", \"- [ ] /code: Instrument the code to track and log the InfoNCE loss and the top eigenvalue of the final self-attention layer's matrix per epoch.\", \"- [ ] /math: Verify that the code agent's implementation correctly reflects the loss function derived.\", \"- [ ] /code: Verify that the empirical results (plots of loss vs. eigenvalue) support the mathematical hypothesis.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"self-supervised-learning\", \"information-theory\", \"attention-mechanism\"], \"acceptance_criteria\": [\"The final report must include a complete mathematical derivation of the InfoNCE loss.\", \"The Python simulation must run successfully and produce plots showing the evolution of the primary and secondary metrics over at least 50 epochs.\", \"The report's conclusion must explicitly state whether the empirical results from the code validate the mathematical hypothesis about the principal eigenvalue.\", \"At least two relevant arXiv pre-prints must be cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 113, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the K-SVD algorithm for dictionary learning, including the sparse coding (OMP) and dictionary update (SVD) stages. Then, implement it in Python and compare its performance for recovering sparsely represented biomedical signals from noisy compressive measurements against the Method of Optimal Directions (MOD). I need a formal analysis of its convergence properties and empirical validation of the reconstruction accuracy.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD dictionary learning convergence proof biomedical signal processing\\\", mode=both)\", \"/math(Derive the full K-SVD update rules, detailing the Orthogonal Matching Pursuit step for sparse coding and the rank-1 approximation via SVD for dictionary atom updates. Outline the proof of convergence by showing the objective function is non-increasing., includes cybersecurity)\", \"/code(Implement K-SVD and MOD algorithms in Python. Simulate a compressed sensing scenario using synthetic sparse biomedical signals, an overcomplete dictionary, and a random measurement matrix. Track reconstruction SNR and dictionary recovery error over iterations.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather seminal literature on K-SVD and MOD. This informs the /math agent's formal derivation of the algorithm and its convergence properties. Finally, the /code agent implements the derived algorithm to provide empirical validation, directly addressing the user's request for a math-plus-code solution.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF report containing the full mathematical derivation of the K-SVD algorithm and its convergence analysis.\", \"- A Python script (`ksvd_simulation.py`) implementing K-SVD, MOD, and the comparative experiment.\", \"- A set of plots (`comparison_plots.png`) showing reconstruction SNR vs. iteration for both algorithms.\", \"- A `metrics.json` file reporting the final SNR, dictionary recovery error, and computation time.\", \"- A bibliography with at least two cited papers on sparse dictionary learning.\"], \"thinking_outline\": [\"1. Research the foundational principles of the K-SVD and Method of Optimal Directions (MOD) algorithms, focusing on their theoretical guarantees for dictionary recovery under noisy conditions.\", \"2. Formally derive the two main stages of the K-SVD iteration: the sparse coding stage using Orthogonal Matching Pursuit (OMP) and the dictionary update stage using Singular Value Decomposition (SVD) for rank-1 approximation.\", \"3. Implement both the K-SVD and MOD algorithms in Python, constructing a synthetic compressed sensing experiment with a known ground-truth dictionary and sparse signals emulating biomedical data.\", \"4. Verify that the Python implementation of K-SVD demonstrates monotonically decreasing reconstruction error across iterations, aligning with the convergence properties derived in the mathematical analysis.\", \"5. Execute the comparative simulation to measure and contrast the final reconstruction Signal-to-Noise Ratio (SNR) and dictionary recovery error between K-SVD and MOD.\", \"6. Validate the experimental outcomes against the theoretical claims and performance benchmarks cited in the literature gathered during the initial research phase.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent provides the formal derivation and pseudocode to the code agent. The code agent produces plots and metrics. Router QA performs final verification: the empirical convergence shown in the plots must match the non-increasing error property derived by the math agent. Fallback: If a discrepancy exists, the math agent must re-audit the derivation for potential errors.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on K-SVD (e.g., by Aharon, Elad, Bruckstein) and MOD.\", \"- [ ] /math: Formally derive the K-SVD objective function and the iterative update rules for both the dictionary atoms and sparse coefficients.\", \"- [ ] /math: Outline the proof of convergence for the K-SVD algorithm.\", \"- [ ] /code: Implement the K-SVD and MOD algorithms from scratch in Python using NumPy/SciPy.\", \"- [ ] /code: Set up and run the comparative simulation on synthetic biomedical signals, generating all required plots and metrics.\", \"- [ ] /math: Verify that the update equations implemented in the Python script are a one-to-one match with the derived formulas.\", \"- [ ] /code: Validate that the generated plots empirically confirm the theoretical convergence behavior.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\", \"signal-processing\"], \"acceptance_criteria\": [\"- The mathematical derivation of the K-SVD algorithm must be self-contained, rigorous, and correct.\", \"- The Python code must be executable and successfully reproduce the key findings reported, including the performance metrics within a 5% tolerance.\", \"- The final report must clearly demonstrate, through plots and metrics, the performance difference between K-SVD and MOD for the specified task.\", \"- At least two relevant academic papers must be cited correctly using their DOI or arXiv ID.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 114, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to synthesize a provably correct Python program for finding amicable number pairs up to a limit 'n'. The synthesis process must be guided by a neural-symbolic framework. Specifically, can you use a large language model to propose loop invariants within a Hoare logic framework, and then use a differentiable SAT solver or a similar neuro-symbolic reasoner to validate these invariants and provide feedback to the model? The goal is to automate the generation of both the algorithm and its correctness proof.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neuro-symbolic program synthesis Hoare logic differentiable SAT solver\\\", mode=both)\", \"/math(Define Hoare logic invariants for the amicable numbers problem, analyzing the convergence properties of the synthesis loop from a Bayesian perspective to ensure proof soundness.)\", \"/code(Implement the neuro-symbolic synthesizer in Python, tracking synthesis complexity metrics and storing verified program-invariant pairs in a local database for curricular learning.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on neuro-symbolic synthesis. This research informs the /math agent's formalization of the problem using Hoare logic. Finally, the /code agent implements the synthesis loop based on the formal specification, creating a tangible and verifiable artifact.\", \"expected_artifacts\": [\"- A formal specification of the Hoare logic proof (pre-conditions, post-conditions, loop invariant) for the amicable numbers algorithm in a `.tex` file.\", \"- A Python script (`.py`) implementing the neural-symbolic synthesis loop.\", \"- The successfully synthesized and verified Python function for finding amicable numbers.\", \"- A summary report (`.md`) detailing the methodology, synthesis success rate, and a bibliography citing at least two relevant academic papers with arXiv IDs.\"], \"thinking_outline\": [\"1. Research current state-of-the-art neural-symbolic methods for program synthesis, focusing on the integration of large language models with formal verifiers like SAT/SMT solvers.\", \"2. Formalize the amicable numbers problem using Hoare logic, precisely defining the pre-conditions, post-conditions, and the structure of the necessary loop invariant for the sum of proper divisors.\", \"3. Implement a Python framework that uses a pre-trained language model to generate candidate invariants and program bodies, and then calls a symbolic solver (e.g., Z3) to validate these proposals.\", \"4. Verify that the logical formulas derived in the math step are correctly translated into the solver's input format within the Python implementation to prevent logical errors.\", \"5. Execute the complete synthesis loop, measure the success rate and computational cost, and validate the final synthesized program by checking its output against known amicable pairs.\", \"6. Compile a final report summarizing the findings, discussing the limitations of the approach, and providing proper citations to the surveyed literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: the math agent verifies that the proposed logical framework is sound based on literature from search; the code agent verifies its implementation against the formal spec from math; router QA audits the final synthesized code for correctness. fallback: if synthesis fails, the code agent will report the logical contradiction and request a revised invariant from the math agent.)\", \"todo_list\": [\"- [ ] /general-search: Find SOTA papers on neural program synthesis with differentiable logic or SAT/SMT solvers.\", \"- [ ] /math: Formulate the complete Hoare triple (pre-condition, program, post-condition) for the amicable numbers problem.\", \"- [ ] /math: Derive the specific loop invariant based on number theory principles for the correctness proof.\", \"- [ ] /code: Implement the Python synthesis loop that interfaces a language model with a symbolic solver like Z3.\", \"- [ ] /code: Implement a verifier function to check the synthesized program's output against a pre-computed list of amicable pairs.\", \"- [ ] /math: Verify that the logical constraints encoded in the Python solver calls accurately represent the derived Hoare logic invariants.\", \"- [ ] /code: Audit the final synthesized program to confirm it is both correct and reasonably efficient.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"program-synthesis\", \"neuro-symbolic-ai\", \"theorem-proving\"], \"acceptance_criteria\": [\"- A complete and correct Hoare logic proof for the amicable numbers algorithm is provided in the `proof.tex` artifact.\", \"- The Python script successfully synthesizes a correct program that can find all amicable pairs up to at least 10000.\", \"- The final report includes a non-zero `synthesis_success_rate` and cites at least two relevant arXiv papers by ID.\", \"- The `synthesized_program.py` artifact contains only the verified, synthesized code, which passes all internal checks.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 115, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Gaussian Process Latent Variable Model (GP-LVM) using stochastic variational inference (SVI). Please derive the variational lower bound (ELBO) objective, explicitly using the reparameterization trick for the inducing point posterior and latent variable approximations. Then, implement this in Python using Pyro or JAX, demonstrating convergence on a high-dimensional Bayesian finance dataset. Analyze the convergence diagnostics, such as the ELBO trace, and visualize the learned latent space.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org gaussian process latent variable model stochastic variational inference reparameterization trick bayesian convergence\\\", mode=rag)\", \"/math(Derive the full Evidence Lower Bound (ELBO) for the Bayesian GP-LVM, detailing the application of the reparameterization trick to obtain low-variance gradient estimators for the variational parameters, ensuring the derivation handles inducing point approximations.)\", \"/code(Implement the SVI algorithm for the GP-LVM in Python/Pyro, optimizing the derived ELBO. Validate the model's convergence on a finance dataset, tracking ELBO metrics and generating plots of the latent space representation.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to find foundational papers on SVI for GP-LVMs. This theoretical grounding is passed to /math to produce a formal, verifiable derivation of the learning objective. Finally, /code implements this derivation to provide an empirical solution and performance analysis, directly addressing the user's request for a working model and validation.\", \"expected_artifacts\": [\"- A LaTeX/Markdown document containing the full mathematical derivation of the SVI objective for the GP-LVM.\", \"- A Python script implementing the GP-LVM with SVI, including data loading and training loops.\", \"- A summary report (.pdf) analyzing the model's performance, including ELBO convergence plots and visualizations of the 2D latent space.\", \"- A BibTeX file with citations for at least two relevant academic papers.\", \"- A JSON file containing the final ELBO value and training time metrics.\"], \"thinking_outline\": [\"1. Research and synthesize the modern formulation of Bayesian GP-LVMs and the SVI framework, focusing on inducing point methods for scalability.\", \"2. Formulate the generative model and the amortized variational posterior. Derive the ELBO, carefully separating the expected log-likelihood term and the KL divergence regularizers.\", \"3. Apply the reparameterization trick to the variational distributions to create a differentiable, low-variance Monte Carlo estimator of the ELBO.\", \"4. Implement the model and the SVI optimization loop in a probabilistic programming language, ensuring correct gradient propagation and minibatching.\", \"5. Validate the implementation by checking that the ELBO is non-decreasing on average during training and qualitatively inspecting the learned latent space for meaningful structure.\", \"6. Audit the code logic against the mathematical derivation to verify that all terms in the objective function are correctly implemented.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent will provide foundational arXiv papers to the /math agent. The /math agent will produce a formal derivation that serves as a specification for the /code agent. The /code agent's implementation will be cross-referenced with the derivation for verification. Fallback: If the initial derivation proves numerically unstable, the /math agent will be re-prompted to explore alternative variational families or KL divergence approximations before re-engaging the /code agent.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 seminal papers on SVI for GP-LVMs.\", \"- [ ] /math: Derive the ELBO for the Bayesian GP-LVM with inducing points.\", \"- [ ] /math: Show the application of the reparameterization trick for gradients.\", \"- [ ] /code: Implement the GP-LVM model and SVI optimizer in Pyro.\", \"- [ ] /code: Train the model on a suitable dataset and log ELBO metrics.\", \"- [ ] /math: Verify that the code's loss function correctly implements the derived ELBO.\", \"- [ ] /code: Generate visualizations of the latent space and ELBO convergence plot.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-processes\", \"bayesian-inference\", \"variational-inference\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO must be complete and correct.\", \"- The implementation must execute and show the ELBO converging (i.e., being non-decreasing on average) over training iterations.\", \"- The final report must contain a coherent analysis, including a latent space visualization that demonstrates meaningful structure (e.g., clustering).\", \"- The solution must cite at least two relevant papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"The final reported value of the Evidence Lower Bound, averaged over the last 100 training iterations. A higher value is better.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"The variance of the ELBO estimates over the last 10% of training. A lower variance indicates a more stable optimization process.\"}}", "categories": ["advanced", "metrics_computation"]}
{"index": 117, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to construct a federated learning system for image classification that preserves client privacy using a hybrid approach. Specifically, implement the federated averaging algorithm where individual client model updates are protected using the Paillier partially homomorphic encryption scheme for secure aggregation at the server. Additionally, apply client-level differential privacy by adding calibrated Laplacian noise before encryption. Provide a formal mathematical proof for the resulting (\u03b5, \u03b4)-differential privacy guarantee of this combined mechanism and analyze the convergence behavior under the noise injection.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning 'homomorphic encryption' 'differential privacy' convergence Paillier\\\", mode=web)\", \"/math(Derive the (\u03b5, \u03b4)-differential privacy guarantee for the federated averaging process with Laplacian noise addition prior to Paillier encryption, analyzing the privacy loss composition over multiple communication rounds.)\", \"/code(Implement the federated learning protocol in Python using a library like `phe` for Paillier encryption. The implementation must include client-side model training, noise addition, update encryption, and server-side secure aggregation.)\", \"/math(Analyze the convergence rate of the implemented federated algorithm, factoring in the variance introduced by the differential privacy noise and potential numerical precision issues from the encryption scheme.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on hybrid privacy-preserving FL. This informs the /math agent's formal derivation of the privacy guarantees. The /code agent then implements this theoretically grounded protocol. A final /math step analyzes the implemented system's convergence, providing a theoretical check on the empirical results.\", \"expected_artifacts\": [\"A formal proof in LaTeX detailing the (\u03b5, \u03b4)-differential privacy derivation.\", \"A Python script implementing the end-to-end federated learning system with hybrid privacy.\", \"A summary report (report.md) including convergence plots (accuracy vs. round) and analysis of the privacy-utility trade-off.\", \"A list of at least two cited arXiv papers with their IDs.\", \"Verification output showing that the noise scale in the code matches the privacy budget derived in the proof.\"], \"thinking_outline\": [\"1. Research established methods for combining homomorphic encryption (HE) and differential privacy (DP) in federated learning, focusing on the Paillier cryptosystem.\", \"2. Formulate the secure aggregation protocol mathematically, specifying the client update clipping, noise addition from the Laplace distribution, and Paillier encryption steps.\", \"3. Derive the formal (\u03b5, \u03b4)-differential privacy guarantee for a single round and compose it over T rounds using techniques like the moments accountant or advanced composition theorem.\", \"4. Implement the client and server logic, ensuring cryptographic operations are correct and the DP noise is sampled and applied appropriately before encryption.\", \"5. Validate the implementation's privacy mechanism by cross-referencing the noise-scaling parameter in the code with the value derived in the mathematical proof for the target \u03b5.\", \"6. Empirically evaluate the model's convergence and final accuracy on a benchmark dataset, and check that the performance degradation due to privacy measures is within acceptable limits.\"], \"handoff_plan\": \"/general-search -> /math (proof derivation) -> /code (implementation) -> /math (convergence analysis) -> router QA. The initial math proof dictates the parameters (e.g., noise multiplier) for the code. The code's empirical results (e.g., convergence plots) are then analyzed by the final math step. Verification: The router will check that the code's privacy parameters exactly match the formal proof. Fallback: If the model fails to converge in the code, the task is routed back to /math to re-evaluate the noise parameter's impact on convergence bounds.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on combining Paillier encryption with differential privacy in FL.\", \"- [ ] /math: Formulate and write the complete (\u03b5, \u03b4)-DP proof, clearly stating all assumptions.\", \"- [ ] /code: Implement the client-side logic for training, clipping, noising, and encrypting model updates.\", \"- [ ] /code: Implement the server-side logic for aggregating encrypted updates and decrypting the final result.\", \"- [ ] /math: Verify that the noise-generation parameters used in the /code implementation correctly correspond to the derived privacy budget (\u03b5) in the proof.\", \"- [ ] /code: Run the full federated training process on MNIST or CIFAR-10 and generate convergence plots for accuracy and loss.\", \"- [ ] /math: Provide a formal analysis of the algorithm's convergence properties, referencing the empirical plots from the /code step.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"differential-privacy\", \"cryptography\"], \"acceptance_criteria\": [\"- The submitted mathematical proof for (\u03b5, \u03b4)-differential privacy is rigorous and self-contained.\", \"- The Python code runs to completion and trains a model on a standard dataset (e.g., MNIST), achieving accuracy > 90%.\", \"- The final report correctly plots the model's test accuracy over communication rounds and cites at least two relevant arXiv papers with IDs.\", \"- The privacy budget (\u03b5) stated in the report is consistent with the parameters used in the code and the formal proof.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 119, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the manifold hypothesis for a Bayesian Variational Autoencoder's latent space. Can you derive the theoretical framework for using persistent homology to characterize its topology, specifically using Vietoris-Rips filtrations, and then implement a pipeline to compute persistence diagrams and Betti numbers to check for topological stability under posterior sampling? How does the topology's convergence relate to the ELBO's convergence?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org persistent homology for deep generative models OR VAE latent space topology\\\", mode=rag, includes numpy)\", \"/math(Derive the construction of a Vietoris-Rips simplicial complex from a point cloud sampled from a VAE latent posterior. Define Betti numbers and persistence diagrams as topological invariants to analyze the manifold's homology and ensure theoretical convergence guarantees.)\", \"/code(Implement a PyTorch VAE for a synthetic manifold dataset. Use giotto-tda to perform a Vietoris-Rips filtration on latent samples, compute persistence diagrams, and plot Betti curves. Validate the recovered homology against the ground truth and check the eigenvalue spectrum of the data covariance matrix., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature from arXiv, providing a theoretical foundation. This is handed to /math to formalize the specific derivations required for this problem. Finally, /code implements and validates the derived theory empirically, providing a complete theory-to-practice workflow.\", \"expected_artifacts\": [\"- A formal mathematical document detailing the application of persistent homology to VAE latent spaces.\", \"- A documented Python script implementing the VAE and TDA pipeline.\", \"- A set of generated persistence diagrams and Betti curve plots visualizing the latent space topology.\", \"- A final report summarizing the findings on topological stability and citing at least two relevant academic papers.\", \"- A JSON file containing computed stability metrics (Wasserstein distance) and VAE performance (MSE).\"], \"thinking_outline\": [\"1. Research the mathematical formalisms for applying persistent homology, specifically Vietoris-Rips filtrations, to characterize the topology of point clouds sampled from a probabilistic distribution.\", \"2. Formulate the precise definitions of simplicial complexes, Betti numbers, and persistence diagrams in the context of a VAE's latent manifold.\", \"3. Implement a VAE trained on a dataset with known topology (e.g., a Swiss roll or torus) and a corresponding TDA pipeline to extract topological features from latent samples.\", \"4. Validate the TDA pipeline by verifying that the computed Betti numbers (e.g., B_0=1, B_1=1 for a circle) from the latent space of a well-trained model match the ground-truth topology of the input data manifold.\", \"5. Assess the topological stability by computing the bottleneck distance between persistence diagrams generated from different samples of the latent posterior and check for convergence as the VAE training progresses.\", \"6. Synthesize all mathematical derivations, code, and experimental results into a comprehensive report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent's derivation of Betti numbers must be directly implemented by the code agent's TDA pipeline. The final metrics from the code must align with the acceptance criteria; fallback: If implementation fails to recover known topology, return to the /math agent to reconsider filtration parameters or distance metrics).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on TDA for VAEs or other generative models from arXiv.\", \"- [ ] /math: Formally derive the Vietoris-Rips filtration process and define persistence diagrams for latent variable models.\", \"- [ ] /code: Implement the VAE and the TDA analysis pipeline in Python using PyTorch and giotto-tda.\", \"- [ ] /math: Verify that the distance metric (Euclidean) and filtration parameter (epsilon) used in the code's Vietoris-Rips complex construction align with the theoretical derivation.\", \"- [ ] /code: Validate the entire pipeline by running it on a synthetic torus dataset and confirming the Betti numbers are B_0=1, B_1=2, B_2=1 within a reasonable tolerance.\", \"- [ ] /code: Generate all required artifacts including plots, metrics JSON, and the final script.\", \"- [ ] /router: Perform final QA review, ensuring all artifacts are present, coherent, and meet acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"generative-models\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly defines the Vietoris-Rips complex, filtration, and persistence diagram.\", \"- The Python implementation successfully recovers the ground-truth Betti numbers for a synthetic manifold dataset (e.g., Swiss roll, torus).\", \"- The final report includes at least two citations to relevant arXiv papers and provides a clear interpretation of the persistence diagrams regarding topological stability.\", \"- The Wasserstein distance between diagrams from different latent samples is calculated and reported in the metrics artifact.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the 1-Wasserstein distance between pairs of persistence diagrams generated from independent batches of latent samples. A lower average distance indicates higher stability.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate the Mean Squared Error between the original input data and the VAE's reconstructed output. This ensures the underlying generative model is performant.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 121, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nPlease provide a full-stack analysis connecting the InfoNCE loss in self-supervised contrastive learning (e.g., SimCLR) to the softmax attention mechanism in transformers. I need a formal mathematical derivation showing their equivalence under certain assumptions, an analysis of how temperature scaling impacts representation collapse by modulating the eigenspectrum of the feature similarity matrix, and a Python simulation to empirically validate this connection using information bottleneck principles.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org InfoNCE loss information bottleneck theory Bayesian interpretation contrastive learning\\\", mode=both)\", \"/math(Derive the formal equivalence between InfoNCE and weighted softmax attention, analyzing the temperature parameter's influence on the feature covariance matrix's eigenvalue distribution and its implications for causal representation learning.)\", \"/code(Implement a PyTorch simulation on CUDA to validate the eigenvalue collapse hypothesis. Track representation uniformity and alignment metrics, and bootstrap confidence intervals for the spectral analysis., using Python)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational literature from arXiv. This theoretical grounding is passed to /math for rigorous derivation and analysis. Finally, /code is used to create a simulation that empirically validates the mathematical theory, providing a complete, verifiable analysis.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation connecting InfoNCE to softmax attention.\", \"A Python script implementing the contrastive learning simulation, capable of generating plots of the feature covariance matrix's eigenvalue distribution vs. temperature.\", \"A summary report in Markdown that integrates the theory, simulation results, and cites at least two relevant papers with arXiv IDs.\", \"Verification logs confirming the alignment between the mathematical formulation and the code implementation.\", \"A JSON file containing the computed uniformity and alignment metrics for different temperature settings.\"], \"thinking_outline\": [\"1. Formulate the InfoNCE loss from a probabilistic, Bayesian perspective as a lower bound on mutual information between augmented views.\", \"2. Demonstrate that optimizing this objective is equivalent to a multi-class classification task using a cross-entropy loss with a softmax function, thereby establishing the link to softmax attention.\", \"3. Analyze the gradient of the InfoNCE loss with respect to feature embeddings, showing how the temperature parameter scales the influence of negative samples and affects the Hessian's eigenvalue spectrum.\", \"4. Validate the theoretical claim that low temperatures lead to sharp eigenvalue distributions (representation collapse) while high temperatures lead to uniform distributions, using a Python simulation.\", \"5. Check that the simulation's metrics for representation uniformity and alignment behave as predicted by the mathematical analysis.\", \"6. Audit the cited literature to ensure the derived theory is consistent with established results in information bottleneck theory and self-supervised learning.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses these to build a formal derivation -> /code implements a simulation based on the derivation -> router QA reviews all artifacts for consistency. (Verification: /math agent cross-checks code logic against its equations. /code agent's plots must match theoretical predictions. Fallback: If simulation contradicts theory, return to /math to check for flawed assumptions.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two foundational papers on InfoNCE and the information bottleneck principle.\", \"- [ ] /math: Produce a step-by-step derivation linking the InfoNCE loss function to a softmax-based cross-entropy objective.\", \"- [ ] /math: Analyze the effect of the temperature parameter on the feature covariance matrix's eigenspectrum and its relation to representation collapse.\", \"- [ ] /code: Implement a toy contrastive learning framework in Python to simulate the training process.\", \"- [ ] /code: Generate plots showing the eigenvalue distribution of the feature covariance matrix under varying temperature settings.\", \"- [ ] /math: Verify that the loss function and metric computations in the Python script from /code are correct implementations of the derived formulas.\", \"- [ ] /code: Validate that the empirical results from the simulation align with the theoretical predictions about representation quality from the /math step.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"self-supervised-learning\", \"representation-learning\", \"information-bottleneck\"], \"acceptance_criteria\": [\"- The provided mathematical derivation must be rigorous and clearly show the equivalence between InfoNCE and softmax cross-entropy.\", \"- The Python simulation must successfully run and generate plots that empirically support the theoretical claims about temperature and representation collapse.\", \"- The final report must clearly explain the findings and include citations for at least two relevant arXiv pre-prints with their IDs.\", \"- The computed uniformity and alignment metrics must trend in the directions predicted by the theory as temperature is varied.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Computed as the logarithm of the average pairwise Gaussian potential over all features: `log E_{p_{data}}[exp(-2||f(x)-f(y)||^2)]`. Lower scores indicate better, more uniform feature distribution on the hypersphere.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Computed as the expected distance between positive pairs: `E_{p_{pos}}[||f(x)-f(x^+)||^2]`. Lower scores indicate that similar samples are mapped to nearby representations.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 122, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous comparison of dictionary learning algorithms for sparse representations, specifically K-SVD versus the Method of Optimal Directions (MOD). Please provide a formal derivation of the update rules for both, potentially framing them within an ADMM context. Implement both algorithms and empirically evaluate their convergence rate, reconstruction accuracy (PSNR), and computational complexity on synthetic signals generated from a known sparse basis.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD MOD dictionary learning convergence analysis ADMM formulation\\\", mode=both)\", \"/math(Derive the Lagrangian and formulate the iterative update rules for both K-SVD and MOD, referencing the ADMM framework. Analyze the convergence guarantees, considering potential eigenvalue constraints on the dictionary atoms.)\", \"/code(Implement K-SVD and MOD in Python with JAX, using bootstrap methods for synthetic data generation. Profile the computational complexity and track reconstruction error against wall-clock time to compare convergence behavior.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather current literature on theoretical convergence properties. This informs the /math agent's formal derivations of the algorithms. Finally, the /code agent implements these verified derivations to produce empirical evidence and performance comparisons.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the K-SVD and MOD update rules.\", \"- A well-documented Python implementation of both algorithms and the experimental setup.\", \"- A summary report (.md) containing plots that compare the algorithms on reconstruction error vs. iteration and vs. wall-clock time.\", \"- A bibliography file with citations for at least two relevant academic papers.\", \"- A JSON file containing the final computed metrics for each algorithm.\"], \"thinking_outline\": [\"1. Research contemporary analyses of K-SVD and MOD, focusing on convergence proofs and interpretations within frameworks like ADMM.\", \"2. Formally derive the optimization problem and iterative update steps for both algorithms from first principles, clearly stating all assumptions.\", \"3. Implement the derived algorithms, paying close attention to the sparse coding stage (e.g., Orthogonal Matching Pursuit) and the dictionary update stage.\", \"4. Design and execute a series of experiments on synthetic data to compare reconstruction error, convergence speed, and per-iteration complexity.\", \"5. Validate the implementation's correctness by checking if the objective function value decreases monotonically (or to a fixed point) across iterations for both algorithms.\", \"6. Verify that the experimental results and performance plots are consistent with the theoretical properties discussed in the researched literature and the mathematical derivations.\"], \"handoff_plan\": \"/general-search -> /math (verification: derivations must align with literature, especially ADMM forms) -> /code (verification: implementation must match mathematical update rules; objective function must decrease) -> router QA (verification: final report plots must reflect theoretical convergence rates; fallback: re-run code agent with corrected implementation if metrics are anomalous).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent arXiv papers comparing K-SVD and MOD, focusing on convergence analysis.\", \"- [ ] /math: Derive the complete update rules for both the K-SVD and MOD algorithms.\", \"- [ ] /math: Formulate the optimization problem in a Lagrangian or ADMM framework.\", \"- [ ] /code: Implement the K-SVD and MOD algorithms, including a sparse coding solver like Orthogonal Matching Pursuit (OMP).\", \"- [ ] /code: Create a synthetic data generation pipeline and an evaluation harness to measure all specified metrics.\", \"- [ ] /math: Verify that the implemented code's logic directly corresponds to the derived mathematical update rules.\", \"- [ ] /code: Validate that the implemented algorithms show a decreasing objective function value over iterations on a test case.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The provided Python code correctly implements both K-SVD and MOD and runs to completion on synthetic data.\", \"- The final report includes plots comparing the Reconstruction MSE vs. iteration and Reconstruction MSE vs. wall-clock time for both algorithms.\", \"- The mathematical derivation of the update rules is complete, correct, and references at least two relevant academic sources.\", \"- The final metrics JSON contains values for Reconstruction MSE and Dictionary Coherence for both algorithms.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 123, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a robust method for causal discovery from a mix of observational and interventional data. Specifically, I'm interested in adapting the Greedy Equivalence Search (GES) algorithm to handle soft interventions. Please provide a formal derivation for an interventional Bayesian Information Criterion (BIC) score, implement the Interventional GES (IGES) algorithm in Python, and validate its performance on a synthetic dataset generated from a known Structural Causal Model (SCM) against the ground truth causal graph.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org interventional greedy equivalence search bayesian information score structural causal model discovery\\\", mode=both)\", \"/math(Derive the interventional Bayesian Information Criterion (BIC) for a linear Gaussian Structural Causal Model, formalizing how intervention targets modify the likelihood function. Prove the score's consistency for causal discovery under faithfulness assumptions.)\", \"/code(Implement the Interventional GES (IGES) algorithm in Python, including the derived interventional BIC score. Generate synthetic data from a known causal DAG with both observational and interventional samples, and run validation checks to measure Structural Hamming Distance.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find foundational papers on interventional causal discovery, ensuring the solution is grounded in current research. This literature is then passed to /math to derive the necessary scoring function formally. Finally, /code implements and validates the derived theory, providing a complete and verified solution.\", \"expected_artifacts\": [\"A LaTeX document containing the formal mathematical derivation of the interventional BIC score and its properties.\", \"A Python script (`iges_implementation.py`) containing the IGES algorithm, the scoring function, and a synthetic data generation process from a known SCM.\", \"A summary report in Markdown (`report.md`) detailing the algorithm's performance, including SHD, TPR, and FPR metrics, with at least two relevant arXiv citations.\", \"A verification log (`verification.log`) confirming the consistency between the mathematical derivation and the Python code's scoring logic.\"], \"thinking_outline\": [\"1. Conduct a literature review to identify the canonical formulation of Greedy Equivalence Search (GES) and its adaptations for interventional data, focusing on scoring functions like interventional BIC.\", \"2. Formally derive the interventional BIC score, starting from the joint likelihood of observational and interventional data under a given Structural Causal Model (SCM).\", \"3. Develop a Python implementation of the IGES algorithm, including the forward and backward search phases, using the derived scoring function.\", \"4. Create a data simulation module that generates samples from a known ground-truth causal graph, incorporating specified intervention targets.\", \"5. Validate the algorithm's output by computing the Structural Hamming Distance (SHD) between the recovered graph and the true graph, and verify that the scoring logic in the code correctly implements the mathematical derivation.\", \"6. Check the implementation for edge cases, such as perfect interventions on multiple nodes, and analyze the algorithm's sensitivity to the number of interventional samples.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The /code agent must confirm that its implementation of the scoring function is algebraically equivalent to the derivation from the /math agent. The final metrics must be calculated on a held-out synthetic dataset. fallback: If the derivation is intractable, the /math agent should notify the router and /general-search will be re-queried for alternative consistent scoring functions.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Interventional GES and consistent scoring functions for causal discovery.\", \"- [ ] /math: Derive the interventional BIC score for linear Gaussian SCMs, including its consistency proof.\", \"- [ ] /code: Implement the IGES algorithm and the derived scoring function in Python.\", \"- [ ] /code: Develop a synthetic data generator for SCMs with interventions and a validation harness.\", \"- [ ] /math: Verify that the scoring function logic in the Python script is a correct translation of the mathematical derivation.\", \"- [ ] /code: Validate the implementation by computing SHD, TPR, and FPR against a known ground truth graph.\", \"- [ ] /router: Perform final QA review of all artifacts, checking for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"structural-causal-model\", \"bayesian-networks\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for the interventional BIC score must be complete and correct.\", \"- The Python implementation must correctly execute the IGES algorithm and reproduce the causal graph from the synthetic dataset with a Structural Hamming Distance (SHD) of less than 3.\", \"- The final report must cite at least two relevant arXiv papers with IDs and correctly report all specified performance metrics.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 126, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the convergence guarantees for a preconditioned SGD optimizer on a transformer's loss landscape. I need a formal proof using Riemannian geometry to model the self-attention parameter space. Specifically, formulate the Fisher Information Metric as the preconditioner, analyze the resulting Hessian eigenvalue distribution, and provide a proof-of-concept JAX implementation validating the accelerated convergence over Adam.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Riemannian optimization transformer Fisher Information Metric attention mechanism convergence\\\", mode=both)\", \"/math(Derive the Fisher Information Metric tensor for a single-headed causal attention layer, formulate the Riemannian gradient update rule with a Bayesian prior, and prove its convergence rate under assumptions about the Hessian eigenvalue spectrum.)\", \"/code(Implement the derived Riemannian preconditioning algorithm in JAX for a small-scale transformer model. Validate the implementation against the mathematical derivation and track convergence speed and the empirical Hessian eigenvalue distribution using bootstrap resampling., using Python)\"], \"route_rationale\": \"The blue general-search agent first gathers foundational literature on Riemannian optimization for transformers. Next, the math agent uses these sources to perform the core theoretical derivation of the optimizer and its convergence proof. Finally, the code agent implements the derived algorithm to provide an empirical proof-of-concept and validate the theoretical claims.\", \"expected_artifacts\": [\"A formal proof of the convergence rate for the Riemannian preconditioned optimizer, delivered as a LaTeX-formatted PDF.\", \"A JAX implementation of the custom optimizer and a minimal transformer model for validation.\", \"A summary report including plots comparing convergence against Adam and visualizing the Hessian eigenvalue spectrum.\", \"A bibliography with at least 2 cited arXiv papers on Riemannian optimization in deep learning.\", \"Verification script output showing numerical alignment between the math derivation and code implementation for a test case.\"], \"thinking_outline\": [\"1. Survey existing literature on Riemannian optimization for neural networks, specifically focusing on the Fisher Information Metric (FIM) as applied to the Transformer architecture's parameter manifold.\", \"2. Formulate the FIM as a metric tensor for the parameter space of a simplified self-attention mechanism, using tensor calculus.\", \"3. Derive the Riemannian gradient descent update rule using this metric and establish a proof for its local convergence rate, noting key assumptions about the loss landscape's geometry (e.g., Polyak-Lojasiewicz condition).\", \"4. Implement the custom optimizer in JAX and verify its core components (e.g., metric tensor computation) against a manually calculated small-scale example to ensure correctness.\", \"5. Conduct experiments comparing the optimizer's convergence speed and final loss against a baseline like Adam on a toy problem. Analyze the empirical Hessian eigenvalue distribution of the final solution.\", \"6. Validate that the experimental results on convergence align with the theoretical bounds derived in the proof.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math produces a formal derivation -> /code implements and validates the derivation. Router QA performs final review. Verification: The code's empirical convergence rate must be consistent with the theoretical bounds from /math. Fallback: If rates diverge, /math re-audits the proof's assumptions and /code instruments the implementation to check for numerical instability.\", \"todo_list\": [\"- [ ] /general-search: Find key papers on Fisher Information Metric as a preconditioner for Transformers.\", \"- [ ] /math: Derive the FIM tensor for a simplified attention mechanism.\", \"- [ ] /math: Formulate the Riemannian gradient update and prove its local convergence rate.\", \"- [ ] /code: Implement the Riemannian optimizer and a toy Transformer model in JAX.\", \"- [ ] /code: Run comparative experiments against Adam, logging loss and Hessian eigenvalues.\", \"- [ ] /math: Verify that the JAX implementation of the FIM tensor matches the symbolic derivation for a 2x2 case.\", \"- [ ] /code: Validate that the empirical convergence rate observed in experiments is consistent with the derived theoretical bounds.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"riemannian-optimization\", \"transformer-optimization\", \"information-geometry\"], \"acceptance_criteria\": [\"- The convergence proof must be mathematically sound, self-contained, and clearly state all assumptions.\", \"- The JAX code must be executable and reproduce the convergence plots shown in the summary report.\", \"- The custom optimizer must demonstrate a statistically significant improvement in convergence speed (steps to target loss) over the Adam baseline.\", \"- The report must cite at least two relevant arXiv papers with correct IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 128, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a physics-informed neural network (PINN) to solve the 2D incompressible Navier-Stokes equations for a lid-driven cavity flow problem, specifically addressing the stiffness and high-frequency components in turbulent regimes. Can you formulate and implement a model using Sobolev training, where the loss function incorporates L2 norms of higher-order derivatives of the network output to improve solution regularity and convergence? The implementation should use JAX for performance.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Sobolev training' 'physics-informed neural networks' 'Navier-Stokes' turbulence JAX implementation\\\", mode=both)\", \"/math(Derive the weak form of the 2D incompressible Navier-Stokes equations and construct the corresponding Sobolev training loss function, which penalizes the Euler-Lagrange PDE residual and L2 norms of the solution's spatial derivatives up to the second order, ensuring proper Fourier space behavior.)\", \"/code(Implement a JAX-based PINN for the lid-driven cavity problem incorporating the derived Sobolev loss. Use automatic differentiation for residuals, optimize with Adam, and validate against a benchmark dataset, tracking PDE residual convergence and CUDA device utilization., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey the latest arXiv literature on Sobolev training for PINNs in fluid dynamics. This informs the /math agent's formal derivation of the specific loss function. Finally, the /code agent implements this mathematical formulation in an optimized JAX environment and validates the results.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the weak form of the Navier-Stokes equations and the Sobolev loss function.\", \"- A documented Python script implementing the JAX-based PINN, including the model architecture, training loop, and loss function.\", \"- A performance report summarizing training metrics (MSE, PDE residual), convergence plots, and visualizations comparing the PINN solution to the Ghia et al. benchmark data.\", \"- A bibliography with at least two cited arXiv papers on Sobolev-trained PINNs.\", \"- A JSON file containing the final primary and secondary metric scores.\"], \"thinking_outline\": [\"1. Research the state-of-the-art in Sobolev training for PINNs applied to fluid dynamics problems, focusing on the Navier-Stokes equations.\", \"2. Formulate the strong and weak forms of the 2D incompressible Navier-Stokes equations. Derive the composite loss function, including terms for the PDE residuals, boundary conditions, and the Sobolev norm regularizer.\", \"3. Implement the PINN architecture and the custom Sobolev loss function in JAX, leveraging `jax.grad` and `vmap` for efficient computation of derivatives and residuals.\", \"4. Train the model on the lid-driven cavity problem, monitoring the convergence of the loss components and solution fields.\", \"5. Validate the implementation by comparing the predicted velocity profiles against established benchmark data (e.g., from Ghia et al., 1982) and verify that the PDE residual decreases to a specified tolerance.\", \"6. Audit the final code for correctness, performance, and adherence to the mathematical formulation, ensuring the automatic differentiation correctly computes the necessary higher-order derivatives.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results from /general-search provide foundational papers for /math to derive the loss function. This derivation is the formal specification for the /code agent. The code agent implements and runs the model, producing plots and metrics. Router QA performs final verification by checking if the code's loss implementation matches the mathematical derivation and if the final metrics meet the acceptance criteria. Fallback: If the model fails to converge, the /code agent will report this to /math to investigate potential issues with the loss formulation, possibly leading to a revised derivation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent, highly-cited papers on Sobolev or variational PINNs for fluid dynamics.\", \"- [ ] /math: Produce a complete derivation of the Sobolev loss for the 2D incompressible Navier-Stokes equations.\", \"- [ ] /code: Implement the PINN architecture and training loop in JAX.\", \"- [ ] /code: Implement the custom Sobolev loss function based on the mathematical derivation.\", \"- [ ] /math: Verify that the implemented loss function in the code correctly matches the derived mathematical formula.\", \"- [ ] /code: Run the training and generate validation plots against the Ghia et al. benchmark data.\", \"- [ ] /router: Perform final QA review of all artifacts, checking for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"partial-differential-equations\", \"computational-fluid-dynamics\"], \"acceptance_criteria\": [\"- The final implementation must be in JAX and use automatic differentiation for all PDE terms.\", \"- The Mean Squared Error (MSE) for the predicted velocity fields (u, v) against the Ghia benchmark must be less than 5e-4.\", \"- The final report must include visualizations (e.g., contour plots of velocity, pressure, and vorticity) that are qualitatively consistent with known results for the lid-driven cavity problem.\", \"- The mathematical derivation must be self-contained and clearly define all terms in the loss function.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 129, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous mathematical derivation of the back-door adjustment formula from Pearl's do-calculus, starting from a specified Structural Causal Model (SCM). Subsequently, please contrast this with the g-formula for estimating causal effects of longitudinal treatments, particularly focusing on its mechanism for handling time-varying confounding that is itself affected by prior treatment. Finally, implement a simulation in Python to demonstrate the estimation bias that arises when the back-door criterion is violated, comparing a naive regression model to a correctly specified g-formula estimator in a simulated biomedical context.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:stat.ucla.edu 'Judea Pearl' back-door criterion derivation 'James Robins' g-formula longitudinal causal inference\\\", mode=web)\", \"/math(Derive the back-door adjustment formula from first principles using do-calculus, then formally derive the g-formula for a longitudinal biomedical setting, analyzing assumptions like sequential exchangeability from a Bayesian perspective.)\", \"/code(Implement a Python simulation of a time-varying treatment. Generate data from a known SCM, then apply both naive regression and the derived g-formula to estimate the causal effect, using bootstrap resampling to calculate confidence intervals and AUPRC metrics.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on do-calculus and the g-formula. This research informs the /math agent, which will produce the core theoretical derivations. Finally, the /code agent implements the derived mathematical formulas in a simulation to provide empirical validation and quantify the theoretical bias.\", \"expected_artifacts\": [\"A LaTeX document containing the formal, step-by-step derivations of both the back-door adjustment and the longitudinal g-formula.\", \"A well-commented Python script (`simulation.py`) that implements the data generating process, the naive estimator, and the g-formula estimator.\", \"A summary report in Markdown (`report.md`) that presents the simulation results, including tables and plots showing estimator bias and variance.\", \"A verification log documenting the check that the code's g-formula implementation matches the mathematical derivation.\", \"A `bibliography.bib` file with at least two citations to authoritative sources on causal inference.\"], \"thinking_outline\": [\"1. Formalize the Structural Causal Model (SCM), including the directed acyclic graph (DAG) and the functional assignments for a time-varying treatment scenario.\", \"2. Using the rules of do-calculus, formally derive the back-door adjustment formula for a simplified, non-longitudinal case from the SCM.\", \"3. Derive the g-formula (g-computation algorithm) for the longitudinal case, clearly stating the assumptions of consistency, positivity, and sequential exchangeability (no unmeasured confounding).\", \"4. Implement the data-generating process in Python based on the SCM, ensuring that a time-varying confounder is affected by past treatment history.\", \"5. Verify that the Python implementation of the g-formula estimator correctly marginalizes over the confounder history as specified in the mathematical derivation.\", \"6. Execute the simulation, compute the bias and variance of the g-formula and a naive regression model, and validate that the results align with causal theory (i.e., the g-formula is unbiased).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to /math. The /math agent produces formal derivations which serve as a specification for the /code agent. The /code agent's implementation must be verified against the /math agent's derivation. Fallback: If the simulation results contradict the theory (e.g., g-formula remains biased), flag for review by the /math agent to check for subtle assumption violations.\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers by Judea Pearl on do-calculus and James Robins on the g-formula.\", \"- [ ] /math: Derive the back-door adjustment formula from the SCM structure.\", \"- [ ] /math: Derive the longitudinal g-formula, clearly stating all assumptions like sequential exchangeability.\", \"- [ ] /code: Implement the simulation's data generating process, including time-varying confounding.\", \"- [ ] /code: Implement both the g-formula estimator and a naive regression estimator.\", \"- [ ] /math: Verify that the code implementation of the g-formula correctly reflects the mathematical derivation provided.\", \"- [ ] /code: Execute the simulation study, generate plots comparing estimator bias, and compute confidence intervals.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"g-formula\", \"biomedical-statistics\"], \"acceptance_criteria\": [\"The provided LaTeX document contains complete and correct derivations for both the back-door adjustment and the g-formula.\", \"The Python simulation runs without error and generates results demonstrating that the g-formula is approximately unbiased while the naive model is substantially biased.\", \"The final report correctly interprets the simulation results, quantifies the bias of each estimator, and cites at least 2 relevant peer-reviewed articles or arXiv preprints with their identifiers.\", \"The code includes comments mapping specific lines or functions back to the equations in the mathematical derivation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 130, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive and implement a steganographic coding scheme for latent diffusion models based on rate-distortion theory. The goal is to embed a binary message within the diffusion process's noise sampling, maximizing the message bit-rate (capacity) subject to a constraint on the Kullback-Leibler divergence between the perturbed and original noise distributions. Your solution must be robust to adversarial JPEG compression and provide a formal proof of the capacity-distortion trade-off using Bayesian principles.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org rate-distortion steganography diffusion models information bottleneck OR Bayesian inference\\\", mode=both)\", \"/math(Derive the Lagrangian for the rate-distortion objective, using KL divergence as the distortion metric. Formulate the optimal message encoding distribution as a function of the latent noise variance and message capacity, applying Bayesian principles and ensuring theoretical convergence.)\", \"/code(Implement the derived steganographic sampler in PyTorch using a pre-trained diffusion model. Validate the embedding process, measure the empirical bit-rate, track the FID score for perceptual quality, and test adversarial robustness using augmentations like JPEG compression., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on information-theoretic methods for generative models. This informs the /math agent's formal derivation of the rate-distortion objective. Finally, the /code agent implements the derived theory and performs empirical validation, a necessary step to ground the theoretical work.\", \"expected_artifacts\": [\"A formal mathematical derivation of the rate-distortion function for the latent noise channel.\", \"Python code implementing the steganographic sampling process for a standard diffusion model.\", \"A report summarizing the empirical results, including a plot of the rate-distortion curve and FID scores.\", \"A list of at least 2 cited arXiv papers that informed the methodology.\", \"A verification log detailing the alignment check between the mathematical derivation and the code's objective function.\"], \"thinking_outline\": [\"1. Research foundational work connecting rate-distortion theory with generative models, particularly diffusion models, focusing on information bottleneck principles and steganography.\", \"2. Formulate the optimization problem: maximize mutual information between the message and the watermarked latent, constrained by the KL divergence between the original and perturbed latent noise distributions.\", \"3. Derive the closed-form solution for the optimal encoding distribution using variational methods and the Euler-Lagrange equation.\", \"4. Implement the derived update rules within a PyTorch DDPM or LDM sampling loop. Verify the implementation's objective function calculation against a manually computed example in the math derivation.\", \"5. Empirically evaluate the trade-off by running experiments at different distortion levels, plotting the resulting message capacity (bit-rate) and visual fidelity (FID). Validate robustness by applying post-hoc transformations.\", \"6. Audit the final results to ensure the empirical curve aligns with the shape predicted by the theoretical derivation and that FID degradation is within acceptable bounds.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent's derived Lagrangian must be directly implemented as the loss function in the code. The code agent must verify this alignment. fallback: If derivation is not implementable, /math agent must reformulate using a tractable approximation, citing literature for justification.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on rate-distortion theory for generative model watermarking/steganography.\", \"- [ ] /math: Derive the capacity-achieving input distribution for the latent channel under a KL-divergence constraint.\", \"- [ ] /math: Formulate the complete objective function and its gradients for optimization.\", \"- [ ] /code: Implement the steganographic noise sampler in PyTorch, integrating it into a pre-trained diffusion pipeline.\", \"- [ ] /code: Implement evaluation metrics: bit recovery accuracy, FID score, and robustness against JPEG compression.\", \"- [ ] /math: Verify that the symbolic gradients derived match the numerical gradients produced by the code's autograd engine for a test case.\", \"- [ ] /code: Run experiments to generate the rate-distortion-fidelity trade-off curve.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"rate-distortion-theory\", \"generative-ai-safety\", \"steganography\"], \"acceptance_criteria\": [\"- Pass: The mathematical derivation of the rate-distortion objective is complete and self-contained.\", \"- Pass: The implemented code successfully embeds and recovers messages with >99% accuracy at a non-zero bit-rate.\", \"- Pass: The final report includes a plot showing the trade-off between bit-rate and FID, consistent with theoretical expectations.\", \"- Fail: The KL-divergence constraint is not correctly implemented or verified in the code.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 131, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the performance of the surface code, a type of topological quantum error-correcting code, under a biased noise model where Pauli-Z errors are more frequent than X or Y errors. Please derive the mapping from the stabilizer syndrome data to a minimum-weight perfect matching (MWPM) problem on a graph, explicitly accounting for the noise bias in the edge weights. Then, implement this MWPM decoder in Python and run a Monte Carlo simulation to determine the logical error rate threshold for a range of noise biases, presenting a phase diagram.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org surface code MWPM decoder biased noise threshold estimation\\\", mode=web, includes cybersecurity)\", \"/math(Derive the mapping from stabilizer syndromes to the MWPM graph problem for biased noise, focusing on the log-likelihood weight calculation and ensuring theoretical convergence guarantees given the problem's complexity.)\", \"/code(Implement the surface code simulation and MWPM decoder in Python, using a bootstrap method to generate confidence interval estimates for the logical error rate at each physical error rate.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the approach in current literature on biased noise decoders. This informs the /math agent's formal derivation of the problem mapping. The derived mathematical model is then handed to the /code agent for a high-fidelity Python implementation and statistical analysis.\", \"expected_artifacts\": [\"A formal mathematical derivation in a LaTeX-formatted PDF detailing the syndrome-to-graph mapping for the biased noise model.\", \"A well-documented Python script implementing the surface code, biased noise model, and MWPM decoder.\", \"A PNG image of the phase diagram showing the error threshold as a function of noise bias.\", \"A JSON file containing the raw simulation data: logical error rates versus physical error rates for different lattice sizes and biases.\", \"A summary report citing at least 2 relevant arXiv papers on the topic.\"], \"thinking_outline\": [\"1. Research state-of-the-art literature on MWPM decoders for the surface code, specifically focusing on adaptations for biased noise models and methods for threshold estimation.\", \"2. Formulate the precise mathematical mapping from the surface code's stabilizer syndrome measurements to a graph problem. The key is deriving the edge weights as negative log-likelihoods based on the biased noise probabilities.\", \"3. Implement the surface code lattice, the biased noise model, syndrome extraction, and the MWPM decoder in Python, leveraging a library like `PyMatching` or `NetworkX` for the graph algorithms.\", \"4. Validate the implementation's correctness by comparing its output for the unbiased case (bias=1) against well-established threshold values from the literature (around 10.3%).\", \"5. Execute large-scale Monte Carlo simulations across a grid of lattice sizes and noise bias parameters to collect logical error rate data.\", \"6. Verify the final phase diagram by checking for consistency and smooth transitions, and ensure the cited literature supports the observed behavior.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: logical error rate curves are monotonic and cross at the expected threshold for the unbiased case; fallback: if thresholds are inconsistent, /math agent to re-audit the log-likelihood derivation for the edge weights).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key arXiv papers on MWPM decoding for surface codes under biased noise.\", \"- [ ] /math: Derive the mapping from stabilizer syndrome to the MWPM graph, defining edge weights based on the log-likelihood of biased Pauli error channels.\", \"- [ ] /code: Implement the surface code lattice, syndrome measurement simulation, and the MWPM decoder.\", \"- [ ] /code: Develop the Monte Carlo simulation framework to compute the logical error rate as a function of physical error rate and noise bias.\", \"- [ ] /math: Verify that the edge weight calculations in the code correctly implement the derived log-likelihood formulas.\", \"- [ ] /code: Validate the decoder's correctness by running it on the unbiased noise model (bias=1) and checking the threshold against published values.\", \"- [ ] /code: Generate the final phase diagram plot and metrics JSON from the simulation data.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"statistical-physics\"], \"acceptance_criteria\": [\"- The Python code must reproduce the known error threshold for the surface code with unbiased depolarizing noise (p_th \u2248 10.3%) to within a 5% margin of error.\", \"- The final report must include a complete mathematical derivation of the mapping from syndrome to the weighted graph for the biased noise model.\", \"- The generated phase diagram must clearly illustrate the relationship between the error threshold and the noise bias \u03b7, showing an increase in threshold for high bias.\", \"- The implementation must be accompanied by citations for at least two relevant arXiv preprints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"For each noise bias, this is the critical physical error rate below which the logical error rate decreases with increasing code distance. Determined by finding the crossing point of logical error rate curves for different code sizes (e.g., d=3, 5, 7).\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"For a given physical error rate and code distance, this is the fraction of Monte Carlo trials where the decoder fails, resulting in a net logical error. Plotted as p_L vs. p_phys.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 132, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm researching neural-symbolic methods for automated theorem proving. I need to develop and benchmark a system that can synthesize a formal proof for the Fundamental Theorem of Arithmetic. The system should use a neural-guided search, like a Monte Carlo Tree Search (MCTS) with a policy network, to generate proof steps in a verifiable formal calculus. How can I structure the mathematical induction, implement the search algorithm in PyTorch, and evaluate its performance against symbolic baselines, ensuring proof soundness?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural theorem proving MCTS program synthesis bayesian search formal methods\\\", mode=both)\", \"/math(Formalize the inductive proof structure for the Fundamental Theorem of Arithmetic, defining the state space, action space, and termination conditions for a search-based prover, analyzing the proof complexity and convergence criteria.)\", \"/code(Implement a CUDA-accelerated MCTS guided by a GNN-based policy network in PyTorch to synthesize proof steps; include a verifier module to check proof soundness and bootstrap the training data from successful proofs., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art neural theorem provers and synthesis techniques. The findings inform the /math agent's formalization of the proof strategy and search space. Finally, the /code agent implements the specified neural-symbolic algorithm based on the mathematical blueprint.\", \"expected_artifacts\": [\"A formal derivation of the inductive proof strategy for the Fundamental Theorem of Arithmetic.\", \"Python source code for the neural-guided MCTS proof synthesizer, including the neural network model and the proof verifier.\", \"A performance report (report.md) comparing the synthesizer's success rate and proof length against a symbolic baseline, with citations.\", \"A JSON file (metrics.json) containing proof success rate, average synthesis time, and proof complexity metrics.\", \"Verification logs (verification.log) confirming the soundness of all generated proofs.\"], \"thinking_outline\": [\"1. Review current literature on neural-symbolic methods, focusing on tree search algorithms for formal proof generation.\", \"2. Decompose the Fundamental Theorem of Arithmetic into a sequence of lemmas suitable for an inductive proof within a formal calculus.\", \"3. Design the neural network architecture (e.g., Graph Neural Network) to act as the policy and value function for the MCTS.\", \"4. Implement the MCTS algorithm and the proof state transition logic, integrating the neural guidance.\", \"5. Validate the implementation by checking that the verifier correctly rejects invalid proof steps and that the complete synthesized proofs are sound.\", \"6. Audit the benchmark results against the symbolic baseline to ensure the comparison is fair and the metrics are computed correctly.\"], \"handoff_plan\": \"/general-search -> /math (verification: literature findings inform the choice of formal calculus and proof strategy) -> /code (verification: implementation must adhere strictly to the mathematical formalism defined by the math agent; fallback: if convergence fails, revisit the math agent to simplify the state representation) -> router QA (verification: final artifacts are cross-checked for consistency and soundness).\", \"todo_list\": [\"- [ ] /general-search: Identify 3-5 key papers on neural-guided search for theorem proving, focusing on MCTS or transformer-based approaches.\", \"- [ ] /math: Define the formal language (syntax and semantics) for the proof steps.\", \"- [ ] /math: Provide a step-by-step inductive proof decomposition of the target theorem.\", \"- [ ] /code: Implement the MCTS-based proof synthesizer and the neural policy network in PyTorch.\", \"- [ ] /code: Implement the symbolic proof verifier module to ensure logical soundness.\", \"- [ ] /math: Verify that the code implementation of the proof state and actions aligns with the formal mathematical definition.\", \"- [ ] /code: Run benchmarks, generate the performance report, and log all metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-theorem-proving\", \"program-synthesis\", \"formal-methods\"], \"acceptance_criteria\": [\"- The synthesized program must generate a complete, formally verifiable proof for the Fundamental Theorem of Arithmetic.\", \"- The final report must include a performance comparison against a baseline (e.g., a pure symbolic BFS/DFS solver) on the specified metrics.\", \"- The provided code must be executable and reproduce the reported metrics within a 5% tolerance.\", \"- The solution must cite at least 3 relevant arXiv papers published within the last 5 years.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 133, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comparative analysis of Model-Agnostic Meta-Learning (MAML) and Prototypical Networks for few-shot image classification. Specifically, derive the update rules for a second-order MAML variant that incorporates a Bayesian prior for regularization, using Hessian-vector products for computational efficiency. Then, implement this and Prototypical Networks to benchmark their performance and convergence speed on the miniImageNet dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org second-order MAML optimization Hessian Bayesian meta-learning Prototypical Networks convergence guarantees\\\", mode=web)\", \"/math(Derive the update rule for Bayesian MAML using second-order Hessian-vector products, analyzing the convergence properties and incorporating a Gaussian prior to regularize the meta-parameters.)\", \"/code(Implement the derived Bayesian MAML and a Prototypical Network baseline in JAX, targeting the miniImageNet benchmark and tracking few-shot accuracy and AUPRC metrics for performance evaluation., using Python)\", \"/math(Perform a formal computational complexity analysis comparing first-order MAML, our second-order Bayesian variant, and Prototypical Networks, referencing the Hessian eigenvalue spectrum's impact on optimization stability.)\"], \"route_rationale\": \"The plan starts with literature review via the blue /general-search agent to ground the work in authoritative sources. Next, /math is used for the core theoretical derivation of the advanced MAML variant. Then, /code implements this theory for empirical validation and benchmarking. Finally, /math returns to provide a formal analysis of the computational trade-offs, creating a complete theoretical and practical investigation.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Bayesian second-order MAML update rule in LaTeX.\", \"- A Python script implementing both the derived MAML variant and Prototypical Networks.\", \"- A summary report including plots comparing the accuracy and convergence rates of the two models on miniImageNet.\", \"- A bibliography with at least two cited arXiv pre-prints with their IDs.\", \"- A computational complexity analysis report.\"], \"thinking_outline\": [\"1. Deconstruct the user query to isolate the core requirements: a) theoretical derivation of Bayesian second-order MAML, b) implementation of both algorithms, and c) empirical comparison.\", \"2. Use targeted search to find seminal papers on MAML, its second-order variants, and Prototypical Networks to establish a theoretical baseline and find existing benchmarks.\", \"3. Execute the formal mathematical derivation, carefully detailing the objective function, the gradient, and the Hessian-vector product formulation.\", \"4. Validate the derivation by checking for dimensional consistency and verifying the logic against established optimization principles.\", \"5. Implement the algorithms in code, ensuring the implementation of the update rule exactly matches the mathematical derivation.\", \"6. Verify the empirical results by comparing the baseline Prototypical Network's performance against published scores to ensure the experimental setup is correct.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation) -> /math (complexity analysis) -> router QA. At each step, artifacts are cross-referenced. Verification: The code implementation in the /code step must pass unit tests based on the symbolic derivatives from the /math step. Fallback: If the second-order MAML proves numerically unstable, the agent should report this and revert to a first-order Bayesian MAML implementation as a comparative model.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on second-order MAML (e.g., MAML++) and Prototypical Networks.\", \"- [ ] /math: Derive the complete update equations for Bayesian second-order MAML.\", \"- [ ] /code: Implement the Prototypical Network baseline for miniImageNet.\", \"- [ ] /code: Implement the Bayesian second-order MAML variant in JAX.\", \"- [ ] /math: Verify that the symbolic Hessian-vector product derivation matches a numerical approximation from the code.\", \"- [ ] /code: Execute the comparative benchmark experiments and generate accuracy/convergence plots.\", \"- [ ] /math: Draft the computational complexity analysis based on the implementations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"few-shot-learning\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Bayesian second-order MAML update is complete and mathematically sound.\", \"- The Python implementation of both models runs successfully on a 5-way 1-shot miniImageNet task, producing classification accuracy metrics.\", \"- The final report includes a plot comparing the learning curves of the two models over meta-training episodes and cites at least two relevant arXiv papers with IDs.\", \"- The complexity analysis correctly identifies the asymptotic runtime differences between the algorithms.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 134, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the performance collapse phenomenon in differentiable architecture search (DARTS). Can you derive a regularization method based on the spectral norm of the Hessian of the validation loss with respect to the architectural parameters? Please formulate the complete bilevel optimization problem, derive the gradient for the regularizer, and then implement this in JAX or PyTorch to demonstrate its efficacy on the CIFAR-10 search space. The goal is to show empirically that this regularization smoothens the loss landscape, mitigates the dominance of skip-connections, and leads to discovering more robust network architectures.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org DARTS performance collapse Hessian eigenvalue regularization bilevel optimization\\\", mode=web)\", \"/math(Formulate the DARTS bilevel optimization objective. Derive a spectral norm regularizer for the validation loss Hessian and compute its gradient using the implicit function theorem, preparing for a practical AutoML implementation.)\", \"/code(Implement the regularized DARTS search algorithm in JAX. Include a Lanczos iteration method to estimate the top Hessian eigenvalue for the regularizer and track CIFAR-10 validation accuracy., using Python)\", \"/math(Analyze the empirical results from the code. Connect the observed reduction in the dominant Hessian eigenvalue to the improved stability and final test accuracy, formally verifying the regularizer's theoretical benefits., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey existing literature on DARTS instability. /math is then used to construct the formal mathematical framework for the proposed regularizer. /code provides the crucial empirical validation of the theory. A final /math step is used to rigorously connect the experimental outputs back to the initial derivation, creating a complete theory-to-practice loop.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the regularized bilevel optimization problem and its gradients.\", \"A Python script or Jupyter Notebook implementing the regularized DARTS algorithm and the Hessian eigenvalue estimation.\", \"A results summary file (e.g., JSON or CSV) logging the validation accuracy, test accuracy, and estimated maximum Hessian eigenvalue over the search process.\", \"A final report synthesizing the theory and results, including plots of the eigenvalue distribution and training curves, with citations to relevant literature.\"], \"thinking_outline\": [\"1. Review the standard DARTS formulation as a bilevel optimization problem and identify the source of instability related to the sharpness of the validation loss landscape.\", \"2. Propose a regularization term based on the spectral norm (maximum eigenvalue) of the Hessian of the validation loss with respect to the architecture parameters (alphas).\", \"3. Formally derive the gradient of the complete regularized objective, carefully applying the implicit function theorem to handle the dependency of network weights on architecture parameters.\", \"4. Implement the search algorithm in code, using an efficient power iteration or Lanczos method to estimate the top Hessian eigenvalue without materializing the full Hessian matrix.\", \"5. Validate the regularizer's effect by running the search on CIFAR-10, plotting the maximum eigenvalue over time, and checking if it remains bounded compared to a non-regularized baseline.\", \"6. Audit the final discovered architecture for signs of reduced skip-connection dominance and evaluate its final performance after retraining from scratch.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results inform the mathematical derivation. The math agent produces a formal specification (equations) for the code agent. The code agent implements this and produces empirical data (plots, logs). The math agent then analyzes this data to verify the original hypothesis. Verification: At each step, the output is checked against the previous step's specification (e.g., code output must match metrics defined in the math spec). Fallback: If code fails to show the expected effect, the router may loop back to the /math agent to reconsider the regularizer's formulation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 key papers on DARTS instability and Hessian-aware optimization.\", \"- [ ] /math: Derive the gradient for the bilevel objective with the Hessian spectral norm regularizer.\", \"- [ ] /code: Implement the regularized DARTS search loop, including the power iteration for eigenvalue estimation.\", \"- [ ] /code: Execute the search on the CIFAR-10 benchmark, saving logs and final architecture genotype.\", \"- [ ] /math: Verify that the implemented gradient in the code (via autodiff) is consistent with the analytical derivation.\", \"- [ ] /math: Analyze the logged eigenvalue data and validation curves to confirm the regularizer's smoothing effect.\", \"- [ ] /router: Perform final QA review, ensuring the final report's conclusions are strongly supported by both the theoretical derivation and empirical results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"automl\", \"differentiable-architecture-search\", \"hessian-analysis\"], \"acceptance_criteria\": [\"- The mathematical derivation must be self-contained and correctly apply the implicit function theorem to compute the hypergradient.\", \"- The Python implementation must successfully run on CIFAR-10 and produce an architecture genotype.\", \"- The final report must contain a plot showing that the maximum Hessian eigenvalue is lower or more stable in the regularized version compared to a baseline.\", \"- The test error of the discovered architecture must be competitive with published DARTS results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 135, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive theoretical and practical analysis of online dictionary learning for sparse coding using the Alternating Direction Method of Multipliers (ADMM). Please derive the full update rules for both the dictionary and the sparse coefficients, provide a rigorous outline for a convergence proof under non-convex constraints common in biomedical signal processing, and then implement this algorithm to demonstrate its efficacy in denoising a synthetic electrocardiogram (ECG) signal corrupted by Gaussian noise.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org online dictionary learning ADMM convergence biomedical signal\\\", mode=web)\", \"/math(Derive the Augmented Lagrangian and corresponding ADMM update rules for the dictionary learning objective function. Outline the convergence proof, analyzing the behavior of the dictionary atom eigenvalue distribution.)\", \"/code(Implement the online ADMM algorithm in Python with JAX for sparse coding and dictionary updates. Evaluate empirical convergence on a synthetic signal, contrasting it with a Bayesian baseline and tracking reconstruction error.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey recent authoritative literature on ADMM for dictionary learning, ensuring the theoretical approach is current. This informs the /math agent's formal derivation and proof structuring. Finally, the /code agent translates the verified mathematics into a practical implementation for empirical validation, creating a complete theory-to-practice workflow.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the ADMM update rules and a structured outline of the convergence proof in a LaTeX PDF.\", \"- A documented Python script implementing the online dictionary learning algorithm and the signal denoising simulation.\", \"- A summary report in Markdown format presenting the PSNR and dictionary recovery metrics, including plots of the original, noisy, and denoised signals.\", \"- A bibliography file citing at least two relevant arXiv papers with their IDs.\", \"- A verification log confirming the alignment between the mathematical derivation and the code's numerical implementation.\"], \"thinking_outline\": [\"1. Formulate the online dictionary learning problem as a constrained optimization task, defining the data fidelity term (Frobenius norm), the sparsity-inducing regularizer (L1-norm), and dictionary constraints.\", \"2. Construct the Augmented Lagrangian for this formulation and derive the iterative ADMM update equations for the sparse codes, the dictionary atoms, and the dual variables.\", \"3. Research and outline the structure of a convergence proof, identifying necessary assumptions on step sizes and signal properties, referencing key literature on non-convex ADMM.\", \"4. Verify the correctness of the derived update rules by checking if they satisfy the first-order optimality conditions (KKT conditions) for the subproblems at each step.\", \"5. Implement the derived algorithm in a Python script, including a function to generate a synthetic sparse ECG signal and another to add noise.\", \"6. Validate the implementation's output by monitoring the objective function's value across iterations to ensure it decreases and by measuring the final reconstruction quality against the ground-truth signal.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent verifies the core update logic in the Python script against its derivation; fallback: if the formal convergence proof is intractable, the /math agent will instead provide a detailed heuristic argument for convergence to be tested empirically by the /code agent).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent arXiv papers on ADMM for online dictionary learning and its convergence analysis.\", \"- [ ] /math: Derive the complete ADMM update equations for the sparse code and dictionary update steps.\", \"- [ ] /math: Outline the key steps and required assumptions for proving the algorithm's convergence.\", \"- [ ] /code: Implement the online dictionary learning algorithm, the signal generator, and the denoising pipeline.\", \"- [ ] /code: Generate plots showing the original, noisy, and reconstructed signals, and report PSNR/dictionary recovery metrics.\", \"- [ ] /math: Audit the Python implementation of the ADMM update loop to verify it exactly matches the derived mathematical formulas.\", \"- [ ] /code: Validate that the empirical convergence rate observed in simulation is consistent with theoretical expectations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"admm\", \"biomedical-signal-processing\"], \"acceptance_criteria\": [\"- The LaTeX document must contain a full, step-by-step derivation of the ADMM update rules from the initial objective function.\", \"- The implemented algorithm must achieve a Peak Signal-to-Noise Ratio (PSNR) improvement of at least 10 dB on the specified synthetic signal denoising task.\", \"- The final report must cite at least two relevant arXiv pre-prints with their full IDs.\", \"- The verification log must explicitly confirm a one-to-one correspondence between the mathematical formulas in the derivation and the update steps in the Python code.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 136, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the evidence lower bound (ELBO) for a Bayesian formulation of Model-Agnostic Meta-Learning (MAML), explicitly connecting the meta-objective to amortized variational inference. Implement this Bayesian MAML variant in PyTorch to solve a sinusoidal regression few-shot learning task. Empirically validate that its predictive uncertainty estimates, derived from the posterior approximation, are better calibrated than the point estimates from vanilla MAML.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian model-agnostic meta-learning variational inference\\\", mode=both)\", \"/math(Derive the evidence lower bound (ELBO) for Bayesian MAML, treating task-specific parameters as latent variables and connecting the objective to amortized variational inference with a focus on Hessian properties.)\", \"/code(Implement Bayesian MAML and a baseline MAML in PyTorch for a few-shot sinusoidal regression problem, ensuring the Bayesian variant uses the reparameterization trick for stable gradient estimation and tracks convergence., using Python)\", \"/math(Formalize the Expected Calibration Error (ECE) metric for regression tasks to quantify the model's uncertainty calibration, referencing the derived Bayesian posterior.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational papers on Bayesian MAML. This informs the /math agent's formal derivation of the learning objective. The derived objective is then handed to the /code agent for a concrete implementation and empirical evaluation. A final call to /math is needed to formalize the specific uncertainty metric required for validation.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the Bayesian MAML objective function from first principles.\", \"A self-contained Python script (`bayesian_maml.py`) implementing the experiment and generating all plots and metrics.\", \"A summary report in `report.md` presenting the methodology, results, and plots comparing the predictive uncertainty of Bayesian MAML and vanilla MAML.\", \"A `metrics.json` file reporting the final Mean Squared Error and Expected Calibration Error for both models.\", \"A bibliography with citations for at least two relevant academic papers.\"], \"thinking_outline\": [\"1. Establish the theoretical framework by reviewing literature on Bayesian MAML and amortized variational inference.\", \"2. Formally derive the evidence lower bound (ELBO) for the meta-learning objective, showing its decomposition into a reconstruction term and a KL divergence regularization term.\", \"3. Implement the derived objective and the reparameterization trick for gradient estimation in a PyTorch model.\", \"4. Design and run a few-shot sinusoidal regression experiment to compare Bayesian MAML against vanilla MAML.\", \"5. Validate the implementation by checking if the model's predictive variance increases for out-of-distribution inputs.\", \"6. Verify that the final reported Expected Calibration Error (ECE) for Bayesian MAML is lower than that of the baseline, confirming improved uncertainty quantification.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: the loss function implemented in /code must directly correspond to the final equation derived in /math; fallback: if the full posterior is intractable, the derivation may be simplified to a mean-field or diagonal covariance assumption, which must be documented).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Bayesian MAML and amortized variational inference for context.\", \"- [ ] /math: Derive the ELBO for the Bayesian MAML objective, clearly defining all terms.\", \"- [ ] /code: Implement the Bayesian MAML and vanilla MAML models in PyTorch.\", \"- [ ] /code: Implement the sinusoidal regression data loader and the training/evaluation loop.\", \"- [ ] /code: Implement the Expected Calibration Error (ECE) metric for regression.\", \"- [ ] /math: Verify that the implemented loss function in the code correctly reflects the derived ELBO.\", \"- [ ] /code: Validate the model's uncertainty by testing on out-of-distribution regression inputs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"variational-inference\", \"bayesian-deep-learning\"], \"acceptance_criteria\": [\"The mathematical derivation of the ELBO must be complete, correct, and self-contained.\", \"The Python code must be executable and reproduce the reported metrics for both Bayesian and vanilla MAML.\", \"The final report must include plots clearly showing that Bayesian MAML provides higher, more meaningful predictive variance for out-of-distribution data points compared to the baseline.\", \"The Expected Calibration Error (ECE) reported for the Bayesian MAML model must be demonstrably lower than that of the vanilla MAML baseline.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 137, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive a variant of the Barlow Twins loss function by incorporating a term based on the information bottleneck principle, specifically penalizing the mutual information between attention head outputs to encourage representational diversity. Investigate the theoretical implications of this modification on representational collapse and provide a PyTorch implementation to validate its convergence properties on a synthetic dataset, comparing its eigenvalue distribution of the cross-correlation matrix against the original formulation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Barlow Twins loss information bottleneck transformer attention head diversity\\\", mode=web)\", \"/math(Formulate a modified Barlow Twins Lagrangian that includes a tractable proxy for mutual information between attention outputs, analyzing its effect on the cross-correlation matrix's eigenvalue spectrum and deriving the gradients for optimization.)\", \"/code(Implement the derived loss in PyTorch, training a vision transformer on a synthetic dataset to validate convergence and analyze the representational collapse by tracking AUPRC on a downstream task and the covariance matrix rank., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the task in current literature on self-supervised learning and the information bottleneck principle. The findings are passed to /math for rigorous mathematical formulation and derivation of the novel loss function. Finally, /code implements the derived mathematics to empirically validate the theoretical claims, completing the research cycle.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the proposed loss function, including gradient calculations.\", \"A Python script (`main.py`) implementing the training loop, the custom loss function, and the evaluation on a synthetic dataset.\", \"A Jupyter notebook (`analysis.ipynb`) visualizing the eigenvalue distribution of the cross-correlation matrix and comparing downstream performance metrics.\", \"A summary report (`report.md`) explaining the findings, including at least two citations to relevant arXiv papers.\", \"A JSON file (`metrics.json`) containing the final primary and secondary metric values.\"], \"thinking_outline\": [\"1. Conduct a literature review to find robust, tractable proxies for mutual information suitable for penalizing redundancy between transformer attention heads, referencing state-of-the-art self-supervised learning papers.\", \"2. Formulate the composite loss function by combining the standard Barlow Twins objective with the selected information bottleneck penalty term, ensuring the new term is differentiable.\", \"3. Derive the analytical gradients of the complete loss function with respect to the model parameters.\", \"4. Implement the loss in PyTorch and design an experiment to compare it against the baseline Barlow Twins, focusing on metrics that quantify representation collapse.\", \"5. Verify that the empirical eigenvalue distribution of the feature cross-correlation matrix from the coded implementation matches the trends predicted by the mathematical analysis.\", \"6. Validate the effectiveness of the new loss function by demonstrating a statistically significant improvement in a downstream classification task or a reduction in representation collapse.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses these to produce a formal derivation and theoretical predictions -> /code implements the derivation and runs experiments. Verification: The /code agent must use assertions and numerical gradient checks to validate the analytical gradients provided by the /math agent. Fallback: If the derivation from /math is numerically unstable, the /code agent flags it, and the router re-engages /math to find an alternative formulation.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize at least two key papers on Barlow Twins and information-theoretic regularization in deep learning.\", \"- [ ] /math: Derive the complete loss function and its gradient, clearly stating all assumptions.\", \"- [ ] /math: Analyze the expected effect of the new term on the eigenvalue spectrum of the feature covariance matrix.\", \"- [ ] /code: Implement the custom PyTorch loss function and a training script for a small vision transformer model.\", \"- [ ] /code: Implement metric tracking for representation collapse (e.g., rank of covariance matrix) and downstream accuracy.\", \"- [ ] /math: Verify that the numerical gradients computed in the PyTorch implementation align with the analytical gradients derived earlier.\", \"- [ ] /code: Generate plots comparing the eigenvalue spectrum and downstream metrics between the baseline and the proposed method.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"self-supervised-learning\", \"attention-mechanism\", \"information-bottleneck\"], \"acceptance_criteria\": [\"The mathematical derivation must be self-contained and free of logical errors.\", \"The Python code must be executable and reproduce the metrics reported in the summary.\", \"The final report must show a quantifiable difference in the primary or secondary metric when comparing the new loss to the baseline Barlow Twins.\", \"The analysis must include at least one visualization of the eigenvalue distribution of the cross-correlation matrix.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 138, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nPlease provide a full theoretical derivation for score-based generative models framed as the solution to a reverse-time stochastic differential equation (SDE). Start from a forward Ornstein-Uhlenbeck process, derive the corresponding reverse-time SDE, and explicitly show its connection to the Fokker-Planck equation. Then, implement a numerical solver in Python for this SDE to sample from a simple 2D distribution (e.g., a Swiss roll), analyzing the convergence and the impact of the score function's Lipschitz continuity on sampling stability and FID.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models stochastic differential equations Fokker-Planck reverse-time SDE\\\", mode=both)\", \"/math(Derive the reverse-time SDE from a forward Ornstein-Uhlenbeck process, leveraging It\u00f4 calculus and the Fokker-Planck equation. Analyze the convergence properties and the theoretical role of the score function's Lipschitz constant on stability.)\", \"/code(Implement a PyTorch-based numerical solver for the derived reverse-time SDE using the Euler-Maruyama method. Train a neural network to approximate the score function on a 2D dataset and generate samples, tracking the score matching loss and FID metrics with CUDA acceleration., using Python)\", \"/math(Verify the numerical stability of the SDE solver by analyzing the empirical Lipschitz constant of the trained score network. Compare this to the theoretical bounds derived previously and check for potential eigenvalue explosion in the Jacobian of the score function.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational arXiv papers. This literature informs the /math agent's formal derivation of the SDE theory. The /code agent then implements this theory for empirical validation. A final /math agent provides a crucial verification step, auditing the numerical properties of the code's trained model against the initial theoretical derivation, ensuring a closed-loop reasoning process.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the reverse-time SDE and its connection to the Fokker-Planck equation.\", \"A commented Python script implementing the SDE solver, score network training, and sampling loop.\", \"A JSON file reporting the final FID score and the score matching loss curve over training epochs.\", \"A final report summarizing the findings, including plots of the generated samples and at least two citations to foundational arXiv papers.\", \"A verification log detailing the comparison between theoretical and empirical Lipschitz constants.\"], \"thinking_outline\": [\"1. Use search to establish the theoretical foundations of score-based models by reviewing seminal papers on score matching and their connection to diffusion processes and SDEs.\", \"2. Formally derive the reverse-time SDE corresponding to a predefined forward SDE (e.g., variance-preserving). This derivation must explicitly use results from stochastic calculus and the Fokker-Planck partial differential equation.\", \"3. Implement a numerical approximation (e.g., Euler-Maruyama or a higher-order method) for the reverse SDE in Python, training a time-dependent neural network to estimate the score function.\", \"4. Validate the implementation by training the model on a known 2D distribution and generating samples. Quantify the quality of the generated distribution using the Fr\u00e9chet Inception Distance (FID) metric.\", \"5. Verify the numerical stability by analyzing the trained score network. Check if its empirical properties, such as its Lipschitz constant, align with the theoretical assumptions required for the SDE solver's convergence.\", \"6. Synthesize all derivations, code, results, and verification checks into a comprehensive final report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation, which the code agent implements. The code agent's trained model is then passed back to the math agent for numerical verification against the initial theory. Verification: The empirical Lipschitz constant from the code must be within the stable bounds derived by the math agent. Fallback: If verification fails, the task is re-routed to the code agent with adjusted hyperparameters for the score network to encourage better conditioning.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on SDE-based diffusion models.\", \"- [ ] /math: Produce a step-by-step derivation of the reverse-time SDE and its link to the Fokker-Planck equation.\", \"- [ ] /code: Implement the SDE solver and score network training loop in PyTorch.\", \"- [ ] /code: Run the training and sampling process to generate samples and compute FID.\", \"- [ ] /math: Analyze the trained PyTorch model to estimate its empirical Lipschitz constant.\", \"- [ ] /math: Verify that the empirical constant is consistent with the stability theory derived earlier.\", \"- [ ] /router: Review all artifacts for correctness, consistency, and fulfillment of acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-modeling\"], \"acceptance_criteria\": [\"- The mathematical derivation of the reverse-time SDE must be correct and self-contained.\", \"- The Python implementation must successfully generate samples that qualitatively resemble the target 2D distribution.\", \"- The final FID score reported in `metrics.json` must be below a reasonable threshold (e.g., 50 for a moderately complex 2D dataset).\", \"- The report must cite at least two relevant arXiv preprints with their IDs.\", \"- The numerical stability verification step must be present and correctly executed.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 139, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am working with a Structural Causal Model (SCM) defined by the graph Z -> X -> Y and Z -> Y. I need a formal derivation of the post-interventional distribution P(Y|do(X=x)) using Pearl's do-calculus. Subsequently, please develop a Python simulation of this Bayesian network to empirically validate that the derived formula correctly identifies the true causal effect, contrasting it with the biased observational conditional probability P(Y|X=x).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Judea Pearl do-calculus rules back-door criterion causal inference\\\", mode=rag)\", \"/math(Derive the expression for P(Y|do(X=x)) for the specified causal graph Z->X->Y, Z->Y by applying the rules of do-calculus, specifically focusing on adjusting for the confounder Z to satisfy the Bayesian back-door criterion.)\", \"/code(Implement the specified SCM in Python with linear-Gaussian relationships. Generate two datasets: one observational and one interventional by fixing X. Use bootstrap resampling to compute empirical estimates and a confidence interval for both P(Y|X=x) and P(Y|do(X=x)).)\", \"/math(Verify that the analytical formula derived previously, when computed using parameters from the observational data, yields a value for E[Y|do(X=x)] that falls within the 95% confidence interval of the empirical estimate from the interventional simulation.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the work in foundational causal inference literature. Next, /math formally derives the interventional distribution, which is the core theoretical task. Then, /code creates a simulation to provide empirical ground truth for the intervention. Finally, a second /math call serves as a crucial cross-validation step, comparing the theoretical formula against the simulated results to ensure correctness.\", \"expected_artifacts\": [\"- A formal mathematical derivation of P(Y|do(X=x)) using do-calculus, presented in LaTeX.\", \"- A Python script (`simulation.py`) that implements the SCM, the intervention, and the estimation procedures.\", \"- A visualization (`comparison.png`) plotting the observational vs. interventional conditional expectations of Y given X.\", \"- A summary report (`report.md`) explaining the results and citing at least two foundational papers on causal inference.\", \"- A JSON file (`metrics.json`) containing the calculated KL Divergence and MAE between the derived and simulated outcomes.\"], \"thinking_outline\": [\"1. Formally define the Structural Causal Model (SCM) and its corresponding Directed Acyclic Graph (DAG), specifying the functional relationships between Z, X, and Y.\", \"2. Apply the rules of do-calculus, particularly Rule 2 (back-door adjustment), to derive the analytical expression for the post-intervention distribution P(Y|do(X=x)).\", \"3. Implement the SCM in Python to generate a large sample of observational data. From this data, compute the associational quantity E[Y|X=x].\", \"4. Create a modified version of the SCM simulation that implements the intervention `do(X=x)`. Generate a large sample of interventional data and compute the causal quantity E[Y|do(X=x)].\", \"5. Validate the derivation by plugging the observational data into the derived analytical formula and checking if the result numerically matches the causal quantity estimated directly from the interventional simulation data.\", \"6. Audit the entire process, documenting the derivation, code, and verification results in a final report, ensuring all claims are supported by evidence from the simulation.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (simulation) -> /math (verification). The /general-search agent provides foundational papers for the /math agent. The /math agent's derived formula and SCM structure inform the /code agent's implementation. The /code agent's simulation output (empirical distributions and confidence intervals) is handed to the final /math agent to perform a numerical verification against the original formula. Fallback: If verification fails (discrepancy > 5%), the router flags the mismatch and requests /code to re-audit its SCM implementation against the formal definition.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers by Judea Pearl on do-calculus and the back-door criterion.\", \"- [ ] /math: Provide a step-by-step derivation for P(Y|do(X=x)) using the back-door adjustment formula.\", \"- [ ] /code: Implement the SCM, ensuring the data generation process for both observational and interventional settings is correct.\", \"- [ ] /code: Calculate the empirical estimates and bootstrap confidence intervals for the causal and associational effects.\", \"- [ ] /math: Receive the derived formula and the simulation outputs from the other agents.\", \"- [ ] /math: Verify that the analytical result aligns with the empirical interventional simulation result within the specified tolerance.\", \"- [ ] /router: Perform final QA check on all artifacts, ensuring consistency between derivation, code, metrics, and the final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"bayesian-networks\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies the back-door adjustment formula to identify the causal effect.\", \"- The Python simulation code runs without errors and correctly implements the specified SCM and the `do(X=x)` intervention.\", \"- The primary metric (KL Divergence) between the analytically derived distribution and the empirically simulated one is below 0.05.\", \"- The report correctly distinguishes between association and causation, citing appropriate literature.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 140, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a formal analysis connecting the spectral properties of the multi-head self-attention operator to the convergence stability of self-supervised contrastive learning. Specifically, derive the spectral norm bounds for the attention matrix under a Lipschitz continuity assumption on the activation functions. Then, theoretically link this bound to the Jacobian of the NT-Xent loss to establish convergence criteria, and provide a numerical verification in PyTorch to confirm the derived bounds.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org spectral norm transformer attention convergence 'contrastive learning' 'Lipschitz continuity'\\\", context=\\\"To ground the derivation, find recent arXiv preprints that analyze the spectral properties and convergence guarantees of attention mechanisms in contrastive learning frameworks.\\\", mode=both)\", \"/math(context=\\\"Derive the upper bound for the spectral norm of the multi-head self-attention operator, considering Lipschitz continuity. Analyze the convergence conditions for the contrastive loss by bounding the largest eigenvalue of its Jacobian, referencing the gathered arxiv literature.\\\")\", \"/code(context=\\\"Implement a PyTorch model on a CUDA device to empirically verify the derived spectral bounds. Track the maximum eigenvalue of the attention matrix during training with a contrastive loss and measure the convergence rate against our theoretical prediction.\\\", using Python)\"], \"route_rationale\": \"The 'blue' general-search agent first gathers state-of-the-art theoretical analyses from arXiv to inform the derivation. The /math agent then performs the core theoretical work based on this research. Finally, the /code agent provides a critical empirical verification of the mathematical theory, ensuring the model's behavior aligns with the derived bounds.\", \"expected_artifacts\": [\"A formal mathematical proof in LaTeX deriving the spectral norm bounds for the self-attention mechanism under stated assumptions.\", \"A Python script using PyTorch to implement the self-supervised learning experiment and empirically calculate the spectral norm of the attention matrices.\", \"A summary report (report.md) linking the theoretical bounds to the empirical convergence behavior, including citations to at least 2 relevant arXiv papers.\", \"A JSON file (metrics.json) containing the tracked maximum eigenvalue and loss values per training epoch.\"], \"thinking_outline\": [\"1. Survey foundational literature on the spectral analysis of transformers and the optimization landscape of contrastive self-supervised learning.\", \"2. Formulate the problem mathematically, defining the self-attention operator and the NT-Xent contrastive loss function with precise notation.\", \"3. Derive the spectral norm bounds for the attention operator, carefully stating all assumptions, such as Lipschitz continuity of non-linearities.\", \"4. Analyze the Jacobian of the contrastive loss function with respect to the learned representations and relate its spectral properties to the derived attention bounds to establish a convergence condition.\", \"5. Validate the theoretical derivation by implementing a numerical experiment to track the empirical spectral norm and convergence rate, and check for alignment between theory and practice.\", \"6. Audit the final report to ensure the mathematical claims are rigorously supported by both the derivation and the empirical results, and that all citations are correctly formatted.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent's derivation of spectral bounds is passed to the code agent to define the verification experiment. The code agent's empirical results are sent back for final review. (Verification: The router will check if the code's measured eigenvalues stay within the math agent's derived bounds. Fallback: If a discrepancy exists, the task is routed back to the /math agent to re-check assumptions in the derivation.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on transformer spectral analysis and SSL convergence guarantees from arXiv.\", \"- [ ] /math: Formally derive the spectral norm bounds for the self-attention operator based on literature review.\", \"- [ ] /math: Analyze the Jacobian of the NT-Xent loss, linking its spectral properties to the attention bounds.\", \"- [ ] /code: Implement a PyTorch model for contrastive learning to measure the empirical spectral norm of the attention matrix.\", \"- [ ] /code: Plot the empirical spectral norm vs. training steps and compare against the theoretical bound.\", \"- [ ] /math: Verify that the code implementation correctly computes the spectral norm and that the results align with the theoretical derivation.\", \"- [ ] /router: Perform final QA check, ensuring the report links the theoretical and empirical results cohesively and addresses the user query.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"self-supervised-learning\", \"spectral-theory\", \"attention-mechanisms\"], \"acceptance_criteria\": [\"The submitted mathematical derivation for the spectral norm bound is rigorous, self-contained, and clearly states all assumptions.\", \"The PyTorch code successfully executes and generates metrics where the empirical spectral norm of the attention matrix does not exceed the derived theoretical upper bound across training.\", \"The final report correctly cites at least 2 relevant arXiv papers and clearly explains the connection between the theoretical analysis and the empirical results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 141, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the loss landscape of a non-convex optimization problem and need to rigorously analyze the behavior of gradient-based algorithms near saddle points. Specifically, I want to understand the formal conditions for a point to be a 'strict' saddle and how algorithms like Perturbed Gradient Descent (PGD) leverage noise to escape such points. Can you provide a formal derivation, an implementation, and a literature review on the topic?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'perturbed gradient descent' strict saddle point convergence analysis\\\", mode=web)\", \"/math(Derive the Hessian eigenvalue conditions defining a strict saddle point and formalize the convergence proof for a noise-injected gradient method, ensuring the escape rate is polynomial in the dimension.)\", \"/code(Implement perturbed gradient descent in Python/JAX to visualize escaping a saddle on a test function like f(x,y)=x^2-y^2. Measure the computational complexity and verify the escape trajectory against theoretical convergence predictions.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey recent arXiv literature for foundational theorems and analysis techniques on PGD. This context is then passed to /math to formally derive the mathematical properties and algorithm guarantees. Finally, /code is used to implement the algorithm and empirically validate the theoretical findings from the previous steps.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of strict saddle conditions and the PGD escape mechanism.\", \"A Python script (JAX or PyTorch) implementing PGD on a canonical saddle point function.\", \"A plot visualizing the optimization trajectory escaping the saddle point.\", \"A summary report (PDF) integrating the findings and citing at least two relevant research papers.\", \"A JSON file containing the computed metrics (escape_time, convergence_rate).\"], \"thinking_outline\": [\"1. Research the formal definition of a strict saddle point and the theoretical guarantees of algorithms designed to escape them, such as Perturbed Gradient Descent.\", \"2. Use Hessian matrix analysis to derive the mathematical conditions (i.e., existence of a negative eigenvalue) that characterize a strict saddle point.\", \"3. Formulate the update rule for Perturbed Gradient Descent and sketch a proof of its convergence to a second-order stationary point.\", \"4. Verify the derived update rule and convergence properties against the literature found in the initial search to ensure correctness and alignment with state-of-the-art.\", \"5. Implement the algorithm and a suitable test function (e.g., a hyperbolic paraboloid) in a numerical computing framework.\", \"6. Validate the implementation by plotting the trajectory of the iterates, confirming they escape the saddle, and comparing the empirical escape time to the theoretical bounds.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Cross-check that the code's stopping criteria and parameterization match the conditions derived in the math proof; fallback: If implementation diverges, flag for /math to review the derivation's assumptions.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on how perturbed gradient methods escape strict saddle points.\", \"- [ ] /math: Define a strict saddle point using Hessian eigenvalue properties and derive the full update rule for Perturbed Gradient Descent.\", \"- [ ] /math: Sketch the proof showing that PGD converges to a second-order stationary point with high probability.\", \"- [ ] /code: Implement PGD and a test function (e.g., f(x,y) = 0.5*x^2 - 0.5*y^2) in Python.\", \"- [ ] /code: Generate a 2D contour plot showing the function landscape and overlay the PGD optimization path demonstrating saddle escape.\", \"- [ ] /math: Verify that the algorithm implemented in the /code step correctly matches the derived mathematical formulation, including the noise injection mechanism.\", \"- [ ] /code: Verify that the generated plot clearly shows the trajectory moving away from the saddle along the direction of negative curvature.\", \"- [ ] router QA: Review all generated artifacts for consistency, correctness, and adherence to acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"gradient-dynamics\"], \"acceptance_criteria\": [\"The mathematical derivation correctly identifies the role of the negative Hessian eigenvalue in the escape direction.\", \"The Python implementation successfully demonstrates an optimization trajectory moving away from the (0,0) saddle point on the specified test function.\", \"The final report correctly cites at least two relevant arXiv papers with IDs.\", \"The primary metric `escape_time` is reported and is finite, indicating successful escape.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 142, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the full expression for the Hessian of the cross-entropy loss for a fully-connected layer employing weight normalization (Salimans & Kingma, 2016). Analyze how this reparameterization affects the spectral norm and the principal eigenvalue (\u03bb_max) of the Hessian, and formally connect these spectral properties to optimization convergence rates, referencing recent literature from arXiv.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org \\\\\\\"weight normalization\\\\\\\" Hessian spectral norm \\\\\\\"convergence rate\\\\\\\" OR eigenvalue\\\", mode=both)\", \"/math(Derive the Hessian matrix of the loss with respect to the pre-normalized weights `v`. Apply tensor calculus and the chain rule, analyzing the structure's impact on its largest eigenvalue and relating it to the model's geometry.)\", \"/code(Implement a PyTorch model with a weight-normalized layer. Use `torch.autograd.functional.hessian` combined with Power Iteration to empirically estimate the maximum eigenvalue (\u03bb_max). Validate convergence speed against a standard layer., using Python)\", \"/math(Synthesize the theoretical derivation and the empirical eigenvalue results from the code. Formulate a rigorous argument connecting the reduced spectral norm of the Hessian to improved convergence guarantees and validate the code's empirical results against theoretical bounds.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature from arXiv as requested. This informs the core theoretical work in /math, which derives the Hessian. The /code agent then provides empirical verification of the theoretical findings by numerically estimating the key spectral metric (\u03bb_max). Finally, /math synthesizes both theoretical and empirical results into a conclusive proof, including a verification step.\", \"expected_artifacts\": [\"A LaTeX document (.pdf) containing the full mathematical derivation of the Hessian for the weight-normalized layer.\", \"A Python script (.py) implementing the network and the Power Iteration method to estimate the Hessian's largest eigenvalue.\", \"A Markdown report (.md) summarizing the findings, embedding plots of eigenvalue evolution, and connecting the theory to empirical convergence results, including citations.\", \"A JSON file (`metrics.json`) logging the largest eigenvalue and training loss per epoch.\", \"Verification notes confirming the consistency between the analytical derivation and numerical estimates.\"], \"thinking_outline\": [\"1. Formulate the weight normalization reparameterization (w = g * v / ||v||) and the cross-entropy loss function for a single layer.\", \"2. Apply matrix calculus, specifically the multivariate chain rule, to derive the gradient and then the Hessian of the loss with respect to the pre-normalized weight vector `v` and scale `g`.\", \"3. Implement a numerical routine in Python to compute the Hessian and estimate its largest eigenvalue. Verify that the empirical results for a test case align with the derived analytical form.\", \"4. Analyze the structure of the derived Hessian to prove how weight normalization projects out gradient components, effectively damping updates and improving the landscape's conditioning (reducing \u03bb_max).\", \"5. Validate the final proof by cross-referencing with established results on optimization and implicit regularization from the literature search, ensuring the argument is sound.\", \"6. Audit the final report to ensure the theoretical claims are directly supported by the empirical data generated by the code.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: code's empirical eigenvalue estimation must align with the analytical bounds from the /math derivation; fallback: if discrepancy > 5%, re-run /math to check derivation for errors.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on weight normalization and Hessian analysis.\", \"- [ ] /math: Complete the formal derivation of the Hessian, clearly showing each step of the chain rule application.\", \"- [ ] /code: Implement the PyTorch model and the Power Iteration algorithm for eigenvalue estimation.\", \"- [ ] /code: Generate plots showing the evolution of the maximum eigenvalue and training loss for both normalized and unnormalized layers.\", \"- [ ] /math: Verify that the code's output for the largest eigenvalue is numerically consistent with the derived Hessian structure.\", \"- [ ] /math: Finalize the proof connecting the spectral analysis to the observed convergence improvements.\", \"- [ ] /router: Perform final QA review, ensuring all artifacts (proof, code, report, citations) are consistent and meet acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"optimization-theory\"], \"acceptance_criteria\": [\"The derived Hessian expression in the LaTeX document must be mathematically correct and fully expanded.\", \"The Python script must execute without errors and generate plots clearly showing a lower maximum Hessian eigenvalue for the weight-normalized layer compared to the baseline.\", \"The final report must cite at least 2 relevant arXiv papers with IDs and cogently link the theoretical derivation to the empirical results.\", \"The primary metric (max_eigenvalue_Hessian) must be reported and show a statistically significant reduction for the normalized model.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"The largest eigenvalue (\u03bb_max) of the loss function's Hessian. Compute using the Power Iteration method on the Hessian matrix, which is obtained via `torch.autograd.functional.hessian`.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"The number of training epochs required to reach a target cross-entropy loss threshold (e.g., < 0.05). A lower value is better.\"}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"index": 144, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the application of persistent homology for manifold learning. Specifically, can you derive the mathematical framework for constructing a Vietoris-Rips filtration on a point cloud sampled from a low-dimensional manifold embedded in a high-dimensional Euclidean space? Then, implement a Python script to compute the persistence diagram for a synthetic Swiss roll dataset, analyze its computational complexity, and verify that the inferred Betti numbers (\u03b2\u2080 and \u03b2\u2081) correctly capture the manifold's topology.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org persistent homology stability Vietoris-Rips manifold learning computational complexity\\\", mode=rag)\", \"/math(Derive the construction of the Vietoris-Rips filtration and the algorithm for computing persistence diagrams. Analyze the stability theorem's implications for convergence and discuss the worst-case computational complexity.)\", \"/code(Implement the TDA pipeline in Python using Ripser to compute persistence diagrams for noisy Swiss roll data. Analyze the eigenvalue spectrum of the graph Laplacian for comparison and verify that Betti numbers stabilize as sample size increases.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational literature from arXiv on persistent homology. This research informs the /math agent, which will formalize the required derivations. Finally, the /code agent will implement the derived theory for empirical validation on a synthetic manifold, providing a complete theory-to-practice workflow.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Vietoris-Rips filtration and persistence diagram computation.\", \"- A Python script implementing the analysis pipeline for the Swiss roll dataset.\", \"- A summary report including generated persistence diagrams, Betti number analysis, and performance metrics.\", \"- A bibliography with citations to at least 2 relevant arXiv preprints on topological data analysis.\", \"- Verification output confirming the computed Betti numbers match the known topology of the synthetic manifold.\"], \"thinking_outline\": [\"1. Research seminal papers on persistent homology, focusing on the stability theorem and its application to manifold learning.\", \"2. Formulate the precise mathematical definition of the Vietoris-Rips complex and the associated filtration process for a given point cloud.\", \"3. Derive the algorithm for computing persistence intervals and the resulting persistence diagram, analyzing its computational complexity.\", \"4. Implement the pipeline in Python and first validate it on a simple dataset with known topology, like points sampled from a circle, to check the correctness of the Betti number calculation.\", \"5. Apply the validated implementation to a noisy Swiss roll dataset, generate its persistence diagram, and interpret the significant features.\", \"6. Verify that the most persistent features in the diagram correspond to the correct Betti numbers (\u03b2\u2080=1, \u03b2\u2081=0) for the Swiss roll manifold, confirming the method's efficacy.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code implementation must match the mathematical derivations from /math; fallback: if Betti numbers are incorrect, /code agent must revisit distance matrix construction and filtration parameters).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on persistent homology and its stability.\", \"- [ ] /math: Provide a step-by-step derivation of the Vietoris-Rips filtration construction.\", \"- [ ] /math: Formally define the persistence diagram and explain how to read Betti numbers from it.\", \"- [ ] /code: Implement a Python script to generate a Swiss roll dataset.\", \"- [ ] /code: Use a library like Ripser or Gudhi to compute the persistence diagram for the dataset.\", \"- [ ] /math: verify the logic in the code for extracting Betti numbers aligns with the mathematical definition.\", \"- [ ] /code: validate that the output Betti numbers (\u03b2\u2080, \u03b2\u2081) match the ground truth for the manifold.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"computational-geometry\"], \"acceptance_criteria\": [\"- The mathematical derivation is clear, correct, and self-contained.\", \"- The Python code successfully runs and generates a persistence diagram for a 2000-point Swiss roll dataset.\", \"- The final report correctly identifies \u03b2\u2080=1 and \u03b2\u2081=0 as the most persistent topological features from the diagram.\", \"- The analysis correctly cites at least two relevant academic papers with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 145, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating Tucker decomposition for compressing neural network weight tensors. Derive the alternating least squares (ALS) update rules for the core tensor and factor matrices, including a proof of convergence under standard assumptions. Also, explore how concepts from algebraic topology, specifically persistent homology, could regularize the decomposition to preserve topological features of the learned representation space. Implement this in PyTorch and validate the compression-accuracy trade-off on a simple convolutional architecture.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org tucker decomposition ALS convergence proof persistent homology regularization representation learning\\\", mode=both)\", \"/math(Derive the Alternating Least Squares update rules for Tucker decomposition. Provide a formal proof of convergence, analyzing the monotonic decrease of the reconstruction error, and considering the eigenvalue properties of the unfolded tensor matricizations.)\", \"/code(Implement the derived ALS algorithm in PyTorch with CUDA support for tensor operations. Add a component for computing persistent homology using Gudhi, tracking reconstruction error vs. Betti numbers., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather seminal papers on ALS for Tucker decomposition and recent work on topological regularization. Next, /math is used for the core theoretical work: deriving the update rules and proving convergence, which is a pure mathematical task. Finally, /code implements the derived math in a practical PyTorch setting, which requires a coding environment to validate the theory empirically.\", \"expected_artifacts\": [\"A LaTeX document detailing the derivation of the ALS update rules and the complete convergence proof.\", \"A Python script implementing the regularized Tucker decomposition, including functions for validation.\", \"A summary report comparing reconstruction error and model accuracy for different ranks and regularization strengths.\", \"A list of at least 2 cited arXiv papers on tensor decomposition and topological data analysis.\", \"A verification log confirming the mathematical derivation matches the code's iterative updates.\"], \"thinking_outline\": [\"1. Establish the mathematical formulation for Tucker decomposition and define the objective function for the reconstruction error (sum of squared differences).\", \"2. Derive the update rules for each factor matrix and the core tensor by setting the partial derivative of the objective function to zero, treating other components as fixed (the core of the Alternating Least Squares approach).\", \"3. Prove that the objective function is non-increasing at each step of the ALS algorithm, ensuring convergence to a local minimum.\", \"4. Validate the mathematical proof for logical soundness and check that assumptions (e.g., non-singularity) are clearly stated.\", \"5. Formulate a regularization term based on persistent homology to penalize changes in the topology of the data's representation and integrate it into the objective function.\", \"6. Verify that the implementation's loss reduction curve aligns with the monotonic convergence proven in the mathematical derivation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent verifies the logic of the ALS update steps from the literature survey; /code agent verifies implementation against the /math derivation's final form; fallback: if convergence proof fails, /math agent reverts to analyzing a simpler CP decomposition case).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on ALS for Tucker decomposition and recent research on topological regularization in deep learning.\", \"- [ ] /math: Derive the ALS update equations for the core tensor and factor matrices from first principles.\", \"- [ ] /math: Construct a formal proof of convergence for the ALS algorithm under standard assumptions.\", \"- [ ] /code: Implement the Tucker-ALS algorithm in PyTorch, ensuring tensor operations are efficient.\", \"- [ ] /code: Integrate a persistent homology regularizer using a TDA library and set up an experiment to test its effect.\", \"- [ ] /math: Verify that the implemented loss function in the /code artifact correctly reflects the derived mathematical objective.\", \"- [ ] /code: Validate that the empirical convergence rate in the implementation is consistent with the theoretical proof's guarantees.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"algebraic-topology\"], \"acceptance_criteria\": [\"The mathematical derivation of the ALS update rules must be complete and self-contained.\", \"The convergence proof must be logically sound and explicitly state all necessary assumptions.\", \"The Python implementation must successfully run and produce a plot showing the trade-off between compression ratio and model accuracy.\", \"The primary metric (Reconstruction Frobenius Norm Error) must decrease monotonically over iterations as predicted by the theory.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 147, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and evaluate a structured amortized variational inference algorithm for a Bayesian Convolutional Neural Network. The task requires a formal derivation of the Evidence Lower Bound (ELBO) for a hierarchical model with both global and local latent variables. Please focus on deriving and implementing a low-variance reparameterization gradient estimator for the ELBO and benchmark its convergence and predictive uncertainty quantification capabilities on the CIFAR-10 dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'structured variational inference' 'bayesian deep learning' 'reparameterization gradient estimator' convergence\\\", mode=rag)\", \"/math(Derive the Evidence Lower Bound (ELBO) for the hierarchical Bayesian model, detailing the reparameterization trick for the structured posterior and analyzing the KL divergence term for asymptotic convergence properties.)\", \"/code(Implement the Bayesian CNN in Pyro, building an efficient Monte Carlo estimator for the ELBO gradient. The implementation must track key convergence metrics and include hooks for future adversarial robustness checks., using Python)\", \"/math(Develop a numerical gradient checking procedure to validate the custom reparameterization gradient implementation. Compare the analytical gradients against the auto-differentiated results to verify correctness and analyze potential variance issues in the Monte Carlo estimator.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art context on structured VI. This informs the /math agent's formal derivation of the ELBO objective. The /code agent then implements this complex mathematical model. A final, critical /math step is included to perform numerical verification of the custom gradient logic, which is notoriously difficult to implement correctly.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the structured ELBO and its reparameterization gradient.\", \"A well-documented Python script implementing the Bayesian CNN and the structured variational inference training loop.\", \"A final report in Markdown format summarizing the model's performance, including ELBO convergence plots and predictive uncertainty visualizations.\", \"A log file from the numerical gradient checker verifying the correctness of the ELBO gradient implementation.\", \"A bibliography file with citations to at least 3 relevant arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the hierarchical Bayesian model for the CNN, clearly defining the priors over weights and the structure of the amortized variational posterior.\", \"2. Derive the Evidence Lower Bound (ELBO) objective, explicitly decomposing it into the expected log-likelihood and the Kullback-Leibler divergence terms for both global and local variables.\", \"3. Derive the low-variance reparameterization gradient estimator for the ELBO with respect to all variational parameters.\", \"4. Implement the Bayesian CNN and the structured variational inference algorithm in Python using the Pyro probabilistic programming language.\", \"5. Validate the correctness of the ELBO gradient implementation by developing and running a numerical gradient checking script on a simplified version of the model.\", \"6. Train the model on the CIFAR-10 dataset, evaluate its performance by tracking the ELBO convergence, predictive accuracy, and calibration of its uncertainty estimates on a held-out test set.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The /math derivation must be self-contained and directly translatable by /code. The /code implementation must expose gradient tensors for the /math verification agent. Verification failure (gradient mismatch > 1e-4) triggers a fallback to /code with diagnostic information for debugging.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 3+ key papers on structured VI for Bayesian neural networks (verify results)\", \"- [ ] /math: Produce a complete, step-by-step derivation of the ELBO and its reparameterization gradient.\", \"- [ ] /code: Implement the Bayesian CNN model, data loaders, and the SVI training loop.\", \"- [ ] /math: Implement a numerical gradient checker and verify the `d(ELBO)/d(phi)` implementation from the /code agent.\", \"- [ ] /code: Train the final model on CIFAR-10, logging metrics (ELBO, accuracy, ECE) to a JSON file.\", \"- [ ] /code: Generate plots for ELBO convergence and a confusion matrix.\", \"- [ ] router QA: Review all artifacts for mathematical correctness, code quality, and alignment with the final report.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"probabilistic-programming\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO gradient must be complete, with all steps justified.\", \"- The Python implementation must be runnable and achieve an ELBO that converges smoothly during training.\", \"- The final report must demonstrate a test accuracy on CIFAR-10 that is within 5% of comparable models cited from the literature search.\", \"- The numerical gradient verification script must pass with a relative error of less than 1e-4 between the implemented and numerical gradients.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 148, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the strong-form residual loss for a Physics-Informed Neural Network (PINN) solving the 2D steady, incompressible Navier-Stokes equations. Implement this using a Fourier feature network in JAX to mitigate spectral bias, validating convergence on the lid-driven cavity problem by tracking the energy spectrum decay and comparing against high-fidelity simulation data.\n", "completion": "{\"route_plan\": [\"/math(Derive the multi-component loss function for the 2D Navier-Stokes PINN, including PDE residual, incompressibility constraint, and boundary conditions. Analyze the theoretical impact of Fourier features on the neural tangent kernel's eigenvalue decay to justify its use for improving convergence.)\", \"/code(Implement the Navier-Stokes PINN in JAX, incorporating Fourier feature mappings. Train on the lid-driven cavity benchmark, logging PDE residual loss, L2 error against a reference solution, and computing the kinetic energy spectrum to verify physical plausibility., using Python)\", \"/general-search(query=\\\"site:arxiv.org physics-informed neural network Navier-Stokes fourier features spectral bias convergence\\\", mode=rag)\"], \"route_rationale\": \"/math is used first to establish the theoretical foundation and derive the specific loss equations. /code follows to translate the mathematical model into a working implementation and generate empirical results. Finally, the blue /general-search agent is used to find authoritative references from arXiv that support the theoretical choices and contextualize the empirical findings, fulfilling the tri-route reasoning pattern.\", \"expected_artifacts\": [\"- A LaTeX document detailing the derivation of the Navier-Stokes PINN loss function.\", \"- A commented Python script (JAX) implementing the PINN, including the training loop and data generation for the lid-driven cavity problem.\", \"- A summary report (.md) containing plots of the loss curves, velocity field visualizations, and a comparison of the predicted vs. reference energy spectrum.\", \"- A bibliography with at least two cited arXiv preprints on PINNs for fluid dynamics or spectral bias.\", \"- A JSON file containing the final computed metrics (L2 error, mean residual loss).\"], \"thinking_outline\": [\"1. Formulate the strong form of the 2D steady, incompressible Navier-Stokes equations and derive the corresponding PDE residual loss, including terms for momentum in x and y, and the continuity (incompressibility) equation.\", \"2. Define the complete loss function by adding terms for the Dirichlet boundary conditions on the cavity walls (u=v=0 on three walls, u=1, v=0 on the lid).\", \"3. Implement the PINN architecture in Python, ensuring the input coordinates are first passed through a Fourier feature mapping layer before entering the main MLP.\", \"4. Validate the implementation by performing a gradient check on the loss function and ensuring the network output correctly satisfies the boundary conditions at initialization.\", \"5. Train the model and evaluate its performance by calculating the L2 relative error against a known high-fidelity solution and plotting the kinetic energy spectrum to check for physical consistency.\", \"6. Audit the results against findings from peer-reviewed literature to confirm that the observed convergence behavior and accuracy are in line with state-of-the-art methods.\"], \"handoff_plan\": \"/math (loss derivation) -> /code (implementation & training). The /code agent must verify its implementation against the equations from /math. /code (results) -> /general-search (literature context). The final report must synthesize the code's empirical results with the theoretical claims in the literature. Fallback: If the /code agent observes poor convergence or high residual loss (>1e-2), it must halt and request a review of the loss weighting scheme from the /math agent.\", \"todo_list\": [\"- [ ] /math: Derive the complete loss function for the 2D Navier-Stokes equations, including all residual and boundary terms.\", \"- [ ] /math: Provide a brief theoretical justification for using Fourier feature networks to combat spectral bias in PINNs.\", \"- [ ] /code: Implement the PINN architecture with Fourier features and the custom loss function in JAX.\", \"- [ ] /code: Train the model on the lid-driven cavity problem and generate all required plots and metric data.\", \"- [ ] /math: Verify that the Python code's loss implementation correctly matches the derived mathematical formula.\", \"- [ ] /general-search: Find and summarize at least two relevant arXiv papers on PINNs for fluid dynamics, focusing on convergence and spectral methods.\", \"- [ ] /code: Verify that the final L2 relative error is below the target threshold of 5%.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"fourier-feature-networks\"], \"acceptance_criteria\": [\"- The final L2 relative error of the velocity field must be less than 5% compared to the reference solution.\", \"- The submitted report must include a clear mathematical derivation of the loss function, consistent with the code implementation.\", \"- The energy spectrum plot must show a physically plausible decay rate for the given Reynolds number.\", \"- The report must cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "math_first", "non_canonical_route"]}
{"index": 152, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nCan you explain the core concept of Differentiable Architecture Search (DARTS) and guide me through a simple implementation? I want to understand how the continuous relaxation of the architecture representation works in practice for AutoML pipelines.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Differentiable Architecture Search' tutorial\\\", mode=both)(Find the foundational arXiv paper on DARTS to understand its continuous relaxation approach for AutoML pipelines.)\", \"/math(Derive the bi-level optimization for DARTS, explaining the first-order approximation for the inner loop gradient update. Contrast this with a purely Bayesian optimization approach.)\", \"/code(Implement a simple DARTS search cell in PyTorch, showing how architecture weights are updated via gradient descent. Include CUDA device placement and track validation accuracy., using Python)\"], \"route_rationale\": \"The 'blue' /general-search agent is used first to gather authoritative sources on DARTS. Then, /math is used to formalize the core optimization problem described in the literature. Finally, /code translates the mathematical theory into a practical, runnable example.\", \"expected_artifacts\": [\"A concise summary of the DARTS methodology with citations to the original paper.\", \"The mathematical formulation of the bi-level optimization problem and the gradient approximation.\", \"A working Python script implementing a simplified DARTS search cell.\", \"A verification report confirming that the code runs and architecture parameters receive gradient updates.\"], \"thinking_outline\": [\"1. Research the original DARTS paper (Liu et al., 2018) to identify the core contributions: continuous relaxation of the search space and bi-level optimization.\", \"2. Formulate the bi-level optimization problem mathematically, clearly defining the upper-level (architecture) and lower-level (weights) objectives.\", \"3. Explain the approximation used to make the gradient calculation with respect to architecture parameters tractable.\", \"4. Implement a minimal DARTS 'cell' in PyTorch, showing how a softmax over operation choices is used to create a mixed operation.\", \"5. Validate the implementation by running a few training steps and checking that the architecture parameters' gradients are non-zero.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses these to derive the formal optimization problem -> /code implements the math in a PyTorch script -> router QA (verification: code runs and architecture gradients are non-zero; fallback: if verification fails, return to /code with specific error trace).\", \"todo_list\": [\"- [ ] Use /general-search to find the original DARTS paper and key tutorials.\", \"- [ ] Use /math to formalize the bi-level optimization and gradient approximation.\", \"- [ ] Use /code to implement a simplified DARTS search cell in PyTorch.\", \"- [ ] Verify that the implemented code correctly updates architecture parameters.\", \"- [ ] Submit all artifacts for final router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"neural-architecture-search\", \"automl\", \"differentiable-programming\"], \"acceptance_criteria\": [\"The mathematical explanation must correctly define the bi-level optimization problem for DARTS.\", \"The Python code must execute without errors and demonstrate non-zero gradient updates to architecture parameters (`alphas`) after one training step.\", \"The final report must cite at least two authoritative sources, including the original DARTS arXiv paper with its ID.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"computation_guidance\": {\"validation_accuracy_improvement\": \"Change in validation accuracy on a held-out dataset after N search epochs.\", \"architecture_alpha_entropy\": \"Shannon entropy of the softmax-normalized architecture parameters (`alphas`) to measure convergence of architectural choices.\"}}}", "categories": ["metrics_computation", "metrics_guidance"]}
{"index": 153, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the spectral properties of a neural network's loss Hessian, specifically its dominant eigenvalue (\u03bb_max), and its connection to generalization. I need a formal derivation of the power iteration method for estimating this eigenvalue using only Hessian-vector products (HVPs). Please implement this in PyTorch for a ResNet-18 on CIFAR-10, demonstrating how to compute HVPs efficiently with autograd. Finally, empirically analyze the convergence of your \u03bb_max estimate and correlate it with the model's generalization gap across different training epochs.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian eigenvalue 'power iteration' 'generalization gap' 'adversarial robustness'\\\", mode=both)\", \"/math(Derive the power iteration algorithm for finding the dominant eigenvalue \u03bb_max of the loss Hessian. Formalize the computation of the Hessian-vector product Hv using matrix calculus principles, and prove the algorithm's convergence under standard assumptions.)\", \"/code(Implement the power iteration for \u03bb_max estimation in PyTorch, using CUDA for acceleration. Apply it to a ResNet-18, track the eigenvalue's evolution during training, and perform a correlation analysis against the generalization gap and adversarial vulnerability., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art literature on Hessian spectral analysis. This informs the /math agent's formal derivation of the power iteration method. Finally, the /code agent implements this verified mathematical framework to conduct empirical analysis, ensuring the solution is both theoretically sound and practically validated.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the power iteration method for the Hessian and its convergence proof.\", \"A Python script implementing the \u03bb_max estimation on CIFAR-10 with a ResNet-18 model, including functions for Hessian-vector products.\", \"A summary report (PDF) containing plots of \u03bb_max convergence, its value over training epochs, and its correlation with the test accuracy.\", \"A bibliography file with citations for at least 2 relevant arXiv papers.\", \"A verification log confirming that the code's output on a small-scale problem matches an analytical or brute-force computation of \u03bb_max.\"], \"thinking_outline\": [\"1. Research foundational papers connecting the Hessian's spectral norm to loss landscape sharpness, generalization, and efficient estimation techniques like power iteration.\", \"2. Formulate the mathematical derivation for applying power iteration to the Hessian operator, emphasizing the role of Hessian-vector products in avoiding explicit matrix formation.\", \"3. Implement the algorithm in PyTorch, focusing on the efficient computation of Hessian-vector products using `torch.autograd.grad`.\", \"4. Validate the numerical implementation by comparing its output on a small, analytically tractable model against the true dominant eigenvalue computed via full Hessian materialization.\", \"5. Conduct experiments on ResNet-18 to track the evolution of \u03bb_max throughout training and analyze its statistical correlation with the generalization gap.\", \"6. Audit the final report to ensure the mathematical derivations, code, and empirical conclusions are mutually consistent and well-supported by the cited literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results from /general-search provide the theoretical foundation for /math's derivation. The formal derivation from /math serves as the specification for the /code agent's implementation. Verification: The /code implementation must pass numerical checks against the /math derivation on a known problem. Fallback: If the derivation is mathematically unsound, /math must revise it based on feedback. If the implementation fails verification, /code must debug against the mathematical specification.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Hessian eigenvalue analysis and power iteration for neural networks.\", \"- [ ] /math: Produce a complete derivation of the power iteration algorithm for the Hessian operator.\", \"- [ ] /math: Prove the convergence of the method to the dominant eigenvalue.\", \"- [ ] /code: Implement the power iteration algorithm and Hessian-vector product computation in PyTorch.\", \"- [ ] /code: Train a ResNet-18 on CIFAR-10 and collect \u03bb_max and generalization gap data at multiple epochs.\", \"- [ ] /math: Verify that the implemented algorithm in the code aligns with the formal derivation.\", \"- [ ] /code: Generate plots and a correlation analysis for the final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"deep-learning-theory\", \"matrix-calculus\"], \"acceptance_criteria\": [\"- The mathematical derivation of the power iteration for the Hessian must be correct and self-contained.\", \"- The Python implementation must successfully compute the dominant eigenvalue for a ResNet-18 model without materializing the full Hessian matrix.\", \"- The final report must present a clear, data-supported analysis correlating the estimated \u03bb_max with the model's generalization gap.\", \"- The code must pass a verification test where its output on a small MLP is compared to the result from `torch.linalg.eigvalsh`.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 156, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive a proof establishing convergence rates for the persistent homology of a Vietoris-Rips complex built on a point cloud sampled from a smooth, compact Riemannian manifold. Specifically, relate the bottleneck distance between the persistence diagram of the sample and the true persistence diagram of the manifold's homology to the Gromov-Hausdorff distance, incorporating constants dependent on the manifold's reach and curvature. I need the full derivation and a simulation in Python to verify the theoretical bounds for a torus embedded in R^3, focusing on eigenvalue analysis of the combinatorial Laplacian to ensure stability.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'persistent homology' 'convergence rates' 'Gromov-Hausdorff' 'manifold reach'\\\", mode=both)\", \"/math(Derive the full convergence proof, connecting bottleneck distance to sampling density via Gromov-Hausdorff approximation. Explicitly formulate dependencies on geometric invariants like reach and curvature, referencing Chebyshev concentration principles for the sampled point cloud.)\", \"/code(Implement a Python simulation using the Gudhi library to sample a torus, construct Vietoris-Rips filtrations, and compute persistence diagrams. Track bottleneck distance against sample size and analyze the combinatorial Laplacian eigenvalue spectrum for stability.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational theorems from arXiv on persistence stability, which is essential for a rigorous proof. Next, /math is tasked with the core theoretical derivation, building upon the sourced literature. Finally, /code provides crucial empirical validation of the abstract mathematical proof, ensuring the theoretical bounds hold in a practical simulation.\", \"expected_artifacts\": [\"A formal proof document (LaTeX/PDF) detailing the convergence rate derivation.\", \"Python source code for the simulation, including data generation, Vietoris-Rips construction, persistence calculation, and plotting.\", \"A summary report comparing theoretical bounds with empirical simulation results, including plots of bottleneck distance vs. sample size.\", \"A bibliography citing at least two relevant arXiv preprints with their IDs.\", \"Verification output showing the combinatorial Laplacian eigenvalue spectrum for a sample complex.\"], \"thinking_outline\": [\"1. Formalize the problem statement, defining all mathematical objects: Riemannian manifold, Vietoris-Rips complex, persistent homology, bottleneck distance, and Gromov-Hausdorff distance.\", \"2. Research existing stability theorems for persistent homology (e.g., the Isometry Theorem and its extensions) to establish a theoretical framework.\", \"3. Derive the specific convergence rate bound by connecting the sampling density to the Gromov-Hausdorff approximation of the manifold.\", \"4. Validate the proof's intermediate steps, specifically checking the dependencies on manifold reach and curvature against established literature.\", \"5. Implement the simulation to empirically measure the bottleneck distance as a function of sample size and verify that the observed decay rate matches the proven theoretical bound.\", \"6. Audit the simulation code to ensure the correct implementation of the Vietoris-Rips filtration, persistence algorithm, and distance calculations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: math proof steps must align with code logic for bound calculation; fallback: if simulation contradicts the proof, /math agent must re-evaluate assumptions on geometric constants or sampling distribution).\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on persistent homology stability and convergence rates from arXiv.\", \"- [ ] /math: Formulate and write the complete proof relating bottleneck distance to sampling density and manifold geometry.\", \"- [ ] /math: Derive the explicit constants in the convergence bound based on manifold reach and curvature.\", \"- [ ] /code: Implement the simulation environment: sampling points from a torus, building the Vietoris-Rips complex.\", \"- [ ] /code: Compute persistence diagrams and bottleneck distances for varying sample sizes, plotting the results against the theoretical bound.\", \"- [ ] /math: Verify that the logic used in the code to calculate the empirical error rate correctly reflects the derived theoretical bound.\", \"- [ ] /code: Add runtime validation to check for numerical stability in the combinatorial Laplacian eigenvalue computation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"computational-topology\"], \"acceptance_criteria\": [\"The submitted proof must be mathematically sound, complete, and rigorously derive an explicit convergence rate bound.\", \"The Python simulation must successfully reproduce the experiment, generating plots that show empirical bottleneck distance converging at a rate consistent with the theoretical prediction.\", \"The final report must correctly cite at least two relevant academic papers (arXiv preprints preferred) and clearly explain the relationship between the theory and the simulation.\", \"The combinatorial Laplacian eigenvalue analysis must be present and correctly interpreted as a measure of topological stability.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 159, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the rate-distortion function for a generative language model where distortion is measured by the success probability of an adversarial classifier attempting to extract private attributes. Formulate this using Bayesian information-theoretic principles and validate the theoretical R(D) bound by simulating an information bottleneck on a transformer's latent space. The simulation must track convergence and adversarial accuracy.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org rate-distortion theory generative models privacy adversarial\\\", mode=web)\", \"/math(Derive the expression for the rate-distortion function R(D) where distortion D is the Bayes error of an adversarial classifier. Use Lagrangian multipliers to solve the constrained optimization problem for mutual information, assuming a specific adversarial loss function and latent space prior.)\", \"/code(Implement a PyTorch simulation using a small transformer encoder-decoder on a synthetic dataset with private attributes. Model the information bottleneck, train the adversarial classifier, and plot the empirical Rate vs. Distortion curve to check convergence against the theoretical bound., using Python)\"], \"route_rationale\": \"The blue /general-search agent is needed first to gather state-of-the-art formulations. Then, /math will formalize the specific problem by deriving the rate-distortion function. Finally, /code will provide empirical validation of the mathematical theory through a targeted simulation, a crucial step for checking the derivation's real-world applicability.\", \"expected_artifacts\": [\"A formal mathematical derivation of the rate-distortion function in LaTeX.\", \"Python source code for the simulation, including the generative model, adversarial classifier, and plotting scripts.\", \"A plot of the empirical rate-distortion curve compared against the theoretical bound.\", \"A summary report (report.md) explaining the results and citing at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Survey recent literature on applying rate-distortion theory and information bottlenecks to privacy in generative AI.\", \"2. Formulate the optimization problem: minimize the mutual information (Rate) between the original and sanitized data, subject to a constraint on the adversarial classifier's success rate (Distortion).\", \"3. Derive the analytical solution for the rate-distortion function R(D) using principles of information theory and constrained optimization.\", \"4. Design and implement a computational experiment to simulate the derived privacy mechanism and validate the theoretical R(D) curve.\", \"5. Verify the correctness of the derivation by checking for consistency with known results in information theory and auditing the logical steps.\", \"6. Validate the simulation code by analyzing convergence properties and comparing empirical results against the theoretical bounds derived in step 3.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /math agent will use foundational papers from /general-search to structure the derivation. The /code agent will implement the exact formulation from /math. Verification: The final empirical R(D) curve from /code must qualitatively match the shape of the theoretical curve from /math. Fallback: If there's a mismatch, the /math agent must re-check the derivation's assumptions and the /code agent must debug the implementation's loss functions and optimizers.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 recent arXiv papers on adversarial privacy and rate-distortion theory.\", \"- [ ] /math: Formulate the optimization problem for the privacy-utility tradeoff.\", \"- [ ] /math: Derive the closed-form or variational bound for the rate-distortion function R(D).\", \"- [ ] /code: Implement the simulation environment with a transformer model and an adversarial classifier in PyTorch.\", \"- [ ] /code: Generate the empirical R(D) curve by training the model across a range of privacy budgets.\", \"- [ ] /math: Verify that the logic in the final derivation is sound and self-consistent.\", \"- [ ] /code: Validate that the empirical R(D) curve from the simulation aligns with the theoretical predictions from the math derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"rate-distortion-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation must be self-contained, logically sound, and correctly apply information-theoretic principles.\", \"- The Python code must be executable, well-commented, and generate a plot that clearly shows the empirical rate-distortion tradeoff.\", \"- The final R(D) plot must show that higher distortion (more privacy) corresponds to a lower rate (less information leakage), consistent with theory.\", \"- The report must cite at least two relevant arXiv pre-prints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 161, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the convergence rate for a Hessian-free Newton-CG optimization algorithm applied to a sparse attention Transformer block. Analyze the spectral properties of the preconditioned Hessian and its impact on the conjugate gradient inner loop. Then, implement a PyTorch simulation to empirically validate the derived convergence bounds and measure computational complexity, specifically tracking wall-clock time and memory footprint against a standard AdamW optimizer under controlled sparsity constraints.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian-free optimization transformer 'conjugate gradient' convergence analysis preconditioning\\\", mode=rag)\", \"/math(Derive the convergence rate bounds for Newton-CG applied to a Transformer layer, incorporating spectral properties of the preconditioned Hessian and assumptions about layer-wise sparsity and its effect on the eigenvalue distribution.)\", \"/code(Implement the Hessian-free Newton-CG optimizer in PyTorch, targeting a sparse attention block. Track convergence metrics like loss reduction per iteration and computational complexity against AdamW for validation., using Python)\", \"/math(Formally analyze the discrepancy between the derived theoretical convergence bounds and the empirical results from the PyTorch simulation, providing a statistical analysis of the residuals and auditing the impact of floating-point precision.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find seminal papers on Hessian-free methods for Transformers. This literature informs the /math agent's formal derivation of convergence rates. The /code agent then implements and empirically tests the theory. A final call to the /math agent is for verification, cross-referencing the theoretical bounds with empirical data to validate the model's correctness.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the convergence rate.\", \"A Python script (`optimizer_comparison.py`) implementing the Hessian-free Newton-CG and the comparison experiment.\", \"A JSON file (`results.json`) containing the logged metrics (loss, wall-clock time, memory) for both optimizers.\", \"A final report (`report.md`) summarizing the findings, including plots comparing theoretical vs. empirical convergence and a discrepancy analysis.\", \"Citations for at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research existing literature on second-order and Hessian-free optimization methods for large neural networks, focusing on convergence theory for non-convex problems.\", \"2. Formulate the optimization problem for a sparse Transformer block and derive the preconditioned Hessian-vector product operator.\", \"3. Derive the theoretical convergence rate for the Newton-CG method, stating all assumptions regarding the Hessian's spectral properties (e.g., condition number, eigenvalue distribution).\", \"4. Implement the custom optimizer in Python and design an experiment to compare it against a baseline like AdamW on a representative task.\", \"5. Validate the implementation by checking gradient correctness and ensuring the Hessian-vector product is computed correctly.\", \"6. Analyze the empirical results, plot the convergence curves, and formally verify if they fall within the derived theoretical bounds, explaining any deviations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: empirical results from /code must be checked against theoretical bounds from /math; fallback: if discrepancy is large, re-run /math to check assumptions or /code to debug implementation).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Hessian-free optimization, Newton-CG, and preconditioning for Transformers on arXiv.\", \"- [ ] /math: Derive the convergence rate for the preconditioned Newton-CG method based on the literature review.\", \"- [ ] /code: Implement the Hessian-free optimizer and the comparison experiment in PyTorch.\", \"- [ ] /code: Log performance metrics (loss, time, memory) to a structured JSON file.\", \"- [ ] /math: Verify that the theoretical derivation's assumptions hold for the implemented model architecture.\", \"- [ ] /math: Compare empirical convergence data from the code agent against the derived theoretical bounds and quantify the error.\", \"- [ ] /router: Perform final QA check, ensuring the report integrates the derivation, code, and validation analysis cohesively.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"llm-optimization\", \"numerical-analysis\", \"convergence-theory\"], \"acceptance_criteria\": [\"- The final report must contain a complete mathematical derivation of the convergence rate.\", \"- The provided Python code must be executable and reproduce the metrics reported in `results.json`.\", \"- The comparison must show that the empirical convergence rate from the simulation is consistent with the derived theoretical bounds, with any deviation explained.\", \"- At least two relevant arXiv pre-prints must be cited in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 162, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and rigorously evaluate Coordinate Ascent Variational Inference (CAVI) for a Bayesian Gaussian Mixture Model with Normal-Wishart priors. Please derive the full update equations for the variational distributions over component responsibilities, means, and precision matrices. Then, implement this in JAX and compare its ELBO convergence against a modern black-box VI technique, like path-derivative-based methods, on a high-dimensional synthetic dataset.\n", "completion": "{\"route_plan\": [\"/math(Derive the closed-form CAVI update equations for a Bayesian Gaussian Mixture Model, ensuring the resulting Evidence Lower Bound (ELBO) formulation is correct to monitor convergence.)\", \"/general-search(query=\\\"site:arxiv.org black-box variational inference bayesian mixture models reparameterization trick\\\", mode=rag)\", \"/code(Implement both the derived CAVI algorithm and the selected black-box VI method in Python, comparing their ELBO convergence and final parameter accuracy on a synthetic dataset, reporting bootstrap confidence intervals on the metrics.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation and specific update equations for the CAVI algorithm, which is essential before any implementation. Next, the blue /general-search agent is tasked with finding a suitable modern black-box VI method to serve as a strong baseline for comparison. Finally, /code implements both the derived theory and the researched baseline to conduct an empirical comparison and validation.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the CAVI update equations and the ELBO.\", \"A Python script implementing both the CAVI and black-box VI algorithms, runnable on a synthetic dataset.\", \"A summary report in Markdown or PDF format, including plots of ELBO convergence versus iteration/time for both methods.\", \"A JSON file containing the final metrics, including final ELBO, convergence time, and parameter estimation error.\", \"A bibliography file citing at least two relevant academic papers with their arXiv IDs.\"], \"thinking_outline\": [\"1. Formulate the Bayesian Gaussian Mixture Model, specifying the likelihood and the conjugate Normal-Wishart priors for the model parameters.\", \"2. Define the mean-field variational family of distributions and write down the expression for the Evidence Lower Bound (ELBO).\", \"3. Derive the CAVI update equations for each variational factor by taking the expectation of the log joint probability with respect to all other factors.\", \"4. Verify the correctness of the derived update equations by checking for consistency with the principles of exponential families and conjugate priors.\", \"5. Implement both the CAVI algorithm and a black-box VI alternative (e.g., using Pyro or TFP's libraries), ensuring the ELBO is correctly computed and monitored for both.\", \"6. Validate the entire pipeline by comparing the performance of the two methods on a synthetic dataset with known ground-truth parameters, analyzing both convergence speed and the quality of the posterior approximation.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivations from /math are passed to /code for implementation. /general-search provides candidate baseline algorithms for /code to implement. The /code agent must verify its ELBO implementation against the formula from /math. Fallback: If the CAVI updates from /math are not yielding a non-decreasing ELBO in the /code implementation, the task is returned to /math to audit the derivations for potential errors.\", \"todo_list\": [\"- [ ] /math: Derive the CAVI update equations for the responsibilities, means, and precision matrices in the BGM model.\", \"- [ ] /math: Provide the final mathematical expression for the ELBO to be used for convergence tracking.\", \"- [ ] /general-search: Identify and summarize two recent black-box variational inference techniques suitable for this comparison.\", \"- [ ] /code: Implement the CAVI algorithm based on the provided mathematical derivations.\", \"- [ ] /code: Implement the chosen black-box VI baseline using a library like Pyro or TensorFlow Probability.\", \"- [ ] /code: Verify that the implemented CAVI ELBO calculation is non-decreasing on a simple test case and matches the derivation.\", \"- [ ] /code: Generate plots comparing the ELBO convergence curves and a table of final performance metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"bayesian-methods\"], \"acceptance_criteria\": [\"The provided LaTeX derivation of the CAVI update equations must be mathematically correct and fully detailed.\", \"The Python implementation of CAVI must show a monotonically non-decreasing ELBO curve during execution.\", \"The final report must contain a comparative plot showing the ELBO convergence of both CAVI and the black-box method over iterations.\", \"The recovered cluster parameters from the synthetic data must have a Mean Squared Error below a threshold of 0.1 relative to the ground truth.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "math_first", "non_canonical_route"]}
{"index": 163, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the convergence properties of Bayesian MAML. Specifically, I need a formal derivation of the convergence rate for the meta-objective under non-convex loss landscapes, assuming a Gaussian prior over the initial parameters. Please support this theoretical analysis with a JAX implementation on a few-shot sinusoidal regression benchmark, and empirically validate the derived bound.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Bayesian MAML convergence non-convex optimization\\\", mode=web)\", \"/math(Derive the convergence rate for Bayesian MAML, treating the inner loop as a stochastic gradient Langevin dynamics update. The analysis must consider the impact of the prior distribution on the meta-objective's Bayesian posterior contraction.)\", \"/code(Implement the derived Bayesian MAML in JAX, targeting CUDA devices for accelerated training. The code must include robust hyperparameter tuning for the learning rates and prior variance, and report final test MSE convergence., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey the latest theoretical literature on Bayesian meta-learning. This informs the /math agent's formal derivation of the convergence bound. Finally, the /code agent implements the algorithm to empirically validate the theoretical findings, providing a complete theory-to-practice workflow.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the convergence rate in a PDF document.\", \"- A Python script with the JAX implementation of Bayesian MAML and the validation experiment.\", \"- A plot (`.png`) comparing the empirically observed convergence against the derived theoretical bound.\", \"- A `report.md` file summarizing the findings and including a bibliography with cited arXiv papers.\", \"- A `metrics.json` file containing the final empirical MSE and the derived theoretical rate.\"], \"thinking_outline\": [\"1. Conduct a literature search to identify key papers and existing theoretical frameworks for the convergence of Bayesian meta-learning algorithms, particularly variants of MAML.\", \"2. Formulate the Bayesian MAML optimization problem precisely, defining the non-convex loss functions, task distribution, and the role of the Gaussian prior over initial parameters.\", \"3. Derive a high-probability upper bound on the meta-objective's gradient norm after T iterations, carefully tracking dependencies on learning rates, task complexity, and prior variance.\", \"4. Implement the Bayesian MAML algorithm and the sinusoidal regression benchmark in JAX, ensuring correct gradient propagation through the inner-loop adaptation steps.\", \"5. Validate the theoretical bound by running the implementation, plotting the empirical training loss against the derived rate, and analyzing any discrepancies.\", \"6. Audit the final derivation for mathematical rigor and check that the experimental setup in the code strictly adheres to the assumptions made in the proof.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation & validation) -> router QA. Verification: The /math agent will verify its assumptions against the papers found by /general-search. The /code agent will validate that its loss computation matches the objective function defined by /math. Fallback: If no direct convergence proofs for Bayesian MAML are found, /general-search will broaden the query to include related gradient-based meta-learning algorithms with variational inference.\", \"todo_list\": [\"- [ ] /general-search: Find seminal and recent arXiv papers on Bayesian MAML and its convergence theory for non-convex optimization.\", \"- [ ] /math: Based on search results, formulate the assumptions and derive the convergence rate for Bayesian MAML.\", \"- [ ] /code: Implement the Bayesian MAML algorithm in JAX for a few-shot sinusoidal regression task.\", \"- [ ] /code: Instrument the implementation to track task-specific adaptation loss and meta-objective convergence.\", \"- [ ] /math: Verify the final derivation steps against the implementation's loss function and optimization logic.\", \"- [ ] /code: Generate plots comparing the empirical convergence curve against the derived theoretical bound.\", \"- [ ] /router: Perform final QA review of all artifacts, ensuring alignment between theory, code, and citations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"bayesian-deep-learning\", \"convergence-theory\"], \"acceptance_criteria\": [\"- The derivation of the convergence bound is mathematically sound, self-contained, and explicitly states all assumptions.\", \"- The JAX implementation correctly reproduces the algorithm described in the derivation and runs without errors to produce the required plot and metrics.\", \"- The final report includes a plot showing a reasonable match between the theoretical bound and empirical performance, with a clear explanation of any discrepancies.\", \"- A minimum of two relevant arXiv papers are cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 164, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous analysis of federated learning schemes that use partially homomorphic encryption for secure model aggregation. Please derive the convergence guarantees for a non-convex objective function under these privacy constraints, specifically considering the impact of noise introduced by the encryption scheme. Then, implement a simulation in Python to validate these theoretical bounds, comparing the performance against a baseline using differential privacy. The analysis should focus on the trade-off between the encryption key size, communication overhead, and the final model's utility (e.g., test accuracy).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning homomorphic encryption convergence non-convex analysis communication overhead\\\", mode=rag)\", \"/math(Derive the convergence proof for federated averaging with partially homomorphic encryption noise, establishing an upper bound on the gradient norm expectation and considering the non-convex federated setting.)\", \"/code(Implement a Python simulation using PyTorch for federated learning with a Paillier encryption scheme, track model utility metrics, and verify the empirical convergence rate against the derived theoretical bounds.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey current literature on homomorphic encryption in federated learning to ground the theoretical work. Next, /math is used to perform the core theoretical derivation of convergence bounds, a necessary prerequisite for the implementation. Finally, /code is used to build a simulation that validates the mathematical theory, providing empirical evidence and analyzing performance trade-offs.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the convergence bounds.\", \"A Python script implementing the federated learning simulation with homomorphic encryption.\", \"A JSON file reporting the final model accuracy, convergence rate, and communication overhead metrics.\", \"A summary report comparing the theoretical bounds with empirical results, including plots and at least 2 arXiv citations.\", \"A verification log detailing the checks performed on the cryptographic primitives and convergence analysis.\"], \"thinking_outline\": [\"1. Formalize the threat model and the federated learning optimization problem with secure aggregation using a partially homomorphic encryption scheme (e.g., Paillier).\", \"2. Derive the convergence rate for the specified algorithm, explicitly accounting for the noise introduced by the encryption and the non-i.i.d. nature of client data.\", \"3. Implement the federated learning system in Python, including clients, a server, and the cryptographic primitives for secure aggregation.\", \"4. Validate the implementation of the homomorphic encryption scheme against known test vectors and properties to ensure correctness.\", \"5. Run simulations on a benchmark dataset (e.g., CIFAR-10) to empirically measure model accuracy and convergence speed, and then verify that these results fall within the derived theoretical bounds.\", \"6. Analyze and plot the trade-off between privacy level (related to key size), communication cost, and model utility.\"], \"handoff_plan\": \"/general-search -> /math (verification: ensure surveyed schemes are suitable for non-convex analysis; fallback: broaden search to include differential privacy analogues) -> /code (verification: check that the implementation correctly models the noise and constraints from the math derivation; fallback: simplify the model or encryption scheme if convergence is not achieved) -> router QA (verification: final review of all artifacts against acceptance criteria).\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 recent arXiv papers on convergence of FL with homomorphic encryption.\", \"- [ ] /math: Formulate the optimization problem and derive the full proof for the convergence upper bound.\", \"- [ ] /code: Implement the federated learning simulation with a Paillier encryption module.\", \"- [ ] /code: Instrument the simulation to collect metrics on accuracy, loss, and communication bits per round.\", \"- [ ] /math: Verify that the noise model used in the derivation accurately reflects the properties of the implemented encryption scheme.\", \"- [ ] /code: Generate plots comparing the empirical convergence curve against the theoretical bound derived by the math agent.\", \"- [ ] /router: Perform final QA review, checking all artifacts against the acceptance criteria and metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"homomorphic-encryption\", \"convergence-analysis\"], \"acceptance_criteria\": [\"The mathematical derivation must be self-contained, clearly state all assumptions (e.g., Lipschitz continuity, gradient variance bounds), and logically sound.\", \"The Python simulation must be runnable and reproduce the key metrics reported in the summary document within a 5% tolerance.\", \"The final report must show a clear comparison between the derived theoretical bound and the empirical results, with the empirical curve lying below the theoretical one.\", \"The implementation must correctly apply a recognized homomorphic encryption scheme for aggregating model updates.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 165, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the computational complexity of determining the P-positions for the impartial game of 'Staircase Nim' with a constraint that moves are restricted to transferring coins to a step at most 'k' steps lower. I need a formal proof of its complexity class, potentially PSPACE-complete, and an efficient Python implementation to compute the Sprague-Grundy values for moderately sized game states. The analysis must consider the impact of the parameter 'k' on the game's period and the resulting algorithmic complexity, referencing relevant literature from arxiv on octal games and computational game theory.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:cs.cmu.edu 'computational complexity' 'impartial games' 'Sprague-Grundy' 'Staircase Nim' 'bounded jump' 'octal games'\\\", mode=both)\", \"/math(Formalize the bounded Staircase Nim game, derive the recurrence for its Sprague-Grundy function, and construct a polynomial-time reduction from a known PSPACE-complete problem to analyze its theoretical complexity and convergence properties.)\", \"/code(Implement a memoized recursive solver in Python for the Sprague-Grundy values. The adversarial nature of the game requires robust code to avoid infinite loops; assess the algorithm's convergence properties and profile its empirical complexity against theoretical bounds.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to survey existing literature on the complexity of similar impartial games, which is crucial for a formal proof. /math is then used to construct the core theoretical argument, deriving the game's mathematical structure and proving its complexity class. Finally, /code provides an empirical validation of the theory by implementing a solver and measuring its performance, which serves as a practical check on the theoretical findings.\", \"expected_artifacts\": [\"A LaTeX document containing the formal proof of the game's complexity class.\", \"A well-documented Python script implementing the Sprague-Grundy value calculator.\", \"A summary report (README.md) linking the literature review, theoretical findings, and empirical results, including performance plots.\", \"A JSON file containing computed Sprague-Grundy values for a set of benchmark game states.\", \"A list of at least two cited academic papers with DOIs or arXiv IDs.\"], \"thinking_outline\": [\"1. Formalize the game rules for Bounded Staircase Nim and define its state representation.\", \"2. Derive the recursive formula for the Sprague-Grundy function, g(S), based on the mex (Minimum Excluded value) of the g-values of reachable states.\", \"3. Research known PSPACE-complete games (e.g., Generalized Geography) to find a suitable candidate for a polynomial-time reduction.\", \"4. Construct the reduction and prove its correctness to establish the PSPACE-hardness of determining the winner.\", \"5. Implement a dynamic programming or memoized recursive algorithm to compute the g-values and validate its correctness by checking its output against manually calculated values for small, trivial game states.\", \"6. Verify that the empirical runtime scaling of the implementation as a function of state size aligns with the established theoretical complexity bounds.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Math agent verifies that the complexity proof correctly uses definitions found in the literature search. Code agent verifies the algorithm correctly implements the recurrence derived by the Math agent. fallback: If reduction fails, /math agent to analyze the problem within the polynomial hierarchy.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on the complexity of impartial games like Nim variants or octal games.\", \"- [ ] /math: Define the game formally and derive the Sprague-Grundy recurrence relation.\", \"- [ ] /math: Develop a formal proof, likely via reduction, for the game's computational complexity class (e.g., PSPACE-complete).\", \"- [ ] /code: Implement a Python solver for the game's Sprague-Grundy values using memoization.\", \"- [ ] /code: Profile the solver's performance (time and memory) and generate plots showing empirical scaling.\", \"- [ ] /math: Verify that the implemented logic in the Python code correctly matches the derived recurrence relation.\", \"- [ ] /code: Validate the implementation's output against at least 5 manually computed small game states.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"computational-logic\"], \"acceptance_criteria\": [\"- The submitted proof must be logically sound and correctly establish the computational complexity class of the game.\", \"- The Python implementation must correctly compute the Sprague-Grundy values for all provided test cases within the given time limits.\", \"- The final report must clearly connect the theoretical complexity with the observed empirical runtime scaling.\", \"- The work must cite at least two relevant academic papers on combinatorial game theory.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 166, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the mathematical underpinnings of secure aggregation in federated learning using the Paillier partially homomorphic encryption scheme. Please provide a formal proof of its privacy properties and then implement a Python simulation on a non-IID dataset to analyze the convergence rate and computational overhead introduced by the cryptographic operations.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning Paillier homomorphic encryption privacy proof convergence\\\", mode=web)\", \"/math(Formally derive the secure aggregation protocol for federated learning using the additive properties of the Paillier encryption scheme, and prove that the central server cannot decrypt individual client model updates.)\", \"/code(Implement a Python simulation of federated learning with Paillier encryption for secure aggregation on MNIST, measuring model convergence (accuracy vs. rounds) and cryptographic overhead.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational literature on Paillier-based federated learning. This context is passed to /math to construct the formal security proofs. Finally, /code implements and simulates the derived protocol to validate the theoretical claims about convergence and measure practical performance overhead.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Paillier-based secure aggregation protocol and its privacy proof.\", \"A commented Python script implementing the federated learning simulation with homomorphic encryption.\", \"A summary report including plots of model accuracy vs. communication rounds and a table of computational overhead.\", \"A list of at least two cited arXiv papers on privacy-preserving federated learning.\"], \"thinking_outline\": [\"1. Research the Paillier cryptosystem and its application in federated learning for secure aggregation, focusing on its additive homomorphic property.\", \"2. Formulate the mathematical steps for key generation, encryption of client updates, homomorphic addition of ciphertexts by the server, and decryption of the aggregated result.\", \"3. Construct a formal proof demonstrating that the server, without the private key, cannot infer information about individual client updates from the aggregated ciphertext.\", \"4. Implement the protocol in Python, simulating multiple clients and a central server. Validate the correctness of the cryptographic operations with test vectors.\", \"5. Run experiments to measure the federated model's convergence on a standard dataset and profile the CPU time required for encryption and aggregation steps.\", \"6. Verify that the experimental convergence behavior aligns with theoretical expectations for federated averaging, accounting for potential noise or precision limitations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The formal proof from /math must match the logic implemented in the /code simulation; specifically, the homomorphic addition property. fallback: If the proof and code diverge, /math must revise the derivation to account for implementation constraints.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on federated learning with Paillier encryption, focusing on security proofs and system design.\", \"- [ ] /math: Derive the secure aggregation protocol using Paillier's additive homomorphism.\", \"- [ ] /math: Formally prove the privacy guarantee: the server cannot decrypt individual updates.\", \"- [ ] /code: Implement the client-server simulation in Python with the Paillier cryptosystem.\", \"- [ ] /code: Instrument the code to measure model accuracy per round and the CPU time for cryptographic operations.\", \"- [ ] /math: Verify that the aggregation logic in the Python code correctly implements the homomorphic addition derived in the proof.\", \"- [ ] /code: Validate that the final decrypted aggregate model update matches the sum of the plaintext updates in a controlled test case.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"homomorphic-encryption\", \"cryptography\"], \"acceptance_criteria\": [\"- The mathematical proof must rigorously demonstrate that individual client data remains private from the server.\", \"- The Python simulation must successfully train a model to a reasonable accuracy (e.g., >90% on MNIST) using the encrypted aggregation protocol.\", \"- The final report must contain plots showing model convergence and a quantitative analysis of the computational overhead introduced by the encryption.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 167, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Lagrangian dual for a neural network pruning problem formulated with L1 regularization on weights, subject to a hard constraint on the network's spectral norm to control its Lipschitz constant. Then, implement a primal-dual algorithm to find the optimal sparse architecture and analyze the Hessian of the Lagrangian to check for second-order sufficiency conditions near the solution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural network pruning spectral norm constraint Lagrangian duality\\\", mode=rag)\", \"/math(Derive the Lagrangian dual for L1-regularized pruning. Formulate the KKT optimality conditions and analyze the Hessian eigenvalue spectrum required for convergence, referencing foundational theory from arxiv.)\", \"/code(Implement a primal-dual algorithm in PyTorch with CUDA support to solve the constrained optimization, tracking convergence and enforcing the spectral norm constraint using power iteration for eigenvalue estimation., using Python)\", \"/math(Verify second-order sufficiency conditions by numerically analyzing the Hessian's eigenvalue distribution at the solution provided by the code agent. Check for saddle points and report on solution stability., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the approach in current literature on spectral norm constraints. The first /math call establishes the core theoretical framework (Lagrangian, KKT). /code operationalizes this theory into a working PyTorch implementation. The final /math call provides a critical verification step, using the numerical output from /code to check the second-order optimality conditions derived in the first step.\", \"expected_artifacts\": [\"A LaTeX document detailing the derivation of the primal problem, Lagrangian dual, and KKT conditions.\", \"A Python script implementing the primal-dual optimization algorithm for the specified pruning problem.\", \"A JSON file containing metrics such as final primal-dual gap, sparsity level, and spectral norm.\", \"A summary report including convergence plots and a formal analysis of the Hessian's eigenvalues at the solution.\", \"A bibliography citing at least two relevant research papers from arXiv with their IDs.\"], \"thinking_outline\": [\"1. Formalize the L1-regularized pruning problem with a spectral norm constraint as a mathematical optimization problem.\", \"2. Derive the Lagrangian, the dual function, and the Karush-Kuhn-Tucker (KKT) necessary conditions for optimality.\", \"3. Validate the conditions under which strong duality holds, for example, by checking Slater's condition.\", \"4. Implement a primal-dual algorithm (e.g., gradient descent-ascent) to find a candidate solution that satisfies the KKT conditions.\", \"5. Verify the nature of the obtained stationary point by computing the Hessian of the Lagrangian and analyzing its eigenvalues to confirm it corresponds to a local minimum (second-order sufficient conditions).\", \"6. Synthesize the mathematical theory, implementation results, and verification analysis into a coherent final report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent. The math agent provides the formal derivations and optimality conditions to the code agent. The code agent returns the numerical solution (optimal weights and Lagrange multiplier) to the second math agent for Hessian analysis. Verification: The final math step validates the numerical solution against the theoretical second-order conditions. Fallback: If the Hessian analysis shows a saddle point or maximum, the code agent must be re-run with different hyperparameters or optimization algorithm.\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on constrained pruning and spectral normalization.\", \"- [ ] /math: Formulate the primal problem, its Lagrangian, and derive the dual problem and KKT conditions.\", \"- [ ] /code: Implement the primal-dual algorithm in PyTorch, including a power iteration method for spectral norm estimation.\", \"- [ ] /code: Instrument the implementation to log the primal-dual gap and weight sparsity over iterations.\", \"- [ ] /math: Derive the Hessian of the Lagrangian with respect to the primal variables at the optimal point found by the code agent.\", \"- [ ] /math: Verify that the implementation's convergence point satisfies the derived KKT conditions numerically.\", \"- [ ] /router: Perform final QA check, ensuring the mathematical derivation, code implementation, and Hessian analysis are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"deep-learning-theory\", \"lagrangian-duality\"], \"acceptance_criteria\": [\"The mathematical derivation of the Lagrangian dual and KKT conditions is complete, rigorous, and correctly stated.\", \"The Python implementation converges to a solution that meets the specified spectral norm constraint within a small tolerance (e.g., 1e-5).\", \"The final report includes a valid second-order sufficiency analysis based on the Hessian and cites at least two relevant arXiv papers with their IDs.\", \"The primal-dual gap at the final solution is demonstrably close to zero, indicating optimality.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 168, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to explore the connection between constructive proofs and program synthesis within the framework of Homotopy Type Theory (HoTT). Please provide a formal derivation of the Curry-Howard correspondence for a dependent product type representing a group homomorphism, then synthesize a Python program from this proof. The task must also include a literature survey of recent advancements in automated theorem provers that leverage cubical type theory for handling the univalence axiom, citing relevant arXiv preprints.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org cubical type theory automated theorem proving homotopy program synthesis\\\", mode=both, includes cybersecurity)\", \"/math(Derive the Curry-Howard correspondence for a dependent product type encoding a group homomorphism, defining the necessary identity types and transport functions. This derivation must be rigorous enough for direct translation into a functional program, referencing the foundational homotopy theory., includes cybersecurity, gradient)\", \"/code(Implement the synthesized group homomorphism function in Python, including a minimal dependent type checker to validate the function's signature against the formal proof. The implementation must include verification tests for associativity and identity element properties, tracking convergence metrics for any iterative components.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather state-of-the-art context on cubical type theory and automated provers, which informs the theoretical work. Next, /math is used for the core theoretical task of formal derivation in HoTT, which is too abstract for a coding agent. Finally, /code translates the abstract proof into a concrete, verifiable Python program, grounding the theory in a practical artifact.\", \"expected_artifacts\": [\"A LaTeX document detailing the formal proof of the program synthesis from the dependent type.\", \"A Python script containing the synthesized function and a lightweight type-checker.\", \"A literature review summary (Markdown) citing at least two arXiv preprints on cubical type theory.\", \"A verification log showing the output of the property tests run against the synthesized program.\"], \"thinking_outline\": [\"1. Use search to identify foundational papers on Homotopy Type Theory, the Curry-Howard correspondence, and recent developments in cubical type theory for automated theorem proving.\", \"2. Formulate the mathematical problem by defining the group structure and the dependent product type that represents a homomorphism within the language of HoTT.\", \"3. Construct the proof of existence for an inhabitant of this type. This constructive proof will serve as the algorithm for the synthesized program.\", \"4. Validate the formal proof for logical soundness, ensuring all steps are justified by the axioms of HoTT.\", \"5. Translate the constructive proof terms (e.g., functions, pairs, identity proofs) into corresponding Python code constructs.\", \"6. Verify that the synthesized Python function correctly implements the group homomorphism properties (e.g., f(a * b) == f(a) * f(b)) through a comprehensive suite of property-based tests.\"], \"handoff_plan\": \"/general-search (literature) -> /math (formal proof) -> /code (implementation & verification) -> router QA (verification: code behavior matches proof specification; fallback: if verification fails, return to /math to audit the derivation for constructive gaps).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 recent arXiv papers on cubical type theory and theorem proving.\", \"- [ ] /math: Produce a formal, step-by-step derivation of the program synthesis in LaTeX.\", \"- [ ] /code: Implement the synthesized function based on the mathematical derivation.\", \"- [ ] /code: Implement property-based tests to verify the homomorphism property of the synthesized code.\", \"- [ ] /math: Audit the generated Python code to confirm its structure directly corresponds to the objects in the formal proof.\", \"- [ ] /code: Generate a verification log file from the property test runs.\", \"- [ ] /router: Perform final QA check, ensuring all artifacts are consistent and meet acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"homotopy-type-theory\", \"program-synthesis\", \"theorem-proving\"], \"acceptance_criteria\": [\"The formal proof must be logically sound, complete, and use standard HoTT notation.\", \"The Python program must execute and pass all property-based tests for a group homomorphism.\", \"The literature review must cite at least two relevant arXiv preprints with their IDs.\", \"A clear mapping between the mathematical proof objects and the corresponding code structures must be evident in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 169, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a high-performance, GPU-accelerated preconditioned conjugate gradient (PCG) solver for large, sparse, symmetric positive-definite linear systems arising from finite element method (FEM) simulations. The focus is on leveraging CUDA via Python to minimize wall-clock time to convergence for matrices exceeding 10^6 x 10^6. Please investigate and implement an effective preconditioner like incomplete Cholesky factorization, analyze the numerical stability, and benchmark its performance against a baseline CPU solver, ensuring the final residual norm is below 1e-9.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org GPU 'incomplete cholesky' OR 'polynomial preconditioner' CUDA sparse solver convergence\\\", mode=both)\", \"/math(Derive the Preconditioned Conjugate Gradient algorithm for sparse matrices, analyzing its computational complexity and the impact of preconditioning on the matrix's eigenvalue distribution to guarantee faster convergence.)\", \"/code(Implement the derived PCG algorithm and an incomplete Cholesky preconditioner in Python using CuPy for CUDA execution. Profile kernel performance and memory bandwidth, tracking the computational complexity against theoretical predictions.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey modern GPU preconditioning techniques. This research informs the /math agent's formal derivation of the algorithm and its convergence analysis. Finally, the /code agent implements the verified algorithm, focusing on performance optimization and benchmarking on CUDA hardware.\", \"expected_artifacts\": [\"A PDF report containing the mathematical derivation of the Preconditioned Conjugate Gradient algorithm and its complexity analysis.\", \"A well-documented Python script (`pcg_solver.py`) implementing the solver using CuPy.\", \"A performance benchmark plot (`benchmark.png`) comparing the GPU implementation's wall-clock time against a standard CPU solver (e.g., SciPy) for various matrix sizes.\", \"A JSON file (`metrics.json`) reporting the final residual norm and time-to-solution for test cases.\", \"A `references.md` file with citations for at least two academic papers on the topic.\"], \"thinking_outline\": [\"1. Research state-of-the-art preconditioners for sparse linear systems on GPUs, focusing on incomplete Cholesky (IC) and polynomial preconditioners.\", \"2. Formally derive the Preconditioned Conjugate Gradient (PCG) algorithm, detailing each step including the application of the preconditioner.\", \"3. Implement the PCG solver and the chosen preconditioner in Python using CuPy, ensuring efficient use of GPU memory and sparse matrix formats (e.g., CSR).\", \"4. Verify the implementation's correctness by comparing its output against a trusted library like `scipy.sparse.linalg.cg` on smaller, well-conditioned test problems.\", \"5. Design and run a series of benchmarks on large-scale matrices to measure performance gains over a CPU baseline and validate that the observed convergence behavior aligns with the theoretical improvements predicted by the mathematical analysis.\", \"6. Consolidate all findings, code, and benchmarks into a comprehensive final report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: research from /general-search provides context for /math's derivation; the derivation from /math provides the exact algorithmic specification for /code; final code and metrics are audited against the initial query requirements; fallback: if the chosen preconditioner is too complex to implement, /code will revert to a simpler Jacobi preconditioner and document the deviation).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on incomplete factorization preconditioners for sparse systems on CUDA.\", \"- [ ] /math: Derive the PCG algorithm and provide a formal analysis of its computational complexity per iteration.\", \"- [ ] /code: Implement the PCG solver using the CuPy library, ensuring support for the CSR sparse matrix format.\", \"- [ ] /code: Implement the incomplete Cholesky preconditioner, or a suitable alternative identified during research.\", \"- [ ] /math: Verify that the implemented code logic is a faithful translation of the mathematical derivation.\", \"- [ ] /code: Create a test suite to validate the solver's correctness against SciPy's CG implementation.\", \"- [ ] /code: Generate performance comparison plots and a final metrics report in JSON format.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"hpc\"], \"acceptance_criteria\": [\"- The implemented solver must correctly solve a 10^6 x 10^6 sparse linear system, achieving a final L2 residual norm of less than 1e-9.\", \"- The GPU implementation must demonstrate at least a 10x speedup in wall-clock time compared to the `scipy.sparse.linalg.cg` baseline on a representative large-scale problem.\", \"- The final report must include a clear mathematical derivation and cite at least two relevant arXiv papers with their IDs.\", \"- The provided code must be well-commented and fully reproducible using the specified environment.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 170, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Variational Quantum Eigensolver (VQE) in Python using PennyLane to find the ground state energy of the LiH molecule. The implementation must use a custom hardware-efficient ansatz, leverage the parameter-shift rule for analytic gradient computation, and incorporate Hamiltonian averaging to reduce measurement overhead. Compare its convergence against a standard UCC ansatz and analyze the final eigenvalue estimation error.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org LiH molecular Hamiltonian VQE hardware-efficient ansatz chemistry\\\", mode=web)\", \"/math(Derive the analytic gradient for a custom hardware-efficient ansatz using the parameter-shift rule, ensuring the formulation accounts for the expected eigenvalue convergence properties.)\", \"/code(Implement the VQE for the LiH molecular Hamiltonian in Python/PennyLane, comparing a custom ansatz against UCC based on convergence rate and final energy. Track all chemistry simulation metrics.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather the specific LiH molecular Hamiltonian data and survey recent literature for suitable hardware-efficient ansatz designs. This information is then passed to /math to establish a rigorous theoretical foundation, deriving the necessary gradient equations. Finally, /code implements the derived theory to perform the numerical experiment and generate the required performance comparisons.\", \"expected_artifacts\": [\"- A Jupyter Notebook (`.ipynb`) containing the full, commented Python implementation using PennyLane.\", \"- A PDF report (`.pdf`) detailing the mathematical derivation of the parameter-shift rule for the chosen ansatz and the Hamiltonian averaging strategy.\", \"- A JSON file (`.json`) containing the final ground state energy estimates, convergence data, and gate counts for both ansatzes.\", \"- A `CITATIONS.md` file listing at least two cited arXiv papers with their IDs.\", \"- A verification script output showing the analytic gradient matches a numerical finite-difference check.\"], \"thinking_outline\": [\"1. Research the qubit representation of the Lithium Hydride (LiH) molecular Hamiltonian and survey recent (post-2020) hardware-efficient ansatz architectures suitable for near-term quantum devices.\", \"2. Formulate the VQE cost function, which is the expectation value of the Hamiltonian, and formally derive the analytic gradient for all variational parameters of the custom ansatz using the parameter-shift rule.\", \"3. Implement the complete VQE algorithm in PennyLane, including the custom hardware-efficient ansatz, a standard Unitary Coupled Cluster (UCC) ansatz for baseline comparison, and an Adam optimizer.\", \"4. Validate the analytic gradient implementation by comparing its output against a numerical gradient calculated using a finite-difference method for a random parameter vector.\", \"5. Execute the VQE optimization for both ansatzes, recording the energy at each iteration, and check the final ground state energy against established values from quantum chemistry literature.\", \"6. Analyze the results by plotting energy convergence curves, comparing the final energy accuracy, and reporting the total CNOT gate count for each ansatz to assess circuit complexity.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent will provide the Hamiltonian structure and candidate ansatz designs to the /math agent. The /math agent will produce a formal specification of the cost function and its gradient, which the /code agent will implement. The final code, plots, and report from /code will be audited by the router QA. Verification: The QA will check if the implemented gradient in code matches the /math derivation. Fallback: If there is a mismatch, the task is routed back to /code with specific instructions to debug the gradient implementation.\", \"todo_list\": [\"- [ ] /general-search: Find the Pauli string representation of the LiH Hamiltonian and identify 2-3 candidate hardware-efficient ansatz papers from arXiv.\", \"- [ ] /math: Derive the analytic gradient expression for the chosen custom ansatz using the parameter-shift rule.\", \"- [ ] /code: Implement the hardware-efficient ansatz and a UCC ansatz in PennyLane.\", \"- [ ] /code: Construct the VQE optimization loop, incorporating the Adam optimizer and Hamiltonian averaging.\", \"- [ ] /math: Verify that the code's gradient calculation matches the mathematical derivation for a simple 2-qubit test case.\", \"- [ ] /code: Implement a verification function that compares the analytic gradient to a finite-difference approximation.\", \"- [ ] /code: Execute the simulations, collect convergence data, and generate comparison plots for the two ansatzes.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-machine-learning\", \"variational-quantum-eigensolver\", \"quantum-chemistry\"], \"acceptance_criteria\": [\"- The final computed ground state energy for LiH must be within 1.5% of the accepted full configuration interaction (FCI) value.\", \"- The report must contain the complete mathematical derivation of the parameter-shift rule as applied to the custom ansatz.\", \"- The submitted code must include a unit test that successfully asserts the equivalence of the analytic and numerical gradients to a tolerance of 1e-5.\", \"- A convergence plot comparing the energy per iteration for both the custom and UCC ansatzes must be generated and included in the report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 171, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of Chebyshev polynomial preconditioning for the Conjugate Gradient method. Derive the optimal Chebyshev polynomial for a symmetric positive-definite matrix A, assuming its eigenvalue spectrum is bounded by [\u03bb_min, \u03bb_max]. Implement a CUDA-accelerated version in Python using JAX to solve a large, sparse linear system Ax=b arising from a 2D Poisson equation, and validate its convergence rate against the theoretical bounds.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Chebyshev polynomial preconditioning' GPU 'conjugate gradient' convergence analysis\\\", mode=rag)\", \"/math(Derive the optimal Chebyshev polynomial preconditioner for a symmetric positive-definite matrix with a known eigenvalue spectrum, establishing the theoretical convergence rate improvement for the Conjugate Gradient method.)\", \"/code(Implement a CUDA-accelerated Conjugate Gradient solver with the derived Chebyshev preconditioner using Python JAX. Apply it to a sparse system from a 2D Poisson discretization, tracking residual norm convergence and eigenvalue estimates.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey recent literature on GPU-accelerated Chebyshev preconditioning. This informs the /math agent's formal derivation of the optimal polynomial and convergence bounds. Finally, the /code agent implements the derived algorithm in a GPU-aware framework (JAX) for empirical validation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the optimal Chebyshev polynomial preconditioner and its impact on the condition number.\", \"A documented Python script using JAX/CUDA for the preconditioned Conjugate Gradient solver.\", \"A convergence plot comparing the preconditioned vs. non-preconditioned solver's residual norm per iteration.\", \"A summary report citing at least two arXiv papers on polynomial preconditioning or iterative methods.\", \"Verification log showing that the observed convergence rate aligns with the theoretical bounds derived.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation of polynomial preconditioning, focusing on how it transforms the eigenvalue spectrum of the matrix to improve its condition number.\", \"2. Derive the specific form of the optimal Chebyshev polynomial that minimizes the condition number of the preconditioned system over a given spectral interval [\u03bb_min, \u03bb_max].\", \"3. Implement the preconditioned Conjugate Gradient (PCG) algorithm, where the preconditioner application step involves evaluating the derived Chebyshev polynomial via a three-term recurrence.\", \"4. Validate the implementation by applying it to a known problem (e.g., discretized 2D Poisson equation) and estimating the extreme eigenvalues of the system matrix using the Lanczos algorithm to feed the preconditioner.\", \"5. Verify that the observed convergence rate (reduction in residual norm per iteration) matches the theoretical predictions based on the transformed condition number.\", \"6. Analyze the performance trade-offs, considering the computational cost of applying the polynomial preconditioner versus the reduction in the number of CG iterations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: math derivation for the optimal Chebyshev polynomial is correctly implemented in the CUDA kernel's three-term recurrence; fallback: if convergence is poor, /math re-evaluates the spectral bounds assumption or polynomial degree)\", \"todo_list\": [\"- [ ] /general-search: Find recent papers (post-2018) on GPU implementations of polynomial preconditioners for iterative solvers. Extract key algorithms and convergence analysis techniques.\", \"- [ ] /math: Derive the optimal k-th degree Chebyshev polynomial for preconditioning a system with eigenvalues in [\u03bb_min, \u03bb_max].\", \"- [ ] /math: Formulate the theoretical convergence rate of PCG using this preconditioner in terms of the new effective condition number.\", \"- [ ] /code: Implement the preconditioned Conjugate Gradient solver in JAX, including a routine to apply the Chebyshev polynomial.\", \"- [ ] /code: Set up a test case using a sparse matrix from a 2D finite difference discretization of the Poisson equation.\", \"- [ ] /math: Verify that the Python code's polynomial recurrence relation correctly matches the mathematical derivation.\", \"- [ ] /code: Run simulations, plot convergence, and check that the empirical convergence rate is within 10% of the theoretical bound.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"iterative-methods\"], \"acceptance_criteria\": [\"The mathematical derivation correctly identifies the optimal Chebyshev polynomial and its roots.\", \"The JAX/CUDA implementation successfully solves a sparse linear system of at least 1,000,000 x 1,000,000.\", \"The final report shows a convergence plot where the preconditioned solver requires at least 3x fewer iterations than the standard CG solver to reach a residual tolerance of 1e-8.\", \"The observed convergence factor \u03c1_obs is consistent with the theoretical factor \u03c1_theory = (sqrt(\u03ba_eff)-1)/(sqrt(\u03ba_eff)+1), where \u03ba_eff is the effective condition number of the preconditioned system.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 172, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the theoretical connection between the reverse-time stochastic differential equation (SDE) in score-based generative models and its corresponding probability flow ordinary differential equation (ODE). Then, implement a simple 2D diffusion model in JAX, showcasing both the Euler-Maruyama SDE solver and a deterministic ODE solver. Empirically verify that both samplers target the same data distribution by comparing the Wasserstein distance of generated samples and visualizing the learned score vector fields.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse SDE probability flow ODE\\\", mode=both)\", \"/math(Derive the probability flow ODE from the reverse-time diffusion SDE, referencing the Fokker-Planck equation and key results from the foundational arXiv papers on convergence.)\", \"/code(Implement a full diffusion model pipeline in JAX for a 2D dataset, including the score network, both SDE and ODE samplers with CUDA support, and metric computation for Wasserstein distance., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative arXiv papers. These papers are handed to /math to construct a formal derivation. Finally, /code implements the derived mathematical concepts for empirical validation.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation with citations.\", \"A Python script (`main.py`) implementing the 2D diffusion model and both samplers.\", \"Generated plots visualizing the learned score field and comparing the sample distributions from the SDE and ODE solvers.\", \"A `results.json` file containing the computed Wasserstein distances.\", \"A summary report (`report.md`) explaining the findings and including arXiv citations.\"], \"thinking_outline\": [\"1. Use /general-search to locate seminal papers by Song, Ermon, et al. on score-based generative modeling, focusing on the SDE and ODE formulations.\", \"2. Formally derive the probability flow ODE from the reverse-time SDE, starting from the Fokker-Planck equation corresponding to the SDE.\", \"3. Implement the forward diffusion process, a score-matching neural network (e.g., a U-Net), and the training loop for a 2D toy dataset like a Gaussian mixture.\", \"4. Implement two sampling functions: one using the Euler-Maruyama method for the SDE and another using a standard ODE solver (like `scipy.integrate.solve_ivp`) for the probability flow ODE.\", \"5. Validate the implementation by ensuring the learned score function is being correctly used in both samplers and that the ODE's vector field aligns with the theoretical form `f(x,t) - g(t)^2 * score(x,t)`.\", \"6. Empirically check for equivalence by generating a large number of samples from both methods and computing the Wasserstein distance between the two resulting distributions and the ground truth.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /math agent verifies that the papers found by /general-search contain the necessary theorems for the derivation. The /code agent verifies that its implementation correctly reflects the equations derived by /math. The final router QA step involves a full audit comparing the empirical results in `results.json` against the theoretical claims in the derivation. Fallback: If the Wasserstein distance is high, the router will request /math to re-check the derivation for potential errors in the drift or diffusion coefficients.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on the SDE/ODE connection in diffusion models.\", \"- [ ] /math: Produce a detailed, step-by-step derivation of the probability flow ODE from the reverse SDE.\", \"- [ ] /code: Implement the score-matching network and training loop for a 2D Swiss roll dataset.\", \"- [ ] /code: Implement the Euler-Maruyama SDE sampler and a deterministic ODE sampler.\", \"- [ ] /math: Verify that the vector field used in the /code agent's ODE solver implementation matches the derived formula.\", \"- [ ] /code: Generate samples, produce visualizations of the vector fields and distributions, and compute final metrics.\", \"- [ ] /router: Perform final QA review, checking consistency between the derivation, implementation, and empirical results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-ai\"], \"acceptance_criteria\": [\"- The submitted LaTeX derivation must be mathematically sound and correctly cite the source papers.\", \"- The Python code must execute without errors and generate the specified plots and `results.json` artifact.\", \"- The final report must show a low Wasserstein distance (<0.1) between the SDE-generated and ODE-generated sample distributions.\", \"- The report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 174, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a neural-symbolic framework for program synthesis guided by a formal specification in Coq's Gallina. The goal is to synthesize a provably correct sorting algorithm by treating proof search as a Monte Carlo Tree Search (MCTS) problem. The MCTS policy and value networks should be trained via self-play, and the final output must be a valid Coq proof term (a \u03bb-term) that can be independently verified. How can I formalize this, derive the Bayesian update rules for the network, and implement a prototype?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural theorem proving MCTS program synthesis Coq Lean tactics convergence\\\", mode=rag)\", \"/math(Derive the Bayesian learning framework for the MCTS value function in proof search, formalizing tactics as actions and proof state as a lattice, analyzing convergence criteria under adversarial tactic selection.)\", \"/code(Implement in Python a JAX-based MCTS agent interacting with a Coq kernel via serapi; the agent's policy network must guide synthesis of an insertion sort function, with federated learning concepts hypothetically informing the model update strategy for robustness and checking the final proof term's eigenvalue properties.)\"], \"route_rationale\": \"The blue /general-search agent first gathers SOTA approaches from arXiv to ground the project. /math then builds the specific theoretical foundation for a Bayesian MCTS-guided proof search, defining the formal model. Finally, /code implements this theory as a validated prototype, demonstrating practical synthesis and verification.\", \"expected_artifacts\": [\"- A literature review summarizing at least 3 relevant arXiv papers on neural theorem proving.\", \"- A formal mathematical derivation of the Bayesian MCTS update rules for proof-term synthesis.\", \"- A Python script implementing the neural-guided program synthesizer for a sorting algorithm.\", \"- A generated Coq script containing the synthesized function and its formal proof of correctness.\", \"- A verification report confirming the Coq proof term is accepted by the Coq kernel.\"], \"thinking_outline\": [\"1. Research state-of-the-art neural-symbolic methods for automated theorem proving and program synthesis, focusing on MCTS and transformer-based models in assistants like Coq or Lean.\", \"2. Formalize the program synthesis task as a pathfinding problem on the graph of partial proof terms, defining states, actions (tactics), and termination conditions.\", \"3. Derive the Bayesian update equations for the policy and value networks that guide the MCTS, ensuring theoretical guarantees on search convergence.\", \"4. Implement the MCTS agent and the neural network components, connecting them to a live Coq kernel to apply tactics and receive feedback.\", \"5. Validate the complete system by tasking it to synthesize a sorting algorithm and its correctness proof from a formal specification.\", \"6. Audit the final generated Coq proof script using an independent `coqc` checker to verify its correctness and completeness.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal model derivation) -> /code (prototype implementation & synthesis). The code's logic must be verified against the mathematical derivations from /math. The final synthesized proof term must be validated by the Coq kernel. Fallback: If synthesis fails, /code will log the failing MCTS search tree, and /math will revisit the reward function or search parameters.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on MCTS for proof search (e.g., AlphaZero adaptations, GamePad).\", \"- [ ] /math: Formulate the proof search problem in the language of Markov Decision Processes.\", \"- [ ] /math: Derive the update rule for the value function using Bayesian principles.\", \"- [ ] /code: Implement the interface to the Coq proof assistant using the `serapi` library.\", \"- [ ] /code: Implement the MCTS algorithm guided by a JAX-based neural network.\", \"- [ ] /math: Verify that the implemented loss function in the code correctly reflects the derived Bayesian update rule.\", \"- [ ] /code: Validate that the synthesized Coq script is successfully type-checked by the `coqc` compiler.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic-methods\", \"program-synthesis\", \"automated-theorem-proving\"], \"acceptance_criteria\": [\"- The system must successfully synthesize a provably correct sorting algorithm (e.g., insertion sort) in Coq.\", \"- The generated proof term must be successfully verified by an independent Coq kernel check (`coqc`).\", \"- The Python implementation of the MCTS update rule must directly correspond to the formal derivation in the math artifact.\", \"- The literature review must cite at least 3 relevant arXiv papers published within the last 5 years.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 175, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the relationship between the Hessian's spectral properties and generalization in deep learning. Can you derive the gradient of the largest eigenvalue of the loss function's Hessian with respect to the network parameters? Then, implement this as a regularizer in PyTorch, using power iteration to approximate the dominant eigenvector, and analyze its effect on model convergence and adversarial robustness on CIFAR-10.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org hessian eigenvalue regularization generalization deep learning\\\", mode=rag)\", \"/math(Derive the gradient of the largest eigenvalue of the Hessian, \u03bb_max(H), with respect to network parameters \u03b8, leveraging the properties of the dominant eigenvector. Specify the update rule for a regularization term based on this eigenvalue to promote convergence.)\", \"/code(Implement a PyTorch training loop for a CNN on CIFAR-10 that incorporates a regularizer for the Hessian's largest eigenvalue. Use power iteration to estimate the eigenvalue and its corresponding eigenvector, then verify the analytical gradient with numerical checks and track adversarial attack success., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather seminal papers on Hessian spectral analysis. /math then uses these theoretical foundations to perform the core matrix calculus derivation. Finally, /code translates the mathematical formula into a verifiable PyTorch implementation to run experiments and measure the impact.\", \"expected_artifacts\": [\"A formal mathematical derivation of the gradient of the largest Hessian eigenvalue (\u03bb_max) in LaTeX.\", \"A Python script implementing the spectral regularizer in PyTorch, including the power iteration method and training loop for CIFAR-10.\", \"A report summarizing the experimental results, including plots of \u03bb_max, training/validation accuracy, and adversarial robustness metrics over epochs.\", \"A list of at least 2 cited arXiv papers that informed the methodology.\"], \"thinking_outline\": [\"1. Research the theoretical link between the Hessian's top eigenvalue, the sharpness of the loss landscape, and a model's generalization gap and adversarial vulnerability.\", \"2. Formulate the optimization problem, defining the loss function L(\u03b8) and the Hessian H = \u2207\u00b2L(\u03b8), and the regularization objective R(\u03b8) = \u03bb_max(H).\", \"3. Derive the expression for \u2207_\u03b8 \u03bb_max(H) using matrix calculus, assuming \u03bb_max is a simple eigenvalue.\", \"4. Validate the analytical gradient derived in the previous step by comparing it against a numerical approximation using finite differences in a simple test case.\", \"5. Implement the full training procedure in PyTorch, including an efficient, backpropagation-compatible power iteration method to estimate \u03bb_max and its gradient.\", \"6. Audit the final results to confirm that the regularizer successfully suppresses the growth of \u03bb_max and analyze its causal effect on test accuracy and robustness to PGD attacks.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to the /math agent. The /math agent produces a formal derivation that the /code agent must implement. Verification: The /code agent's numerical gradient check must match the /math agent's analytical derivation within a small tolerance. Fallback: If verification fails, the task is routed back to /math for a derivation review and /code for an implementation audit.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Hessian spectra and generalization.\", \"- [ ] /math: Produce a step-by-step derivation for the gradient of the largest Hessian eigenvalue.\", \"- [ ] /code: Implement the power iteration method to find the dominant eigenvector and eigenvalue of the Hessian.\", \"- [ ] /code: Integrate the spectral regularizer into a CIFAR-10 training script and log metrics.\", \"- [ ] /math: Verify that the analytical gradient expression is correct and consistent with literature.\", \"- [ ] /code: Validate the implementation by comparing the analytical gradient to a finite difference approximation.\", \"- [ ] /code: Generate plots for accuracy, loss, and \u03bb_max over training epochs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for \u2207_\u03b8 \u03bb_max(H) must be complete and correct.\", \"- The Python code must successfully run and train a model on CIFAR-10, demonstrably reducing the Hessian's largest eigenvalue compared to a baseline without regularization.\", \"- The final report must include plots showing a comparison of training metrics (accuracy, \u03bb_max) between the regularized and baseline models.\", \"- The numerical gradient check in the code must pass, showing alignment with the analytical formula within a tolerance of 1e-4.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 178, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the formal convergence guarantee for a federated averaging algorithm under (\u03b5, \u03b4)-differential privacy, specifically when using a secure aggregation protocol based on additively homomorphic encryption. How does the noise from encryption and DP mechanisms affect the spectral properties of the global model's Hessian, and can you provide a proof that bounds the optimization error as a function of the privacy budget and the number of communication rounds?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning convergence proof homomorphic encryption secure aggregation differential privacy\\\", mode=web)\", \"/math(Derive the convergence proof for federated averaging with encryption, analyzing the impact of noise from the privacy mechanism on the global Hessian's eigenvalue distribution and satisfying the specified convergence criteria.)\", \"/code(Implement the federated learning protocol in Python with a Paillier encryption scheme for secure aggregation, tracking AUPRC metrics and using CUDA for acceleration of client-side training steps.)\", \"/math(Verify the empirical convergence from the simulation against the derived theoretical bounds, analyzing the asymptotic behavior and the impact of federated learning hyperparameters like client participation rate.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey existing literature for foundational proofs. This informs the /math agent's formal derivation of the convergence bounds. The /code agent then implements a simulation to empirically test the theory. A final /math step is crucial for verification, comparing theoretical predictions with practical outcomes.\", \"expected_artifacts\": [\"- A formal mathematical proof of convergence in a PDF document, detailing all assumptions and steps.\", \"- A Python script implementing the federated learning simulation with homomorphic aggregation.\", \"- A summary report in Markdown, including plots of model accuracy vs. privacy budget and empirical vs. theoretical convergence rates.\", \"- A bibliography file with citations for at least two relevant arXiv preprints.\", \"- A verification log documenting the alignment between the theoretical proof and the simulation's output.\"], \"thinking_outline\": [\"1. Research current literature on convergence proofs for federated learning under the dual constraints of differential privacy and secure aggregation using homomorphic encryption.\", \"2. Formulate the optimization problem, defining the global objective and local updates, explicitly modeling the noise introduced by both the DP mechanism and the cryptographic protocol.\", \"3. Derive the convergence rate by carefully bounding the expected norm of the global gradient update, accounting for the variance introduced by the privacy mechanisms.\", \"4. Implement the federated averaging algorithm with a simulated homomorphic encryption scheme to validate the theoretical analysis on a benchmark dataset like CIFAR-10.\", \"5. Validate that the empirical convergence curve from the simulation falls within the derived theoretical bounds for different privacy levels.\", \"6. Audit the privacy budget (epsilon) calculation and analyze the trade-off between the strength of the privacy guarantee and the final model's utility (test accuracy).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results provide the theoretical foundation for the math agent's proof. The completed proof provides the specification for the code agent's simulation. The simulation output is then handed back to the math agent for formal verification against the proof. The router performs a final QA check on all artifacts. Fallback: If the empirical results deviate significantly from the theory, the code agent will be tasked to debug the implementation's privacy mechanism, and the math agent will re-check the proof's assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on the convergence of private federated learning.\", \"- [ ] /math: Formulate and derive the complete convergence proof based on the literature.\", \"- [ ] /code: Implement the simulation environment, the federated averaging algorithm, and the secure aggregation primitive.\", \"- [ ] /code: Run experiments for different privacy budgets (epsilon values) and plot the results.\", \"- [ ] /math: Verify that the empirical convergence rates from the plots are consistent with the derived theoretical bounds.\", \"- [ ] /math: Verify the correctness of the privacy loss calculation for the combined mechanism.\", \"- [ ] Router: Perform final QA review of the proof, code, and summary report for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"differential-privacy\", \"homomorphic-encryption\"], \"acceptance_criteria\": [\"- The submitted mathematical proof is logically sound, complete, and formally bounds the convergence rate.\", \"- The simulation code is executable and reproduces the key plots showing the privacy-utility trade-off.\", \"- The final report accurately summarizes the methodology and results, citing at least 2 relevant arXiv papers with IDs.\", \"- The verification log confirms that the empirical results align with the theoretical predictions within an acceptable margin of error.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 179, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for dictionary learning using both the Method of Optimal Directions (MOD) and the K-SVD algorithm for sparse coding. Subsequently, implement a Python simulation to demonstrate compressed sensing recovery of a sparse, high-dimensional signal from undersampled Fourier measurements, analyzing the reconstruction error versus the signal's sparsity level (k) and the learned dictionary's coherence. The derivation must address convergence properties and computational complexity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD dictionary learning convergence proof compressed sensing fourier\\\", mode=web)\", \"/math(Derive the update rules for the K-SVD algorithm for dictionary learning, analyzing the objective function's convergence properties and the overall computational complexity for sparse representation over a learned, incoherent dictionary.)\", \"/code(Implement K-SVD in Python to recover a sparse signal from undersampled Fourier measurements. Systematically evaluate reconstruction error against signal sparsity and dictionary coherence, providing a complexity analysis.)\"], \"route_rationale\": \"/general-search (blue agent) first gathers foundational literature from arXiv on K-SVD and its convergence analysis. /math then formalizes the algorithm, deriving the specific update rules and analyzing its theoretical properties. Finally, /code translates the mathematical framework into a verifiable Python simulation to test its empirical performance on a challenging compressed sensing problem.\", \"expected_artifacts\": [\"A formal mathematical derivation of the K-SVD update rules, including a discussion of its convergence and complexity.\", \"A well-documented Python script implementing the K-SVD algorithm for compressed sensing signal recovery.\", \"A set of plots showing reconstruction Signal-to-Noise Ratio (SNR) as a function of signal sparsity and measurement count.\", \"A summary report comparing the theoretical complexity with empirical runtime measurements and citing at least two relevant papers.\", \"A JSON file containing the final primary and secondary metric values.\"], \"thinking_outline\": [\"1. Formulate the dictionary learning problem as an optimization task, defining the objective function that combines a data fidelity term with an L0-norm sparsity constraint.\", \"2. Derive the two-stage iterative update process for K-SVD: a sparse coding stage using an algorithm like Orthogonal Matching Pursuit (OMP) and a dictionary update stage using Singular Value Decomposition (SVD) on residual matrices.\", \"3. Analyze the convergence of the K-SVD algorithm by showing that the objective function is non-increasing at each step.\", \"4. Implement the complete K-SVD algorithm in Python, along with a simulation environment to generate sparse signals and undersampled Fourier measurements.\", \"5. Validate the implementation by ensuring the reconstruction error decreases monotonically over iterations for a fixed test case.\", \"6. Perform a parameter sweep to empirically verify the relationship between the number of measurements, signal sparsity, and successful reconstruction probability, comparing results to theoretical bounds from compressed sensing.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent will audit the /code implementation to ensure the SVD-based dictionary update step correctly reflects the derived mathematical formulation; fallback: if the simulation fails to converge, /code will add logging to inspect dictionary atom norms and residuals, handing back to /math for re-evaluation of assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on K-SVD, focusing on convergence proofs and applications in signal processing.\", \"- [ ] /math: Formulate the sparse coding and dictionary learning optimization problem with appropriate constraints.\", \"- [ ] /math: Derive the iterative update rules for K-SVD, detailing the sparse coding (OMP) and dictionary update (SVD) stages.\", \"- [ ] /code: Implement the K-SVD algorithm in Python using NumPy/SciPy.\", \"- [ ] /code: Set up a simulation to generate sparse signals, acquire undersampled Fourier measurements, and apply K-SVD for reconstruction.\", \"- [ ] /math: Verify that the loss function implemented in the /code artifact monotonically decreases, which is a key convergence property.\", \"- [ ] /code: Validate the final reconstruction performance by measuring the Signal-to-Noise Ratio (SNR) against the ground truth signal.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"compressed-sensing\", \"numerical-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the K-SVD update steps is complete, rigorous, and logically sound.\", \"- The Python implementation successfully reconstructs a known k-sparse signal from M random Fourier measurements (where M < N) with a final reconstruction SNR exceeding 30 dB.\", \"- The generated plots correctly illustrate that reconstruction quality degrades as sparsity (k) increases or the number of measurements (M) decreases, consistent with compressed sensing theory.\", \"- The final report includes proper citations for at least two relevant academic papers, including their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 180, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous analysis of saddle point escape dynamics in non-convex optimization. Please derive the theoretical conditions for a second-order stationary point using Hessian eigenvalue analysis and then implement a perturbed gradient descent algorithm in Python to empirically validate the theory on a suitable test function, demonstrating how the algorithm leverages negative curvature to escape a sharp saddle.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org perturbed gradient descent non-convex optimization Hessian eigenvalue analysis\\\", mode=rag)\", \"/math(Derive the second-order Taylor expansion for a non-convex objective to analyze the Hessian eigenvalue spectrum at a saddle point, establishing the theoretical convergence guarantees.)\", \"/code(Implement a perturbed gradient descent algorithm in Python to escape a strict saddle in an adversarial optimization setting, tracking the smallest eigenvalue of the Hessian to validate the escape dynamics.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather state-of-the-art theoretical foundations from arXiv. This literature informs the /math agent's formal derivation of saddle point conditions and escape algorithms. Finally, the /code agent implements the derived theory for empirical validation, creating a closed loop of theory and practice.\", \"expected_artifacts\": [\"A formal mathematical derivation (.tex or .md) of the strict saddle point conditions and the perturbed gradient descent update rule.\", \"A Python script (.py) implementing the algorithm and a test function with clear saddle points, using libraries like NumPy and SciPy.\", \"A set of plots (.png) visualizing the optimization trajectory escaping the saddle and the evolution of the Hessian's minimum eigenvalue over time.\", \"A summary report (.md) synthesizing the findings and citing at least two relevant arXiv papers.\", \"A JSON file containing the final metrics, including escape iterations and minimum eigenvalue at escape.\"], \"thinking_outline\": [\"1. Define a suitable non-convex test function (e.g., f(x, y) = x^2 - y^2) that possesses a clear strict saddle point at the origin.\", \"2. Use the literature search to ground the mathematical derivation of first-order (gradient is zero) and second-order (Hessian has at least one negative eigenvalue) conditions for a strict saddle point.\", \"3. Formulate the update rule for a perturbed gradient descent algorithm, which adds noise when the gradient is small to find a direction of negative curvature.\", \"4. Implement the algorithm and the test function in Python, ensuring the Hessian matrix and its eigenvalues are computed correctly at each step.\", \"5. Validate that the implemented algorithm successfully escapes the saddle point by plotting its trajectory and verify that the escape direction aligns with the eigenvector corresponding to the Hessian's most negative eigenvalue.\", \"6. Audit the empirical number of iterations required for escape against the theoretical complexity guarantees discussed in the sourced literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent verifies theoretical claims from /general-search; /code agent's implementation is cross-referenced against /math agent's derivation. fallback: if /code fails validation tests, the discrepancy is flagged for /math to review the derivation for potential errors or edge cases.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers (e.g., from Jin et al.) on escaping saddle points and Hessian analysis in non-convex settings.\", \"- [ ] /math: Formally define a strict saddle point and derive the conditions based on the Hessian's eigenvalue spectrum.\", \"- [ ] /math: Formulate the update steps for a perturbed gradient descent algorithm designed to leverage negative curvature.\", \"- [ ] /code: Implement the non-convex test function and the perturbed gradient descent algorithm in Python.\", \"- [ ] /code: Implement logic to compute the Hessian matrix and its eigenvalues at each relevant iteration.\", \"- [ ] /math: Verify that the algorithm's implementation in the /code step correctly reflects the mathematical derivation.\", \"- [ ] /code: Validate the escape trajectory by plotting the optimization path and the minimum eigenvalue over iterations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-matrix\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly identifies the conditions for a strict saddle and provides a clear, step-by-step logic for the perturbed update rule.\", \"- The Python implementation successfully demonstrates the algorithm escaping a pre-defined saddle point, with visualization showing a clear departure from the saddle.\", \"- The final report includes plots of both the 2D optimization trajectory and the minimum eigenvalue over iterations, and correctly cites at least 2 relevant arXiv preprints with their IDs.\", \"- The computed `escape_iterations` metric is finite and reasonable for the chosen test problem.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 183, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive the complete variational inference update for a Bayesian Neural Network where the posterior q(\u03b8|D) is approximated by a planar normalizing flow. This requires deriving the ELBO objective, paying special attention to the log-determinant of the Jacobian for the flow transformations. Can you also implement this using PyTorch, verify its convergence on a synthetic regression task, and compare the posterior uncertainty quantification against standard mean-field Bayesian inference?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian neural network 'planar normalizing flow' variational inference\\\", mode=web)\", \"/math(Derive the Evidence Lower Bound (ELBO) for a Bayesian neural network using a planar normalizing flow posterior. Explicitly detail the change of variables formula and the resulting log-determinant-Jacobian term for convergence analysis.)\", \"/code(Implement the derived ELBO in PyTorch for a Bayesian regression model. Use automatic differentiation for gradients and track the ELBO convergence and predictive uncertainty metrics like AUPRC., using Python)\"], \"route_rationale\": \"/general-search (blue agent) is needed first to gather seminal literature on normalizing flows for variational inference, ensuring the mathematical derivation is grounded in established theory. /math will then formalize the derivation of the ELBO objective, focusing on the complex Jacobian term. Finally, /code will translate the mathematical model into a working PyTorch implementation for empirical validation.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the ELBO for a BNN with a planar flow posterior.\", \"A Python script implementing the model, training loop, and visualization of posterior predictive distributions.\", \"A summary report comparing the ELBO convergence and uncertainty estimates against a mean-field baseline.\", \"A verification log confirming the analytical Jacobian term matches the computational graph's automatic differentiation results.\"], \"thinking_outline\": [\"1. Formulate the Bayesian Neural Network as a probabilistic graphical model and define the prior p(\u03b8) and likelihood p(D|\u03b8).\", \"2. Define the variational posterior q(\u03b8) using a sequence of planar normalizing flow transformations, starting from a base Gaussian distribution.\", \"3. Derive the Evidence Lower Bound (ELBO), L(q) = E_q[log p(D|\u03b8)] - KL(q(\u03b8) || p(\u03b8)), paying close attention to the log |det J| term arising from the flow's change of variables.\", \"4. Implement the derived objective function and the BNN model, using the reparameterization trick to get low-variance gradient estimates.\", \"5. Validate the implementation by checking for ELBO convergence on a synthetic dataset and visualizing the learned posterior predictive uncertainty.\", \"6. Audit the analytical log-determinant-Jacobian derivation against the values computed by the framework's automatic differentiation to ensure correctness.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent verifies that the ELBO derivation in the final report matches the implemented loss function in the /code agent's script; fallback: if a mismatch is found, route back to /math for correction of the Jacobian term).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Normalizing Flows for VI (e.g., Rezende & Mohamed, 2015).\", \"- [ ] /math: Derive the full ELBO objective for the BNN with a planar flow posterior, showing all steps for the Jacobian determinant.\", \"- [ ] /code: Implement the BNN model, the planar flow transformation, and the training loop in PyTorch.\", \"- [ ] /code: Generate a synthetic regression dataset and train the model, plotting the ELBO curve.\", \"- [ ] /math: Verify that the analytical derivation of the log-determinant-Jacobian aligns with the numerical output from the PyTorch implementation.\", \"- [ ] /code: Add code to visualize the posterior predictive distribution and its uncertainty.\", \"- [ ] router: Perform final QA check on all artifacts for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"normalizing-flows\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO is complete, correct, and includes the specific form for the planar flow Jacobian.\", \"- The Python code runs without error, demonstrates ELBO convergence on a synthetic dataset, and generates visualizations of the predictive uncertainty.\", \"- The final report correctly cites at least two relevant academic papers with their arXiv IDs.\", \"- The verification log confirms numerical alignment between the analytical Jacobian term and its implementation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 185, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the complete Karush-Kuhn-Tucker (KKT) conditions for a continuous-time policy optimization problem where safety must be guaranteed via a stochastic Control Barrier Function (CBF). Formulate a corresponding primal-dual learning scheme using neural network approximators for both the policy and the CBF, and formally analyze the conditions for convergence under non-convex policy parameterizations within control theory.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org stochastic control barrier function deep reinforcement learning lagrangian duality convergence\\\", mode=both)\", \"/math(Derive the Lagrangian for the policy optimization problem with a Bayesian Control Barrier Function constraint. Analyze the Karush-Kuhn-Tucker conditions and formulate the primal-dual update rules for ensuring convergence and control theory guarantees.)\", \"/code(Implement the derived primal-dual algorithm in PyTorch with CUDA support, creating a simple robotics simulation to validate the CBF effectiveness and track convergence metrics against a baseline., using Python)\"], \"route_rationale\": \"The blue /general-search agent is used first to survey recent authoritative literature on stochastic CBFs in RL. This informs the /math agent's formal derivation of the primal-dual optimization framework. Finally, the /code agent implements this mathematical framework to provide an empirical validation of the theoretical results.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF containing the full mathematical derivation of the Lagrangian, KKT conditions, and primal-dual update rules.\", \"- A documented Python script implementing the safety-constrained RL agent and the simulation environment.\", \"- A summary report in Markdown that presents the experimental results, including plots and interpretations.\", \"- A JSON file containing the final evaluation metrics (Safety_Violation_Rate, Cumulative_Reward).\", \"- A verification log confirming numerical alignment between the mathematical derivation and the code's gradient implementation.\"], \"thinking_outline\": [\"1. Formulate the safety-constrained problem as a Constrained Markov Decision Process (CMDP) where the constraint is defined by the stochastic Control Barrier Function.\", \"2. Construct the Lagrangian for the policy optimization objective, incorporating the CBF constraint with a Lagrange multiplier.\", \"3. Derive the Karush-Kuhn-Tucker (KKT) optimality conditions for this non-convex problem and formulate the corresponding primal-dual gradient update rules for the policy network and the Lagrange multiplier.\", \"4. Analyze the theoretical conditions for convergence, referencing Lyapunov stability and control theory principles for systems with learned components.\", \"5. Verify that the implemented gradient updates in the Python code numerically match the symbolic derivatives derived in the mathematical analysis.\", \"6. Validate the algorithm's performance in simulation by confirming that the agent's safety violation rate is below a specified threshold while maintaining reasonable task performance.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: router will check the numerical gradient validation script from /code against the symbolic derivation from /math; fallback: if convergence is not achieved, /math will re-evaluate assumptions on Lipschitz continuity and propose a regularizer, which /code will then implement).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on stochastic CBFs and primal-dual methods in deep RL.\", \"- [ ] /math: Formulate the CMDP and derive the Lagrangian dual for the safety-constrained policy objective.\", \"- [ ] /math: Derive the KKT conditions and the final primal-dual gradient update equations.\", \"- [ ] /code: Implement the policy network, value function, and the Lagrange multiplier update rule in PyTorch.\", \"- [ ] /code: Set up a simulation environment and run experiments to collect safety violation rates and reward curves.\", \"- [ ] /math: Verify that the implemented gradient computations in the code numerically match the symbolic derivations.\", \"- [ ] /code: Validate that the agent respects the safety constraints in the simulation, reporting key metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"control-theory\", \"constrained-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the primal-dual updates must be complete, rigorous, and self-contained.\", \"- The Python implementation must execute and demonstrate a statistically significant reduction in the Safety_Violation_Rate metric compared to an unconstrained baseline agent.\", \"- The final report must cite at least 2 relevant arXiv papers with IDs and include plots showing both reward and constraint satisfaction over training epochs.\", \"- The numerical gradient verification check must pass with a tolerance of 1e-5.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 186, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the connection between loss landscape geometry and generalization in deep networks. Can you provide a detailed analysis of the Hessian's spectral properties? Specifically, I need you to derive the power iteration method for efficiently computing the dominant eigenvalue of the Hessian matrix using only Hessian-vector products, implement this in PyTorch for a pre-trained ResNet on CIFAR-10, and analyze the resulting spectrum in the context of recent arXiv literature on Sharpness-Aware Minimization (SAM).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org hessian eigenvalue power iteration deep learning 'sharpness aware minimization'\\\", mode=both)\", \"/math(Derive the power iteration algorithm for computing the top Hessian eigenvalue using only Hessian-vector products. Formulate the convergence criteria based on the eigenvalue gap.)\", \"/code(Implement the derived power iteration in PyTorch to find the dominant Hessian eigenvalue for a pre-trained ResNet-18 on CIFAR-10. Utilize CUDA for acceleration and track convergence., using Python)\"], \"route_rationale\": \"/general-search (blue agent) is used first to gather foundational papers on Hessian spectral analysis and modern techniques like SAM. /math is then used for the formal derivation of the required numerical method. Finally, /code implements the derived algorithm for empirical validation on a concrete deep learning model.\", \"expected_artifacts\": [\"A formal mathematical derivation of the power iteration method for Hessian-vector products.\", \"A Python script (PyTorch) implementing the algorithm and applying it to a ResNet-18.\", \"A plot visualizing the convergence of the top eigenvalue estimate over iterations.\", \"A summary report (.md) interpreting the results, citing at least two relevant arXiv papers.\", \"A verification log showing the implementation's correctness on a small test matrix.\"], \"thinking_outline\": [\"1. Survey recent literature on the Hessian's role in generalization and methods for spectral analysis, particularly power iteration and its connection to sharpness-aware optimization.\", \"2. Formulate the mathematical problem: define the Hessian of the loss function and derive the update rule for the power iteration method using only Hessian-vector products, which avoids materializing the full Hessian.\", \"3. Implement the derived algorithm in a deep learning framework, including functionality for automatic differentiation to compute the Hessian-vector products efficiently on a GPU.\", \"4. Validate the code's correctness by applying it to a simple quadratic function where the Hessian and its eigenvalues can be computed analytically, and check that the implementation recovers the known top eigenvalue.\", \"5. Execute the validated implementation on a pre-trained deep neural network (e.g., ResNet-18 on CIFAR-10) to compute the top eigenvalue of its loss Hessian.\", \"6. Analyze the resulting eigenvalue and discuss its implications for model generalization, referencing the findings from the initial literature survey.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The literature from /general-search informs the derivation in /math. The formal derivation from /math provides the algorithm for /code to implement. /code must verify its implementation against an analytical solution; if verification fails, it flags a potential mismatch with the /math derivation, triggering a review. The final code, plots, and report are passed to the router for a final quality check.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arxiv.org on Hessian spectral analysis and sharpness-aware minimization in deep learning.\", \"- [ ] /math: Provide a step-by-step derivation of the power iteration method for finding the dominant eigenvalue of the Hessian using only Hessian-vector products.\", \"- [ ] /code: Implement the power iteration algorithm in PyTorch, leveraging `torch.autograd` to compute Hessian-vector products efficiently on a GPU.\", \"- [ ] /code: Create a small test case with a known quadratic function to validate the correctness of the power iteration implementation.\", \"- [ ] /code: Apply the validated code to a pre-trained ResNet-18 model on the CIFAR-10 dataset and generate a plot showing the eigenvalue estimate's convergence.\", \"- [ ] /math: Verify that the algorithm implemented in the code correctly reflects the mathematical derivation, paying close attention to the normalization step.\", \"- [ ] /router: Perform final QA on all artifacts, checking for consistency between the derivation, code, results, and the summary report's conclusions.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"matrix-calculus\", \"spectral-analysis\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"The mathematical derivation for power iteration using Hessian-vector products is complete and correct.\", \"The Python code successfully runs and computes a plausible top eigenvalue for a pre-trained ResNet-18, and its correctness is demonstrated on a small analytical test case.\", \"The final report cites at least two relevant arXiv papers and correctly interprets the computed eigenvalue in the context of loss landscape flatness and generalization.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 187, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a detailed analysis comparing two privacy-preserving aggregation protocols for federated learning: Secure Aggregation (SecAgg) using homomorphic encryption primitives versus a federated averaging scheme augmented with user-level differential privacy via Gaussian noise addition. The goal is to derive the convergence bounds for a non-convex objective function under both privacy models and implement a simulation to empirically validate the theoretical trade-off between the privacy budget (epsilon, delta) and the final model's test accuracy, while also benchmarking communication overhead.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning secure aggregation convergence analysis vs differential privacy non-convex\\\", mode=both)\", \"/math(Derive the convergence rate for federated averaging with a non-convex objective, then incorporate the error terms introduced by both homomorphic encryption noise in a simplified SecAgg model and the Gaussian mechanism for user-level differential privacy.)\", \"/code(Implement federated learning simulations in Python using TensorFlow Privacy to compare the final model accuracy against the privacy budget (epsilon) for the DP-based approach and the communication cost for the encryption-based federated scheme.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art papers on federated learning privacy, which is essential for grounding the theoretical work. Next, the /math agent is tasked with the formal derivation of convergence bounds, a necessary prerequisite for understanding the theoretical trade-offs. Finally, the /code agent implements the derived concepts to provide empirical evidence and benchmark performance, directly addressing the user's validation requirement.\", \"expected_artifacts\": [\"A formal mathematical derivation of convergence bounds for both privacy-preserving schemes.\", \"A summary report citing at least two relevant research papers with arXiv IDs, comparing the theoretical guarantees.\", \"Python source code for the simulation, including data loaders, model definitions, and federated training loops for both methods.\", \"A JSON file containing the simulation results, including accuracy, privacy budget (epsilon), and communication metrics.\", \"A plot visualizing the trade-off between test accuracy and the privacy budget.\"], \"thinking_outline\": [\"1. Survey foundational literature on Secure Aggregation (e.g., Bonawitz et al.) and Differentially Private Federated Learning (e.g., McMahan et al.) to establish theoretical baselines.\", \"2. Formulate the optimization problem for a non-convex loss function and derive the convergence proof for standard Federated Averaging.\", \"3. Extend the proof to incorporate the noise term from user-level differential privacy, explicitly showing the dependency of the convergence rate on the privacy budget (epsilon, delta).\", \"4. Model the communication and computation overhead of a simplified homomorphic encryption scheme for Secure Aggregation and compare its complexity against the DP approach.\", \"5. Validate the theoretical convergence bounds by implementing both protocols and running simulations on a benchmark dataset like CIFAR-10, plotting accuracy vs. privacy budget.\", \"6. Audit the implementation to ensure the noise injection correctly satisfies the (epsilon, delta)-differential privacy definition and that the secure aggregation logic is sound.\"], \"handoff_plan\": \"/general-search (key papers) -> /math (derive bounds based on literature) -> /code (implement and validate derived bounds) -> router QA (verification: code outputs must align with mathematical derivations; fallback: if simulation contradicts theory, flag for review and potentially re-run /math with revised assumptions).\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize at least two seminal papers on SecAgg and DP in federated learning.\", \"- [ ] /math: Provide a step-by-step derivation of the convergence bound for FedAvg with user-level DP noise.\", \"- [ ] /math: Formulate the privacy and correctness guarantees for the Secure Aggregation protocol.\", \"- [ ] /code: Implement the federated learning simulation framework for a non-convex problem (e.g., CNN on CIFAR-10).\", \"- [ ] /code: Implement both the DP-based and a simplified encryption-based aggregation mechanism.\", \"- [ ] /math: Verify that the noise scaling implemented in the code correctly corresponds to the (epsilon, delta)-DP definition from the derivation.\", \"- [ ] /code: Generate plots comparing test accuracy vs. epsilon for the DP scheme and report communication overhead for both.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"differential-privacy\", \"secure-aggregation\"], \"acceptance_criteria\": [\"- The mathematical derivation must clearly show how privacy parameters (epsilon, delta) or encryption overhead impact the convergence error terms.\", \"- The final report must contain a plot showing a clear trade-off curve between model accuracy and privacy level (epsilon).\", \"- The implementation must correctly apply noise for differential privacy, as verifiable by inspecting the relevant code sections and their output distributions.\", \"- The solution must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 189, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating how Tucker decomposition can be constrained by topological invariants, specifically Betti numbers derived from persistent homology, to learn equivariant representations. Can you derive the optimization problem for a Tucker decomposition where the core tensor's structure is regularized by a topological loss term computed on the data manifold? Please then implement this using a higher-order singular value decomposition (HOSVD) initialization in JAX and demonstrate its convergence on a synthetic dataset like a torus, analyzing the resulting factor matrices.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org tensor decomposition persistent homology representation learning topological autoencoder\\\", mode=both)\", \"/math(Formulate the Lagrangian for Tucker decomposition with a Betti number regularization term. Derive the Karush-Kuhn-Tucker conditions and the gradient expressions for the core tensor and factor matrices, analyzing the problem's complexity and convergence properties.)\", \"/code(Implement the topologically-constrained decomposition in JAX with HOSVD initialization. Use CUDA for acceleration and a persistent homology library to compute the topological loss. Validate the implementation's eigenvalue spectrum against theoretical predictions from the /math derivation., using Python)\"], \"route_rationale\": \"The blue /general-search agent will first gather state-of-the-art literature to ground the approach. The /math agent will then formalize the novel optimization problem and derive the necessary equations. Finally, the /code agent will implement and empirically validate the derived mathematical framework, creating a tight loop between theory and practice.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the optimization problem, gradients, and convergence analysis.\", \"- A JAX/Python script implementing the algorithm, including data generation, HOSVD initialization, and training loop with integrated topological loss.\", \"- A results report (PDF or Markdown) comparing the reconstruction error and topological invariance metric against a standard Tucker decomposition baseline, with citations to at least two relevant arXiv papers.\", \"- A verification log file showing the output of the numerical gradient check, confirming alignment between the mathematical derivation and the code.\"], \"thinking_outline\": [\"1. Review state-of-the-art literature connecting tensor methods with topological data analysis (TDA) for representation learning.\", \"2. Formulate a composite loss function combining the standard tensor reconstruction error with a term penalizing divergence from target Betti numbers calculated via persistent homology.\", \"3. Derive the partial derivatives of this composite loss with respect to the core tensor and factor matrices using tensor calculus and the chain rule.\", \"4. Verify the derived analytical gradients numerically using finite differences (JAX's grad functionality can be compared against a manual implementation) to ensure correctness before full-scale training.\", \"5. Implement the optimization algorithm and validate that the learned representations preserve the underlying topological structure (i.e., the Betti numbers) of the synthetic torus manifold.\", \"6. Audit the final model's convergence behavior and the interpretability of the learned factor matrices in relation to the manifold's geometric properties.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers (arXiv IDs) to the math agent. The math agent produces a formal derivation (LaTeX PDF) for the code agent. The code agent implements the derivation and produces a verification log comparing its numerical gradients to the math agent's analytical ones. Fallback: If numerical gradients do not match analytical ones within a tolerance of 1e-6, the task reverts to the /math agent for derivation correction, citing the specific component that failed.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Tucker decomposition and persistent homology for representation learning from arXiv.\", \"- [ ] /math: Formulate the objective function and derive the gradient expressions for all components (core tensor, factor matrices).\", \"- [ ] /math: Analyze the computational complexity of the gradient updates and propose a stable optimization scheme (e.g., alternating updates with momentum).\", \"- [ ] /code: Implement the HOSVD initialization and the gradient descent update loop in JAX.\", \"- [ ] /code: Integrate a persistent homology library (e.g., giotto-tda, Ripser) to compute the topological loss term within the JAX computational graph.\", \"- [ ] /math: Verify that the gradients derived analytically match numerical approximations from the /code implementation's gradient check log.\", \"- [ ] /code: Run experiments on a synthetic torus manifold and generate plots for the convergence of the primary and secondary metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"algebraic-topology\"], \"acceptance_criteria\": [\"- The mathematical derivation of the gradients for the topologically-regularized loss must be complete, self-contained, and mathematically sound.\", \"- The JAX implementation must pass a numerical gradient check against the analytical derivation with a relative error tolerance of 1e-5.\", \"- The final model must demonstrate a statistically significant improvement in the `betti_invariance_score` compared to a standard Tucker decomposition baseline on the specified synthetic dataset, while maintaining a comparable `reconstruction_error`.\", \"- The solution must cite at least two relevant academic papers published after 2018.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 190, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to design a safe reinforcement learning agent for a robotic arm manipulator using a Lagrangian-based approach for a Constrained Markov Decision Process (CMDP). Please provide the full derivation of the primal-dual update rule for both the policy and the Lagrange multiplier, a PyTorch implementation demonstrating convergence on a simulated environment, and a comparison against state-of-the-art methods from arXiv for ensuring near-zero constraint violation during curricular learning.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Constrained Markov Decision Process Lagrangian robotics curricular learning\\\", mode=both)\", \"/math(Derive the primal-dual update rules for the CMDP policy and Lagrange multiplier, ensuring the formulation satisfies KKT conditions and analyzing theoretical convergence from a control theory perspective.)\", \"/code(Implement the Lagrangian-based PPO agent in PyTorch, tracking the average constraint violation and the Bayesian confidence interval of the return to ensure robust safety performance., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find foundational papers on Lagrangian methods for CMDPs. This research informs the /math agent, which will formalize the theory and derive the necessary update equations. Finally, the /code agent will translate the verified mathematical formulation into a working PyTorch implementation and evaluate its performance against the safety constraints.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical formulation of the CMDP and the complete derivation of the primal-dual update rules.\", \"A standalone Python script containing the PyTorch implementation of the safe RL agent and the simulation environment.\", \"A summary report in Markdown with plots showing learning curves for reward and constraint violation, plus a bibliography citing at least two arXiv papers.\", \"A `metrics.json` file containing the final evaluation results for primary and secondary metrics.\"], \"thinking_outline\": [\"1. Research state-of-the-art Lagrangian methods for solving Constrained Markov Decision Processes (CMDPs), focusing on applications in robotics and safe exploration.\", \"2. Formulate the robotic manipulation task as a CMDP, defining the state space, action space, reward function, and a cost function for safety constraint violations (e.g., collisions).\", \"3. Derive the primal-dual optimization algorithm, including the gradient update rules for the policy parameters and the update rule for the Lagrange multiplier, based on the Lagrangian formulation.\", \"4. Validate the implementation by performing gradient checks to ensure the code's backward pass correctly matches the analytical gradients derived in the mathematical formulation.\", \"5. Train the agent in the simulated environment and empirically analyze its performance, plotting the episodic return and the cumulative constraint cost over time.\", \"6. Verify that the trained agent respects the safety budget by evaluating it over 100 episodes and confirming the average cumulative constraint cost is below the predefined threshold.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: research from /general-search provides the theoretical basis for the derivation in /math, which in turn provides the exact equations for the /code implementation; fallback: if the derivation proves intractable, /math will request /general-search finds alternative formulations like a projection-based method).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on PPO-Lagrangian or similar methods for CMDPs.\", \"- [ ] /math: Derive the policy gradient and Lagrange multiplier update equations from the Lagrangian objective.\", \"- [ ] /code: Implement the CMDP environment for the robotic arm, including the safety cost function.\", \"- [ ] /code: Implement the PPO-Lagrangian agent based on the derived equations.\", \"- [ ] /math: Verify that the gradient logic implemented in the /code agent's backward pass aligns with the derived equations.\", \"- [ ] /code: Validate that the trained agent's average constraint violation is below the specified threshold `d` over 100 evaluation episodes.\", \"- [ ] /router: Perform a final quality assurance review of all generated artifacts, checking for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"robotics\", \"constrained-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the primal-dual updates is complete, consistent with KKT conditions, and correctly documented.\", \"- The Python code executes without errors, and the agent demonstrates learning (increasing rewards) while the average cumulative constraint violation remains below the predefined budget `d`.\", \"- The final report includes convergence plots for both reward and constraint cost and cites at least two relevant arXiv papers with their IDs.\", \"- All expected artifacts are delivered in the specified formats.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 191, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the information-theoretic privacy-utility trade-off for generative models. Specifically, can you formulate the rate-distortion problem for a diffusion model where 'distortion' is the mutual information leakage about training samples and 'rate' is constrained by a target Frechet Inception Distance (FID)? Please derive the governing equations for the optimal channel and propose an iterative algorithm, analogous to Blahut-Arimoto, to empirically estimate the R-D curve.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org information bottleneck theory generative models privacy leakage 'Blahut-Arimoto algorithm' differential privacy\\\", mode=rag, includes cybersecurity)\", \"/math(Derive the rate-distortion function for adversarial privacy leakage, formulating distortion as mutual information and rate via an FID budget, using Euler-Lagrange equations to characterize the optimal data transformation.)\", \"/code(Implement a PyTorch model with a custom Bayesian information bottleneck regularizer based on the math derivation, tracking the AUPRC of a membership inference attack and ensuring convergence of the iterative updates., using Python)\"], \"route_rationale\": \"The blue /general-search agent will first curate foundational papers on information-theoretic privacy and relevant algorithms. This literature grounds the formal derivation, which will be performed by the /math agent. Finally, the /code agent will translate the derived mathematical principles into a concrete PyTorch implementation for empirical validation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the rate-distortion function for model privacy, delivered as a LaTeX-formatted PDF.\", \"A well-documented Python script implementing the model and the iterative algorithm to estimate the privacy-utility curve.\", \"A plot (`privacy_utility_curve.png`) visualizing the empirical rate-distortion trade-off (MIA AUPRC vs. FID).\", \"A summary report (`report.md`) explaining the results and citing at least two relevant arXiv papers.\", \"A verification log (`verification.log`) confirming the consistency between the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Formalize the problem by defining 'rate' as a function of generative utility (e.g., an upper bound on FID) and 'distortion' as a measure of privacy leakage (e.g., mutual information between a training sample and model output).\", \"2. Consult information theory literature to derive the rate-distortion function D(R) that provides the theoretical lower bound on privacy leakage for a given utility level.\", \"3. Adapt the Blahut-Arimoto algorithm to our context, defining the iterative update rules for the conditional distribution (the generative process) that minimizes the R-D Lagrangian.\", \"4. Implement this algorithm in code, including a membership inference adversary to empirically estimate the mutual information (distortion).\", \"5. Verify that the implemented loss function in the code is a correct discrete approximation of the continuous Lagrangian derived by the math agent.\", \"6. Validate the final empirical R-D curve by checking for expected properties like convexity and monotonicity, and compare it against theoretical bounds if available.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal derivation) -> /code (implementation & empirical results). The core handoff is from /math to /code. Verification: The code's loss function must be rigorously audited against the LaTeX equations from /math. Fallback: If verification fails, the derivation is returned to /math for correction, citing the specific implementation conflict. Final artifacts are bundled for router QA.\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on rate-distortion theory for privacy and the Blahut-Arimoto algorithm.\", \"- [ ] /math: Derive the Lagrangian for the privacy-utility optimization problem.\", \"- [ ] /code: Implement the generative model and the membership inference attack adversary.\", \"- [ ] /code: Implement the iterative Blahut-Arimoto update loop to trace the R-D curve.\", \"- [ ] /math: Verify that the gradients computed by the code's loss function align with the analytical gradients from the derivation.\", \"- [ ] /code: Audit the statistical estimators used for mutual information and FID to ensure they are unbiased and have low variance.\", \"- [ ] /code: Generate the final plot and metrics JSON file.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"rate-distortion-theory\"], \"acceptance_criteria\": [\"- The generated rate-distortion curve must be monotonic and convex, consistent with information theory principles.\", \"- The final report must cite at least two relevant arXiv pre-prints with their IDs and correctly interpret the empirical results.\", \"- The implemented code must be runnable and reproduce the reported plot and metrics within a 5% tolerance.\", \"- The verification log must confirm that the code's core optimization objective matches the one derived in the math artifact.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the area under the empirical Rate-Distortion curve, where Rate is 1/FID and Distortion is MIA AUPRC. A lower area indicates a better privacy-utility trade-off. Use the trapezoidal rule for numerical integration of the plotted points.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"The number of iterations for the Blahut-Arimoto algorithm to converge to a stable point on the R-D curve for a given rate. Lower is better.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 193, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for a Stochastic Variational Gaussian Process (SVGP) using inducing points. Then, implement this model in Python with GPyTorch to analyze a time-series dataset, demonstrating convergence and comparing predictive uncertainty against a standard GP. The derivation must explicitly handle the Kullback-Leibler divergence between the variational distribution and the prior.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Gaussian Processes for Big Data' Hensman 2013 OR 'Scalable Variational Gaussian Process Classification'\\\", mode=both)\", \"/math(Derive the complete ELBO for a Bayesian SVGP model, focusing on the KL-divergence between the variational posterior and the prior, ensuring mathematical rigor and readiness for implementation.)\", \"/code(Implement the SVGP in Python/GPyTorch, creating a training loop to optimize the ELBO, and validate the model's convergence and predictive log-likelihood on a benchmark dataset.)\"], \"route_rationale\": \"The plan begins with the blue /general-search agent to locate the seminal papers on SVGPs, which provides the theoretical foundation. This literature is passed to the /math agent to produce a rigorous, step-by-step derivation of the ELBO. Finally, the /code agent translates the derived mathematics into a working GPyTorch model, which serves as an empirical verification of the theory by testing for convergence and predictive accuracy.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF detailing the full derivation of the SVGP ELBO.\", \"A commented Python script implementing the SVGP model, including data loading, training, and evaluation functions.\", \"A summary report in Markdown containing plots of ELBO convergence over epochs and a comparison of predictive uncertainty against a baseline GP.\", \"A bibliography file citing at least two seminal papers on variational inference for Gaussian Processes with their arXiv IDs.\"], \"thinking_outline\": [\"1. Review the foundational theory of Gaussian Processes and variational inference, specifically how inducing points create a scalable, global approximation.\", \"2. Formulate the variational distribution over the inducing point function values and derive the Evidence Lower Bound (ELBO), carefully decomposing it into an expected log-likelihood term and a KL-divergence term.\", \"3. Implement the derived SVGP model and its corresponding ELBO loss function using a suitable framework like GPyTorch, paying close attention to minibatching for stochastic optimization.\", \"4. Validate the implementation by training the model on a known time-series dataset and plotting the ELBO to verify its monotonic convergence over training epochs.\", \"5. Audit the model's performance by comparing its predictive log-likelihood and uncertainty estimates on a held-out test set against a non-scalable, exact GP.\", \"6. Synthesize all findings\u2014derivation, code, and validation results\u2014into a comprehensive report.\"], \"handoff_plan\": \"/general-search -> /math (verification: check derived ELBO matches formulation in cited papers) -> /code (verification: ensure implemented loss function exactly matches the derived mathematical expression; fallback: revisit /math if ELBO does not converge) -> router QA (verification: all acceptance criteria met).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Stochastic Variational Gaussian Processes (e.g., Hensman et al., 2013).\", \"- [ ] /math: Derive the complete ELBO for the SVGP, clearly showing the decomposition into the expected log-likelihood and the KL term.\", \"- [ ] /code: Implement the SVGP model in Python with GPyTorch.\", \"- [ ] /code: Train the model and generate plots for ELBO convergence and predictive uncertainty.\", \"- [ ] /math: Verify that the symbolic form of the KL-divergence in the derivation matches the standard formulation for multivariate Gaussians.\", \"- [ ] /code: Validate the implementation by ensuring the predictive log-likelihood on a test set is reasonable and improves over random chance.\", \"- [ ] /router: Perform final QA check, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-processes\", \"bayesian-inference\", \"gaussian-processes\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO must be complete, self-contained, and consistent with published literature.\", \"- The Python implementation must successfully train, and the ELBO metric must show a clear trend of convergence (monotonic increase).\", \"- The final report must include a plot comparing the predictive mean and uncertainty bounds of the SVGP against the ground truth on a test set.\", \"- The SVGP model must achieve a higher predictive log-likelihood on the test set than a simple mean-predictor baseline.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 194, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for the K-SVD dictionary learning algorithm and prove its convergence under the assumption of a fixed sparsity level. Then, implement the algorithm in Python, using Orthogonal Matching Pursuit (OMP) for the sparse coding step, and validate its performance on a synthetic dataset by measuring reconstruction error and dictionary atom coherence. The analysis should also touch upon the Restricted Isometry Property (RIP) and its relevance to convergence guarantees, referencing recent econometrics literature on high-dimensional estimators.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD convergence proof 'restricted isometry property' econometrics\\\", mode=rag)\", \"/math(Derive the K-SVD update rules and formally prove its convergence, analyzing the objective function's behavior and bounding the eigenvalue of intermediate operators.)\", \"/code(Implement K-SVD and OMP in a CUDA-aware Python environment, using bootstrap methods to establish confidence intervals for the reconstruction error metric.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative literature on K-SVD theory and its connection to econometrics. This informs the /math agent's formal derivation and proof of convergence. Finally, the /code agent implements the derived algorithm and performs empirical validation, providing a complete theoretical and practical analysis.\", \"expected_artifacts\": [\"A formal mathematical document detailing the derivation of K-SVD update rules and a complete proof of its convergence.\", \"A well-documented Python script implementing the K-SVD algorithm, OMP, and the experimental validation harness.\", \"A summary report containing plots of reconstruction error vs. signal sparsity, the final dictionary's mutual coherence value, and a discussion of the results.\", \"A verification document confirming the alignment between the mathematical derivation and the code's logic.\", \"A bibliography file citing at least two relevant arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Research foundational literature on K-SVD, Orthogonal Matching Pursuit (OMP), and the Restricted Isometry Property (RIP) to establish the theoretical context.\", \"2. Formally derive the K-SVD update steps by treating it as an alternating minimization problem, and construct a rigorous proof for its convergence to a local minimum.\", \"3. Implement the K-SVD algorithm in Python, including a modular OMP function for the sparse coding stage, and a synthetic data generator.\", \"4. Validate the implementation's correctness by checking the dictionary update step against a known small-scale example and ensuring the reconstruction error decreases monotonically across iterations.\", \"5. Design and execute experiments on synthetic data to measure reconstruction error as a function of sparsity and to compute the mutual coherence of the learned dictionary.\", \"6. Audit the final report to verify that the experimental results and the implemented algorithm are fully consistent with the mathematical derivations and cited literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: at each step, the output artifact is checked for consistency with the previous one, e.g., math derivations must align with sourced literature, and code logic must match the derivation; fallback: if the code's performance contradicts the theory, loop back to /math for a review of the proof's assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on K-SVD and RIP from arXiv and IEEE Xplore.\", \"- [ ] /math: Derive the K-SVD dictionary and sparse code update rules via alternating minimization.\", \"- [ ] /math: Formulate and write the proof of convergence for the K-SVD algorithm under fixed sparsity.\", \"- [ ] /code: Implement the K-SVD algorithm and the OMP subroutine in Python using NumPy/SciPy.\", \"- [ ] /code: Develop a synthetic data generation pipeline and compute reconstruction error and dictionary coherence metrics.\", \"- [ ] /math: Verify that the logic in the Python implementation of the dictionary update step exactly matches the derived mathematical formulas.\", \"- [ ] /code: Validate the OMP implementation's output against a trusted library's implementation for a small test case.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"compressed-sensing\", \"dictionary-learning\"], \"acceptance_criteria\": [\"The mathematical proof of convergence for K-SVD must be complete, logically sound, and clearly articulated.\", \"The Python implementation must execute without errors and demonstrate a monotonically decreasing reconstruction error on the provided synthetic dataset.\", \"The final report must contain plots showing reconstruction error versus signal sparsity and a computed value for dictionary coherence.\", \"The solution must cite at least two relevant academic papers, including their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute ||X - D*Alpha||_F^2 / N, where X is the original signal matrix, D is the learned dictionary, Alpha is the sparse code matrix, and N is the number of signals.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Compute max_{i!=j} |<d_i, d_j>|, where d_i and d_j are normalized dictionary atoms. A lower value indicates a better-conditioned dictionary.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 195, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the fundamental trade-offs in watermarking large language models for AI safety. Please derive the rate-distortion function for a generative text watermark under a Kullback-Leibler divergence constraint on the output distribution. Then, develop a Python simulation to empirically validate this theoretical bound and assess its adversarial robustness against a simple paraphrasing attack, measuring detection power versus text utility.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org information theory llm watermarking rate-distortion Kullback-Leibler\\\", mode=both, includes cybersecurity)\", \"/math(Derive the rate-distortion function D(R) for a Bernoulli watermark embedded in a categorical distribution, using Bayesian principles to define the distortion as the KL-divergence between the watermarked and original distributions.)\", \"/code(Implement the derived watermarking algorithm in Python with PyTorch. Simulate its application on a pre-trained model's logits, including an adversarial paraphrasing module, and compute the AUPRC and perplexity metrics.)\", \"/math(Analyze the simulation outputs to plot the empirical rate-distortion curve. Validate the convergence of the empirical results to the theoretical derivation and establish a formal confidence interval for the discrepancy.)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature. The math agent then establishes the core theoretical foundation by deriving the rate-distortion function. The code agent implements this theory to provide empirical evidence and test adversarial resilience. A final math agent step validates that the experimental results align with the theory, closing the verification loop.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the rate-distortion function in LaTeX.\", \"- A Python script implementing the watermarking simulation and adversarial attack.\", \"- A final report (.md) summarizing the findings, including plots of the theoretical vs. empirical rate-distortion curve and AUPRC scores.\", \"- A JSON file containing the final computed metrics.\", \"- A bibliography with at least two cited arXiv preprints.\"], \"thinking_outline\": [\"1. Review existing literature on information-theoretic methods for LLM watermarking to establish a formal problem setting.\", \"2. Formulate the watermarking problem as a rate-distortion optimization problem, where 'rate' is the watermark strength and 'distortion' is the KL-divergence from the original text distribution.\", \"3. Derive the closed-form solution for the optimal watermarking strategy and the corresponding rate-distortion function D(R).\", \"4. Implement the watermarking algorithm and a simple adversarial paraphraser in Python to test the watermark's robustness.\", \"5. Execute simulations to generate an empirical rate-distortion curve and validate that it aligns with the theoretical derivation within a statistically significant margin.\", \"6. Audit the Python code to check for numerical stability issues in the KL-divergence calculation and ensure the sampling process correctly reflects the derived distribution.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (validation) -> router QA. Verification: The code agent will verify the computability of the math agent's derivation. The second math agent will verify the code's output against its theoretical model. Fallback: If the empirical curve from /code deviates significantly (>10%) from the theoretical one from /math, the task is routed back to the /math agent to re-check the derivation's assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 seminal arXiv papers on LLM watermarking theory.\", \"- [ ] /math: Produce a step-by-step derivation of the rate-distortion function for the specified problem.\", \"- [ ] /code: Develop a Python simulation to apply the watermark and measure its AUPRC against an adversary.\", \"- [ ] /math: Plot empirical results from /code and formally verify their alignment with the initial derivation.\", \"- [ ] /code: Add unit tests to validate the KL-divergence and watermark sampling implementations.\", \"- [ ] /math: Cross-check the core logic of the Python simulation against the assumptions made in the derivation.\", \"- [ ] /router: Perform final QA, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"rate-distortion-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation must be complete, self-contained, and mathematically sound.\", \"- The Python simulation must execute without errors and generate the specified plots and metrics JSON file.\", \"- The empirical rate-distortion curve from the simulation must match the theoretical curve with less than 5% mean squared error.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 197, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the connection between the spectral properties of the loss Hessian and generalization in neural networks. Specifically, can you analyze the Hessian eigenvalue density for a ResNet-18 on CIFAR-10, contrasting its behavior at initialization versus after convergence? Please use concepts from random matrix theory to model the empirical spectral density and derive the update rule for a power iteration method to estimate the top eigenvalue, which serves as a sharpness metric.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:jmlr.org Hessian eigenvalue spectral density generalization deep learning random matrix theory\\\", mode=both)\", \"/math(Derive the power iteration method for finding the dominant eigenvalue of the Hessian matrix H, and formalize the connection between this eigenvalue and the sharpness of the loss landscape for a given network convergence point.)\", \"/code(Implement in Python with PyTorch and CUDA a routine to compute the Hessian spectral density for a ResNet-18 on CIFAR-10 using the Lanczos algorithm, tracking the top eigenvalue during training.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative literature from arXiv and JMLR on Hessian spectral analysis, which is a complex research area. The findings inform the /math agent's formal derivation of the power iteration method for eigenvalue estimation. Finally, the /code agent implements this method to run an empirical experiment, grounding the theory with practical validation on a standard deep learning benchmark.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the power iteration method for Hessian eigenvalue estimation.\", \"- A Python script implementing the spectral analysis on a ResNet-18, including data loading and training loops.\", \"- A JSON file containing the computed top eigenvalue at different training epochs and the final empirical spectral density.\", \"- A plot visualizing the evolution of the Hessian's empirical spectral density over the course of training.\", \"- A summary report citing at least two relevant arXiv papers connecting spectral properties to generalization.\"], \"thinking_outline\": [\"1. Survey foundational literature on the spectral properties of the Hessian in deep neural networks, focusing on its connection to generalization gaps and loss landscape geometry.\", \"2. Formally derive the mathematical procedure for estimating the top eigenvalue (lambda_max) of the Hessian using power iteration or the Lanczos algorithm, and prove its convergence.\", \"3. Implement the derived algorithm in a deep learning framework to empirically measure the Hessian's spectral density for a standard computer vision model.\", \"4. Validate the implementation by checking for numerical stability and comparing the output on a small, known matrix with its analytically computed eigenvalues.\", \"5. Execute the experiment, tracking lambda_max and the full spectral density throughout the training process, and correlate these measurements with the model's training and validation accuracy.\", \"6. Verify that the final report's conclusions are supported by both the theoretical derivations and the empirical results, auditing the plotted data against the raw metric outputs.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent's derivation of the power iteration must match the algorithm implemented by the code agent. The code agent must verify numerical stability and convergence of the eigenvalue estimates. fallback: If convergence is not achieved, the code agent should flag the issue and the math agent may need to suggest an alternative algorithm like Randomized SVD.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv or JMLR on Hessian spectral density and its relation to generalization.\", \"- [ ] /math: Provide a step-by-step derivation of the Lanczos algorithm for estimating the eigenvalues of a symmetric matrix.\", \"- [ ] /math: Formally define 'sharpness' in terms of the maximum Hessian eigenvalue and connect it to generalization bounds.\", \"- [ ] /code: Implement the Lanczos algorithm in PyTorch to compute the Hessian spectral density for a given model and loss.\", \"- [ ] /code: Train a ResNet-18 on CIFAR-10 and use the implemented algorithm to collect spectral data at initialization, mid-training, and convergence.\", \"- [ ] /math: Verify that the algorithm implemented in the code correctly reflects the mathematical derivation provided.\", \"- [ ] /code: Generate plots of the spectral density evolution and a table correlating the top eigenvalue with test accuracy.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"hessian-eigenvalues\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The submitted code must successfully run and reproduce the reported eigenvalue spectrum with the provided seed.\", \"- The mathematical derivation for the eigenvalue estimation algorithm must be complete and free of errors.\", \"- The final report must include a plot showing a clear difference between the Hessian spectral density at initialization versus at convergence.\", \"- The analysis must cite at least two relevant papers with their arXiv IDs, as per the citation policy.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 198, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Natural Policy Gradient update rule for a stochastic policy parameterized by a neural network, assuming a Riemannian manifold structure defined by the Fisher Information Matrix. Implement this using JAX for a continuous control theory benchmark, and analyze its convergence properties against vanilla REINFORCE, paying close attention to the second-order approximation of the KL-divergence constraint.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'natural policy gradient' 'fisher information matrix' convergence analysis control theory\\\", mode=both)\", \"/math(Derive the Natural Policy Gradient update rule by formulating the policy improvement as a constrained optimization problem. Formalize the Fisher Information Matrix for a Gaussian policy and analyze its eigenvalue properties to ensure numerical stability in the control theory context.)\", \"/code(Implement the Natural Policy Gradient algorithm and a REINFORCE baseline in JAX, targeting a classic control problem on a CUDA device. Track reward curves with a 95% confidence interval over multiple seeds to validate performance., using Python)\"], \"route_rationale\": \"The blue /general-search agent first gathers the latest theoretical literature on Natural Policy Gradients. The /math agent then uses these sources to perform the formal derivation. Finally, the /code agent implements the derived algorithm and runs experiments to validate the theory, creating a closed loop from theory to practice.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the Natural Policy Gradient update rule, including the Fisher Information Matrix (FIM) for a Gaussian policy.\", \"A Python script in JAX implementing both the NPG and a baseline REINFORCE agent for a continuous control task.\", \"A summary report (PDF) with plots comparing the learning curves (average reward vs. episodes) of both agents, including a citation list with at least two relevant arXiv papers.\", \"A verification log file confirming the numerical stability of the FIM inverse and the gradient norms during training.\"], \"thinking_outline\": [\"1. Review foundational literature on policy gradients, trust regions, and information geometry, focusing on the derivation of the Natural Policy Gradient (NPG).\", \"2. Formulate the policy improvement step as a constrained optimization problem, maximizing expected reward while constraining the KL-divergence between old and new policies.\", \"3. Derive the NPG update rule using a second-order Taylor expansion of the KL-divergence constraint, identifying the Fisher Information Matrix (FIM) as the Hessian.\", \"4. Implement both NPG (using the derived FIM) and a vanilla REINFORCE baseline in JAX for a continuous control environment.\", \"5. Validate the implementation by checking the FIM's properties (e.g., positive semi-definiteness) and ensuring the gradient updates are numerically stable.\", \"6. Compare the empirical convergence rates and final performance of NPG versus REINFORCE, and verify that the results align with theoretical expectations from the literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Math derivation is correctly implemented in the JAX code, especially the FIM computation and its inverse; fallback: if derivation is flawed or implementation diverges, return to /math with specific error notes from the /code agent).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Natural Policy Gradients and Fisher Information Matrix, focusing on convergence proofs and connections to control theory.\", \"- [ ] /math: Derive the NPG update rule from first principles, showing the second-order approximation of the KL-divergence.\", \"- [ ] /math: Formulate the explicit expression for the Fisher Information Matrix for a multivariate Gaussian policy.\", \"- [ ] /code: Implement the NPG algorithm in JAX, including a stable method for inverting the FIM (e.g., Tikhonov regularization).\", \"- [ ] /code: Implement a baseline REINFORCE agent for comparison on a standard continuous control environment (e.g., Pendulum-v1).\", \"- [ ] /math: Verify that the JAX implementation of the FIM matches the derived mathematical expression by testing with known inputs.\", \"- [ ] /code: Verify that the empirical performance improvement of NPG over REINFORCE is statistically significant across multiple random seeds.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"information-geometry\", \"policy-gradient\"], \"acceptance_criteria\": [\"The mathematical derivation correctly identifies the Fisher Information Matrix as the metric for the steepest ascent direction in policy space.\", \"The JAX implementation runs without numerical errors (e.g., NaN gradients, singular matrix inversion) for at least 1000 training episodes.\", \"The final report shows a statistically significant improvement in sample efficiency or final asymptotic performance for NPG over the REINFORCE baseline, as measured by the primary metric.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 199, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the energy conservation properties of equivariant graph neural networks for simulating a Lennard-Jones fluid. Specifically, can you derive the force-field from a SchNet-like GNN potential, implement a Velocity Verlet integrator, and benchmark the total energy drift over a microcanonical ensemble (NVE) simulation? Contrast this with a baseline analytical potential to quantify the GNN's ability to preserve Hamiltonian dynamics.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org equivariant graph neural network 'energy conservation' molecular dynamics SchNet DimeNet\\\", mode=both)\", \"/math(Derive the force field as the negative gradient of the GNN's potential energy output. Formulate the discrete update rules for the Velocity Verlet algorithm to ensure symplectic integration and minimize long-term energy drift.)\", \"/code(Implement a SchNet GNN in PyTorch Geometric to learn a Lennard-Jones potential. Integrate it with a Velocity Verlet simulation loop, tracking total system energy and force prediction accuracy against the analytical chemistry baseline., using Python)\"], \"route_rationale\": \"The plan starts with a search by the blue general-search agent to gather state-of-the-art models and theoretical grounding. This research informs the /math agent's derivation of the core physics equations (force fields and integrator). Finally, the /code agent implements the derived model and simulation, using the mathematical formulation for verification and the research for architectural choices.\", \"expected_artifacts\": [\"- A LaTeX document with the derivation of the GNN-based force field and the Velocity Verlet update equations.\", \"- A Python script implementing the SchNet model and the NVE simulation loop.\", \"- A plot comparing the total system energy drift over time for the GNN potential vs. the analytical Lennard-Jones potential.\", \"- A summary report citing at least two relevant arXiv papers on equivariant GNNs for MD.\", \"- A JSON file containing the final energy drift metrics and force prediction MAE.\"], \"thinking_outline\": [\"1. Research state-of-the-art equivariant GNN architectures (e.g., SchNet, PaiNN) used for learning interatomic potentials and their performance in conserving energy.\", \"2. Formulate the Hamiltonian for the N-particle system and formally derive the force on each particle as the negative gradient of the GNN's potential energy output (F_i = -\u2207_{r_i} U_GNN).\", \"3. Implement the chosen GNN architecture and the Velocity Verlet integration scheme, ensuring correct handling of periodic boundary conditions.\", \"4. Validate the force-field implementation by comparing the GNN-computed forces against analytical forces from the true Lennard-Jones potential for a set of reference configurations.\", \"5. Run a long NVE simulation, plotting the total system energy (kinetic + potential) as a function of time to visually and quantitatively assess energy drift.\", \"6. Check that the observed energy drift is within physically acceptable bounds and analyze its sources, such as numerical precision or model inaccuracies.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides papers and model architectures to the math agent. The math agent provides the precise force-field and integrator equations to the code agent. The code agent implements these equations and generates plots/metrics. The router QA performs final validation. Verification: The code agent must verify its force implementation against analytical gradients provided by the math agent for a simple two-particle system. Fallback: If verification fails, use PyTorch's autograd as a reference implementation for debugging.\", \"todo_list\": [\"- [ ] /general-search: Find SOTA equivariant GNNs for MD and benchmarks on energy conservation.\", \"- [ ] /math: Derive the force calculation F = -\u2207U_GNN.\", \"- [ ] /math: Formulate the Velocity Verlet integrator steps.\", \"- [ ] /code: Implement the SchNet GNN model and data loaders for atomic positions.\", \"- [ ] /code: Implement the NVE simulation loop using the derived integrator.\", \"- [ ] /math: Verify that the force derivation from the math agent matches the autograd implementation in the code agent's script for a test case.\", \"- [ ] /code: Generate plots of total energy vs. time and compute the final drift metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-network\", \"molecular-dynamics\", \"computational-chemistry\"], \"acceptance_criteria\": [\"- The final report must include a plot showing total energy drift over at least 10,000 timesteps, with the GNN-based simulation demonstrating stability (drift < 0.1% of initial energy).\", \"- The implemented code must pass a verification test where its GNN-computed forces have a Mean Absolute Error (MAE) < 1e-5 relative to analytical forces on a validation set.\", \"- The solution must cite at least two arXiv papers published after 2020 on the topic of learned potentials for molecular dynamics.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 200, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive and implement the Trust Region Policy Optimization (TRPO) algorithm from first principles. Focus on the theoretical justification for using the natural policy gradient and the Fisher Information Matrix (FIM) as the metric tensor. The implementation must use the conjugate gradient method to efficiently compute the FIM inverse-vector product without explicitly forming the FIM, and demonstrate its performance on a classic control theory benchmark like CartPole or Pendulum, analyzing the policy's convergence properties.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Trust Region Policy Optimization natural policy gradient conjugate gradient\\\", mode=rag)\", \"/math(Derive the TRPO surrogate objective function with a Kullback-Leibler (KL) divergence constraint, and formulate the conjugate gradient steps for solving the Fisher-vector product `Fx = g` required for the natural gradient update. Analyze the convergence properties.)\", \"/code(Implement the TRPO algorithm in Python with PyTorch/JAX, including the generalized advantage estimation (GAE) and the conjugate gradient solver. Ensure the implementation includes diagnostics to track KL divergence and policy entropy for debugging and analysis.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather seminal papers on TRPO, providing a solid theoretical foundation. This research informs the /math agent's task of performing the formal derivations. Finally, the verified mathematical formulation is passed to the /code agent for a robust implementation and empirical validation.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the TRPO update rule and the conjugate gradient approximation.\", \"- A Python script implementing the TRPO algorithm and the training loop for a classic control environment.\", \"- A JSON file containing the final evaluation metrics, including average episodic return and KL divergence per epoch.\", \"- A brief report summarizing the empirical results, including learning curves and a discussion of convergence.\", \"- A bibliography file with citations for at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the policy optimization problem as maximizing a surrogate objective subject to a trust region constraint defined by KL divergence.\", \"2. Derive the Taylor series expansion of both the objective and the constraint, leading to a quadratic approximation that can be solved using the natural policy gradient.\", \"3. Show how the natural gradient direction `F^-1 * g` can be calculated efficiently by solving the linear system `Fx = g` using the conjugate gradient algorithm, avoiding explicit FIM computation.\", \"4. Verify the mathematical derivation for the Fisher-vector product, ensuring it can be computed using only first-order gradient information.\", \"5. Implement the complete algorithm, including advantage estimation (GAE) and the line search mechanism to enforce the KL constraint.\", \"6. Validate the implementation by monitoring key diagnostics during training, such as the actual KL divergence between policy updates, policy entropy, and reward convergence.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers (e.g., arXiv IDs) to the /math agent. The /math agent produces a formal derivation (PDF/LaTeX) which the /code agent uses as a specification for implementation. Verification: The /code agent must validate its conjugate gradient solver against the mathematical derivation and check for numerical stability. Fallback: If the conjugate gradient implementation is unstable, the /code agent should report the issue and revert to a simpler steepest descent method with a smaller, fixed learning rate for baseline comparison.\", \"todo_list\": [\"- [ ] /general-search: Find the original TRPO paper by Schulman et al. (arXiv:1502.05477) and at least one other paper discussing its practical implementation details.\", \"- [ ] /math: Derive the surrogate objective function and the quadratic approximation used in TRPO.\", \"- [ ] /math: Detail the steps of the conjugate gradient algorithm for solving `Fx = g` and derive the formula for the Fisher-vector product.\", \"- [ ] /code: Implement the TRPO agent, including policy and value networks, using PyTorch.\", \"- [ ] /code: Implement the conjugate gradient solver and the backtracking line search to enforce the KL constraint.\", \"- [ ] /code: Verify the numerical stability and correctness of the conjugate gradient solver against a simple known linear system.\", \"- [ ] /math: Verify that the code's implementation of the Fisher-vector product aligns with the derived mathematical expression.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"control-theory\"], \"acceptance_criteria\": [\"- The agent must successfully solve the CartPole-v1 environment (average return > 475 over 100 consecutive episodes).\", \"- The provided LaTeX derivation must be mathematically sound and fully justify the implemented algorithm.\", \"- The implementation must correctly use the conjugate gradient algorithm, verified by intermediate diagnostic logs.\", \"- The final report must include a plot of the average episodic return vs. training epochs, demonstrating clear learning progress.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 201, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to design a safe controller for a robotic manipulator using reinforcement learning. Can you help me formulate this as a Constrained Markov Decision Process (CMDP) and derive the policy update rules using Lagrangian relaxation? I'm specifically interested in the primal-dual optimization approach, its convergence guarantees, and how to handle the duality gap.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org constrained markov decision process lagrangian relaxation convergence robotics control theory\\\", mode=web)\", \"/math(Derive the primal-dual update rules for policy optimization in a CMDP using Lagrangian relaxation, analyzing the convergence properties and conditions required for a zero duality gap.)\", \"/code(Implement a Python prototype of the Lagrangian policy gradient for a robotics control theory task, using Bayesian methods to estimate cost uncertainty and track convergence metrics.)\"], \"route_rationale\": \"The blue /general-search agent will first curate foundational literature on Lagrangian methods for CMDPs to ground the solution. Next, the /math agent will perform the formal derivation of the optimization algorithm. Finally, the /code agent will translate the derived mathematics into a working implementation and validate its performance.\", \"expected_artifacts\": [\"A formal mathematical derivation of the primal-dual update rules for the policy and Lagrange multiplier.\", \"A Python script implementing the Lagrangian policy gradient algorithm in a simulated robotics environment (e.g., using Gymnasium).\", \"A summary report in Markdown that includes plots of episodic return and cost, and cites at least two relevant arXiv papers.\", \"A JSON file containing the final evaluation metrics, including average constraint violation and episodic return.\", \"Verification output confirming the alignment between the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Formulate the safe robotics task as a Constrained Markov Decision Process (CMDP), defining the state space, action space, reward function, cost function, and the safety constraint threshold.\", \"2. Construct the Lagrangian for the CMDP objective and derive the dual function. From this, derive the gradient-based update rules for both the policy parameters (primal update) and the Lagrange multiplier (dual update).\", \"3. Implement the derived primal-dual algorithm in Python, ensuring stable learning by carefully tuning the learning rates for both the policy and the multiplier.\", \"4. Validate the implementation by running experiments in a simulated environment. Plot the episodic reward, episodic cost, and the value of the Lagrange multiplier over training epochs to observe learning dynamics.\", \"5. Check the final trained policy to verify that the average constraint violation is below the predefined threshold, confirming that the safety constraint has been successfully enforced.\", \"6. Audit the final report to ensure the cited literature supports the chosen methodology and that the empirical results align with theoretical expectations regarding the reward-safety trade-off.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The /code implementation must correctly instantiate the update equations derived by /math, and the methodology must be supported by the literature found by /general-search; fallback: If /math finds the standard Lagrangian approach has poor convergence, it will ask /general-search to find papers on alternative methods like interior-point policy optimization.)\", \"todo_list\": [\"- [ ] /general-search: Find at least two seminal papers on Lagrangian relaxation for CMDPs, focusing on robotics applications and convergence proofs.\", \"- [ ] /math: Formulate the CMDP and derive the complete primal-dual update equations for the policy parameters and the Lagrange multiplier.\", \"- [ ] /math: Analyze the conditions for strong duality in this CMDP formulation.\", \"- [ ] /code: Implement the algorithm in Python using a standard RL framework like PyTorch and a simulation environment like Gymnasium.\", \"- [ ] /code: Instrument the code to log key metrics: episodic return, episodic cost, and the Lagrange multiplier's value.\", \"- [ ] /math: Verify that the update rule implemented in the /code step is a correct representation of the derived equations.\", \"- [ ] /code: Validate the final policy by running 100 evaluation episodes and calculating the average constraint violation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"constrained-optimization\", \"robotics-safety\"], \"acceptance_criteria\": [\"- The provided mathematical derivation of the primal-dual updates must be complete and correct.\", \"- The Python code must execute and show that the agent's average cost per episode is below the specified constraint threshold after training.\", \"- The final report must contain plots illustrating the convergence of both reward and cost, and cite at least two relevant arXiv papers with IDs.\", \"- The relationship between the learned Lagrange multiplier and the active constraint must be discussed in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 203, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the training dynamics of a LoRA-finetuned Transformer model as a gradient flow on the manifold of low-rank matrices. Derive the corresponding Riemannian gradient and Hessian, relating its spectral properties to training stability and convergence rate. Also, implement a PyTorch prototype to empirically validate the derived Hessian eigenvalue distribution under specific adversarial training conditions, tracking the Fr\u00e9chet inception distance (FID).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org \\\\\\\"gradient flow\\\\\\\" \\\\\\\"low-rank adaptation\\\\\\\" LoRA \\\\\\\"Riemannian manifold\\\\\\\" transformer convergence\\\", mode=both)\", \"/math(Derive the Riemannian gradient and Hessian for LoRA training, treating it as a gradient flow on a low-rank manifold. Analyze the Hessian eigenvalue spectrum's impact on convergence, referencing Euler-Lagrange conditions for the optimization path.)\", \"/code(Implement a PyTorch prototype using CUDA to simulate LoRA fine-tuning under adversarial perturbations. Numerically estimate the Hessian eigenvalue distribution and validate its correlation with FID score degradation., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to build a foundation from authoritative arXiv research on Riemannian optimization and LoRA. This literature review informs the /math agent's formal derivation of the Hessian. Finally, the /code agent implements the theory to provide empirical validation, directly testing the mathematical model's predictions.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Riemannian gradient and Hessian for LoRA.\", \"- A Python script implementing the LoRA fine-tuning experiment and Hessian eigenvalue estimation.\", \"- A summary report (.md) containing plots that correlate the Hessian eigenvalue distribution with FID scores.\", \"- A list of cited arXiv papers with their IDs.\", \"- A verification log confirming the alignment between the mathematical derivation and the code's numerical methods.\"], \"thinking_outline\": [\"1. Review seminal literature on Riemannian optimization for deep learning models, specifically focusing on low-rank matrix manifolds and their application to LoRA.\", \"2. Formulate the LoRA update rule as a continuous-time gradient flow on the manifold of matrices with a fixed rank, defining the appropriate metric tensor.\", \"3. Derive the analytical expressions for the Riemannian gradient and the Hessian operator associated with the training loss function.\", \"4. Implement a numerical method in PyTorch, such as the Lanczos algorithm, to efficiently estimate the top eigenvalues of the empirical Hessian during training.\", \"5. Validate that the numerical Hessian estimation in the code is a correct and stable discretization of the derived analytical Hessian operator.\", \"6. Check that the final report's conclusions about training stability are directly supported by both the mathematical derivation and the empirical correlation with the FID metric.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The code's numerical Hessian estimation must align with the structure of the analytical Hessian from the math derivation; fallback: If the derivation is intractable, the /math agent will flag it and request /general-search to find relevant approximation techniques, which will then be passed back to /math).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least three key papers on LoRA, Riemannian manifolds, and gradient flow in deep learning.\", \"- [ ] /math: Produce the full derivation for the Riemannian gradient and Hessian of the LoRA update.\", \"- [ ] /math: Analyze the spectral properties of the derived Hessian and hypothesize their link to convergence rates.\", \"- [ ] /code: Implement the LoRA fine-tuning loop and a Lanczos-based method for Hessian eigenvalue estimation.\", \"- [ ] /code: Run experiments under adversarial conditions, generating plots of eigenvalue distribution vs. FID score.\", \"- [ ] /math: Verify that the numerical Hessian estimation implemented in the code is a valid discretization of the derived analytical Hessian.\", \"- [ ] /code: Audit the experimental results to confirm they align with the theoretical claims from the math derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"transformer-optimization\", \"gradient-flow\", \"riemannian-geometry\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Riemannian Hessian is provided, is self-consistent, and references concepts from the sourced literature.\", \"- The Python code runs successfully and produces plots clearly correlating Hessian eigenvalues with the FID metric under adversarial stress.\", \"- The final report cites at least 3 relevant arXiv papers and explicitly connects the theoretical derivation to the empirical results.\", \"- The verification log confirms that the code's implementation of the Hessian matches the mathematical formula's structure.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 204, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a custom equivariant graph neural network potential for a molecular dynamics simulation of liquid argon. Please derive the Hamiltonian and the corresponding conservative force field, ensuring the potential is invariant to SE(3) transformations. Then, implement this GNN potential in Python using PyTorch Geometric, validate the energy conservation over a long NVE ensemble simulation, and compare the radial distribution function against established empirical potentials.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org SE(3) equivariant graph neural network molecular dynamics hamiltonian\\\", mode=rag)\", \"/math(Derive the conservative force field F = -\u2207U from the learned GNN potential U(R), ensuring the Hessian of the potential respects permutation and SE(3) equivariance constraints for energy conservation.)\", \"/code(Implement the equivariant GNN potential and a Verlet integrator in Python. Run an NVE simulation for liquid argon, tracking total energy drift and computing the radial distribution function for final state chemistry analysis.)\"], \"route_rationale\": \"The plan starts with the 'blue' /general-search agent to survey state-of-the-art equivariant GNN architectures. The findings inform the /math agent's formal derivation of the Hamiltonian and force field. Finally, the /code agent implements the derived model, runs the physics simulation, and performs validation checks.\", \"expected_artifacts\": [\"A formal mathematical derivation of the SE(3)-equivariant force field from the GNN potential.\", \"A documented Python script implementing the GNN potential, Verlet integrator, and the NVE molecular dynamics simulation.\", \"A verification report containing plots of total energy vs. time (to check conservation) and the computed radial distribution function.\", \"A summary of findings with at least 2 citations to relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Research current SE(3)-equivariant GNN architectures (e.g., SchNet, PaiNN, NequIP) suitable for learning interatomic potentials.\", \"2. Formulate the system's Hamiltonian and derive the analytical expression for the force on each atom as the negative gradient of the GNN's potential energy output.\", \"3. Implement the GNN and the molecular dynamics simulation loop, paying close attention to numerical stability and boundary conditions.\", \"4. Validate the implementation by running an NVE simulation and verifying that the total energy (kinetic + potential) is conserved within a small tolerance, indicating a correct force field derivation.\", \"5. Further audit the physical realism of the model by computing the radial distribution function (RDF) and comparing it to known results for liquid argon.\", \"6. Summarize the model's performance, limitations, and cite the foundational literature.\"], \"handoff_plan\": \"/general-search -> /math (verification: mathematical formulations in papers are sound) -> /code (verification: implementation logic matches the derived force field equations; fallback: if energy is not conserved, return to /math to check derivation for errors) -> router QA (verification: all artifacts generated, metrics meet acceptance criteria).\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 state-of-the-art SE(3) equivariant GNN architectures for molecular potentials from arXiv.\", \"- [ ] /math: Derive the force field F = -\u2207U from the GNN potential U, explicitly showing how equivariance is maintained.\", \"- [ ] /code: Implement the GNN potential and a Verlet integrator for a system of argon atoms.\", \"- [ ] /code: Run a 10,000-step NVE simulation and log the total system energy and atomic positions at each step.\", \"- [ ] /math: Verify that the implemented force calculation in the code correctly reflects the gradient of the potential derived earlier.\", \"- [ ] /code: Compute and plot the radial distribution function from the simulation trajectory and check for physical realism.\", \"- [ ] /router: Perform final QA check on all artifacts, ensuring acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"hamiltonian-mechanics\"], \"acceptance_criteria\": [\"* The mathematical derivation of the force field must be complete and consistent with Hamiltonian mechanics.\", \"* The NVE simulation must demonstrate energy conservation, with a total energy drift of less than 0.1% over 10,000 timesteps.\", \"* The computed radial distribution function must show characteristic peaks for a liquid state, comparable to established argon models.\", \"* The final report must cite at least two relevant arXiv preprints on equivariant GNNs for physics.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 205, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a Python implementation of a persistent homology-based regularizer for a generative diffusion model. The goal is to analyze the manifold structure of the generated data cloud using Vietoris-Rips complexes and penalize the model for creating spurious topological features (e.g., undesirable Betti numbers). Please derive the necessary gradients for this topological loss term, implement it using PyTorch and the Gudhi library, and validate its convergence properties on a synthetic 'Swiss roll with a hole' dataset, measuring both FID and the stability of the persistence diagrams.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'persistent homology' 'diffusion model' regularization\\\", mode=both)\", \"/math(Derive the gradient of a persistence-based loss for manifold learning, analyzing convergence properties and its relation to the Hessian eigenvalue spectrum.)\", \"/code(Implement the full CUDA-accelerated pipeline in PyTorch, integrating the Gudhi library to compute persistence diagrams from Vietoris-Rips complexes, and track FID score for quality control., using Python)\"], \"route_rationale\": \"The plan begins with a literature survey using the blue /general-search agent to ground the approach in existing research. Next, /math is used for the formal derivation of the novel loss function's gradient, which is a prerequisite for implementation. Finally, /code implements the derived mathematics and validates the entire system, leveraging GPU acceleration for the computationally intensive TDA and diffusion steps.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the topological loss gradient.\", \"- A Python script implementing the diffusion model with the persistent homology regularizer.\", \"- A report including persistence diagram plots, FID score progression, and a list of at least two cited arXiv papers.\", \"- A verification summary confirming the alignment between the mathematical derivation and the code's numerical gradients.\"], \"thinking_outline\": [\"1. Research state-of-the-art methods for integrating topological constraints into deep generative models, focusing on persistence-based losses for diffusion models.\", \"2. Formulate a differentiable loss function based on the persistence of the H1-homology group in the Vietoris-Rips filtration of point clouds sampled from the model.\", \"3. Derive the analytical subgradient of this topological loss with respect to the generated data points, preparing it for backpropagation.\", \"4. Implement the complete training pipeline in Python, combining a standard diffusion model with a TDA library (Gudhi) to compute the loss and its gradient.\", \"5. Verify the correctness of the gradient implementation by comparing the analytical gradient values with those obtained through numerical finite differences on a toy example.\", \"6. Audit the final trained model by visually inspecting the persistence diagrams of the generated data and quantifying the improvement in topological accuracy and FID score over a baseline.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation which is handed to the code agent for implementation. The code agent delivers the final implementation, plots, and metrics for review. Verification: The router will cross-reference the math derivation against the code's gradient logic and check if the final persistence diagrams match the target topology. Fallback: If the gradient derivation is intractable, the router will task /general-search to find alternative differentiable topological loss formulations.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize key papers on topological regularization for diffusion models (verify results)\", \"- [ ] /math: Produce a complete derivation for the gradient of the H1-persistence loss function.\", \"- [ ] /code: Implement the diffusion model and the data generation pipeline for the 'Swiss roll with a hole' dataset.\", \"- [ ] /code: Integrate the Gudhi library to compute the Vietoris-Rips persistence diagram and implement the topological loss backpropagation.\", \"- [ ] /math: Verify that the analytical gradient derived matches the numerical gradient computed from the Python code.\", \"- [ ] /code: Train the model, generate all plots for the final report (persistence diagrams, FID scores), and package the code.\", \"- [ ] Router QA: Review all artifacts for correctness, consistency, and adherence to acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"persistent-homology\"], \"acceptance_criteria\": [\"- The final Python script must run and train a model that demonstrably reduces the persistence of spurious H0 and H1 features compared to a baseline without the regularizer.\", \"- The submitted LaTeX document must contain a clear, step-by-step mathematical derivation of the topological loss gradient.\", \"- The persistence diagram generated from the final model's output must show a single, high-persistence point corresponding to the H1-homology group (the 'hole').\", \"- The Fr\u00e9chet Inception Distance (FID) score must not degrade by more than 15% compared to the baseline model, ensuring visual quality is maintained.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 206, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the performance of the 7-qubit Steane code. Please derive the first-order approximation for the logical error rate per cycle under a depolarizing noise model with physical error probability `p`. Then, develop a Monte Carlo simulation in Python to validate this theoretical threshold. The analysis must incorporate recent benchmarks for fault-tolerant Clifford gate fidelities from the literature.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 7-qubit Steane code logical error rate depolarizing noise fault-tolerant Clifford gates\\\", mode=rag, includes optimality)\", \"/math(Derive the logical error rate P_L as a function of physical error rate p for the Steane code, analyzing the combinatorial complexity of weight-1 and weight-2 error events and referencing recent arXiv findings.)\", \"/code(Implement a Monte Carlo simulation in Python to model the Steane code's error correction cycle under a depolarizing noise model, tracking logical error convergence and performing statistical bootstrap analysis.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather current literature on Steane code performance and gate fidelities. This information is passed to the math agent to construct a formal, analytical derivation of the error rate. Finally, the code agent implements a simulation to numerically verify the mathematical model, providing a robust, cross-validated result.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the logical error rate, showing P_L is proportional to p^2.\", \"Python source code for the Monte Carlo simulation, including a requirements file and unit tests for the syndrome decoder.\", \"A JSON file containing the raw simulation results (physical vs. logical error rates) and the fitted model parameters.\", \"A final report in PDF format summarizing the methodology, comparing analytical vs. simulation results with a log-log plot, and providing at least 2 citations to relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Review foundational literature on the 7-qubit Steane code, its stabilizer generators, and the standard depolarizing noise channel to establish the physical model.\", \"2. Formulate the mathematical derivation for the logical error rate P_L, focusing on the dominant error events (weight-1 correctable, weight-2 uncorrectable) and expressing it as P_L \u2248 C * p^2.\", \"3. Implement a Monte Carlo simulation of the full error correction cycle: state preparation, noise application, syndrome measurement, and recovery operation.\", \"4. Validate the simulation's syndrome decoding logic against a known set of single-qubit Pauli error test cases to ensure correctness.\", \"5. Execute the simulation across a range of physical error probabilities `p` and fit the resulting logical error rates to the derived analytical model P_L \u2248 C * p^2 to estimate the constant C.\", \"6. Verify that the simulated results converge to the analytical prediction in the low-p limit and document any deviations, providing a statistical analysis of the fit's quality (e.g., chi-squared test).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code agent validates math agent's P_L \u2248 C*p^2 formula by fitting simulation data; fallback: if fit is poor, router re-engages math agent to check for missing higher-order terms).\", \"todo_list\": [\"- [ ] /general-search: Find recent (last 3 years) arXiv papers on Steane code performance and fault-tolerant gate benchmarks.\", \"- [ ] /math: Derive the first-order approximation for the logical error rate `P_L` as a function of `p`.\", \"- [ ] /code: Implement the Monte Carlo simulation for the 7-qubit Steane code's error correction cycle.\", \"- [ ] /code: Execute the simulation, collect data, and plot logical vs. physical error rates on a log-log scale.\", \"- [ ] /math: Verify that the simulation data fits the derived `P_L \u2248 C*p^2` model and provide a best-fit estimate for the constant C.\", \"- [ ] /code: Validate the Python code's syndrome decoding logic against known single-qubit error test cases.\", \"- [ ] /router: Perform final QA review, ensuring the analytical derivation matches the simulation's asymptotic behavior and all artifacts are generated.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"fault-tolerance\"], \"acceptance_criteria\": [\"The derived logical error rate `P_L` must be shown to be proportional to `p^2` for small `p`.\", \"The Monte Carlo simulation code must be runnable and reproduce the reported logical error rates within a 5% statistical margin given the same random seed.\", \"The final report must correctly cite at least two relevant arXiv preprints on quantum error correction or fault-tolerant computation.\", \"The comparison plot must show clear agreement between the analytical slope (a slope of 2 on a log-log plot) and the simulation data in the low-p regime (p < 1e-2).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 207, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to design a differentiable AutoML pipeline for neural architecture search under federated learning constraints. Specifically, derive the bilevel optimization objective for DARTS with a relaxed search space and formulate a computationally tractable hypergradient approximation using the Neumann series to avoid explicit Hessian inversion. Please provide the mathematical derivation, analyze the convergence guarantees concerning the spectral radius of the weight update operator, and implement the search algorithm in JAX to find an optimal convolutional cell on the CIFAR-10 dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org differentiable architecture search federated learning hypergradient approximation Neumann series DARTS\\\", mode=both)\", \"/math(Derive the federated bilevel optimization objective and the hypergradient approximation using the Neumann series. Analyze the convergence conditions based on the Hessian eigenvalue spectrum and constraints from the federated setting.)\", \"/code(Implement the DARTS search loop in JAX with CUDA acceleration, including the derived hypergradient estimator. Validate the architecture performance against a baseline, tracking AUPRC metrics and resource usage., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on differentiable AutoML and hypergradient methods. This research grounds the formal derivation, which is handled by the /math agent. Finally, the /code agent implements and validates the derived algorithm, translating theory into a practical, verifiable artifact.\", \"expected_artifacts\": [\"A formal mathematical derivation of the federated DARTS bilevel objective and its second-order hypergradient approximation.\", \"A summary report citing at least 3 relevant arXiv papers on the topic.\", \"A Python script using JAX that implements the complete NAS pipeline and can reproduce the final architecture evaluation.\", \"A verification log detailing the numerical stability checks for the inverse Hessian approximation.\", \"Plots showing the validation accuracy and loss curves during the architecture search phase.\"], \"thinking_outline\": [\"1. Research existing literature to identify state-of-the-art techniques for second-order optimization in Differentiable Architecture Search (DARTS) and their adaptations for federated or constrained environments.\", \"2. Formulate the precise bilevel optimization problem, defining the inner loop for weight training and the outer loop for architecture parameter updates under federated averaging constraints.\", \"3. Derive the hypergradient using the implicit function theorem and approximate the inverse Hessian-vector product using a truncated Neumann series, detailing all assumptions.\", \"4. Verify the mathematical derivation, paying close attention to the conditions under which the Neumann series converges and how this impacts the stability and correctness of the hypergradient estimate.\", \"5. Implement the complete algorithm, including the search space, the bilevel optimizer, and the evaluation protocol on a standard benchmark like CIFAR-10.\", \"6. Validate the final discovered architecture's performance against a known baseline and audit the implementation's numerical outputs against the theoretical derivation for key steps.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent will provide foundational arXiv papers to the /math agent. The /math agent will produce a formal PDF derivation, which the /code agent will use as a specification. The /code agent's implementation and metric logs will be reviewed against the derivation for the final QA. Verification: The router will check that the JAX implementation of the Hessian approximation matches the formula derived by /math. Fallback: If the derivation is numerically unstable, the /math agent will be re-engaged to investigate regularization or alternative approximations.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22653 recent arXiv papers on second-order DARTS variants and hypergradient estimation.\", \"- [ ] /math: Derive the federated bilevel optimization objective and the Neumann series approximation for the hypergradient.\", \"- [ ] /math: Analyze and state the convergence conditions for the hypergradient approximation.\", \"- [ ] /code: Implement the DARTS search space, bilevel optimizer, and evaluation logic in JAX.\", \"- [ ] /code: Profile the search process and report final architecture performance on CIFAR-10.\", \"- [ ] /math: Verify that the code agent's implementation of the inverse-Hessian-vector product is a correct translation of the derived mathematical formula.\", \"- [ ] /code: Add unit tests to validate the gradient calculations for the inner and outer loop updates.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"automl\", \"bilevel-optimization\", \"neural-architecture-search\"], \"acceptance_criteria\": [\"- The mathematical derivation of the hypergradient must be complete and self-contained.\", \"- The JAX implementation must successfully run the architecture search to completion and produce a valid architecture specification.\", \"- The final report must cite at least 3 relevant arXiv papers with their IDs.\", \"- The discovered architecture must achieve a test accuracy on CIFAR-10 that is competitive with published DARTS baselines.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 208, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a physics-informed neural network (PINN) to solve the inverse problem for the 2D incompressible Navier-Stokes equations. Specifically, I want to infer the unknown viscosity and an inlet velocity profile from sparse, noisy velocity field data. Please formulate a Bayesian inference framework to quantify uncertainty in the inferred parameters, leveraging Fourier feature mappings to handle high-frequency solution components and provide a rigorous derivation of the evidence lower bound (ELBO) objective.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Bayesian physics informed neural networks Navier-Stokes inverse problem uncertainty quantification Fourier features\\\", mode=web)\", \"/math(Derive the variational inference objective (ELBO) for a Bayesian PINN constrained by the Navier-Stokes PDE residual, incorporating a Gaussian prior over network weights and defining the likelihood from sparse observation data. Analyze the convergence properties of this formulation.)\", \"/code(Implement the Bayesian PINN in JAX with the Flax/NumPyro stack. Use Fourier feature mapping for the input coordinates, validate the PDE residual computation for the energy equation, and track ELBO convergence and posterior variance metrics during training., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find state-of-the-art architectures and loss formulations from recent arXiv papers. This research informs the /math agent, which will perform the rigorous derivation of the variational inference objective. Finally, the /code agent implements the derived mathematical model in a suitable framework, JAX, which is excellent for gradient-based optimization and probabilistic programming.\", \"expected_artifacts\": [\"A formal mathematical derivation of the ELBO for the Bayesian PINN applied to the Navier-Stokes equations.\", \"A Python script implementing the model, training loop, and inference logic using JAX/NumPyro.\", \"A summary report including plots of the inferred posterior distributions for viscosity and inlet velocity parameters, and a comparison against ground truth.\", \"A JSON file containing final metrics: Mean Squared Error of the inferred parameters, final ELBO value, and mean PDE residual.\", \"A bibliography with at least two cited arXiv preprints on Bayesian PINNs.\"], \"thinking_outline\": [\"1. Survey existing literature on Bayesian PINNs and their application to inverse problems in fluid dynamics, focusing on uncertainty quantification.\", \"2. Formulate the problem mathematically: define the neural network architecture with Fourier features, specify the prior distributions for the unknown parameters (viscosity, etc.) and network weights, and write down the full evidence lower bound (ELBO) objective, including the data likelihood term and the PDE residual as a soft constraint.\", \"3. Implement the model and the Stochastic Variational Inference (SVI) training loop in JAX/NumPyro.\", \"4. Verify the correctness of the coded Navier-Stokes residual function by comparing its output against a manually computed example with known analytical derivatives.\", \"5. Train the model on a synthetic dataset generated from a known ground truth, monitoring the ELBO, PDE residual, and parameter posterior estimates.\", \"6. Validate the final uncertainty estimates by checking if the ground truth parameters are captured within the 95% credible intervals of the inferred posterior distributions.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to the /math agent. The /math agent's formal derivation of the ELBO is the direct specification for the /code agent's implementation. A final review by the router ensures the code's loss function exactly matches the derivation. (Verification: Cross-reference equations in the math document with the loss implementation in the code; Fallback: If a discrepancy is found, route back to /math for clarification on the derivation.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Bayesian PINNs for inverse fluid problems.\", \"- [ ] /math: Derive the complete ELBO for the Navier-Stokes inverse problem, clearly defining all terms.\", \"- [ ] /math: Verify that the derived loss function's components are dimensionally consistent.\", \"- [ ] /code: Implement the PINN architecture with Fourier feature mapping and the SVI training loop.\", \"- [ ] /code: Implement a unit test to verify the Navier-Stokes residual calculation for a simple flow field.\", \"- [ ] /code: Train the model and generate plots of the posterior distributions and the learned velocity field.\", \"- [ ] /router: Perform final QA by checking that the implemented loss in code matches the /math derivation and that acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"bayesian-inference\", \"partial-differential-equations\"], \"acceptance_criteria\": [\"- The implemented code must successfully infer the ground truth viscosity parameter within a 10% margin of error.\", \"- The final report must include a clear mathematical derivation of the ELBO objective function.\", \"- The 95% credible interval of the inferred viscosity posterior must contain the ground truth value.\", \"- The solution must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 209, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating Bayesian meta-learning for few-shot image classification. I need to derive the evidence lower bound (ELBO) for a hierarchical model where task-specific parameters are drawn from a shared meta-prior. Then, implement a variant of Reptile or MAML that uses amortized variational inference with a reparameterization trick to optimize this ELBO on the Mini-ImageNet benchmark, and validate the convergence properties against a non-Bayesian baseline.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian model-agnostic meta-learning amortized variational inference ELBO reparameterization\\\", mode=web)\", \"/math(Based on the literature survey from arxiv, formally derive the evidence lower bound (ELBO) for the hierarchical Bayesian model, ensuring the reparameterization trick is correctly applied to handle the stochastic gradient estimation for both meta-parameters and task-specific parameter posteriors. Track the convergence criteria.)\", \"/code(Implement the Bayesian meta-learning algorithm in PyTorch, using the derived ELBO as the loss. Validate the implementation on the Mini-ImageNet few-shot benchmark, tracking both classification accuracy and ELBO convergence against a non-Bayesian MAML baseline. Utilize CUDA for acceleration., using Python)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature to ground the derivation. /math then provides the formal mathematical foundation required for the implementation. Finally, /code translates the theory into a working experiment, providing empirical validation and performance metrics.\", \"expected_artifacts\": [\"A LaTeX document detailing the full ELBO derivation with all intermediate steps.\", \"Python source code for the Bayesian meta-learning implementation and the non-Bayesian baseline.\", \"A results report comparing the Bayesian vs. non-Bayesian MAML on Mini-ImageNet, including plots of accuracy and loss curves.\", \"A bibliography file with at least two cited arXiv papers.\", \"A verification log confirming the gradient implementation matches the mathematical derivation via numerical checks.\"], \"thinking_outline\": [\"1. Research foundational papers on Bayesian meta-learning (e.g., Bayesian MAML) and amortized variational inference to establish the theoretical framework.\", \"2. Formulate the hierarchical generative model for few-shot learning and derive the corresponding evidence lower bound (ELBO) objective function.\", \"3. Implement the meta-training loop using the reparameterization trick to get low-variance gradients of the ELBO.\", \"4. Validate the gradient computation numerically to check for implementation errors against the mathematical derivation.\", \"5. Conduct experiments on a standard few-shot benchmark (e.g., Mini-ImageNet) and compare performance (accuracy, ELBO values) against a non-Bayesian baseline.\", \"6. Audit the final results, ensuring the observed convergence behavior is consistent with theoretical expectations and that the reported metrics are reproducible.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code's loss function must match the /math derivation and reported metrics must align with acceptance criteria; fallback: if derivation is flawed or implementation diverges, return to /math with feedback from the /code agent).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 seminal papers on Bayesian meta-learning and amortized VI.\", \"- [ ] /math: Derive the ELBO for the hierarchical Bayesian few-shot model, showing all steps.\", \"- [ ] /code: Implement the base MAML algorithm to serve as a performance baseline.\", \"- [ ] /code: Implement the Bayesian meta-learning algorithm using the derived ELBO and reparameterization trick.\", \"- [ ] /math: Verify that the gradient estimator implemented in the code is an unbiased estimator of the gradient of the derived ELBO.\", \"- [ ] /code: Run experiments on Mini-ImageNet, generating plots for accuracy and loss convergence.\", \"- [ ] router: QA review of final report, code, and derivations against acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"bayesian-inference\", \"few-shot-learning\"], \"acceptance_criteria\": [\"- The LaTeX derivation of the ELBO must be mathematically sound, complete, and fully justified.\", \"- The Python code must successfully run on the Mini-ImageNet 5-way 1-shot benchmark and achieve an accuracy demonstrably better than random chance.\", \"- The final report must include convergence plots for both the Bayesian and non-Bayesian models and cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 210, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the analytical gradients for a hardware-efficient, entangling layered ansatz for the Variational Quantum Eigensolver (VQE). Implement this in Python using PennyLane to find the ground state energy of the LiH molecule with a STO-3G basis. Your implementation must include a parameter shift rule for gradient calculation and validate the convergence against the exact eigenvalue from classical FCI.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org hardware-efficient ansatz VQE parameter shift rule LiH STO-3G\\\", mode=both)\", \"/math(Derive the explicit unitary matrix representation and the analytical gradient expressions via the parameter shift rule for a two-qubit rotation gate within the proposed chemistry-inspired ansatz, ensuring the resulting eigenvalue is real.)\", \"/code(Implement the VQE algorithm in Python/PennyLane for the LiH Hamiltonian using the derived ansatz and gradients. Validate convergence of the ground state energy against the known FCI value and track the optimizer's performance.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather current literature on efficient VQE ansatze and gradient methods. The findings inform the /math agent, which will perform the rigorous derivation of the analytical gradients. Finally, the /code agent implements the derived mathematical framework in a quantum simulation, running experiments to validate the theoretical results against established benchmarks.\", \"expected_artifacts\": [\"A LaTeX document detailing the derivation of the hardware-efficient ansatz and the analytical gradients using the parameter shift rule.\", \"A Python script (Jupyter Notebook preferred) implementing the VQE simulation for LiH, including the custom ansatz and optimizer loop.\", \"A summary report comparing the VQE-computed ground state energy against the full configuration interaction (FCI) value, with plots showing energy convergence.\", \"A bibliography citing at least two relevant arXiv preprints on VQE or quantum chemistry.\", \"A verification log documenting the numerical check of the analytical gradient against a finite-difference approximation.\"], \"thinking_outline\": [\"1. Research hardware-efficient and chemistry-inspired ansatze suitable for near-term quantum devices and the LiH molecule, focusing on expressibility and trainability.\", \"2. Formulate the mathematical representation of the chosen ansatz and derive the analytical gradients for its parameterized gates using the parameter shift rule.\", \"3. Implement the molecular Hamiltonian for LiH in the STO-3G basis and the custom VQE ansatz using a quantum computing framework like PennyLane.\", \"4. Validate the analytical gradient implementation by comparing its output to a numerical finite-difference approximation for a set of random parameters.\", \"5. Execute the VQE optimization loop to find the ground state energy and check for convergence towards the known FCI eigenvalue.\", \"6. Analyze the results, plotting the energy convergence curve and discussing any observed optimization challenges like barren plateaus.\"], \"handoff_plan\": \"/general-search -> /math (verification: ensure chosen ansatz has established literature support) -> /code (verification: numerically check analytical gradients from /math against finite differences; fallback: revert to /math for derivation correction if gradients mismatch) -> router QA (verification: final energy error is within chemical accuracy of 1.6e-3 Hartree; fallback: re-run /code with different optimizer/hyperparameters).\", \"todo_list\": [\"- [ ] /general-search: Find reference papers on hardware-efficient VQE ansatze and parameter shift rules for molecular Hamiltonians.\", \"- [ ] /math: Derive the analytical gradients for all parameterized gates in the chosen ansatz.\", \"- [ ] /code: Construct the LiH molecular Hamiltonian (STO-3G basis) using PennyLane's quantum chemistry module.\", \"- [ ] /code: Implement the VQE algorithm with the custom ansatz and the analytically derived gradients.\", \"- [ ] /math: Verify the mathematical derivation of the parameter shift rule aligns with the literature and the code implementation's logic.\", \"- [ ] /code: Numerically validate the implemented analytical gradients against a finite-difference approximation to ensure correctness.\", \"- [ ] /code: Run the simulation, plot the energy convergence, and compare the final result to the FCI benchmark.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"variational-quantum-eigensolver\", \"quantum-chemistry\"], \"acceptance_criteria\": [\"The final computed ground state energy for LiH must be within 1.6 x 10^-3 Hartree (chemical accuracy) of the FCI calculated value.\", \"The Python implementation must correctly use the parameter shift rule for gradients, verified by a numerical comparison to finite differences with a tolerance of 1e-6.\", \"The final report must include a convergence plot showing the energy value per optimization step.\", \"The submission must cite at least two relevant arXiv preprints that informed the choice of ansatz or methodology.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 212, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the identifiability of causal effects in semi-Markovian structural causal models (SCMs) under multiple, simultaneous interventions. Can you derive a formal proof for the conditions under which the post-interventional distribution P(Y|do(X=x), do(Z=z)) is identifiable from the observational distribution, given a specific directed acyclic graph (DAG) structure where a backdoor path from X to Y is mediated by an unobserved confounder? Please use Pearl's do-calculus and provide the resulting functional mapping from P(V) to P(Y|do(X,Z)).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org causal identifiability semi-markovian SCM joint interventions do-calculus econometrics\\\", mode=rag)\", \"/math(Derive the proof for identifying the causal effect P(Y|do(X,Z)) from observational data P(V) using the rules of do-calculus. The derivation must account for the specified Bayesian network structure with unobserved confounders and provide the final functional form.)\", \"/code(Implement the derived causal identifiability formula in Python using a symbolic library like SymPy to represent the graph and the Bayesian probability manipulations. Validate the formula against a simple, known case from the arxiv literature.)\"], \"route_rationale\": \"The blue /general-search agent is needed first to gather state-of-the-art literature on causal identifiability in complex SCMs. The /math agent will then perform the core task of deriving the formal proof using do-calculus. Finally, the /code agent will translate the abstract mathematical formula into a concrete symbolic representation for verification and clarity.\", \"expected_artifacts\": [\"A formal mathematical proof in LaTeX detailing the step-by-step application of do-calculus rules for the specified causal identifiability problem.\", \"A Python script (using SymPy) that symbolically represents the SCM's DAG and the derived post-interventional distribution formula.\", \"A summary report (PDF) integrating the proof, the code representation, and a discussion of the assumptions and limitations.\", \"A bibliography citing at least 2 relevant arXiv preprints on causal identifiability.\", \"A verification log documenting the symbolic check of the derived formula against a known case.\"], \"thinking_outline\": [\"1. Formalize the user's query into a precise graphical model (DAG) and define the set of observed variables V, intervened variables {X, Z}, outcome Y, and any unobserved confounders U.\", \"2. Apply the three rules of do-calculus systematically to manipulate the expression P(Y|do(X=x), do(Z=z)) with the goal of removing all `do` operators.\", \"3. Verify at each step of the derivation that the graph-theoretic conditions for applying each rule of do-calculus (e.g., d-separation) are met by the specified DAG.\", \"4. Once all `do` operators are eliminated, express the final result as an explicit functional of the observational distribution P(V).\", \"5. Validate the final derived formula by implementing it symbolically and testing it on a simpler, known graphical structure where the identifiability result is well-established, checking for consistency.\", \"6. Document the complete proof, including all assumptions, graphical criteria checks, and the final functional form, alongside the verification code.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent cross-references its derivation against principles found by /general-search; /code agent's symbolic implementation must match the final formula from /math. fallback: If identifiability is not possible, /math must prove non-identifiability and explain which graphical criterion fails.)\", \"todo_list\": [\"- [ ] /general-search: Execute query 'site:arxiv.org causal identifiability semi-markovian SCM joint interventions do-calculus' to find foundational papers.\", \"- [ ] /math: Define the SCM and DAG based on the user's problem statement.\", \"- [ ] /math: Perform the step-by-step derivation using do-calculus to prove identifiability.\", \"- [ ] /code: Implement the SCM's DAG structure using a suitable Python library (e.g., NetworkX, SymPy).\", \"- [ ] /code: Translate the final mathematical formula from the /math agent into a symbolic expression.\", \"- [ ] /math: Verify that the symbolic expression from /code correctly represents the derived mathematical formula.\", \"- [ ] /code: Validate the implemented formula against a known textbook example of causal identifiability.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"structural-causal-model\", \"do-calculus\"], \"acceptance_criteria\": [\"The final output must include a complete, step-by-step mathematical proof of identifiability using do-calculus rules.\", \"A runnable Python script must be provided that symbolically represents the derived formula and passes a verification test against a known case.\", \"The proof must correctly identify all graphical conditions (e.g., d-separation criteria) required for each step of the derivation.\", \"The report must cite at least two relevant academic papers (arXiv preprints preferred) on causal identifiability.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 213, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm new to AutoML and want to understand the core concept of differentiable architecture search (DARTS). Can you explain the continuous relaxation of the search space and derive its bilevel optimization objective?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Differentiable Architecture Search' continuous relaxation bilevel optimization AutoML\\\", mode=both)\", \"/math(Derive the bilevel optimization objective for DARTS, defining the continuous relaxation of the architecture representation and explaining the gradient approximation for architecture parameters to ensure training convergence.)\", \"/code(Implement a minimal DARTS search cell in Python using PyTorch, demonstrating the mixed operation computation and the update step for architecture parameters. Include a CUDA check for GPU availability.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather the seminal DARTS paper and authoritative tutorials. This grounds the subsequent steps in verified sources. Next, /math is used to formally derive the core mathematical concepts. Finally, /code translates the mathematical theory into a practical, minimal implementation to solidify understanding.\", \"expected_artifacts\": [\"A summary report explaining the DARTS method, including its continuous relaxation and optimization strategy, with proper citations.\", \"A formal mathematical derivation of the bilevel optimization objective and the approximate gradient calculation for architecture parameters.\", \"A Python script (`darts_cell.py`) implementing a simplified DARTS search cell using PyTorch.\", \"A verification log confirming that the gradient computation in the code aligns with the mathematical derivation.\"], \"thinking_outline\": [\"1. Research the foundational principles of Differentiable Architecture Search (DARTS), focusing on how it achieves a continuous relaxation of the discrete architectural search space.\", \"2. Formulate the bilevel optimization problem, clearly defining the upper-level objective (minimizing validation loss w.r.t. architecture parameters \u03b1) and the lower-level objective (minimizing training loss w.r.t. network weights w).\", \"3. Derive the approximate gradient for the architecture parameters (\u03b1) using the finite difference approximation or the implicit function theorem as described in the original paper.\", \"4. Implement a simplified version of a DARTS cell in PyTorch, focusing on the `MixedOp` class which computes a softmax-weighted sum of candidate operations.\", \"5. Validate the implementation by performing a numerical gradient check to verify that the backpropagated gradients for \u03b1 match the derived mathematical formula for a small, controlled example.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses these papers to create a formal derivation -> /code implements the derivation. The result from /code is then checked against the /math derivation. Handoff to router QA for final review. (Verification: The gradient logic in the code must numerically match the final expression in the math derivation. Fallback: If a mismatch occurs, the task is returned to the /math agent to re-check the derivation and its assumptions).\", \"todo_list\": [\"- [ ] Use /general-search to retrieve the original DARTS paper (Liu et al., 2018) and key tutorials on its methodology.\", \"- [ ] Use /math to formally derive the bilevel optimization objective and the first-order gradient approximation for architecture parameters.\", \"- [ ] Use /code to implement a simplified DARTS search cell, including the MixedOp, in PyTorch.\", \"- [ ] Verify that the gradient computation for architecture parameters in the code numerically aligns with the mathematical derivation.\", \"- [ ] Submit all artifacts (report, derivation, code, verification log) for router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"automl\", \"neural-architecture-search\", \"differentiable-search\"], \"acceptance_criteria\": [\"The mathematical derivation correctly presents the bilevel objective and the final gradient approximation formula for the architecture parameters.\", \"The Python code successfully implements a `MixedOp` that computes a weighted sum of operations and demonstrates a valid update step for the architecture parameters.\", \"The final report cites at least two authoritative sources, including the original DARTS arXiv paper with its ID.\", \"The verification log shows a numerical gradient check with a mismatch error below a small tolerance (e.g., 1e-5).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"computation_guidance\": {\"primary\": \"Report the validation loss on a hold-out set after a fixed number of search epochs.\", \"secondary\": \"Compute the L2 norm of the difference between the analytical gradient (from /math) and a numerically computed gradient (e.g., using finite differences) for the architecture parameters.\"}}}", "categories": ["metrics_computation", "metrics_guidance"]}
{"index": 214, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous analysis and implementation of a federated learning system using homomorphic encryption for secure aggregation. Formally derive the convergence bounds for FedAvg under the Paillier cryptosystem, assuming a non-IID data distribution across clients. Then, implement this system in Python, simulating client updates and the encrypted aggregation server, and empirically validate the derived convergence rate against a baseline without encryption, measuring the computational overhead.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning convergence proof homomorphic encryption non-IID\\\", mode=rag)\", \"/math(Derive the convergence proof for federated averaging under Paillier homomorphic encryption, analyzing the impact of encryption noise and non-IID data distributions on the global model's Hessian eigenvalue spectrum.)\", \"/code(Implement a federated learning simulation in Python with a Paillier encryption scheme for secure aggregation. Track model convergence metrics and measure the compute overhead introduced by cryptographic operations.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather seminal papers on the convergence of private federated learning. This literature informs the /math agent's formal derivation of the convergence bounds. Finally, the /code agent implements the system based on the verified mathematical specifications to provide empirical validation.\", \"expected_artifacts\": [\"- A formal mathematical proof of convergence in a PDF document, including all assumptions and lemmas.\", \"- A commented Python script implementing the federated learning simulation with the Paillier cryptosystem.\", \"- A JSON file containing the final metrics (convergence rate, aggregation overhead).\", \"- A verification report comparing theoretical bounds to empirical results, including plots of the loss curves.\"], \"thinking_outline\": [\"1. Research existing literature on convergence proofs for FedAvg, specifically with additive homomorphic encryption schemes like Paillier under non-IID settings.\", \"2. Formulate the optimization problem and formally derive the one-step progress bound, accounting for noise from encryption and data heterogeneity (client drift).\", \"3. Implement the Paillier cryptosystem and the federated averaging protocol with the secure aggregation step, ensuring cryptographic best practices.\", \"4. Design an experiment simulating a non-IID data distribution (e.g., using a Dirichlet distribution over labels on CIFAR-10) to test the implementation.\", \"5. Validate that the empirical convergence rate from the simulation falls within the derived theoretical bounds and analyze any discrepancies.\", \"6. Audit the cryptographic implementation for potential vulnerabilities, such as incorrect parameter selection or susceptibility to timing attacks.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The derived convergence bound from /math is checked against the empirical loss curves generated by /code; fallback: If divergence is >15%, /math must re-evaluate assumptions about data heterogeneity or encryption noise.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least 2 key papers on FedAvg convergence with homomorphic encryption.\", \"- [ ] /math: Derive the full convergence proof, clearly stating all assumptions regarding client data distributions and the cryptosystem.\", \"- [ ] /code: Implement the Paillier cryptosystem and the federated learning simulation environment.\", \"- [ ] /code: Run experiments on a non-IID partitioned dataset (e.g., CIFAR-10) and collect metrics.\", \"- [ ] /math: Verify that the aggregation logic in the Python code correctly implements the derived secure aggregation formula from the proof.\", \"- [ ] /code: Generate plots comparing empirical convergence of the encrypted model vs. a plaintext baseline.\", \"- [ ] /router: Perform final QA review, checking for alignment between the mathematical proof, code implementation, and empirical results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"cryptography\", \"privacy-preserving-ml\"], \"acceptance_criteria\": [\"- The mathematical proof is rigorous, self-contained, and clearly articulates its assumptions.\", \"- The Python simulation runs without errors and accurately implements the federated learning protocol with Paillier-based secure aggregation.\", \"- The final report shows empirical convergence results that are consistent with the derived theoretical bounds, with any deviations clearly explained.\", \"- The code includes comments linking implementation details back to specific equations in the mathematical derivation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate the slope of a log-linear regression fit to the global model's training loss curve over communication rounds. A more negative slope indicates faster convergence.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the average wall-clock time in milliseconds per round for encrypted aggregation minus the average time for plaintext aggregation.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 215, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the primal-dual formulation for Lasso with elastic net regularization, then implement a linearized ADMM solver in Python, analyzing its convergence rate against the standard ADMM variant for a synthetic high-dimensional dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org linearized ADMM convergence rate elastic net lasso\\\", mode=web, context=\\\"Find key papers on linearized ADMM for Lasso-type problems, focusing on deriving convergence rate guarantees and comparing against standard ADMM performance.\\\")\", \"/math(context=\\\"Derive the augmented Lagrangian for elastic net Lasso, formulate the standard and linearized ADMM update steps for both primal and dual variables, and establish the theoretical convergence conditions.\\\")\", \"/code(context=\\\"Implement both standard and linearized ADMM in Python for the elastic net problem, then benchmark their convergence speed and solution accuracy on a synthetic sparse dataset, reporting the final objective value and iteration count.\\\")\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on linearized ADMM and its convergence properties. This theoretical grounding informs the /math agent's formal derivation of the specific update equations for the elastic net problem. Finally, the /code agent implements the derived algorithms to empirically validate the theoretical performance claims on a concrete benchmark.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the standard and linearized ADMM update rules for elastic net.\", \"A standalone Python script (`admm_comparison.py`) that implements both algorithms and reproduces the benchmark results.\", \"A summary report in Markdown (`report.md`) presenting the convergence plots, performance tables, and analysis, with proper citations.\", \"A JSON file (`metrics.json`) containing the final convergence rates, wall-clock times, and objective values for both methods.\"], \"thinking_outline\": [\"1. Research linearized Alternating Direction Method of Multipliers (ADMM) variants, specifically their application to composite objective functions like elastic net, to understand the theoretical motivation and expected performance gains.\", \"2. Formulate the elastic net problem in a consensus form suitable for ADMM, clearly defining the objective function, constraints, and splitting variables.\", \"3. Derive the detailed iterative update rules for both the standard and linearized ADMM algorithms, paying close attention to the derivation of the proximal operators for the L1 and L2 penalty terms.\", \"4. Implement both algorithms in Python, including a reproducible synthetic data generator for sparse linear regression problems.\", \"5. Validate the correctness of the implementations by comparing the final solution for a pure Lasso case (alpha=1) against the result from a trusted library like scikit-learn's `Lasso` model to ensure the solver is sound.\", \"6. Execute the comparative benchmark, plot the convergence curves (residual vs. iteration), and analyze the empirical results in the context of the theoretical findings from the initial literature search.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The blue agent's literature findings (papers on convergence rates) are passed to /math to guide the derivation. The finalized update equations from /math are the direct specification for the /code implementation. The /code agent produces empirical results (plots, metrics) that are handed to the router for final quality assurance. Verification: The router checks if the empirical convergence from code aligns with the theoretical rates found by /general-search. Fallback: If code fails to converge or shows unexpected behavior, the logic of the proximal operators will be re-audited against the /math derivation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize >=2 key papers from arXiv on linearized ADMM for Lasso/elastic net, focusing on convergence analysis.\", \"- [ ] /math: Provide a step-by-step derivation of the augmented Lagrangian and the update equations for both standard and linearized ADMM.\", \"- [ ] /code: Implement both ADMM algorithms and the synthetic sparse data generation framework using NumPy/SciPy.\", \"- [ ] /code: Run the benchmark comparison, generating convergence plots (log-residual vs. iteration) and performance tables (time, objective value).\", \"- [ ] /code: Validate the implementation's final solution for a simple Lasso case (lambda2=0) against scikit-learn's `Lasso` model.\", \"- [ ] router: QA the final report to ensure the empirical results are correctly interpreted and align with the theoretical claims from the cited papers.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"convex-optimization\", \"admm\", \"machine-learning\"], \"acceptance_criteria\": [\"The mathematical derivation of all ADMM update rules must be complete, self-contained, and mathematically sound.\", \"The Python implementation must be functional, reproducible given the provided seed, and include a specific validation function that checks its output against scikit-learn's Lasso on a test case.\", \"The final report must contain plots clearly demonstrating the superior convergence rate of linearized ADMM and cite at least two relevant academic papers by their arXiv IDs.\", \"The reported metrics in `metrics.json` must be consistent with the data presented in the plots and tables of the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the combined primal and dual residual norm `sqrt(||r_k||^2 + ||s_k||^2)` at each iteration `k`. Plot the natural logarithm of this norm against the iteration number. The slope of this line in the linear region indicates the convergence rate.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the total execution time in seconds for each algorithm to reach a combined residual norm below a fixed tolerance (e.g., 1e-6).\"}}", "categories": ["metrics_guidance"]}
{"index": 216, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am working with high-dimensional biomedical point cloud data and need to validate the manifold hypothesis. Can you derive the update rules for a gradient-based optimization to find a low-dimensional embedding that preserves the persistent homology (specifically the 0- and 1-dimensional Betti numbers) of the original data's Vietoris-Rips complex? Also, implement this using a Python library like Giotto-TDA or Ripser and benchmark its convergence on a synthetic Swiss roll dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'persistent homology' 'differentiable' manifold learning optimization biomedical\\\", mode=both)\", \"/math(Derive the loss function and its gradient for an embedding that minimizes the Wasserstein distance between persistence diagrams of the original high-dimensional data and the low-dimensional embedding, ensuring Bayesian convergence guarantees.)\", \"/code(Implement the derived optimization in Python using JAX for automatic differentiation and Giotto-TDA for persistence diagram computation. Benchmark convergence and energy-based topological feature preservation on a synthetic manifold.)\"], \"route_rationale\": \"The blue general-search agent first identifies state-of-the-art differentiable frameworks for topological data analysis. Next, the /math tool formalizes the optimization problem and derives the necessary gradients based on the literature. Finally, the /code tool implements this mathematical formulation and provides an empirical benchmark of its performance.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the topological loss function and its gradient.\", \"- A documented Python script implementing the optimization procedure.\", \"- A summary report including convergence plots and a comparison of persistence diagrams for the original and embedded data.\", \"- A list of at least two cited arXiv papers with full IDs.\", \"- A verification log confirming the consistency between the mathematical derivation and the code's numerical gradients.\"], \"thinking_outline\": [\"1. Research differentiable and optimization-based approaches for topological feature preservation in dimensionality reduction, focusing on methods that use persistent homology.\", \"2. Formulate a loss function based on a metric between persistence diagrams, such as the Wasserstein or bottleneck distance, which is amenable to gradient-based optimization.\", \"3. Derive the analytical gradient of this topological loss function with respect to the coordinates of the low-dimensional embedding.\", \"4. Implement the gradient descent optimization loop, leveraging existing libraries for persistent homology calculations and automatic differentiation for the gradient computation.\", \"5. Validate the implementation by comparing the persistence diagrams of the original and embedded data at various optimization stages and verify that the loss function converges.\", \"6. Check the final Betti numbers of the embedded manifold against the source data's Betti numbers to audit whether the primary topological features were preserved.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The /math agent's symbolic gradient derivation must be checked against the numerical gradient computed by the /code agent's JAX implementation; fallback: If a mismatch >1e-4 is found, the /math agent must re-derive the gradient, and the /code agent must re-implement the loss function.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on differentiable topology and preserving persistent homology in manifold learning, focusing on arXiv.\", \"- [ ] /math: Based on search results, formally define the optimization problem using the Wasserstein distance between persistence diagrams.\", \"- [ ] /math: Derive the complete gradient of the topological loss with respect to the embedded point coordinates.\", \"- [ ] /code: Implement the manifold embedding and the differentiable topological loss function in Python using JAX and Giotto-TDA.\", \"- [ ] /code: Set up a benchmark using a synthetic dataset (e.g., Swiss roll) to test the implementation's convergence.\", \"- [ ] /math: Verify that the symbolic gradient derived matches the numerical gradient computed by the JAX implementation to within a small tolerance.\", \"- [ ] /code: Generate plots showing the convergence of the loss function and persistence diagrams before and after optimization.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"persistent-homology\"], \"acceptance_criteria\": [\"- The implemented code reduces the Wasserstein distance between persistence diagrams by at least 50% on the synthetic test dataset.\", \"- The final report includes a complete mathematical derivation that is verified to be consistent with the code's behavior.\", \"- The Betti numbers (dimensions 0 and 1) of the final embedded manifold match those of the original synthetic data.\", \"- At least two relevant arXiv papers on differentiable topology are cited with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 219, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a provably safe reinforcement learning agent for a robotic manipulation task. Please formulate the problem as a Constrained Markov Decision Process (CMDP) and derive a primal-dual algorithm that uses a Lagrangian relaxation. Critically, you must provide a formal proof of convergence to a locally optimal policy using a Lyapunov stability analysis, referencing modern control theory concepts. The final implementation should be benchmarked in a high-fidelity simulator.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org primal-dual reinforcement learning CMDP Lyapunov stability convergence control theory\\\", mode=web)\", \"/math(Derive the Lagrangian for the CMDP, formulate the primal-dual saddle-point problem, and prove convergence guarantees using a discrete-time Lyapunov function based on the dual variable's update rule and policy gradient error.)\", \"/code(Implement the derived primal-dual algorithm in Python with PyTorch for a Safety-Gym environment. Add runtime validation to track constraint violations and Lagrangian multiplier convergence, ensuring the code reflects the mathematical derivation.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art theoretical foundations for CMDP optimization and stability proofs. This literature informs the /math agent's formal derivation and convergence proof. Finally, the /code agent implements and empirically validates the derived theory, turning the abstract mathematics into a testable solution.\", \"expected_artifacts\": [\"A PDF report containing the full mathematical derivation of the primal-dual updates and the Lyapunov convergence proof.\", \"Well-commented Python source code for the agent and the simulation environment.\", \"A `results.json` file logging the primary and secondary metrics across training epochs.\", \"Plots visualizing the cumulative reward, constraint violations, and the trajectory of the Lagrangian multipliers over time.\", \"A bibliography section in the report citing at least two relevant academic papers with arXiv IDs.\"], \"thinking_outline\": [\"1. Formalize the robotic safety problem as a Constrained Markov Decision Process (CMDP), defining state/action spaces, transition dynamics, reward function, and cost function.\", \"2. Use Lagrangian relaxation to transform the constrained optimization problem into an unconstrained saddle-point problem, identifying the primal (policy) and dual (Lagrange multiplier) variables.\", \"3. Derive the gradient-based update rules for both the primal and dual variables, forming the core of the primal-dual algorithm.\", \"4. Verify the theoretical convergence of the algorithm by constructing a suitable Lyapunov function and showing that it decreases over iterations, guaranteeing stability around the optimal point.\", \"5. Implement the algorithm and validate its performance in a simulated environment, ensuring the rate of constraint violations remains below the specified threshold.\", \"6. Audit the empirical results by comparing the observed convergence of the Lagrange multipliers and policy performance against the theoretical predictions from the proof.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math agent uses these to produce a formal derivation -> /code agent implements the derivation. Verification: The /code agent's implementation of the update rules must be audited against the final equations in the /math agent's report. Fallback: If the implementation fails to converge or consistently violates constraints, the task is routed back to /math to re-evaluate the assumptions in the Lyapunov stability proof.\", \"todo_list\": [\"- [ ] /general-search: Find at least two key papers on primal-dual methods for CMDPs, focusing on those with convergence proofs.\", \"- [ ] /math: Derive the complete primal-dual update equations from first principles based on the CMDP formulation.\", \"- [ ] /math: Construct the Lyapunov stability proof for the derived algorithm, clearly stating all assumptions.\", \"- [ ] /code: Implement the policy and cost critics, along with the policy and dual variable update logic in PyTorch.\", \"- [ ] /code: Set up the Safety-Gym experiment, including logging for all required metrics.\", \"- [ ] /math: Verify that the implemented update rules in the code exactly match the derived equations.\", \"- [ ] /code: Validate that the logged constraint violations per episode remain below the predefined safety threshold after convergence.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"safe-reinforcement-learning\", \"constrained-mdp\", \"control-theory\"], \"acceptance_criteria\": [\"The final report must contain a complete mathematical derivation and a valid Lyapunov-based convergence proof.\", \"The implemented agent must achieve a cumulative reward of at least 80% of the unconstrained optimum while keeping the average constraint violation rate below a threshold of 0.05.\", \"The plot of the Lagrangian multiplier must show clear convergence to a stable value.\", \"The submitted code must be runnable and reproduce the key metrics reported in the results summary.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 220, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating GPU-accelerated solvers for large, sparse, symmetric positive-definite linear systems arising from finite element discretizations. I need to implement a preconditioned conjugate gradient (PCG) method in CUDA using Python. Can you help me derive the algorithm, find a suitable modern preconditioner like Incomplete Cholesky factorization (specifically IC0), and then implement and benchmark it, analyzing its convergence rate and floating-point throughput?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org GPU Incomplete Cholesky IC0 preconditioner CUDA sparse conjugate gradient convergence\\\", mode=rag)\", \"/math(Derive the Preconditioned Conjugate Gradient algorithm for sparse matrices, detailing the application of the IC0 preconditioner and analyzing the expected impact on the problem's eigenvalue distribution.)\", \"/code(Implement the PCG solver and IC0 preconditioner using Python with CUDA bindings like CuPy, ensuring robust convergence checks and profiling of kernel execution time for performance analysis.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather current research on efficient GPU-based preconditioners. This informs the /math agent's formal derivation of the PCG algorithm and preconditioner application. Finally, the /code agent implements and benchmarks the derived algorithm, leveraging the theoretical foundation for a correct and performant solution.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the PCG algorithm with the IC0 preconditioner.\", \"- A Python implementation file using a CUDA backend (e.g., CuPy) for the solver.\", \"- A performance report in Markdown comparing the custom solver's convergence and runtime against a baseline, including plots and citations.\", \"- A JSON file containing raw metrics data from the benchmark runs.\", \"- Verification output confirming numerical equivalence with a trusted library solver on a test matrix.\"], \"thinking_outline\": [\"1. Conduct a literature review to identify state-of-the-art techniques and challenges for implementing Incomplete Cholesky preconditioners on GPU architectures for sparse matrices.\", \"2. Formally derive the Preconditioned Conjugate Gradient algorithm, clearly defining each step and the role of the preconditioner in transforming the system's eigenvalue spectrum.\", \"3. Implement the solver in Python using a library like CuPy for CUDA acceleration, paying special attention to efficient sparse matrix-vector multiplication (SpMV) and memory management on the GPU.\", \"4. Validate the numerical correctness of the custom implementation by comparing its solution for a known sparse positive-definite system against the output from a reliable library like SciPy.linalg, ensuring the residual norms match to a high precision.\", \"5. Design and execute a benchmark to evaluate the solver's performance, measuring convergence rate (iterations to tolerance) and wall-clock time against a standard unpreconditioned conjugate gradient solver.\", \"6. Check the final analysis to ensure the benchmark results are correctly interpreted in the context of the mathematical derivations and the findings from the initial literature review.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /code agent must verify its implementation against the derivation from /math and a known numerical result; fallback: if numerical instability is found, /code requests /math to analyze the matrix properties and suggest stabilization techniques).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent papers on GPU-accelerated IC0 preconditioners for sparse matrices.\", \"- [ ] /math: Provide a step-by-step derivation of the PCG algorithm incorporating a generic preconditioner M.\", \"- [ ] /math: Detail how the IC0 factorization serves as the preconditioner M.\", \"- [ ] /code: Implement the PCG solver and IC0 preconditioner in a Python script using CuPy.\", \"- [ ] /code: Create a benchmark function that compares the custom solver's speed and convergence with `scipy.sparse.linalg.cg`.\", \"- [ ] /math: Verify that the algorithm implemented in the code correctly reflects the mathematical derivation.\", \"- [ ] /code: Validate the implementation's numerical output against a trusted library on a sample problem.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"conjugate-gradient\"], \"acceptance_criteria\": [\"- The implemented solver must converge to the correct solution for a given sparse positive-definite test matrix, matching a reference solver's output within a tolerance of 1e-6.\", \"- The final report must contain at least one plot showing the convergence history (residual norm vs. iteration) for both the preconditioned and unpreconditioned solvers.\", \"- The implementation must demonstrate a measurable performance improvement (either faster convergence or lower wall-clock time) compared to the unpreconditioned baseline.\", \"- The solution must cite at least two relevant academic papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 221, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous derivation and implementation of amortized variational inference for a Bayesian neural network. Please derive the gradient of the evidence lower bound (ELBO) using the reparameterization trick for a Gaussian posterior. Then, implement this in PyTorch and compare its convergence behavior against a standard mean-field VI approach on a benchmark dataset, analyzing the variance of the gradient estimators.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org amortized variational inference reparameterization trick bayesian neural network\\\", mode=rag)\", \"/math(Derive the ELBO gradient for a Bayesian model with Gaussian posterior, applying the reparameterization trick to achieve a low-variance estimator suitable for stochastic optimization. Define convergence criteria.)\", \"/code(Implement the amortized VI scheme in PyTorch with a CUDA backend, track the ELBO on a validation set, and generate plots comparing its convergence profile against a mean-field VI baseline. Reference the arxiv papers., using Python)\"], \"route_rationale\": \"The blue /general-search agent will first retrieve foundational papers on Amortized VI to ground the theoretical work. Next, the /math agent will produce a formal, step-by-step derivation of the necessary gradient estimator. Finally, the /code agent will translate the mathematical theory into a working implementation for empirical validation and comparison.\", \"expected_artifacts\": [\"A formal LaTeX document detailing the derivation of the ELBO and its reparameterized gradient.\", \"A commented Python script implementing the Bayesian neural network, the inference network, and the training loop.\", \"A generated plot (`.png`) comparing the ELBO convergence curves for amortized and mean-field variational inference.\", \"A metrics file (`.json`) containing the final test set ELBO and Predictive Log-Likelihood.\", \"A summary report citing at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Define the probabilistic model for a Bayesian Neural Network and the objective of variational inference: maximizing the Evidence Lower Bound (ELBO).\", \"2. Formally derive the ELBO and its gradient, highlighting why the naive Monte Carlo estimator has high variance or is intractable.\", \"3. Introduce the reparameterization trick for a Gaussian variational posterior and derive the low-variance gradient estimator.\", \"4. Validate the mathematical derivation against the foundational literature sourced by the search agent to ensure theoretical correctness.\", \"5. Implement the derived estimator within a PyTorch model and verify the analytical gradient implementation using numerical gradient checking.\", \"6. Execute the experiment, collect metrics, and analyze the results to conclude on the comparative performance of amortized versus mean-field VI.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent cross-references its derivation against the arXiv papers from /general-search; /code agent uses numerical gradient checking to validate the implemented analytical gradient from /math; fallback: if derivation is flawed, return to /math with corrected assumptions from literature).\", \"todo_list\": [\"- [ ] /general-search: Find the seminal papers on 'Auto-Encoding Variational Bayes' and 'Stochastic Backpropagation and Approximate Inference in Deep Generative Models'.\", \"- [ ] /math: Derive the ELBO gradient for a BNN using the reparameterization trick.\", \"- [ ] /math: Formulate the update rules for a non-amortized mean-field VI baseline for comparison.\", \"- [ ] /code: Implement the Bayesian neural network, inference network, and training loops in PyTorch.\", \"- [ ] /code: Instrument the code to log ELBO and Predictive Log-Likelihood metrics during training.\", \"- [ ] /math: Verify that the logic in the /code implementation correctly maps to the mathematical derivation.\", \"- [ ] /code: Add a unit test that uses `torch.autograd.gradcheck` to validate the analytical gradient against a numerical estimate.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"probabilistic-models\"], \"acceptance_criteria\": [\"- The LaTeX derivation of the reparameterized gradient must be complete and mathematically sound.\", \"- The implemented code must run without errors, and the ELBO metric must demonstrate a clear trend of convergence during training.\", \"- The final report must include a plot comparing ELBO convergence for the two VI methods and cite at least two foundational papers with arXiv IDs.\", \"- The numerical gradient check for the custom ELBO implementation must pass.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Computed on a held-out test set after each training epoch. Should be a scalar value that, on average, monotonically increases, indicating successful optimization.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"The average log-likelihood of the test data under the posterior predictive distribution, approximated via Monte Carlo sampling from the variational posterior. Measures generalization.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 222, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for a score-based generative model, starting from a forward Ornstein-Uhlenbeck process. Subsequently, use the Fokker-Planck equation to prove that the marginal probability densities of the reverse process match the forward process. Finally, implement a numerical solver in Python to validate this theory by sampling from a simple 2D distribution, showing convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models stochastic differential equation reverse-time SDE diffusion\\\", mode=web)\", \"/math(Derive the reverse-time SDE using It\u00f4 calculus and Anderson's theorem. Then, formulate the forward and reverse Fokker-Planck equations to prove the convergence of marginal distributions, referencing Euler-Lagrange principles for the score matching objective.)\", \"/code(Implement a score network and the Euler-Maruyama solver for the derived SDE in PyTorch with CUDA support. Train the model on a 2D Gaussian mixture and track the Fr\u00e9chet Inception Distance (FID) to measure sample quality., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature from arXiv. This theoretical grounding is then passed to /math for rigorous derivation and proof. Finally, /code implements the derived mathematics for empirical validation, creating a closed loop of theory and practice.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the reverse-time SDE and the Fokker-Planck proof.\", \"- A Python script implementing the numerical solver and score-based model training.\", \"- A plot visualizing the generated samples against the true data distribution over training epochs.\", \"- A JSON file reporting the final FID score and training loss curves.\", \"- A bibliography file with citations to at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research foundational literature on the connection between score matching, diffusion models, and SDEs, focusing on the work by Song et al. and Anderson.\", \"2. Formally derive the reverse-time SDE using the established properties of It\u00f4 processes and the definition of the score function, \u2207_x log p_t(x).\", \"3. Formulate the Fokker-Planck partial differential equation for both the forward and derived reverse-time SDEs to prove that p_t(x) is a valid solution for both, ensuring the generative process is a valid time-reversal.\", \"4. Implement the Euler-Maruyama numerical integration scheme for the reverse-time SDE, using a neural network to approximate the score function.\", \"5. Validate the implementation by training the model on a known distribution and measuring the FID between generated samples and true samples, ensuring it decreases to an acceptable level.\", \"6. Audit the mathematical derivation against the Python code to ensure the drift and diffusion coefficient implementations precisely match the derived equations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers (e.g., arXiv preprints) to the /math agent. The /math agent produces a formal derivation, which serves as a specification for the /code agent. The /code agent's empirical results (FID score, sample plots) are used to verify the correctness of the /math agent's derivation. Verification: Cross-check implemented SDE coefficients in code against the math derivation. Fallback: If FID fails to converge, re-audit the derivation in /math for potential errors in the drift term.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on score-based generative modeling with SDEs.\", \"- [ ] /math: Provide a step-by-step derivation of the reverse-time SDE from a forward process.\", \"- [ ] /math: Use the Fokker-Planck equation to formally prove the validity of the time-reversed process.\", \"- [ ] /code: Implement the Euler-Maruyama solver for the derived SDE in PyTorch.\", \"- [ ] /code: Train a score network on a 2D dataset and generate samples.\", \"- [ ] /math: Verify that the drift and diffusion terms implemented in the /code agent's solver exactly match the derived mathematical expressions.\", \"- [ ] /code: Validate model convergence by ensuring the final FID score is below the target threshold defined in metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-ai\"], \"acceptance_criteria\": [\"- The LaTeX derivation of the reverse-time SDE is complete, mathematically sound, and aligns with established literature.\", \"- The Python implementation successfully generates samples that visually converge to the target 2D distribution.\", \"- The final reported Fr\u00e9chet Inception Distance (FID) is less than 50 on the test distribution.\", \"- The response correctly cites at least two foundational arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 223, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the mathematical formulation for a Variational Quantum Eigensolver (VQE) using a Hardware-Efficient Ansatz to find the ground state energy of the H2 molecule. Specifically, detail the decomposition of the molecular Hamiltonian into a sum of Pauli strings and derive the parameter shift rule for gradient computation. Implement this in Python using PennyLane, validate the convergence against known results, and analyze the impact of circuit depth on the final energy eigenvalue.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational quantum eigensolver H2 Hamiltonian Pauli decomposition 'hardware efficient ansatz'\\\", mode=both)\", \"/math(Derive the Pauli string decomposition for the H2 molecular Hamiltonian and the parameter-shift rule for analytic gradients, ensuring the derived energy functional's eigenvalue properties are consistent with quantum chemistry principles.)\", \"/code(Implement the VQE algorithm in Python/PennyLane to find the ground state energy of the H2 molecule, validating the convergence against the exact eigenvalue and analyzing the effect of ansatz depth on optimization performance.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on VQE for H2, which is essential for the theoretical work. Next, /math is used to perform the formal derivations of the Hamiltonian decomposition and gradient rules, which are non-trivial. Finally, /code implements and validates the derived theory, providing an empirical test of the mathematical model.\", \"expected_artifacts\": [\"- A LaTeX document detailing the derivation of the H2 qubit Hamiltonian and the parameter shift rule.\", \"- A Python script implementing the VQE algorithm using PennyLane, capable of reproducing the results.\", \"- A summary report including a plot of energy convergence vs. optimizer steps and a table showing final energy vs. ansatz depth.\", \"- A log file from the code execution verifying that the analytical gradient matches a finite-difference approximation.\", \"- A bibliography file citing at least two relevant arXiv preprints on VQE or quantum chemistry.\"], \"thinking_outline\": [\"1. Research the principles of VQE and its application to quantum chemistry, specifically for finding the ground state of the H2 molecule using authoritative sources.\", \"2. Formally derive the qubit Hamiltonian for H2 by applying a fermion-to-qubit mapping (e.g., Jordan-Wigner) and express it as a linear combination of Pauli strings.\", \"3. Derive the parameter shift rule for analytically calculating the gradient of the energy expectation value with respect to the ansatz parameters.\", \"4. Implement the VQE loop in Python, combining a parameterized quantum circuit (Hardware-Efficient Ansatz), the derived Hamiltonian for expectation value measurement, and a classical optimizer (e.g., Adam).\", \"5. Validate the implementation by comparing the converged ground state energy to the known Full Configuration Interaction (FCI) value for H2 at a standard bond length (e.g., 0.7414 \u00c5).\", \"6. Audit the gradient implementation by comparing the output of the parameter shift rule against a numerical finite-difference gradient for a set of random parameters.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent produces derivations that serve as a formal specification for the code agent. The code agent implements this spec and performs a critical verification step: comparing its analytical gradient to a numerical one. If they mismatch, this triggers a fallback to the math agent to review the derivation for errors. The final, validated results are passed to the router for quality assurance.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on VQE for H2, focusing on Hamiltonian representation and gradient calculation techniques.\", \"- [ ] /math: Produce a formal derivation of the H2 qubit Hamiltonian, expressed as a weighted sum of Pauli operators.\", \"- [ ] /math: Derive the formula for the parameter shift rule as it applies to the chosen ansatz gates (e.g., single-qubit rotations).\", \"- [ ] /code: Implement the VQE algorithm in PennyLane, including the ansatz, Hamiltonian expectation, and optimization loop.\", \"- [ ] /code: Implement a verification function to compare the analytical gradient from the parameter shift rule with a finite-difference approximation.\", \"- [ ] /code: Run the simulation, plot the energy convergence curve, and report the final ground state energy.\", \"- [ ] /math: Verify that the implemented Hamiltonian in the code correctly matches the derived Pauli string decomposition.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"variational-algorithms\", \"hamiltonian-simulation\"], \"acceptance_criteria\": [\"- The derived H2 Hamiltonian must be correctly expressed as a sum of weighted Pauli strings, matching established literature.\", \"- The implemented VQE algorithm must converge to a ground state energy within 1e-3 Hartrees of the known FCI value for H2 at the specified bond length.\", \"- The analytical gradient calculated via the parameter shift rule must match a central finite-difference approximation to a tolerance of 1e-6.\", \"- The final report must include a convergence plot and citations for at least two relevant arXiv preprints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 224, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Variational Quantum Eigensolver (VQE) in Python to find the ground state energy of the LiH molecule. Please formulate the problem by deriving the qubit Hamiltonian using a Jordan-Wigner transformation, design an appropriate Unitary Coupled Cluster (UCC) ansatz, and benchmark the VQE's convergence and final eigenvalue accuracy against established computational chemistry results.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational quantum eigensolver LiH UCC ansatz Hamiltonian\\\", mode=web)\", \"/math(Derive the qubit Hamiltonian for LiH using the Jordan-Wigner transformation and formulate the expectation value calculation for the UCC ansatz, focusing on eigenvalue convergence.)\", \"/code(Implement the VQE algorithm using Python and Qiskit, including the UCC ansatz circuit and a classical optimizer like COBYLA, ensuring robust convergence checks and tracking the final energy eigenvalue.)\"], \"route_rationale\": \"The blue general-search agent will first gather foundational research papers and benchmark values for VQE applied to LiH. The math agent will then use this context to derive the specific mathematical formalism, including the qubit Hamiltonian. Finally, the code agent will translate the mathematical derivation into a working Python simulation to compute the final energy eigenvalue and assess convergence.\", \"expected_artifacts\": [\"A LaTeX document detailing the derivation of the LiH qubit Hamiltonian via the Jordan-Wigner transformation.\", \"A well-documented Python script implementing the VQE simulation for LiH using Qiskit or a similar framework.\", \"A summary report comparing the calculated ground state energy to literature values, including convergence plots of energy vs. optimizer iteration.\", \"A bibliography with at least two cited arXiv preprints relevant to the methodology.\", \"A verification log confirming the one-to-one mapping between the derived Hamiltonian terms and the coded Pauli strings.\"], \"thinking_outline\": [\"1. Research the electronic structure of the LiH molecule to obtain the one- and two-electron integrals required for constructing the fermionic Hamiltonian.\", \"2. Apply the Jordan-Wigner transformation to map the fermionic Hamiltonian to a multi-qubit Pauli operator representation (the qubit Hamiltonian).\", \"3. Formulate the Unitary Coupled Cluster Singles and Doubles (UCCSD) ansatz circuit, which provides a systematically improvable approximation to the true ground state.\", \"4. Implement the complete VQE algorithm, which integrates the UCCSD ansatz, the qubit Hamiltonian for energy evaluation, and a classical optimization loop to update the ansatz parameters.\", \"5. Validate the implementation by checking the commutation relations and coefficients of the Pauli terms in the coded Hamiltonian against the formal mathematical derivation.\", \"6. Execute the simulation and verify the final ground state energy by comparing it against established benchmark values for LiH from high-accuracy computational chemistry methods like Full Configuration Interaction (FCI).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The calculated ground state energy must be within 1% of literature FCI values; fallback: If the optimizer fails to converge, /code will attempt an alternative optimizer, such as SPSA, and re-run the simulation).\", \"todo_list\": [\"- [ ] /general-search: Find authoritative papers on VQE for LiH, specifically identifying the UCCSD ansatz structure and benchmark FCI energy values.\", \"- [ ] /math: Derive the explicit Pauli string representation for the LiH qubit Hamiltonian from its fermionic form.\", \"- [ ] /math: Formulate the cost function as the expectation value of the Hamiltonian with respect to the parameterized ansatz state.\", \"- [ ] /code: Implement the UCCSD ansatz circuit in Python, parameterized by variational angles.\", \"- [ ] /code: Implement the full VQE loop using a classical optimizer (e.g., COBYLA) to minimize the energy expectation.\", \"- [ ] /math: Verify that the Pauli terms and their coefficients in the code's Hamiltonian implementation match the derived mathematical expression exactly.\", \"- [ ] /code: Run the simulation to find the ground state energy and generate convergence plots for the energy eigenvalue.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"variational-algorithms\", \"quantum-chemistry\"], \"acceptance_criteria\": [\"- The implemented VQE must converge to a ground state energy for LiH that is within 1% of the accepted Full Configuration Interaction (FCI) value (~ -7.88 Hartrees).\", \"- The final report must contain a complete mathematical derivation of the qubit Hamiltonian from the fermionic operators.\", \"- The provided source code must be well-documented and execute successfully to reproduce the reported energy value and convergence plots.\", \"- At least two relevant arXiv preprints detailing VQE or its application to molecular systems must be cited in the report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 225, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a distributed Alternating Direction Method of Multipliers (ADMM) solver for L1-regularized logistic regression. Please provide a full derivation, a Python implementation, and an analysis of its convergence rate on a non-separable synthetic dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org distributed ADMM L1-regularized logistic regression convergence analysis\\\", mode=web)\", \"/math(Derive the specific primal-dual update steps for the ADMM formulation of sparse logistic regression, ensuring the derivation explicitly handles the non-smooth L1 norm and establishes the conditions for guaranteed convergence.)\", \"/code(Implement the derived ADMM algorithm in Python using NumPy, validate its convergence on a synthetic dataset, and analyze its computational complexity against a standard solver like scikit-learn's LogisticRegression.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art ADMM formulations for this problem class. This informs the /math agent, which will produce a rigorous mathematical derivation of the update steps. Finally, the /code agent will implement the derived algorithm and perform empirical validation, providing a complete theoretical and practical solution.\", \"expected_artifacts\": [\"A markdown document containing the complete mathematical derivation of the ADMM update equations for L1-regularized logistic regression.\", \"A Python script (`admm_solver.py`) implementing the algorithm.\", \"A results report in PDF format showing convergence plots (objective value, primal/dual residuals vs. iteration) and a performance comparison table.\", \"A `CITATIONS.md` file listing at least two relevant arXiv papers with their IDs.\", \"A log file (`verification_output.txt`) showing the output of the validation check against the baseline solver.\"], \"thinking_outline\": [\"1. Research standard ADMM formulations for L1-regularized problems, particularly the 'consensus' or 'sharing' form suitable for distributed computation.\", \"2. Formulate the L1-regularized logistic regression problem in a structure amenable to ADMM, clearly defining the objective function parts f(x) and g(z).\", \"3. Derive the iterative update equations for the primal variables (x-update, z-update) and the dual variable (u-update). The z-update should result in a soft-thresholding operation.\", \"4. Implement the algorithm in Python and validate its correctness by checking that the primal and dual residuals converge to zero and the objective function value matches a trusted baseline solver on a known problem.\", \"5. Systematically analyze the algorithm's performance, generating plots to visualize the convergence behavior and comparing the final solution vector to a benchmark.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to /math. The /math agent provides the exact update equations to /code. The /code agent provides the implementation, plots, and metrics to the router for final review. Verification: The router will audit the code's output against the benchmark solver's solution. Fallback: If convergence fails, the router will request the /math agent to re-verify the derivations; if the derivation is correct, the /code agent will debug the implementation, focusing on the soft-thresholding step.\", \"todo_list\": [\"- [ ] /general-search: Find 2-3 key papers on ADMM for statistical learning and distributed optimization.\", \"- [ ] /math: Derive the scaled-form ADMM update rules for L1-regularized logistic regression.\", \"- [ ] /math: State the theoretical convergence conditions based on the problem parameters.\", \"- [ ] /code: Implement the ADMM solver as a Python class.\", \"- [ ] /code: Develop a test harness to generate synthetic data and run the solver against a scikit-learn baseline.\", \"- [ ] /code: Generate plots of primal/dual residuals and objective function value per iteration to validate convergence.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"convex-optimization\", \"admm\", \"primal-dual-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation of the x, z, and u updates must be complete and correct.\", \"- The Python implementation must converge on a test dataset to a solution vector with an L2 norm difference of less than 1e-3 from scikit-learn's 'saga' solver.\", \"- The final report must include convergence plots showing both primal and dual residuals approaching zero.\", \"- The `CITATIONS.md` file must contain at least two valid and relevant arXiv paper IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"primary_computation\": \"L2 norm of the difference between the ADMM solution vector and the solution from the scikit-learn baseline solver.\", \"secondary_computation\": \"Number of iterations required to reach a primal and dual residual tolerance of 1e-4.\"}}", "categories": ["metrics_computation"]}
{"index": 227, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to solve the Maximum Weight Independent Set problem on a general graph by leveraging its treewidth. Can you derive the dynamic programming formulation for a rooted tree decomposition, analyze its time complexity in terms of n and the treewidth k, and implement a Python solution that includes finding a near-optimal tree decomposition and then running the DP? I'm particularly interested in the state representation for each bag in the decomposition and the transitions for introduce, forget, and join nodes, considering vertex weights from a Bernoulli distribution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:.edu treewidth dynamic programming 'maximum independent set' complexity analysis\\\", mode=rag)\", \"/math(Derive the DP state representation for Maximum Weight Independent Set on a tree decomposition, formulating recurrence relations for introduce, forget, and join nodes. Analyze the complexity in terms of vertex count n and treewidth k, considering the state space size and potential for causal analysis.)\", \"/code(Implement the tree decomposition DP in Python, including a function for heuristic treewidth decomposition. Validate the algorithm's correctness on graphs with known solutions and profile its runtime performance against the theoretical complexity, analyzing convergence properties.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational academic literature on tree decomposition DP. This research informs the /math agent's formal derivation of the DP recurrence. Finally, the /code agent implements and validates the derived theory, providing an end-to-end solution.\", \"expected_artifacts\": [\"A formal mathematical derivation of the DP state and recurrence relations for introduce, forget, and join nodes.\", \"A Python implementation of the complete algorithm, including a heuristic for tree decomposition and the DP solver.\", \"A verification report comparing the implementation's output with known solutions on test graphs and a runtime analysis plot.\", \"A list of at least 2 cited arXiv papers or academic articles on parameterized complexity and tree decompositions.\"], \"thinking_outline\": [\"1. Research the formal definition of tree decompositions and the standard dynamic programming approach for Maximum Weight Independent Set on them.\", \"2. Formulate the DP state for a bag in the tree decomposition. The state must encode, for each subset of vertices in the bag, the maximum weight of an independent set in the corresponding subgraph that is consistent with that subset.\", \"3. Derive the recurrence relations for the three types of tree decomposition nodes: introduce, forget, and join. Validate the logic for each state transition.\", \"4. Analyze the time complexity of the algorithm, showing its dependence on the number of vertices (n) and the treewidth (k). This should be O(c^k * poly(n)) for some small constant c, typically c=2.\", \"5. Implement the algorithm and test it on various graphs. Verify the correctness of the output against a brute-force solver on small graphs to ensure the logic is sound.\", \"6. Audit the implementation's performance profile to check if it aligns with the derived theoretical complexity for graphs of varying treewidth.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Cross-reference the derived recurrence in /math with the implementation logic in /code; fallback: if recurrence is incorrect, rerun /math with refined constraints from /general-search).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers and lecture notes on DP over tree decompositions for graph problems like Independent Set.\", \"- [ ] /math: Define the DP state as a mapping from subsets of a bag's vertices to maximum weights.\", \"- [ ] /math: Derive the transition formulas for introduce, forget, and join nodes based on the state definition.\", \"- [ ] /code: Implement a function to find a heuristic tree decomposition of an input graph (e.g., using min-degree heuristic).\", \"- [ ] /code: Implement the DP traversal over the tree decomposition, applying the derived math formulas.\", \"- [ ] /math: Verify that the implemented DP logic in the /code artifact correctly reflects the derived recurrence relations.\", \"- [ ] /code: Create a test suite with small graphs to validate the DP output against a known-correct brute-force solver.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"dynamic-programming\", \"graph-theory\", \"parameterized-complexity\"], \"acceptance_criteria\": [\"- The mathematical derivation of the DP recurrence relations for all node types must be correct and self-contained.\", \"- The Python code must pass all unit tests, correctly solving Maximum Weight Independent Set on graphs with treewidth up to 5.\", \"- The final report must include a plot demonstrating that runtime scales polynomially with `n` for a fixed treewidth `k`, consistent with the O(2^k * poly(n)) theoretical complexity.\", \"- At least two relevant academic sources (e.g., from arXiv) must be cited.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 229, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the convergence properties of federated learning algorithms under additive homomorphic encryption for secure aggregation. Can you derive the convergence proof for gradient descent with a Paillier cryptosystem, considering the noise introduced by the encryption scheme and its impact on the convexity assumptions? Please also model the communication overhead and simulate the trade-off between privacy budget and model accuracy for a non-IID data distribution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org paillier homomorphic encryption federated learning convergence proof noise analysis\\\", mode=web)\", \"/math(Derive the convergence proof for federated stochastic gradient descent under Paillier encryption, explicitly modeling the noise term in the aggregated gradient and its effect on the expected squared gradient norm. Analyze the impact on Lipschitz continuity and convexity assumptions.)\", \"/code(Implement a Python simulation of this federated learning protocol using a library for the Paillier cryptosystem. Track convergence rate against a non-encrypted baseline and measure the computational overhead due to encryption and decryption operations. Use a standard convex loss function.)\"], \"route_rationale\": \"The plan starts with the 'blue' /general-search agent to gather state-of-the-art literature on homomorphic encryption in federated learning. This informs the /math agent's formal derivation of the convergence proof. Finally, the /code agent implements the derived protocol to simulate and validate the theoretical findings on privacy-accuracy trade-offs.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the convergence proof.\", \"A Python script implementing the federated learning simulation with the Paillier cryptosystem.\", \"A verification report comparing the simulated convergence rate to the theoretical bounds derived in the proof.\", \"A summary of at least two relevant academic papers from arXiv with their IDs.\", \"A JSON file containing the final metrics (convergence rate, overhead, accuracy).\"], \"thinking_outline\": [\"1. Formulate the federated optimization problem with secure aggregation using additive homomorphic encryption (specifically Paillier).\", \"2. Characterize the noise distribution introduced by the encryption/decryption process and its effect on the aggregated model updates.\", \"3. Derive the step-by-step convergence proof, adapting standard SGD convergence analysis to account for the cryptographic noise, and establish the conditions under which convergence is guaranteed.\", \"4. Implement the protocol in Python, simulating multiple clients, a central server, and the Paillier key generation, encryption, and decryption steps.\", \"5. Validate the implementation by comparing the empirical convergence rate from the simulation with the theoretical bounds derived in the mathematical proof.\", \"6. Audit the code for cryptographic best practices and check the mathematical proof for logical consistency and correctness of assumptions.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The /math agent's proof must explicitly reference literature found by /general-search. The /code agent's simulation parameters must match the assumptions in the /math proof. router QA will verify this alignment. fallback: If the proof assumptions are too strong for practical simulation, /math will be asked to relax them and re-derive.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on convergence of federated learning with homomorphic encryption, focusing on Paillier.\", \"- [ ] /math: Formulate the optimization problem and derive the convergence proof, accounting for encryption noise.\", \"- [ ] /code: Implement the federated learning simulation with Paillier encryption and a non-IID data split.\", \"- [ ] /code: Instrument the simulation to collect metrics on convergence, accuracy, and computational overhead.\", \"- [ ] /math: Verify that the assumptions used in the proof (e.g., Lipschitz constant, smoothness) are clearly stated and justified.\", \"- [ ] /code: Verify that the Python implementation correctly models the noise and aggregation steps described in the mathematical derivation.\", \"- [ ] /router: Perform final QA review, ensuring the theoretical proof and empirical simulation results are consistent and all acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"cryptography\", \"optimization-theory\"], \"acceptance_criteria\": [\"The submitted proof must correctly derive a convergence bound that explicitly includes a term for the cryptographic noise.\", \"The Python simulation must successfully run and generate plots showing the trade-off between cryptographic key size and model accuracy/convergence speed.\", \"The final report must compare the theoretical bounds with the empirical results and cite at least two relevant arXiv papers.\", \"The computational overhead metric must be reported in seconds-per-round for the cryptographic operations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 230, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a provably safe reinforcement learning controller for a robotic manipulator described by nonlinear dynamics on a Riemannian manifold. Please derive a safety filter using Control Barrier Functions (CBFs) based on Lie derivatives, formulating it as a Quadratic Program (QP). Then, provide a detailed implementation plan in JAX for integrating this QP solver with a SAC agent, and formally prove the forward invariance of the defined safe set under the resulting control law, analyzing convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org control barrier function reinforcement learning Lie derivative robotics\\\", mode=both)\", \"/math(Derive the CBF-based safety certificate for the given nonlinear dynamics. Formulate the Quadratic Program (QP) that minimally modifies the nominal RL action subject to the CBF constraint, referencing control theory principles., includes cybersecurity)\", \"/code(Implement a JAX-based QP solver as a safety layer for a SAC agent. The implementation must accept the Lie derivative terms from the /math agent's derivation and ensure real-time convergence for the control loop., using Python)\", \"/math(Prove the forward invariance of the safe set under the proposed CBF-QP controller. Analyze the conditions for feasibility of the QP and discuss the stability and convergence properties, potentially analyzing the Hessian eigenvalue structure.)\"], \"route_rationale\": \"The blue /general-search agent first gathers state-of-the-art literature. The first /math agent then performs the core theoretical derivation of the safety controller. The /code agent translates this complex mathematical formulation into a practical, high-performance JAX implementation. The final /math agent provides the crucial theoretical validation and proof of the safety guarantees, which is a required part of the user's request.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the CBF-QP safety filter, including the Lie derivative calculations.\", \"A Python script using JAX that implements the safety filter and integrates it with a standard SAC agent.\", \"A verification report containing the formal proof of forward set invariance and an analysis of the controller's convergence properties.\", \"A literature review summary with citations to at least two relevant arXiv preprints.\", \"A JSON file containing the final evaluation metrics (SafetyViolationRate, PolicyDeviationNorm).\"], \"thinking_outline\": [\"1. Formalize the robotic system's nonlinear dynamics and define the safe operating region mathematically.\", \"2. Propose a suitable Control Barrier Function (CBF) and derive the safety constraint inequality using Lie derivatives along the system dynamics.\", \"3. Formulate the safety filter as a Quadratic Program (QP) that computes the minimal perturbation to the learning agent's action to satisfy the CBF constraint.\", \"4. Verify the conditions under which the QP is guaranteed to be feasible, ensuring a safe control action can always be found.\", \"5. Outline the JAX implementation, focusing on the differentiable QP layer and its integration with the RL agent's policy update.\", \"6. Formally prove that the closed-loop system guarantees forward invariance of the safe set and validate the theoretical assumptions.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (proof) -> router QA. The initial search results from /general-search provide the theoretical foundation for the /math agent. The resulting derivation from /math is the specification for the /code agent. The code implementation and initial derivation are then passed to the second /math agent for the final proof. Verification: The router will check that the JAX code's QP formulation exactly matches the derived mathematical expressions. Fallback: If the proof in the final /math step fails, the task will be routed back to the first /math agent to reformulate the CBF or constraints.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on CBFs for RL, focusing on those with formal proofs.\", \"- [ ] /math: Derive the CBF and QP formulation, clearly defining all terms and matrices.\", \"- [ ] /code: Implement the QP solver layer in JAX and a wrapper for a standard RL agent.\", \"- [ ] /math: Verify that the derived QP conditions guarantee safety as per the CBF definition.\", \"- [ ] /code: Add unit tests to validate the QP solver's output against known solutions.\", \"- [ ] /math: Complete the formal proof of forward invariance and convergence.\", \"- [ ] /math: Cross-verify that the assumptions made in the proof are consistent with the implementation's constraints.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"control-theory\", \"safety-critical-ai\"], \"acceptance_criteria\": [\"- The mathematical derivation of the CBF-QP must be complete and rigorous.\", \"- The formal proof of safety (forward invariance of the safe set) must be valid under the stated assumptions.\", \"- The JAX implementation must correctly solve the derived QP and integrate with a SAC agent.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 231, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time SDE for a variance-preserving diffusion process and implement a Python simulator using the Euler-Maruyama method to sample from a 2D Gaussian mixture, analyzing the FID score convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time SDE diffusion\\\", mode=both)\", \"/math(Derive the reverse-time SDE for a variance-preserving diffusion process, starting from the forward process dxt = f(xt, t)dt + g(t)dwt, and clearly defining the score function \u2207x log p_t(x).)\", \"/code(Implement an Euler-Maruyama solver in Python for the derived SDE to sample from a 2D Gaussian mixture. Track the Fr\u00e9chet Inception Distance (FID) over training steps to measure convergence.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on score-based models. This theoretical basis is then handed to the math agent for a rigorous derivation of the core reverse-time SDE. Finally, the code agent implements the derived equation to provide empirical validation, checking if the theory holds in practice.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the reverse-time SDE.\", \"A self-contained Python script implementing the Euler-Maruyama solver for the diffusion model.\", \"A JSON file containing the final FID score and a list of FID values over sampling iterations.\", \"A brief summary report (Markdown) that includes a plot of FID convergence and cites at least two relevant papers.\"], \"thinking_outline\": [\"1. Review the mathematical formulation of forward diffusion processes and Stochastic Differential Equations (SDEs) from authoritative sources.\", \"2. Formally derive the corresponding reverse-time SDE, explicitly showing the dependency on the score function, \u2207x log p_t(x).\", \"3. Discretize the derived continuous-time SDE using the Euler-Maruyama method to formulate a practical sampling algorithm.\", \"4. Implement this algorithm in Python, targeting a simple, known 2D distribution like a Gaussian mixture model for clear evaluation.\", \"5. Validate the implementation by checking if the distribution of generated samples converges to the target distribution, quantitatively measured by the Fr\u00e9chet Inception Distance (FID).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers (e.g., Song et al.) to the math agent. The math agent provides the final, derived SDE to the code agent. The code agent provides the implementation, generated samples, and convergence plots back to the router for final quality assurance. Verification: The FID score from the code step must show a clear decreasing trend. Fallback: If FID does not converge, the math agent will re-audit the SDE derivation and the code agent will check the solver's numerical stability.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on score-based generative modeling and reverse-time SDEs (e.g., by Yang Song or Jascha Sohl-Dickstein).\", \"- [ ] /math: Derive the complete reverse-time SDE from the forward process defined in the literature, showing all intermediate steps.\", \"- [ ] /code: Implement the Euler-Maruyama solver for the derived SDE in Python using numpy/scipy.\", \"- [ ] /code: Generate 5000 samples from a 2D Gaussian mixture and compute the FID score against the ground truth distribution.\", \"- [ ] /code: Validate that the FID score decreases over sampling iterations, plotting the results to show convergence.\", \"- [ ] /router: Perform final QA check on the derivation's correctness, code implementation, and reported metrics against acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"diffusion-models\", \"generative-modeling\", \"stochastic-calculus\"], \"acceptance_criteria\": [\"The mathematical derivation of the reverse-time SDE is correct, complete, and references the source literature.\", \"The Python implementation successfully generates samples and achieves a final FID score below 50 on the target 2D Gaussian mixture distribution.\", \"The final report includes a plot showing the FID score converging over sampling iterations and correctly cites at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Lower is better. Compute the FID between 5000 generated samples and 5000 ground-truth samples. As no standard Inception network exists for 2D data, use the moments (mean and covariance) of the 2D point clouds directly: d^2 = ||\u03bc_x - \u03bc_y||^2 + Tr(\u03a3_x + \u03a3_y - 2(\u03a3_x \u03a3_y)^(1/2)).\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Lower is better. Estimate the KL divergence by discretizing the 2D space into a 100x100 grid, computing the empirical probability masses for both generated and target distributions, and applying the standard KL formula. Add a small epsilon to avoid log(0).\"}}", "categories": ["metrics_guidance"]}
{"index": 233, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a novel pruning algorithm for vision transformers. How can I formulate a constrained optimization problem using the method of Lagrange multipliers to minimize the L2 norm of the weights, subject to a constraint on the maximum Hessian eigenvalue to control the loss landscape sharpness and a budget for post-pruning inference latency?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org constrained optimization neural network pruning hessian eigenvalue lagrangian\\\", mode=both)\", \"/math(Formulate the Lagrangian for L2 weight norm minimization with inequality constraints on the Hessian's maximum eigenvalue and model latency. Derive the full Karush-Kuhn-Tucker (KKT) conditions for this specific non-convex problem, considering adversarial robustness.)\", \"/code(Implement the derived optimization algorithm in PyTorch, using power iteration for Hessian eigenvalue estimation. Validate the implementation by checking the KKT conditions' satisfaction at convergence on a pre-trained ViT model for a given latency budget., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey recent literature on Hessian-aware pruning from authoritative sources like arXiv. This informs the /math agent's formal derivation of the specific Lagrangian and KKT conditions. Finally, the /code agent implements and validates the mathematical formulation, providing an empirical test of the theory.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Lagrangian and KKT conditions for the specified pruning problem.\", \"- A Python script implementing the optimization algorithm and applying it to a vision transformer.\", \"- A results summary in markdown, including plots of the validation accuracy vs. sparsity and verification logs showing KKT condition satisfaction.\", \"- A bibliography with at least two cited arXiv preprints relevant to spectral conditioning and model pruning.\"], \"thinking_outline\": [\"1. Research existing literature on Hessian-based regularization and constrained optimization for neural network pruning to identify state-of-the-art approaches.\", \"2. Formulate the precise optimization problem, defining the objective function (L2 weight norm) and the inequality constraints (maximum Hessian eigenvalue, inference latency).\", \"3. Construct the Lagrangian and derive the Karush-Kuhn-Tucker (KKT) optimality conditions for the formulated problem.\", \"4. Implement a numerical solver in Python to find a solution that satisfies the derived KKT conditions for a vision transformer.\", \"5. Validate the solver's output by numerically checking the primal feasibility, dual feasibility, and complementary slackness conditions at the point of convergence.\", \"6. Verify that the empirical results (pruning level, accuracy, latency) align with the theoretical constraints defined in the mathematical formulation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. /general-search provides foundational papers to /math. /math provides a complete theoretical derivation to /code. /code implements the theory and produces numerical results and verification logs. Router QA performs a final audit. Verification: The KKT conditions derived by /math must be the same ones checked by /code. Fallback: If the KKT conditions are too complex to implement directly, /code will implement a dual gradient descent approach and report on convergence issues.\", \"todo_list\": [\"- [ ] /general-search: Find \u22652 recent arXiv papers on Hessian-aware pruning and Lagrangian methods.\", \"- [ ] /math: Derive the complete KKT conditions for the pruning optimization problem.\", \"- [ ] /math: Define the primal and dual update steps for an iterative solver.\", \"- [ ] /code: Implement the optimization algorithm in PyTorch for a Vision Transformer.\", \"- [ ] /code: Add logging to track primal/dual variables and KKT condition satisfaction.\", \"- [ ] /math: Verify that the numerical KKT checks in the code correctly implement the derived mathematical conditions.\", \"- [ ] /code: Generate plots showing accuracy vs. sparsity trade-off and final verification metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"lagrangian-multipliers\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the KKT conditions must be complete and correct.\", \"- The Python code must successfully execute and produce a pruned model whose performance is reported.\", \"- The final report must include a verification section that numerically demonstrates the satisfaction of the KKT conditions (within a tolerance) at the solution.\", \"- At least two relevant arXiv papers with IDs must be cited in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 235, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a formal derivation and implementation for a dynamic programming algorithm using matrix exponentiation to count the number of simple paths of length exactly L between all pairs of vertices in a directed acyclic graph (DAG). Additionally, analyze its time complexity and compare its convergence properties against iterative methods like Bellman-Ford for all-pairs shortest path. Finally, can you prove whether this technique can be adapted to find the K-th shortest path, not just count paths, while maintaining polynomial complexity?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org path counting matrix exponentiation 'k-th shortest path' complexity analysis\\\", mode=web)\", \"/math(Derive the dynamic programming recurrence for all-pairs path counting of length L. Formalize this using adjacency matrix exponentiation. Prove the time complexity is O(V^3 log L) and analyze the algebraic structure for extension to the K-th shortest path problem, considering convergence constraints.)\", \"/code(Implement the matrix exponentiation algorithm in Python using NumPy for efficient matrix operations. Develop a suite of test cases for DAGs to validate the implementation's correctness and profile its performance against the theoretical complexity.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on matrix methods for path problems. This informs the /math agent, which will perform the core theoretical work: deriving the DP recurrence, formalizing the proof, and analyzing complexity. Finally, the /code agent implements the verified mathematical formulation and validates it empirically.\", \"expected_artifacts\": [\"A formal mathematical proof of the DP recurrence relation and its time complexity analysis in a PDF document.\", \"A Python script implementing the matrix exponentiation algorithm for path counting.\", \"A verification report containing the output of the implementation on test graphs and a performance profile.\", \"A summary of findings from the literature search, including at least two cited arXiv preprints.\"], \"thinking_outline\": [\"1. Research existing literature on algebraic path problems, specifically using matrix exponentiation for path counting and its relation to the K-th shortest path problem.\", \"2. Formulate the dynamic programming recurrence relation for counting paths of length L between any two vertices i and j.\", \"3. Abstract this recurrence into a matrix form and leverage the property of matrix exponentiation to solve for length L efficiently. Derive the O(V^3 log L) time complexity.\", \"4. Validate the correctness of the mathematical derivation, ensuring base cases (L=0, L=1) are handled properly.\", \"5. Investigate the algebraic structure (e.g., semirings) required to extend this from path counting to finding the K-th shortest path and prove the complexity bounds for such an extension.\", \"6. Check the final implementation against the formal proof and a set of hand-crafted test cases with known solutions.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to /math. The /math agent produces a formal specification and proof, which is handed to /code for implementation. The /code agent's output is then checked against the mathematical specification during router QA. Fallback: If the /code implementation fails verification on test cases, the discrepancy is reported back to the /math agent to audit the proof for edge cases.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on algebraic path problems and K-th shortest path algorithms.\", \"- [ ] /math: Derive the DP recurrence and prove the correctness of the matrix exponentiation method.\", \"- [ ] /math: Analyze the algebraic structure for extending the solution to the K-th shortest path problem.\", \"- [ ] /code: Implement the path counting algorithm using NumPy.\", \"- [ ] /code: Create a validation suite with diverse DAGs, including disconnected and dense graphs.\", \"- [ ] /math: Verify that the implemented code logic is a faithful representation of the derived mathematical formula.\", \"- [ ] /code: Profile the implementation's runtime and compare it against the theoretical O(V^3 log L) complexity.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"dynamic-programming\", \"graph-theory\", \"computational-complexity\"], \"acceptance_criteria\": [\"- The mathematical proof for the O(V^3 log L) complexity must be correct and rigorously argued.\", \"- The Python implementation must pass all test cases in the provided validation suite, correctly counting paths for various lengths L.\", \"- The analysis of the K-th shortest path extension must correctly identify the required algebraic structures (e.g., tropical semiring) and prove the resulting complexity.\", \"- At least two relevant academic papers (arXiv or peer-reviewed) must be cited in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 236, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the evidence lower bound (ELBO) for a variational autoencoder where the posterior is approximated by the solution to a stochastic differential equation, specifically a Neural SDE. Then, implement a Bayesian Neural Network using this framework to model a time-series with heteroscedastic noise, and validate the convergence properties of the stochastic gradient Langevin dynamics sampler used for inference.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Neural SDE' 'variational inference' Bayesian posterior convergence\\\", mode=both)\", \"/math(Derive the evidence lower bound (ELBO) for a variational posterior governed by a Neural SDE, applying Ito's lemma and Girsanov's theorem for the change of measure. This derivation must inform the Bayesian model.)\", \"/code(Implement a Bayesian Neural Network in JAX using the derived Neural SDE posterior. Use a stochastic gradient Langevin dynamics sampler and evaluate convergence diagnostics like the Gelman-Rubin statistic., using Python)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational literature on Neural SDEs. This research informs the /math agent's formal derivation of the ELBO. The resulting mathematical expression is then handed to the /code agent for implementation and empirical validation of the MCMC sampler's convergence.\", \"expected_artifacts\": [\"A LaTeX document providing the step-by-step derivation of the Neural SDE ELBO, including all assumptions and cited theorems.\", \"A JAX/Flax Python script implementing the Bayesian Neural Network with the SDE-based variational posterior.\", \"A summary report in Markdown presenting convergence diagnostics (e.g., Gelman-Rubin plots) and predictive performance on a synthetic dataset.\", \"A bibliography file with citations for at least two relevant academic papers, including arXiv IDs.\"], \"thinking_outline\": [\"1. Research state-of-the-art literature on Neural SDEs for variational inference to establish a theoretical foundation.\", \"2. Formally derive the ELBO for the specified model, clearly stating all assumptions and applying relevant theorems from stochastic calculus like Ito's lemma and Girsanov's theorem.\", \"3. Implement the derived objective function and the SDE solver within a JAX-based Bayesian Neural Network framework.\", \"4. Validate the implementation by checking gradient correctness using JAX's tools and ensuring the SDE solver's numerical stability on simple cases.\", \"5. Run experiments on a synthetic time-series dataset, collecting MCMC convergence diagnostics across multiple chains.\", \"6. Verify that the empirical results and convergence diagnostics (e.g., R-hat values) are consistent with the theoretical properties of the Langevin dynamics sampler.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The /math agent's final ELBO expression is used as the loss function by the /code agent. The code agent's convergence plots and metrics are passed to the router for final review against theoretical expectations; fallback: If convergence fails, the /code agent will simplify the SDE drift/diffusion networks and rerun).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Neural SDEs and variational inference, focusing on the ELBO derivation and Girsanov's theorem.\", \"- [ ] /math: Derive the ELBO using Ito's lemma and clearly define all terms.\", \"- [ ] /code: Implement the Neural SDE solver and the Bayesian Neural Network architecture in JAX.\", \"- [ ] /code: Implement the training loop using stochastic gradient Langevin dynamics and necessary MCMC diagnostics.\", \"- [ ] /math: Formally define the Gelman-Rubin (R-hat) statistic to be used for convergence analysis.\", \"- [ ] /code: Run experiments on a synthetic dataset and generate plots for convergence diagnostics and predictive uncertainty.\", \"- [ ] /math: Verify that the implemented loss function in the code repository correctly matches the derived ELBO expression from the LaTeX document.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-deep-learning\", \"stochastic-differential-equations\", \"variational-inference\"], \"acceptance_criteria\": [\"The ELBO derivation provided by the /math agent must be mathematically sound and correctly apply theorems from stochastic calculus.\", \"The code implementation must run without errors and demonstrate MCMC sampler convergence with a Gelman-Rubin statistic (R-hat) below 1.1 for all major parameters.\", \"The final report must clearly present the model's predictive uncertainty on a hold-out dataset, visualized with credible intervals.\", \"At least two relevant arXiv papers must be cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 237, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a variational inference procedure for a hierarchical Bayesian model in bioinformatics. The posterior is non-Gaussian and complex. I want to use a more expressive approximating family than mean-field, specifically a planar normalizing flow. Please derive the evidence lower bound (ELBO) for this setup, detail the reparameterization trick for the flow, and implement it in PyTorch to check for convergence and posterior quality.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference normalizing flows 'planar flow' Rezende Mohamed\\\", mode=web)\", \"/math(Derive the full Evidence Lower Bound (ELBO) for a Bayesian hierarchical model where the variational posterior is parameterized by a multi-layer planar normalizing flow. The derivation must explicitly handle the log-determinant of the Jacobian and the Kullback-Leibler divergence term.)\", \"/code(Implement the amortized variational inference scheme in PyTorch for the specified Bayesian bioinformatics model. The implementation must use a planar flow, track ELBO convergence, and include posterior predictive checks for model validation., using Python)\", \"/math(Verify the analytical gradient of the ELBO derived previously by comparing it to a numerical gradient computed via finite differences on the Python implementation for a single data point. Audit the convergence criteria for stability.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on normalizing flows. This informs the /math agent's formal derivation of the model's objective function (ELBO). The /code agent then implements this mathematical specification. A final /math call provides a critical verification step, ensuring the theoretical derivation and practical implementation are consistent before final analysis.\", \"expected_artifacts\": [\"- A LaTeX/PDF document containing the full mathematical derivation of the ELBO for a planar normalizing flow.\", \"- A Python script (`.py`) implementing the variational inference procedure.\", \"- A summary report in Markdown format, including plots of the ELBO convergence and results from posterior predictive checks.\", \"- A `citations.bib` file with at least two relevant papers on normalizing flows and variational inference.\", \"- A verification log showing the comparison between analytical and numerical gradients.\"], \"thinking_outline\": [\"1. Formalize the target Bayesian hierarchical model and its posterior distribution, identifying the sources of complexity that motivate using normalizing flows.\", \"2. Use search to find the original papers on normalizing flows (e.g., by Rezende & Mohamed) to understand the mechanics of planar flows, including the transformation and the calculation of the Jacobian determinant.\", \"3. Derive the ELBO for the model, carefully applying the change of variables formula to account for the normalizing flow and detailing how the reparameterization trick enables low-variance gradient estimation.\", \"4. Implement the model, the variational distribution (planar flow), and the stochastic optimization loop in PyTorch, ensuring metrics like the ELBO are logged correctly.\", \"5. Validate the implementation's correctness by implementing a numerical gradient checker and comparing its output to the analytical gradients produced by PyTorch's autograd for a small, fixed input.\", \"6. Evaluate the quality of the learned posterior approximation by analyzing the ELBO convergence curve and performing posterior predictive checks to see if the model can generate realistic data.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The /math agent uses papers from /general-search as a theoretical basis. The /code agent implements the exact formulas from the /math agent's derivation. The second /math call verifies the /code output against the theory. Verification failure at the last step triggers a loop back to the first /math agent to check the derivation. Fallback: If planar flows prove unstable, the /code agent will revert to a simpler radial flow and notify the router.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on normalizing flows for variational inference (e.g., Rezende & Mohamed, 2015; Kingma et al., 2016).\", \"- [ ] /math: Provide a step-by-step derivation of the ELBO, including the log-prob of the base distribution and the log-determinant Jacobian term.\", \"- [ ] /code: Implement the Bayesian hierarchical model and the planar normalizing flow variational family in PyTorch.\", \"- [ ] /code: Write the training loop using the Adam optimizer to maximize the ELBO via stochastic gradient ascent.\", \"- [ ] /math: Write the mathematical formula for the numerical gradient using the finite difference method.\", \"- [ ] /code: Implement the gradient checker to compare the analytical gradient from autograd with the numerical one.\", \"- [ ] /code: Generate and plot the ELBO convergence curve and posterior predictive check visualizations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"normalizing-flows\", \"bayesian-deep-learning\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO is complete, self-contained, and mathematically sound.\", \"- The Python implementation runs without errors, and the reported ELBO shows a clear trend of convergence during training.\", \"- The verification step confirms that the analytical and numerical gradients match within a small tolerance (e.g., 1e-4).\", \"- The final report correctly interprets the convergence plots and cites at least two relevant academic papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"Computed as E_q[log p(x,z) - log q(z|x)]. Track its value per training epoch. A monotonically increasing curve that flattens out indicates successful optimization.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"Calculated from MCMC diagnostics on the resulting posterior samples if applicable, or visual inspection of posterior predictive distributions. Higher ESS or realistic predictive samples indicate a better approximation.\"}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "metrics_computation", "non_canonical_route"]}
{"index": 238, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to solve the 2D incompressible Navier-Stokes equations for lid-driven cavity flow using a Physics-Informed Neural Network (PINN). Please derive the specific loss function, including terms for the momentum equations, the incompressibility constraint (continuity equation), and the boundary conditions. Then, implement this in JAX, providing a verification against a known benchmark result, like from Ghia et al. (1982), and analyze the convergence properties of the training.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org physics-informed neural network Navier-Stokes lid-driven cavity Ghia benchmark\\\", mode=both)\", \"/math(Derive the full PINN loss function for the Navier-Stokes equations, incorporating the momentum, continuity (incompressibility), and Dirichlet boundary conditions. Analyze the Euler-Lagrange form and its implications for convergence.)\", \"/code(Implement the Navier-Stokes PINN in JAX using automatic differentiation for the PDE residuals. Validate the model by comparing velocity profiles against Ghia et al. benchmark data, reporting L2 error and ensuring Bayesian convergence diagnostics., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather seminal papers on PINNs for fluid dynamics, establishing a theoretical foundation. Next, /math is used for the rigorous derivation of the PDE-based loss function, a critical step before implementation. Finally, /code implements the derived theory in JAX and performs a quantitative validation against a known benchmark, ensuring the model's physical accuracy.\", \"expected_artifacts\": [\"A complete mathematical derivation of the PINN loss function for the 2D Navier-Stokes equations.\", \"A well-documented JAX implementation of the PINN solver, including training and validation scripts.\", \"A plot comparing the PINN's predicted velocity profiles (u and v) against the Ghia et al. (1982) benchmark data at the cavity's geometric centerlines.\", \"A report summarizing the final PDE residual loss, the L2 error metric against the benchmark, and a list of at least 2 cited arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the 2D incompressible Navier-Stokes equations in their residual form, identifying all terms (momentum, continuity, boundary conditions) that must be enforced.\", \"2. Theoretically derive the composite loss function for the PINN, ensuring each component of the PDE residual and boundary conditions is correctly weighted and represented.\", \"3. Implement the PINN architecture and the custom loss function in JAX, leveraging `vmap` and `grad` for efficient computation of derivatives.\", \"4. Validate the implementation by running a test case with a known analytical solution (e.g., Kovasznay flow) to check for correctness before tackling the full problem.\", \"5. Train the model on the lid-driven cavity problem and compare the resulting velocity profiles against the Ghia et al. benchmark data to quantify accuracy.\", \"6. Audit the convergence behavior of the loss components to check for issues like stiffness or gradient pathologies during training.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation, which serves as the blueprint for the code agent. The code agent implements and validates this blueprint. Verification: The code agent must confirm its implementation matches the math agent's derivation, and the final results must align with the benchmark data found by the search agent. Fallback: If validation fails, the code agent flags the discrepancy, and the task may be re-routed to the math agent to check for errors in the loss formulation.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers (e.g., Raissi et al.) on PINNs for Navier-Stokes and locate the Ghia et al. (1982) benchmark dataset.\", \"- [ ] /math: Derive the complete loss function for the 2D incompressible Navier-Stokes equations, clearly defining terms for momentum, continuity, and boundary conditions.\", \"- [ ] /code: Implement the neural network architecture and the custom loss function in JAX.\", \"- [ ] /code: Implement the training loop, data sampling strategy (collocation points), and load the Ghia benchmark data for comparison.\", \"- [ ] /math: Verify that the derivatives implemented in the JAX code correspond exactly to the terms derived in the mathematical formulation.\", \"- [ ] /code: Generate plots comparing the u and v velocity profiles from the trained PINN against the benchmark data and compute the L2 error metric.\", \"- [ ] /router: Perform final QA review of all artifacts, ensuring the L2 error is within an acceptable range and the derivation is sound.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-neural-networks\", \"pde-solvers\", \"computational-fluid-dynamics\"], \"acceptance_criteria\": [\"- The mathematical derivation of the loss function must be complete and correct.\", \"- The JAX implementation must successfully train and produce a velocity field for the lid-driven cavity problem.\", \"- The L2 relative error of the predicted velocity profiles against the Ghia et al. benchmark data must be less than 5%.\", \"- The final report must include at least two citations to relevant arXiv preprints or peer-reviewed articles.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 239, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the theoretical underpinnings of the softmax temporal kernel in transformers, as proposed by Tsai et al. (2019). Can you derive the connection between the self-attention mechanism and kernel methods, specifically proving that the attention operation can be framed as a kernel smoother? Furthermore, implement a custom attention layer in PyTorch that explicitly computes the kernel matrix and analyze its eigenvalue decay to verify its low-rank approximation potential, targeting a high AUPRC on a synthetic sequence modeling task.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org transformer as kernel smoother softmax temporal kernel eigenvalue decay\\\", mode=web)\", \"/math(Derive the equivalence of the scaled dot-product attention mechanism to a kernel smoother, detailing the role of the softmax function and proving the resulting kernel is positive semi-definite. Analyze the convergence properties and reference Bayesian interpretations.)\", \"/code(Implement a custom PyTorch attention module that explicitly materializes the softmax kernel matrix. Perform singular value decomposition (SVD) on this matrix for synthetic sequences and plot the eigenvalue decay to test the low-rank hypothesis. Validate with a high AUPRC., using Python)\"], \"route_rationale\": \"/general-search is invoked first using the blue agent to ground the task in existing literature as requested by the user. /math is then used for the formal derivation before any implementation. Finally, /code implements the derived theory and performs empirical validation through eigenvalue analysis, which requires a computational framework.\", \"expected_artifacts\": [\"A formal mathematical derivation in LaTeX connecting self-attention to kernel smoothing.\", \"A Python script (`attention_kernel.py`) containing the PyTorch implementation of the custom attention layer and the SVD analysis.\", \"A plot (`eigenvalue_decay.png`) showing the sorted eigenvalues of the attention kernel matrix.\", \"A summary report (`report.md`) detailing the findings, including the AUPRC metric and citations to at least two relevant arXiv papers.\", \"A verification log (`verification.log`) documenting the check between the mathematical derivation and the code's computational graph.\"], \"thinking_outline\": [\"1. Review foundational papers (e.g., Tsai et al., 2019) on transformers as kernel methods to understand the existing theoretical framework.\", \"2. Formulate the self-attention mechanism as a weighted sum and formally derive its representation as a kernel smoother, paying close attention to the properties induced by the softmax function.\", \"3. Implement the custom attention mechanism in PyTorch, ensuring the kernel matrix is explicitly computed and accessible.\", \"4. Design an experiment to compute the eigenvalue spectrum of the kernel matrix for sequences of varying lengths and validate the low-rank approximation hypothesis by observing the decay.\", \"5. Verify that the mathematical derivation for the kernel smoother is correctly implemented in the PyTorch code by checking the computational graph's logic against the derived equations.\", \"6. Check the final AUPRC score on the synthetic task to ensure the implementation is not just theoretically sound but also performs as expected.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The SVD implementation in /code must analyze the kernel derived in /math, and the report must cite sources from /general-search; fallback: If eigenvalue decay does not indicate a low-rank structure, re-run /code with different synthetic data distributions or review the /math derivation for incorrect assumptions.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on transformers as kernel methods, focusing on the softmax temporal kernel.\", \"- [ ] /math: Produce a step-by-step derivation proving self-attention's equivalence to a kernel smoother.\", \"- [ ] /math: Analyze the positive semi-definite property of the resulting kernel matrix.\", \"- [ ] /code: Implement the custom attention layer in PyTorch, exposing the kernel matrix.\", \"- [ ] /code: Write the SVD analysis script to compute and plot the eigenvalue decay.\", \"- [ ] /math: Verify that the code's kernel computation logic in the forward pass correctly implements the derived mathematical formula.\", \"- [ ] /code: Validate the implementation achieves a high AUPRC on the specified synthetic task.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"attention-mechanisms\", \"kernel-methods\", \"self-supervised-learning\"], \"acceptance_criteria\": [\"The provided LaTeX derivation correctly and completely shows the equivalence between scaled dot-product attention and a kernel smoother.\", \"The Python script runs without errors and generates a plot clearly showing rapid eigenvalue decay for the attention kernel matrix.\", \"The final report achieves an AUPRC score of > 0.9 on the synthetic sequence modeling task.\", \"The report includes citations for at least two relevant arXiv papers with valid IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 240, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze a non-trivial Structural Causal Model (SCM) involving unobserved confounders. Can you derive the causal effect of an intervention, P(Y|do(X=x)), using Pearl's do-calculus rules? The goal is to find a valid adjustment formula and then implement a corresponding semi-parametric estimator, like an augmented inverse probability weighting (AIPW) estimator. Please provide the full derivation, a Python implementation using `dowhy` and `econml`, and a simulation study to validate the estimator's consistency and bootstrap confidence interval coverage.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:causal.cs.ucla.edu do-calculus rules identification unobserved confounders front-door criterion causal bayesian networks\\\", mode=rag)\", \"/math(Derive the causal estimand P(Y|do(X=x)) from the provided Structural Causal Model using the three rules of do-calculus. The derivation must explicitly handle unobserved confounders and justify each step, leading to a non-parametric identification formula suitable for Bayesian econometrics.)\", \"/code(Implement an Augmented Inverse Propensity Weighting (AIPW) estimator in Python for the derived causal estimand. Use `dowhy` for the graph and `econml` for the estimator, and conduct a simulation study to verify unbiasedness and calculate bootstrap confidence intervals.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather seminal literature on do-calculus and identification strategies for causal inference, ensuring a strong theoretical foundation. This feeds into the /math agent, which performs the core symbolic derivation. Finally, the /code agent translates the derived mathematical formula into a robust Python implementation with a simulation study for empirical validation of the causal estimator.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the causal estimand using do-calculus rules.\", \"A Python script implementing the simulation study and the AIPW estimator.\", \"A final report in PDF format summarizing the methodology, derivation, simulation results, and including plots of estimator bias and confidence interval coverage.\", \"A JSON file containing the computed metrics from the simulation (ATE bias, variance, CI coverage).\", \"A list of at least 2 cited arXiv papers on causal identification or semi-parametric estimation.\"], \"thinking_outline\": [\"1. Formulate the problem by formally defining the Structural Causal Model (SCM) and the target causal query P(Y|do(X=x)).\", \"2. Apply the rules of do-calculus systematically to the SCM's graphical representation to manipulate the expression and remove the `do` operator.\", \"3. Derive the final identifiable, non-parametric estimand expressed in terms of observational probabilities.\", \"4. Verify the derived formula by checking if all do-calculus rules were applied correctly and ensuring the final expression contains only observable quantities.\", \"5. Implement the corresponding semi-parametric estimator (AIPW) and a data-generating process based on the SCM for a simulation study.\", \"6. Validate the implementation by running the simulation to check for estimator consistency, unbiasedness, and the empirical coverage of the bootstrap confidence intervals, comparing them to theoretical expectations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent verifies literature aligns with the SCM assumptions; /code agent validates that the implementation matches the derived formula from /math; fallback: if identification fails, /math agent reports non-identifiability and suggests alternative assumptions or data requirements to the router).\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on do-calculus and AIPW estimators, focusing on identification with unobserved confounders.\", \"- [ ] /math: Using the SCM graph, derive the causal identification formula for P(Y|do(X=x)).\", \"- [ ] /math: Document each step of the do-calculus derivation in LaTeX, citing the specific rule used.\", \"- [ ] /code: Implement the data generating process from the SCM and the AIPW estimator.\", \"- [ ] /code: Run a Monte Carlo simulation to evaluate the estimator's bias, variance, and confidence interval coverage.\", \"- [ ] /math: Verify that the implemented propensity score and outcome models in the /code step are consistent with the assumptions made during the /math derivation.\", \"- [ ] /code: Verify that the simulation results (e.g., low bias) confirm the correctness of the derived formula from /math.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"econometrics\"], \"acceptance_criteria\": [\"- The mathematical derivation must correctly apply the rules of do-calculus to arrive at a valid, identifiable expression.\", \"- The Python implementation must correctly implement the AIPW estimator and a simulation that demonstrates unbiasedness of the estimator as the sample size grows.\", \"- The final report must clearly connect the theoretical derivation to the implementation and simulation results, including visualizations.\", \"- The project must cite at least two relevant academic papers (e.g., from arXiv) on causal inference.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 241, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the application of Physics-Informed Neural Networks (PINNs) for solving the 2D incompressible Navier-Stokes equations. I need a comprehensive analysis that starts with a literature review of modern loss function formulations, particularly those addressing convergence issues in high Reynolds number regimes. Subsequently, derive the complete loss function, including the PDE residual, boundary, and initial conditions. Finally, implement this in JAX, train it on the lid-driven cavity problem, and validate the solution against established benchmark data, analyzing the convergence behavior of the loss components.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org physics-informed neural networks Navier-Stokes loss function convergence high Reynolds number\\\", mode=rag)\", \"/math(Formulate the PINN loss for Navier-Stokes, incorporating insights on adversarial weighting and curriculum learning from the literature search. Derive all partial derivatives for the PDE residual and analyze the theoretical convergence criteria.)\", \"/code(Implement the derived PINN model and loss function in JAX. Validate the implementation against a known benchmark for the lid-driven cavity problem, tracking mean squared error and ensuring CUDA optimization for the energy budget., using Python)\"], \"route_rationale\": \"The blue /general-search agent is needed first to survey the latest arXiv research on PINN loss functions for fluid dynamics. The /math agent then formalizes these findings into a specific mathematical derivation. Finally, the /code agent implements and empirically validates the derived model, providing concrete performance metrics.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the Navier-Stokes PINN loss function.\", \"A Python script (JAX) implementing the PINN model, training loop, and validation.\", \"A plot comparing the PINN-predicted velocity field to the benchmark data for the lid-driven cavity problem.\", \"A JSON file containing the final L2 relative error, PDE residual loss, and training history.\", \"A bibliography citing at least two relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Survey recent literature on advanced PINN loss formulations for the Navier-Stokes equations, focusing on stability and convergence.\", \"2. Formulate the complete loss function, including data, boundary, and PDE residual terms, based on the literature review.\", \"3. Derive the analytical form of the partial derivatives required for the Navier-Stokes PDE residual.\", \"4. Implement the network and loss function in JAX, and verify the automatic differentiation output for the PDE residual against a manually computed sample point.\", \"5. Train the model on the lid-driven cavity benchmark and validate the final velocity and pressure fields against Ghia et al.'s benchmark data.\", \"6. Analyze the training dynamics, document the final error metrics, and check for signs of training pathologies like vanishing gradients.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /code implementation is audited against the /math derivation for correctness of the PDE residual terms; fallback: If implementation deviates, /code must re-implement the specific tensor operations or /math must clarify the derivation).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 recent arXiv papers on PINN loss functions for Navier-Stokes.\", \"- [ ] /math: Derive the complete loss function and all PDE residual terms based on the literature.\", \"- [ ] /math: Formally document the derivation in a shareable format (LaTeX/PDF).\", \"- [ ] /code: Implement the PINN architecture and the custom loss function in JAX.\", \"- [ ] /code: Train the model on the lid-driven cavity problem and generate prediction plots.\", \"- [ ] /math: Verify that the gradients computed in the /code implementation match the formal derivation for a test input.\", \"- [ ] /code: Validate the final model's output against the Ghia et al. benchmark data and compute the L2 relative error.\", \"- [ ] router QA: Review all artifacts for consistency, correctness, and adherence to acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"fluid-dynamics\"], \"acceptance_criteria\": [\"- The final L2 relative error for the velocity field must be below 5e-2 when compared to the Ghia et al. benchmark data.\", \"- The implemented PDE residual in the code must exactly match the mathematical derivation.\", \"- The final report must cite at least two relevant arXiv papers with IDs, published within the last 3 years.\", \"- The generated plots must clearly visualize the predicted u and v velocity fields and the point-wise error.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 242, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the closed-form expression for the Expected Improvement (EI) acquisition function for Gaussian Process-based Bayesian Optimization. Then, implement a sequential Bayesian optimization loop in Python using this EI function to find the minimum of the multi-dimensional Ackley function. Analyze the convergence rate empirically and compare it to theoretical guarantees from recent literature, particularly concerning the choice of kernel and its impact on the posterior variance.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org gaussian process bayesian optimization convergence guarantees regret bounds\\\", mode=web)\", \"/math(Derive the closed-form expression for Expected Improvement under a Gaussian Process posterior, showing how it depends on the current best value, mean, and variance. This Bayesian derivation must be self-contained.)\", \"/code(Implement a Bayesian optimization loop in Python using JAX or NumPy. The implementation must include a Gaussian Process with an RBF kernel and the derived Expected Improvement function, targeting the Ackley function and tracking regret_at_T.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather theoretical context on convergence guarantees, which is essential for the final analysis. Next, the /math agent performs the core theoretical work of deriving the acquisition function. Finally, the /code agent implements the derived mathematics and runs the experiment, providing the empirical results needed for the analysis.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the Expected Improvement acquisition function.\", \"A standalone Python script (`bayesian_optimizer.py`) implementing the Bayesian optimization loop with a Gaussian Process model.\", \"A final report in Markdown (`report.md`) containing convergence plots, the primary/secondary metrics, and a discussion comparing empirical results to theoretical guarantees with citations.\", \"A `verification_log.txt` detailing the consistency check between the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Research theoretical convergence guarantees for Bayesian Optimization using Gaussian Processes, focusing on the role of the kernel and the resulting regret bounds.\", \"2. Derive the closed-form expression for the Expected Improvement (EI) acquisition function, clearly stating all assumptions about the Gaussian Process posterior distribution.\", \"3. Implement the Gaussian Process model and the derived EI function in Python, using a standard radial basis function (RBF) kernel.\", \"4. Validate the implementation by checking the EI computation against a known numerical example or a trusted library's output for a single, deterministic step.\", \"5. Construct the sequential optimization loop to minimize the Ackley function, logging the chosen points and observed values at each iteration.\", \"6. Verify that the observed convergence performance aligns with the theoretical bounds discovered in the research phase and document any discrepancies in the final report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides arXiv papers to the math agent to frame the derivation. The math agent's final derivation is the direct specification for the code agent's implementation of the EI function. The code agent produces the script and results report. Router QA performs a final consistency check. Fallback: If the code's output seems inconsistent with theory, the math agent will be re-engaged to audit the code's implementation of the EI formula.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 recent arXiv papers on Bayesian optimization convergence guarantees.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the Expected Improvement (EI) closed-form expression.\", \"- [ ] /code: Implement the Gaussian Process model with a configurable kernel (defaulting to RBF).\", \"- [ ] /code: Implement the EI acquisition function precisely according to the mathematical derivation.\", \"- [ ] /math: Verify that the code agent's EI implementation is a correct translation of the derived formula by testing it on a fixed input vector.\", \"- [ ] /code: Execute the full optimization loop on the 2D Ackley function for 100 iterations and generate convergence plots and metrics.\", \"- [ ] /code: Audit the final regret metric to ensure the observed convergence rate is plausible given the theory from the literature search.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-processes\", \"bayesian-optimization\", \"gaussian-processes\"], \"acceptance_criteria\": [\"The mathematical derivation for Expected Improvement must be complete, correct, and self-contained.\", \"The Python implementation must successfully run and demonstrate a clear trend of minimizing the Ackley function over 100 iterations.\", \"The final report must cite at least two relevant arXiv papers and explicitly compare the empirical `regret_at_T` to theoretical expectations.\", \"The verification log must confirm a positive match between the math derivation and the code's EI calculation logic.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"computation_guidance\": {\"regret_at_T\": \"Difference between the true minimum of the Ackley function (0) and the minimum value found by the optimizer after T=100 iterations.\", \"mean_posterior_variance\": \"Average posterior variance at the queried points over the T=100 iterations, indicating the balance of exploration vs. exploitation.\"}}}", "categories": ["advanced", "metrics_computation", "metrics_guidance"]}
{"index": 243, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a federated learning system for image classification on a non-IID partitioned CIFAR-10 dataset. The core requirement is to use the Paillier partially homomorphic encryption scheme for secure aggregation of model gradients to ensure privacy. Please provide a formal derivation of the secure aggregation protocol, analyze its impact on convergence compared to standard FedAvg, and quantify the communication and computational overhead introduced by the encryption.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org OR site:eprint.iacr.org 'Paillier cryptosystem' 'federated learning' 'secure aggregation' convergence analysis non-IID\\\", mode=web)\", \"/math(Derive the secure federated aggregation protocol using Paillier's additive homomorphism. Formally define the key generation, client-side encryption, server-side homomorphic addition, and decryption steps, ensuring the convergence properties are considered.)\", \"/code(Implement the federated learning system in Python with PyTorch, integrating a Paillier encryption library. Add runtime validation to check that decrypted aggregated gradients match the sum of plaintext gradients and track communication overhead metrics.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art literature on homomorphic encryption in federated learning. The findings inform the /math agent's formal derivation of the cryptographic protocol. Finally, the /code agent implements this verified protocol, adding its own runtime checks.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Paillier-based secure aggregation protocol in LaTeX/PDF format.\", \"A well-documented Python implementation of the federated learning system, including simulation scripts.\", \"A results report comparing the secure protocol against baseline FedAvg, with plots showing test accuracy vs. communication rounds and tables detailing computational overhead.\", \"A bibliography file with citations for at least two relevant research papers.\"], \"thinking_outline\": [\"1. Research the Paillier cryptosystem and its application in secure aggregation for federated learning to understand the protocol's mechanics and security assumptions.\", \"2. Formulate the mathematical protocol, detailing key distribution, client-side gradient encryption, server-side homomorphic addition of ciphertexts, and final decryption of the aggregated model update.\", \"3. Implement the protocol in Python, simulating a federated network with a non-IID data distribution (e.g., Dirichlet distribution over labels).\", \"4. Verify the correctness of the cryptographic implementation by designing a unit test where the decrypted sum of encrypted vectors matches the plaintext sum of those vectors.\", \"5. Conduct experiments to measure the impact on model accuracy (convergence) and the performance overhead (latency, communication size) compared to a non-private baseline.\", \"6. Validate that the final report's claims regarding privacy, convergence, and overhead are rigorously supported by the experimental metrics and the mathematical derivation.\"], \"handoff_plan\": \"/general-search (papers/concepts) -> /math (formal derivation) -> /code (verified implementation). The /code agent must implement the exact protocol from /math. Verification: The /code agent will run unit tests to confirm cryptographic integrity (dec(enc(a) + enc(b)) == a + b). Fallback: If the derivation from /math is unimplementable or numerically unstable, the /code agent will flag the issue with specific examples and request a revised derivation from /math. The final artifacts are reviewed by router QA.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on using Paillier encryption for federated learning.\", \"- [ ] /math: Produce a complete derivation of the secure aggregation protocol, including security parameter choices.\", \"- [ ] /code: Set up the federated learning simulation environment with a non-IID data partition for CIFAR-10.\", \"- [ ] /code: Implement the client and server logic, integrating the Paillier cryptosystem for gradient updates.\", \"- [ ] /math: Verify that the Python implementation's cryptographic operations align with the formal derivation.\", \"- [ ] /code: Run experiments, collect metrics on accuracy and overhead, and generate comparison plots.\", \"- [ ] /code: Write the final report summarizing the methodology, results, and analysis.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"privacy-preserving-ml\", \"homomorphic-encryption\"], \"acceptance_criteria\": [\"- The Python code must successfully run a simulation for at least 50 communication rounds without crashing.\", \"- The final report must contain at least one plot showing the test accuracy curve of the secure model versus the non-private baseline.\", \"- The implemented cryptographic protocol must pass a unit test confirming that the decrypted aggregated gradient is numerically equivalent to the sum of the plaintext gradients.\", \"- The project must cite at least two relevant arXiv papers on the topic, with IDs provided.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 246, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement the Alternating Direction Method of Multipliers (ADMM) to enforce group-lasso sparsity on the convolutional filter weights of a VGG-style network. Can you derive the full ADMM update rules for this problem, accounting for the non-differentiable group L1/L2 norm, and then implement this optimization scheme in JAX? The implementation must demonstrate convergence by tracking primal and dual residuals and achieve a target filter pruning rate without catastrophic forgetting on CIFAR-10.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Alternating Direction Method of Multipliers group lasso deep learning convergence\\\", mode=rag)\", \"/math(Derive the augmented Lagrangian and the three subproblem update steps for ADMM applied to a cross-entropy loss with a group-lasso penalty term, analyzing the expected convergence properties.)\", \"/code(Implement the derived ADMM updates in JAX to train a VGG network on CIFAR-10, ensuring CUDA device placement and tracking both primal/dual residuals and the filter sparsity percentage to validate convergence., using Python)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to find authoritative literature on ADMM for structured sparsity, providing a theoretical foundation. /math is then tasked with the formal derivation of the specific update equations required for this problem. Finally, /code implements and validates the mathematical formulation, testing its empirical performance and convergence properties.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the ADMM update rules for group-lasso regularization.\", \"A JAX implementation of the ADMM optimizer and the training script for the VGG network on CIFAR-10.\", \"A results summary including plots of primal/dual residual convergence, training/validation accuracy curves, and a visualization of the pruned filters.\", \"A final report comparing the performance of the pruned model to a dense baseline, with citations to at least two relevant academic papers.\", \"A verification log confirming that the code's proximal operator for the group-lasso matches the mathematical derivation.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem for training a CNN with group-lasso regularization, using an auxiliary variable to split the weights from the non-smooth regularizer.\", \"2. Construct the augmented Lagrangian for this problem formulation.\", \"3. Derive the iterative update steps for the network weights (x-update), the auxiliary variable (z-update, involving a group-wise soft-thresholding operator), and the dual variable (u-update).\", \"4. Implement the complete algorithm in JAX, including the custom training loop that alternates between the three update steps.\", \"5. Verify the correctness of the implementation by monitoring the primal and dual residuals; they must converge towards zero as the algorithm proceeds.\", \"6. Validate the final model's performance by checking its classification accuracy on the test set and ensuring the filter sparsity meets the specified target without significant performance degradation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers (e.g., from arXiv) to the /math agent. The /math agent produces a formal derivation that serves as a precise specification for the /code agent. Verification: The /code agent's implementation of the group-wise proximal operator must be audited against the derivation from /math. Fallback: If convergence fails in the /code step, the /math agent will be re-engaged to check the derivation for potential issues with the Lagrangian penalty parameter.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on using ADMM for structured sparsity in deep neural networks.\", \"- [ ] /math: Formulate the group-lasso optimization problem and derive the complete set of ADMM update equations.\", \"- [ ] /code: Implement the VGG model and the ADMM training loop in JAX.\", \"- [ ] /code: Add instrumentation to log primal residuals, dual residuals, and filter sparsity throughout training.\", \"- [ ] /math: Verify that the JAX implementation of the group-wise soft-thresholding operator is mathematically equivalent to the derived z-update subproblem solution.\", \"- [ ] /code: Run experiments to demonstrate convergence and compare the final sparse model's accuracy against a dense baseline.\", \"- [ ] /router: Perform a final QA review, ensuring the mathematical derivation, code, and experimental results are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"admm\", \"structured-sparsity\"], \"acceptance_criteria\": [\"- The primal and dual residuals of the ADMM algorithm must converge to values below 1e-4.\", \"- The final trained model must achieve at least 50% filter sparsity while maintaining a test accuracy within 3% of the dense baseline on CIFAR-10.\", \"- The submitted code must be written in JAX and include a script to reproduce the reported metrics.\", \"- The final report must include the full mathematical derivation and cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 247, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous comparison of constraint-based (like FCI) versus score-based (like GES) causal discovery algorithms when dealing with non-Gaussian noise distributions and unobserved confounding. Specifically, derive the identifiability conditions for a simple structural causal model (SCM) with a latent confounder under both observational and soft interventional data, then implement a simulation benchmark to evaluate the structural Hamming distance (SHD) and edge precision-recall of FCI and GES implementations under these conditions. The benchmark should use synthetic data generated from a known causal graph with varying intervention strengths.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org causal discovery algorithms FCI GES latent confounders interventional data identifiability\\\", mode=rag)\", \"/math(Formalize the structural causal model (SCM) with latent confounding and non-Gaussian noise, then derive the local causal identifiability conditions under soft Bayesian interventions, referencing the faithfulness and Markov assumptions.)\", \"/code(Implement a Python simulation framework using numpy and the 'cdt' library to generate data from the derived SCM, apply FCI and GES algorithms, and compute Structural Hamming Distance and AUPRC metrics against the ground truth causal graph.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey the literature on causal discovery with confounders, which is essential for grounding the theoretical work. Next, the /math agent formalizes the problem based on the research, deriving the key theoretical conditions for identifiability. Finally, the /code agent implements a simulation to empirically validate the theoretical findings and compare the algorithms based on specified metrics.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of identifiability conditions for the SCM.\", \"A Python script implementing the simulation benchmark for FCI vs. GES algorithms.\", \"A results summary in JSON format containing SHD, precision, recall, and AUPRC scores for each algorithm under different intervention strengths.\", \"A summary report (.md) synthesizing the theoretical findings, experimental results, and citing at least 3 relevant papers.\", \"A log of the verification process for the mathematical derivation and code implementation.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of FCI and GES, focusing on their assumptions and limitations regarding latent confounders and interventional data.\", \"2. Formulate a specific structural causal model (SCM) that includes a latent common cause and non-Gaussian noise terms.\", \"3. Derive the conditions under which the causal structure is identifiable from a combination of observational and soft interventional data, using principles from intervention-calculus or equivalent formalisms.\", \"4. Implement a data generation process based on the SCM and a benchmarking pipeline to apply FCI and GES and compute performance metrics.\", \"5. Validate the mathematical derivation by cross-referencing it with established literature and checking for logical consistency.\", \"6. Verify the code implementation by running unit tests on small, known graphs and checking that the metric calculations are correct.\"], \"handoff_plan\": \"/general-search -> /math (verification: findings from search must provide sufficient theoretical basis for the derivation; fallback: broaden search query) -> /code (verification: implementation must match the SCM defined by /math; fallback: simplify the SCM in /math) -> router QA (verification: all artifacts are present and acceptance criteria are met; fallback: re-route to specific agent for correction).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers and recent advancements on FCI, GES, and causal identifiability with confounders.\", \"- [ ] /math: Derive the identifiability conditions for the specified SCM, clearly stating all assumptions like faithfulness.\", \"- [ ] /code: Implement the synthetic data generator based on the SCM from the /math agent.\", \"- [ ] /code: Implement the benchmarking logic to run FCI and GES and compute SHD and AUPRC metrics.\", \"- [ ] /math: Verify that the conditional independence tests assumed in the derivation are appropriate for non-Gaussian data.\", \"- [ ] /code: Verify the correctness of the metric computation functions (SHD, precision, recall) with known examples.\", \"- [ ] router: Review all generated artifacts against the acceptance criteria and produce the final summary report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"structural-causal-model\", \"causal-discovery\", \"intervention-calculus\"], \"acceptance_criteria\": [\"The mathematical derivation must be self-contained, logically sound, and correctly reference causal assumptions (e.g., Causal Markov Condition, Faithfulness).\", \"The Python code must be executable, well-commented, and successfully reproduce the benchmark results reported.\", \"The final report must correctly compare FCI and GES performance, linking empirical results back to the theoretical identifiability conditions.\", \"At least 3 relevant academic papers must be cited with their DOIs or arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 249, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of second-order Model-Agnostic Meta-Learning (MAML) for few-shot regression tasks. First, provide a full mathematical derivation of the MAML objective's meta-gradient, explicitly showing the Hessian-vector product term that arises from the inner-loop update. Then, implement a Python version using JAX that approximates this expensive term with a finite-difference method. Finally, empirically compare its convergence rate and final task accuracy against the standard first-order MAML approximation on a sinusoidal regression benchmark, and briefly discuss how a Bayesian perspective on the inner-loop updates could regularize the meta-optimizer.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'second order MAML' 'hessian-vector product' bayesian meta-learning convergence analysis\\\", mode=web)\", \"/math(Derive the full second-order meta-gradient for MAML, isolating the Hessian-vector product. Formulate a finite-difference approximation and analyze its first-order error term's impact on convergence.)\", \"/code(Implement MAML in JAX for a sinusoidal regression task. Code both first-order and the derived second-order finite-difference meta-optimizer, tracking mean squared error and using a Bayesian prior on weights to ensure numerical stability., using Python)\"], \"route_rationale\": \"The 'blue' general-search agent first gathers state-of-the-art context on second-order MAML and Hessian approximations. This literature informs the /math agent's formal derivation of the meta-gradient. The /code agent then implements this derivation to provide an empirical validation of the theoretical claims, comparing the methods on a standard benchmark.\", \"expected_artifacts\": [\"A PDF report containing the complete mathematical derivation of the second-order MAML meta-gradient and its finite-difference approximation.\", \"A Python script (`maml_benchmark.py`) with a JAX-based implementation of both first-order and second-order MAML.\", \"A PNG image (`convergence_plot.png`) comparing the meta-test loss over training iterations for both algorithms.\", \"A summary section in the report citing at least two relevant research papers with their arXiv IDs.\", \"A JSON file (`metrics.json`) reporting the final meta-test MSE and convergence rate for each method.\"], \"thinking_outline\": [\"1. Review the foundational literature on MAML (Finn et al., 2017) and second-order optimization methods to establish a theoretical baseline.\", \"2. Formally derive the meta-gradient using the chain rule on the meta-objective, carefully differentiating through the inner-loop SGD update to expose the Hessian-vector product term.\", \"3. Validate the mathematical derivation by cross-referencing it with established results from the literature, ensuring all terms are correctly represented.\", \"4. Implement the algorithm and the sinusoidal regression benchmark, ensuring the finite-difference approximation for the Hessian-vector product is numerically stable and computationally efficient.\", \"5. Execute the comparative experiments, collecting metrics on both meta-test performance and convergence speed.\", \"6. Verify that the experimental results, such as the potentially faster convergence of the second-order method, align with theoretical expectations and analyze any discrepancies.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: search results provide the basis for the derivation; the math derivation is the formal spec for the code implementation; fallback: if the derivation is intractable, the math agent should propose a simpler but related problem, like analyzing the Hessian for a linear model).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on second-order MAML or Hessian-free optimization (verify results)\", \"- [ ] /math: Produce the step-by-step derivation of the meta-gradient, highlighting the Hessian term.\", \"- [ ] /math: Formulate the finite-difference approximation for the Hessian-vector product and discuss its error bounds.\", \"- [ ] /code: Implement the sinusoidal regression task generator and the base neural network model in JAX.\", \"- [ ] /code: Implement the first-order MAML and the second-order MAML meta-optimizers.\", \"- [ ] /math: Verify that a gradient check on a toy version of the JAX implementation matches the formal derivation.\", \"- [ ] /code: Run the comparison experiment for at least 5000 meta-iterations and generate the convergence plot.\", \"- [ ] Router QA: Review all artifacts for correctness, consistency, and fulfillment of acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"few-shot-learning\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation in the report must be complete, correct, and match the specified formulation.\", \"- The provided Python script must run without errors and successfully reproduce the plot comparing the two MAML variants.\", \"- The final report must contain the specified convergence plot and cite at least two relevant arXiv papers by ID.\", \"- The final meta-test MSE reported in `metrics.json` for the second-order method must be competitive with or better than the first-order method.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 251, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the theoretical underpinnings for a SE(3)-equivariant graph neural network that learns the Hamiltonian for a molecular system. Show how to formulate the network's message passing to conserve total energy and momentum, and implement a proof-of-concept in PyTorch Geometric demonstrating long-term trajectory stability for a small peptide system compared to a non-conservative baseline.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org SE(3)-equivariant graph neural network Hamiltonian dynamics molecular chemistry\\\", mode=web)\", \"/math(Derive the symplectic gradient structure from the learned Hamiltonian, prove that the SE(3)-equivariant message passing preserves the system's total energy based on Noether's theorem, and formulate the Euler-Lagrange update rule.)\", \"/code(Implement the Hamiltonian GNN and a symplectic integrator in PyTorch with CUDA support. Add runtime validation checks for energy drift and equivariance error during the molecular dynamics simulation., using Python)\"], \"route_rationale\": \"The route starts with the blue general-search agent to survey current literature on equivariant GNNs and Hamiltonian mechanics. Next, /math is used to establish the rigorous theoretical foundation and proofs of energy conservation. Finally, /code implements the derived model and runs experiments to empirically validate the theoretical claims.\", \"expected_artifacts\": [\"- A formal mathematical proof deriving the energy conservation property of the proposed SE(3)-equivariant GNN architecture.\", \"- A Python script implementing the Hamiltonian GNN and the molecular dynamics simulation using PyTorch Geometric.\", \"- A report summarizing the findings, including plots of long-term energy drift and a list of at least two cited arXiv papers.\", \"- A JSON file containing the final calculated metrics for energy drift and equivariance error.\"], \"thinking_outline\": [\"1. Research state-of-the-art SE(3)-equivariant graph architectures and Hamiltonian Neural Networks (HNNs) for learning physical systems.\", \"2. Formally derive the Hamiltonian equations of motion from a parameterized GNN and prove that the architecture's structure enforces conservation of total energy, linking it to continuous symmetries via Noether's theorem.\", \"3. Implement the GNN architecture and a symplectic integrator (e.g., Velocity Verlet) to ensure the numerical time-stepping also respects the system's geometric structure.\", \"4. Numerically validate the implementation by designing a unit test to check for SE(3) equivariance; the output of a transformed input must equal the transformed output of the original input.\", \"5. Execute a long-term molecular dynamics simulation and verify that the learned Hamiltonian model exhibits minimal energy drift compared to a non-Hamiltonian baseline (e.g., a standard GNN force field).\", \"6. Collate the mathematical proofs, implementation details, and empirical results into a comprehensive final report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent's proof of energy conservation must be explicitly checked against the code's implementation of the gradient updates. The final energy drift metrics from the code agent must align with theoretical expectations. fallback: If energy drift is high, the code agent must flag this for the math agent to re-check the proof for potential unstated assumptions.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on Hamiltonian or Lagrangian GNNs for molecular simulation.\", \"- [ ] /math: Provide a complete derivation of the symplectic gradient updates from the learned Hamiltonian function.\", \"- [ ] /math: Write a formal proof that the proposed equivariant architecture conserves energy.\", \"- [ ] /code: Implement the SE(3)-equivariant GNN layers and a symplectic integrator in PyTorch Geometric.\", \"- [ ] /code: Set up a simulation for a small molecular system (e.g., Alanine dipeptide) and train the model.\", \"- [ ] /math: Verify the code's automatic differentiation output for the Hamiltonian gradients matches the analytically derived symplectic gradient.\", \"- [ ] /code: Generate plots comparing the long-term energy drift of the Hamiltonian GNN vs. a non-conservative baseline GNN.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"hamiltonian-mechanics\"], \"acceptance_criteria\": [\"- The submitted mathematical proof of energy conservation is rigorous and complete.\", \"- The implementation correctly passes a numerical test for SE(3) equivariance with an L2 error below 1e-5.\", \"- The final simulation results demonstrate that the Hamiltonian GNN has at least one order of magnitude less relative energy drift over 10,000 steps compared to a baseline model.\", \"- The final report includes citations for at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 252, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Alternating Least Squares (ALS) update rules for Tucker decomposition with an explicit orthogonality constraint on the factor matrices. Then, implement this constrained Tucker-ALS in Python using TensorLy to compress a pre-trained convolutional layer's weight tensor. Analyze the trade-off between compression ratio (controlled by the core tensor's rank) and representation fidelity, specifically focusing on how the orthogonality constraint impacts convergence speed and reconstruction error.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Tucker decomposition' 'orthogonality constraint' 'neural network compression' convergence\\\", mode=both)\", \"/math(Derive the ALS update rules for Tucker decomposition under orthogonality constraints using the method of Lagrange multipliers, referencing the Euler-Lagrange equations and analyzing the connection to the eigenvalue problem.)\", \"/code(Implement the constrained ALS algorithm in Python with TensorLy/JAX, leveraging CUDA for acceleration, to compress a weight tensor and evaluate reconstruction error versus Bayesian information criterion.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather seminal literature on constrained tensor decomposition, which is crucial for grounding the theoretical derivation. This feeds into the /math agent to formally derive the update rules. Finally, the /code agent implements the derived algorithm to validate the theory empirically, creating a full-stack reasoning loop from theory to practice.\", \"expected_artifacts\": [\"- A PDF report containing the complete mathematical derivation of the constrained Tucker-ALS update rules.\", \"- A well-commented Python script implementing the algorithm and the compression experiment.\", \"- A JSON file logging the final metrics, including reconstruction error and orthogonality violation for different ranks.\", \"- A plot visualizing the trade-off between Tucker rank (compression) and reconstruction error.\", \"- A bibliography in the report citing at least two relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Review the foundational theory of Tucker decomposition and the standard unconstrained Alternating Least Squares (ALS) algorithm.\", \"2. Formulate the optimization problem for Tucker decomposition with orthogonality constraints on the factor matrices using the method of Lagrange multipliers.\", \"3. Derive the specific update rules for the core tensor and each factor matrix by solving the Karush-Kuhn-Tucker (KKT) conditions for the formulated Lagrangian.\", \"4. Validate the derived update rules by showing they simplify to a Procrustes problem, whose solution involves Singular Value Decomposition (SVD), ensuring orthogonality is maintained.\", \"5. Implement the derived algorithm and apply it to a sample tensor, verifying that the factor matrices remain orthogonal (i.e., ||U^T U - I||_F < 1e-6) after each update step.\", \"6. Analyze the empirical results, connecting the observed convergence behavior and performance trade-offs back to the properties of the constrained optimization.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Math agent's derived update equations must precisely match the algorithm implemented by the Code agent, particularly the SVD step for enforcing orthogonality. Fallback: If the derivation is intractable, /general-search must find an alternative formulation from a cited paper for the /math agent to analyze.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on constrained Tucker decomposition (e.g., HOSVD, HOOI) and its application in deep learning compression.\", \"- [ ] /math: Formulate the Lagrangian for Tucker decomposition with orthogonality constraints on the factor matrices.\", \"- [ ] /math: Derive the complete ALS update rules for the core tensor and each factor matrix.\", \"- [ ] /code: Implement the constrained ALS algorithm in Python using TensorLy and NumPy/JAX.\", \"- [ ] /code: Design and run an experiment to compress a sample weight tensor and plot reconstruction error vs. rank.\", \"- [ ] /math: Verify that the implemented update logic in the Python script correctly reflects the derived mathematical expressions.\", \"- [ ] /code: Validate the orthogonality of the factor matrices at each iteration using a matrix norm check (`||F^T F - I|| < epsilon`).\", \"- [ ] router QA: Review all artifacts for consistency between theory, implementation, and cited literature.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"numerical-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation for the constrained ALS update rules is complete, self-contained, and mathematically sound.\", \"- The Python implementation successfully converges and maintains factor matrix orthogonality within a small tolerance (e.g., Frobenius norm of `U^T U - I` is < 1e-6) throughout the optimization process.\", \"- The final report includes a plot showing the trade-off between Tucker rank and final reconstruction error, supported by at least 2 citations from arXiv.\", \"- The code is runnable and reproduces the metrics and plot presented in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 253, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the gradient and Hessian of the cross-entropy loss function for a deep feedforward neural network with respect to its weight matrices. Then, analyze the spectral norm of the Hessian and its connection to generalization bounds. I need a formal proof connecting the largest eigenvalue (\u03bb_max) of the Hessian to the network's Lipschitz constant, and a Python simulation in JAX to empirically track the Hessian's eigenvalue distribution during training on a synthetic dataset, validating the theoretical claims about convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian eigenvalue deep learning generalization Lipschitz constant convergence\\\", mode=rag)\", \"/math(Derive the gradient and Hessian of a neural network loss using matrix calculus, then prove the relationship between the Hessian's maximum eigenvalue and the network's Lipschitz constant, referencing key theorems on convergence.)\", \"/code(Implement a JAX-based simulation to train a small MLP, using the Lanczos algorithm to estimate the Hessian's top eigenvalue at each epoch and plot its evolution against the training loss to validate our convergence theory., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on Hessian analysis in deep learning. This research informs the /math agent, which will perform the rigorous matrix calculus derivations and proofs. Finally, the /code agent implements a simulation to empirically validate the mathematical theory, providing a crucial link between theory and practice.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the Hessian and the proof connecting its spectral norm to the Lipschitz constant.\", \"A well-documented Python script using JAX to run the simulation and compute the Hessian's top eigenvalue.\", \"A PNG plot visualizing the evolution of the maximum Hessian eigenvalue versus the training loss over epochs.\", \"A summary report (Markdown) that synthesizes the theoretical findings and empirical results, including citations.\", \"A JSON file containing the final computed correlation metrics.\"], \"thinking_outline\": [\"1. Research foundational literature on the spectral properties of the neural network Hessian and its role in optimization and generalization.\", \"2. Formally derive the gradient and Hessian of the loss function for a multi-layer perceptron using matrix calculus, paying close attention to the chain rule for matrix derivatives.\", \"3. Construct a formal proof linking the maximum eigenvalue of the Hessian to the local Lipschitz constant of the network's gradient map.\", \"4. Implement a numerical simulation in JAX to track the Hessian's top eigenvalue during training using an efficient power iteration or Lanczos method to avoid materializing the full Hessian.\", \"5. Validate the implementation by checking numerical gradients computed by JAX's autodiff against the analytical derivations for a small test case.\", \"6. Analyze the simulation results to empirically verify the theoretical relationship between the Hessian's spectral norm, training convergence speed, and generalization performance.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results provide the theoretical foundation for the math agent's derivations. The math agent's final expressions are then handed to the code agent for implementation. The code agent must verify its implementation against the math agent's formulas. (Verification: numerical gradient checks; Fallback: if checks fail, the code agent flags the discrepancy and requests a review from the math agent). The final artifacts are bundled for router QA.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 key papers from arXiv on Hessian spectral analysis and generalization in deep learning.\", \"- [ ] /math: Derive the gradient and Hessian expressions using matrix calculus for an L-layer MLP with ReLU activations.\", \"- [ ] /math: Formulate and prove the theorem connecting the Hessian's \u03bb_max to the network's local Lipschitz constant.\", \"- [ ] /code: Implement the MLP, the training loop, and the synthetic data generator in JAX.\", \"- [ ] /code: Implement a power iteration or Lanczos method to efficiently estimate the Hessian's top eigenvalue without materializing the full matrix.\", \"- [ ] /math: Verify that the analytical Hessian derivation from the math agent matches the numerical Hessian computed by JAX's autodiff for a small, deterministic test case.\", \"- [ ] /code: Generate plots showing the correlation between the max eigenvalue, training loss, and test accuracy over training epochs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"neural-network-theory\"], \"acceptance_criteria\": [\"The mathematical derivation of the Hessian is complete, correct, and presented with clear notation.\", \"The Python simulation runs without errors and produces a plot clearly showing the evolution of the maximum eigenvalue over training epochs.\", \"The final report explicitly connects the theoretical proof to the empirical simulation results, discussing any discrepancies.\", \"The provided code includes a unit test that verifies the analytical Hessian derivation against JAX's automatic differentiation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 254, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the analytical form of the Jacobian for a single multi-head self-attention layer with respect to its input embeddings. Subsequently, using tools from Riemannian geometry, analyze the spectral properties of the Jacobian's Gramian matrix to characterize the local curvature and its implications for training stability and convergence guarantees, particularly concerning exploding or vanishing gradients.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Riemannian geometry transformer attention Jacobian spectral analysis\\\", mode=rag)\", \"/math(Derive the tensor calculus expression for the single-layer multi-head attention Jacobian, then formulate the Gramian and its characteristic polynomial to analyze its eigenvalue distribution for convergence.)\", \"/code(Implement the derived Jacobian using PyTorch's autograd or JAX for a toy transformer. Use CUDA to compute the eigenvalue spectrum of the Gramian and validate against the analytical predictions, tracking numerical precision., using Python)\"], \"route_rationale\": \"Start with a literature review via the blue /general-search agent to ground the work in current research from arXiv. Transition to /math for the rigorous tensor calculus derivation. Finally, use /code to empirically validate the theoretical findings on a GPU, comparing numerical results to the analytical derivation.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the Jacobian and the spectral analysis proof.\", \"A Python script implementing the Jacobian computation and eigenvalue analysis for a sample attention layer.\", \"A summary report (.md) linking the theoretical curvature to empirical eigenvalue distributions and citing at least two relevant arXiv papers.\", \"A JSON file containing the computed eigenvalues and the maximum singular value for verification.\"], \"thinking_outline\": [\"1. Establish the mathematical formalism for a multi-head self-attention layer using tensor notation, drawing from foundational literature.\", \"2. Perform the symbolic differentiation with respect to the input token embeddings to derive the Jacobian tensor.\", \"3. Formulate the Gramian of the Jacobian and derive its characteristic equation to study the eigenvalue spectrum.\", \"4. Implement the Jacobian calculation numerically and verify its output against finite-difference approximations for a set of test vectors.\", \"5. Compare the numerically computed eigenvalue distribution from the implementation with the theoretical predictions from the derivation to validate the model's correctness.\", \"6. Synthesize findings into a report discussing how the geometric curvature, as measured by the spectral properties, influences gradient flow and training dynamics.\"], \"handoff_plan\": \"/general-search -> /math (verification: check if cited papers provide a valid starting point for the derivation) -> /code (verification: ensure numerical implementation matches the derived Jacobian's structure; fallback: if divergence, revisit /math to check for errors in the tensor calculus) -> router QA (verification: final review of proof, code, and report for consistency).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on the geometric analysis of transformer layers from arXiv.\", \"- [ ] /math: Derive the complete Jacobian for the self-attention mechanism using tensor calculus.\", \"- [ ] /math: Analyze the spectral properties of the resulting Gramian matrix to predict convergence behavior.\", \"- [ ] /code: Implement the Jacobian computation in Python using JAX.\", \"- [ ] /code: Compute the eigenvalue spectrum numerically on a CUDA-enabled device.\", \"- [ ] /math: Verify that the analytical eigenvalue bounds from the derivation contain the numerically computed eigenvalues from the code.\", \"- [ ] /code: Validate the Jacobian implementation using numerical gradient checking (finite differences).\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"differential-geometry\", \"jacobian-analysis\", \"transformer-stability\"], \"acceptance_criteria\": [\"The mathematical proof must be self-contained, rigorous, and free of logical errors.\", \"The Python code must correctly compute the Jacobian and its eigenvalue spectrum, passing verification against a finite-difference baseline with a tolerance of 1e-5.\", \"The final report must correctly link the maximum eigenvalue of the Jacobian's Gramian to potential training instability, supported by both the proof and numerical results.\", \"The submission must cite at least two relevant arXiv papers with valid IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 255, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the optimization landscape of Transformer attention heads. Can you derive the geodesic update rules for an attention weight matrix constrained to the Stiefel manifold, treating it as an application of Riemannian optimization? Please provide the formal derivation of the Riemannian gradient and exponential map (retraction), and then implement a proof-of-concept to compare its convergence against a standard Euclidean optimizer like Adam on a synthetic task.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Riemannian optimization Stiefel manifold transformer attention geodesic\\\", mode=both)\", \"/math(Derive the Riemannian gradient and geodesic update rule for an attention weight matrix on the Stiefel manifold St(n, d), analyzing the Hessian eigenvalue spectrum to prove local convergence guarantees.)\", \"/code(Implement the geodesic optimizer in JAX, comparing its convergence against Adam. Use a bootstrap method to estimate confidence intervals for loss curves and a Bayesian approach for hyperparameter tuning., using Python)\"], \"route_rationale\": \"The blue /general-search agent will first gather foundational literature on Riemannian optimization in deep learning. Next, the /math agent will perform the core theoretical work of deriving the specific update equations for this geometric context. Finally, the /code agent will implement the derived algorithm to provide empirical validation and comparison, grounding the theory in practice.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Riemannian gradient and geodesic update rule, formatted in LaTeX.\", \"- A Python script (JAX) implementing the geodesic optimizer and a baseline Euclidean optimizer.\", \"- A plot comparing the convergence rates (loss vs. iteration) and constraint violation of the two optimizers.\", \"- A summary report (.md) explaining the methodology, results, and citing at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research and formalize the representation of an attention weight matrix as a point on the Stiefel manifold St(n, d), identifying the relevant geometric properties such as the tangent space and metric.\", \"2. Derive the expression for the Riemannian gradient of a given loss function by projecting the Euclidean gradient onto the tangent space of the manifold at the current weight matrix.\", \"3. Formulate the geodesic update rule using a retraction, such as the exponential map or a QR-based projection, to move along the manifold while preserving the orthogonality constraint.\", \"4. Implement the derived update rule in JAX and set up a comparative experiment against a standard optimizer like Adam on a simple regression or classification task.\", \"5. Validate the implementation by continuously monitoring the orthogonality constraint (W^T W = I) throughout training to ensure the optimizer correctly operates on the manifold.\", \"6. Analyze and document the empirical results, comparing convergence speed and final performance, and verify that the findings are consistent with the initial theoretical derivation.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation & validation) -> router QA (final review). Verification: /math agent verifies the literature is applicable; /code agent verifies the implementation numerically satisfies the derived constraints. Fallback: If derivation is intractable, /math agent flags it and suggests a simpler manifold or approximation for router review.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Riemannian optimization for deep learning, focusing on the Stiefel manifold.\", \"- [ ] /math: Derive the projection operator onto the tangent space for St(n, d).\", \"- [ ] /math: Derive the Riemannian gradient and a retraction-based update rule (e.g., QR retraction).\", \"- [ ] /code: Implement the Riemannian optimizer and a comparable Adam baseline in JAX.\", \"- [ ] /code: Design a synthetic experiment to compare the convergence and constraint satisfaction of both optimizers.\", \"- [ ] /math: Verify that the numerical gradients computed in the code align with the analytically derived Riemannian gradient.\", \"- [ ] /code: Generate plots for loss curves and constraint violation metrics, and write a summary report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"riemannian-optimization\", \"transformer-architecture\", \"differential-geometry\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for the Riemannian gradient and update rule must be correct and self-contained.\", \"- The code implementation must run without errors and maintain the Stiefel manifold constraint (||W^T W - I||_F < 1e-6) throughout optimization.\", \"- The final report must clearly present the comparative results and include citations for at least two relevant arXiv papers with their IDs.\", \"- The performance of the geodesic optimizer must be demonstrably different from the Euclidean baseline, as measured by the primary and secondary metrics.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 256, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a provably safe reinforcement learning agent for a robotic manipulator. Please formulate the problem as a Constrained Markov Decision Process (CMDP) and implement a Lagrangian relaxation-based algorithm. The agent must maximize task completion reward while ensuring the end-effector velocity stays below a critical safety threshold to avoid catastrophic failure. Show the derivation for the primal-dual updates and validate the implementation's convergence guarantees.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Lagrangian relaxation 'constrained policy optimization' robotics control theory convergence\\\", mode=both)\", \"/math(Derive the primal-dual update equations for a CMDP using Lagrangian multipliers, establishing the objective function with a cost penalty. Analyze the Karush-Kuhn-Tucker conditions for policy convergence.)\", \"/code(Implement the PPO-Lagrangian agent in PyTorch with CUDA acceleration. The implementation must include monitoring for constraint violations and use Bayesian optimization for hyperparameter tuning., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on constrained RL. This informs the /math agent's formal derivation of the optimization problem. Finally, the /code agent implements the derived algorithm, providing an empirical testbed for the theoretical model.\", \"expected_artifacts\": [\"A PDF report containing the full mathematical derivation of the CMDP formulation and the Lagrangian-based update rules.\", \"A git repository URL with the complete, commented Python source code for the PPO-Lagrangian agent and the simulation environment.\", \"A metrics.json file logging the primary and secondary metrics over the course of training.\", \"A literature review summary with citations to at least 2 relevant arXiv preprints.\", \"A verification log detailing the outcomes of the checks performed in the `todo_list`.\"], \"thinking_outline\": [\"1. Research current state-of-the-art approaches for solving Constrained Markov Decision Processes using Lagrangian methods, focusing on robotics applications.\", \"2. Formulate the specific robotic control problem as a CMDP, clearly defining states, actions, transition probabilities, rewards, and the safety constraint on end-effector velocity.\", \"3. Derive the update rules for the policy parameters and the Lagrangian multiplier using principles of constrained optimization and control theory.\", \"4. Implement the derived algorithm and validate its correctness by comparing the code's computational graph against the mathematical derivation.\", \"5. Execute training runs, systematically collecting performance and safety metrics to empirically verify the agent's ability to learn while respecting constraints.\", \"6. Audit the final results against the acceptance criteria, ensuring the agent achieves the target reward without exceeding the constraint violation budget.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to /math. /math derives the core equations and hands them off to /code for implementation. Verification: The /code agent must verify that its implementation of the loss function and gradient updates directly matches the formulas from /math. Fallback: If the training is unstable, the task is routed back to /math to re-check the convergence properties of the derivation.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 key papers on PPO-Lagrangian or CPO for robotics.\", \"- [ ] /math: Formulate the CMDP and derive the primal-dual update rules for the policy and Lagrange multiplier.\", \"- [ ] /code: Implement the PPO-Lagrangian agent and the robotic arm simulation environment.\", \"- [ ] /code: Add logging for all specified primary and secondary metrics.\", \"- [ ] /math: Verify that the implemented loss function in the code is a correct representation of the derived mathematical objective.\", \"- [ ] /code: Write unit tests to check the gradient flow through the constrained policy update.\", \"- [ ] /code: Run the training experiment and generate plots for reward and constraint violations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"robotics\", \"constrained-optimization\"], \"acceptance_criteria\": [\"- The agent must achieve an average episodic reward of at least 250 while maintaining an average constraint violation rate below 5% over the final 100 episodes.\", \"- The mathematical derivation must be complete, self-contained, and consistent with the cited literature.\", \"- The provided code must be executable and reproduce the reported metrics within a 5% tolerance.\", \"- The final report must include plots showing the convergence of both the episodic reward and the value of the Lagrange multiplier.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 258, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement the K-SVD algorithm for dictionary learning from first principles. Please provide a full mathematical derivation of the update rules for both the dictionary atoms via SVD and the sparse coefficients, assuming an Orthogonal Matching Pursuit (OMP) approach for the pursuit stage. Then, code this in Python and demonstrate its performance on a synthetic sparse signal, analyzing its convergence and reconstruction fidelity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD dictionary learning algorithm convergence proof Orthogonal Matching Pursuit\\\", mode=rag)\", \"/math(Derive the K-SVD update rules for dictionary atoms and sparse coefficients, detailing the Singular Value Decomposition step for atom updates and proving convergence under assumptions found in arxiv literature.)\", \"/code(Implement the K-SVD algorithm in Python with NumPy, including an OMP subroutine. Validate its performance on synthetic data by tracking reconstruction SNR and ensuring convergence.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational academic papers on K-SVD and OMP. These sources are passed to /math to produce a formal, citable derivation of the algorithm's mechanics. Finally, /code translates the formal mathematics into a working implementation and performs empirical validation, completing the theory-to-practice workflow.\", \"expected_artifacts\": [\"A Markdown or LaTeX document containing the full mathematical derivation of the K-SVD update rules.\", \"A Python script (`k_svd.py`) with a complete, commented implementation of the K-SVD algorithm and the OMP subroutine.\", \"A plot (`convergence.png`) showing the reconstruction error (MSE or SNR) as a function of the K-SVD iteration number.\", \"A `report.md` file summarizing the results and citing at least two relevant academic papers with their arXiv IDs.\", \"A `metrics.json` file reporting the final reconstruction SNR and average sparsity level.\"], \"thinking_outline\": [\"1. Research the K-SVD algorithm and Orthogonal Matching Pursuit (OMP) to understand the theoretical underpinnings, focusing on the dictionary update step and the sparse coding stage.\", \"2. Formulate and derive the mathematical update rules for the dictionary atoms using an SVD-based approach and for the sparse coefficients using OMP.\", \"3. Implement the complete K-SVD algorithm in Python, ensuring the OMP subroutine correctly identifies the support of the sparse signal.\", \"4. Validate the implementation by creating synthetic sparse signals and a known dictionary, then check if the algorithm can recover both.\", \"5. Analyze the algorithm's performance by plotting the reconstruction error over iterations to verify convergence and measure the final signal-to-noise ratio (SNR).\", \"6. Compare the empirical convergence behavior and final reconstruction quality against the theoretical guarantees discussed in the sourced literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation that serves as a blueprint for the code agent. The code agent implements and runs experiments, producing plots and metrics. Router QA performs the final check. Verification: The implementation's convergence plot must align with the theoretical expectation of monotonic error decrease. Fallback: If the code fails to converge, it is flagged for the math agent to review the derivation's assumptions or potential instabilities.\", \"todo_list\": [\"- [ ] /general-search: Find the seminal papers on K-SVD and OMP, focusing on algorithmic derivation and convergence guarantees.\", \"- [ ] /math: Derive the K-SVD update equations for dictionary atoms (via SVD) and sparse codes (via OMP).\", \"- [ ] /code: Implement the K-SVD algorithm and its OMP subroutine in Python using NumPy.\", \"- [ ] /code: Develop a test suite to generate synthetic sparse signals and a ground-truth dictionary for validation.\", \"- [ ] /math: Verify that the derivation for the atom update correctly isolates the rank-1 approximation step.\", \"- [ ] /code: Validate that the reconstruction SNR metric improves monotonically across iterations in the generated convergence plot.\", \"- [ ] /router: Perform final QA review, ensuring the `metrics.json` file shows high reconstruction SNR and the report correctly cites the source papers.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The mathematical derivation for the K-SVD update step is complete and correctly utilizes Singular Value Decomposition (SVD).\", \"- The Python implementation successfully reconstructs synthetic signals, achieving a final reconstruction SNR greater than 30 dB.\", \"- The generated convergence plot clearly shows a monotonically decreasing reconstruction error over at least 20 iterations.\", \"- The final report correctly cites at least two authoritative sources (e.g., from arXiv) for K-SVD and/or OMP.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 259, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm exploring advanced techniques for model compression and need to formalize a second-order optimization method for structured pruning of large transformer models. Can you derive the full Lagrangian dual formulation for minimizing training loss subject to a hard spectral norm constraint on each weight matrix to control the model's Lipschitz constant, and then implement this as a practical algorithm in PyTorch, citing recent work from arXiv on similar constrained optimization problems?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org structured pruning 'Lagrangian relaxation' 'spectral norm constraint' second-order optimization for adversarial robustness\\\", mode=rag)\", \"/math(Derive the Euler-Lagrange equations and KKT conditions for the structured pruning objective with a spectral norm constraint, analyzing the Hessian eigenvalue distribution for convergence guarantees.)\", \"/code(Implement the second-order pruning algorithm in PyTorch with CUDA support, using Bayesian optimization to tune hyperparameters and ensuring the spectral norm constraint holds throughout training., using Python)\"], \"route_rationale\": \"The route starts with the blue general-search agent to survey current literature on arXiv for state-of-the-art constrained optimization techniques in model pruning. This research informs the /math agent's formal derivation of the specific Lagrangian and KKT conditions requested. Finally, the /code agent translates the verified mathematical framework into a functional PyTorch implementation, capable of running on accelerated hardware.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the Lagrangian, its gradient, Hessian, and the KKT optimality conditions.\", \"A Python script (`pruning_optimizer.py`) implementing the second-order constrained optimization algorithm.\", \"A JSON file (`metrics.json`) logging the training loss, model sparsity, and the spectral norm of constrained weight matrices per epoch.\", \"A markdown report (`report.md`) summarizing the method, results, and citing at least two relevant arXiv papers with IDs.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem for structured pruning, defining the objective function (e.g., cross-entropy loss) and the inequality constraint (spectral norm of weight matrices less than a constant).\", \"2. Construct the Lagrangian for this problem and derive the Karush-Kuhn-Tucker (KKT) conditions that define the criteria for an optimal solution.\", \"3. Derive the analytical gradient and Hessian of the Lagrangian with respect to both the model parameters and the Lagrange multipliers to enable a second-order update.\", \"4. Implement the derived update rule in a Python framework like PyTorch, including robust methods for estimating the spectral norm and solving the inner KKT system.\", \"5. Verify the correctness of the implementation by numerically checking the gradients against the analytical derivation using finite differences on a small-scale model.\", \"6. Validate the algorithm's performance on a benchmark task by plotting the sparsity-accuracy trade-off curve and ensuring the spectral norm constraint is not violated during training.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: search results provide the theoretical basis for the math derivation; the math derivation provides the exact equations for the code implementation; the code's output metrics are validated against the KKT conditions. fallback: if derivation is intractable, /math agent will propose a first-order relaxation and notify the router for approval).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 recent arXiv papers on second-order methods for constrained neural network optimization.\", \"- [ ] /math: Derive the Lagrangian and state the complete KKT conditions for the spectral norm constrained pruning problem.\", \"- [ ] /math: Derive the Hessian of the Lagrangian and formulate the Newton-Raphson update step for the primal-dual optimization.\", \"- [ ] /code: Implement the pruning algorithm in PyTorch, including a power-iteration method for spectral norm estimation.\", \"- [ ] /code: Instrument the code to log all metrics specified in the `metrics` field to `metrics.json`.\", \"- [ ] /math: Verify that the gradients computed by the /code agent's implementation match the analytical derivation via a finite difference check.\", \"- [ ] /code: Add assertions to the training loop to confirm the spectral norm constraint is satisfied at each step within a tolerance of 1e-5.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"deep-learning-theory\", \"structured-pruning\"], \"acceptance_criteria\": [\"- The mathematical derivation of the KKT conditions must be complete, correct, and self-contained in the provided PDF.\", \"- The Python implementation must execute without errors and demonstrate convergence on a benchmark dataset (e.g., pruning a ResNet-18 on CIFAR-10).\", \"- The final report must cite at least two relevant arXiv preprints with IDs, published within the last three years.\", \"- The algorithm must achieve at least 70% unstructured sparsity while maintaining classification accuracy within 2% of the dense model's baseline.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 261, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the Hessian eigenvalue spectrum at saddle points for a non-convex minimax problem. Derive the conditions for escaping these saddles and implement a perturbed gradient descent algorithm in Python to demonstrate faster convergence compared to vanilla GD.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'perturbed gradient descent' non-convex saddle point escape Hessian eigenvalue analysis\\\", mode=both, context=\\\"Find seminal papers on non-convex optimization, focusing on perturbed gradient descent methods for escaping saddle points and analyzing the Hessian eigenvalue properties for convergence guarantees.\\\")\", \"/math(context=\\\"Derive the theoretical conditions under which perturbed gradient descent escapes saddle points in non-convex settings, focusing on the role of the Hessian and proving faster convergence rates.\\\")\", \"/code(context=\\\"Implement perturbed gradient descent in Python for a non-convex test function, visualize the optimization trajectory, and compare its convergence against vanilla GD, ensuring the perturbation strategy aligns with control theory principles for system stability.\\\")\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational literature on saddle point escape mechanisms. This informs the /math agent, which formalizes the theory and derives the specific escape conditions. Finally, the /code agent implements the algorithms to empirically validate the mathematical derivations, providing concrete evidence for the theory.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the saddle point escape conditions for perturbed gradient descent.\", \"- A Python script implementing both vanilla and perturbed gradient descent, with visualization of their optimization paths.\", \"- A summary report in Markdown that synthesizes the theory, presents the empirical results with plots, and includes at least two citations to arXiv papers.\", \"- A metrics file in JSON format reporting the iterations to escape and the empirical convergence rate.\", \"- A verification log comparing the empirical convergence rate with the derived theoretical bound.\"], \"thinking_outline\": [\"1. Review foundational literature on non-convex optimization, specifically the challenges posed by saddle points versus local minima and the role of the Hessian.\", \"2. Formulate the mathematical problem by defining a suitable non-convex test function (e.g., z = x^2 - y^2) and the update rules for vanilla and perturbed gradient descent.\", \"3. Derive the conditions under which injected noise allows the algorithm to escape a saddle point, focusing on the relationship between the noise, learning rate, and the Hessian's negative eigenvalues.\", \"4. Implement both algorithms in Python, visualizing their optimization paths on the test function's contour plot to provide a qualitative comparison.\", \"5. Validate the implementation by quantitatively comparing the empirical convergence rates to the theoretical guarantees derived in step 3 and analyzing any discrepancies.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to the /math agent. The /math agent derives theoretical guarantees and passes the formal conditions to the /code agent. The /code agent implements the algorithms and generates plots and metrics for final review. Verification: Empirical convergence from the code must match the theoretical bounds from the math derivation within a 10% tolerance. Fallback: If verification fails, the /code agent must re-audit its implementation against the math derivation; if the issue persists, the /math agent must refine the theoretical bounds.\", \"todo_list\": [\"- [ ] /general-search: Find 2-3 key papers on perturbed gradient descent for non-convex optimization and saddle point escape.\", \"- [ ] /math: Derive the update rule for perturbed GD and the theoretical conditions on the Hessian for saddle point escape.\", \"- [ ] /code: Implement vanilla GD and perturbed GD for a 2D non-convex function with a prominent saddle point (e.g., f(x,y) = x^2 - y^2).\", \"- [ ] /code: Generate contour plots showing the optimization trajectories for both algorithms from the same starting point near the saddle.\", \"- [ ] /code: Validate the empirical convergence rate of perturbed GD against the derived theoretical bound.\", \"- [ ] /router: Perform final QA check, ensuring the theoretical derivation, code implementation, and final report are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"gradient-descent\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly identifies the role of the Hessian's negative eigenvalues in the escape mechanism.\", \"- The Python implementation demonstrates that perturbed GD successfully escapes the saddle point in significantly fewer iterations than vanilla GD.\", \"- The final report includes convergence plots and correctly cites at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Count the number of iterations required for the algorithm to move a Euclidean distance of >1.0 from the saddle point at (0,0).\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"After escaping the saddle, calculate the slope of the log-log plot of ||f(x_k) - f(x*)|| vs. iteration k to estimate the convergence rate.\"}}", "categories": ["metrics_guidance"]}
{"index": 262, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nPlease provide a complete derivation of the evidence lower bound (ELBO) for a continuous-time generative model specified by a variance-exploding (VE) stochastic differential equation. Subsequently, implement this score-based generative model using JAX and Flax to learn the data distribution of the MNIST dataset. Your implementation must include a method for quantifying posterior predictive uncertainty via a Bayesian approximation like Langevin dynamics sampling.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models stochastic differential equations ELBO bayesian\\\", mode=web)\", \"/math(Derive the evidence lower bound for a variance-exploding SDE generative model, showing the connection between the score function and the reverse-time SDE. Analyze the convergence properties of the Bayesian posterior approximation.)\", \"/code(Implement a score-based generative model in JAX using the derived ELBO. Train on MNIST, track FID metrics for sample quality, and use Langevin dynamics for Bayesian posterior sampling to quantify uncertainty., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find foundational papers on SDE-based models and their connection to Bayesian inference. The findings are handed off to /math for a rigorous mathematical derivation of the loss function. Finally, /code implements the verified mathematical framework and runs the experiment, producing the required metrics and artifacts.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the ELBO for the VE-SDE model.\", \"A Python script implementing the JAX/Flax score-based model, trainer, and sampler.\", \"A final report in Markdown summarizing the methodology, FID score, and NLL estimate, including plots of generated samples and their predictive uncertainty.\", \"A bibliography file citing at least two relevant arXiv papers.\", \"A JSON file containing the final evaluation metrics.\"], \"thinking_outline\": [\"1. Use /general-search to find foundational papers, such as those by Song et al., that formally define score-based generative modeling via stochastic differential equations.\", \"2. Following the literature, use /math to derive the evidence lower bound (ELBO) as the objective function, explicitly connecting the score-matching loss to the reverse-time SDE.\", \"3. Implement the neural network for score approximation, the SDE solver (e.g., Euler-Maruyama), and the training loop in JAX.\", \"4. Train the model on the MNIST dataset and validate its generative performance by computing the Fr\u00e9chet Inception Distance (FID) against the test set.\", \"5. Implement a method for uncertainty quantification, such as running Langevin MCMC on the posterior distribution of the model parameters or latent variables, and check that the resulting uncertainty estimates are plausible.\", \"6. Synthesize all derivations, code, and results into a coherent final report, and verify that the metrics in the report match the outputs logged by the code.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent's derivation of the ELBO must match the loss function implemented by the code agent. The code agent's reported FID score must be consistent with visual sample quality; fallback: If the derivation and implementation diverge, re-run /math with specific implementation constraints. If FID is poor despite a correct implementation, flag for review of hyperparameters or model architecture.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on score-based generative models with SDEs.\", \"- [ ] /math: Provide a step-by-step derivation of the ELBO for the VE-SDE.\", \"- [ ] /code: Implement the score network, SDE solver, and training loop in JAX/Flax.\", \"- [ ] /code: Implement the FID calculation and the Langevin dynamics sampler for uncertainty estimation.\", \"- [ ] /math: Verify that the final loss function in the code is a correct implementation of the derived mathematical expression.\", \"- [ ] /code: Validate that the final FID score is below 20 for the MNIST dataset.\", \"- [ ] /router: Perform final QA review on all artifacts, checking for consistency between derivation, code, and reported results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-differential-equations\", \"generative-models\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for the ELBO must be complete, correct, and consistent with the cited literature.\", \"- The JAX implementation must be runnable and train to completion, achieving an FID score of less than 20 on the MNIST 10k test set.\", \"- The final report must include visualizations of generated samples and a clear depiction of posterior predictive uncertainty for at least two generated digits.\", \"- At least two relevant arXiv papers must be cited correctly.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 263, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to formulate a safe reinforcement learning policy for a robotic arm using a Constrained Markov Decision Process (CMDP). Derive the Lagrangian dual formulation for this CMDP, showing how to update the policy and the Lagrange multiplier using a primal-dual approach. Then, implement this algorithm to train an agent that maximizes reward while ensuring the expected cost, representing a safety constraint like collision avoidance, remains below a predefined threshold.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org primal-dual lagrangian reinforcement learning CMDP convergence guarantees control theory\\\", mode=web)\", \"/math(Derive the Lagrangian relaxation for a CMDP objective, formulate the primal-dual update rules for both the policy parameters and the Lagrange multiplier, and analyze the conditions for convergence to a safe policy.)\", \"/code(Implement the derived primal-dual algorithm in Python using PyTorch, simulating a simple control theory environment. Validate the safety constraint satisfaction and track policy convergence throughout training.)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature on CMDP optimization. /math then formalizes the specific primal-dual Lagrangian derivation based on these findings. Finally, /code implements and simulates the derived algorithm to validate its performance on a control problem.\", \"expected_artifacts\": [\"A formal mathematical derivation of the primal-dual update rules for the CMDP.\", \"A Python script implementing the safe RL agent and a simple simulation environment.\", \"A brief report summarizing the convergence results and constraint satisfaction, including plots and at least 2 arXiv citations.\", \"A JSON file containing final metrics for reward and cost constraint violation.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of Constrained Markov Decision Processes (CMDPs) and primal-dual optimization methods in reinforcement learning.\", \"2. Formulate the Lagrangian for the CMDP, derive the policy gradient update, and the update rule for the Lagrange multiplier.\", \"3. Implement the derived algorithm, including the environment, policy network, and the dual update step.\", \"4. Validate the implementation by checking if the safety constraint is satisfied throughout training and if the agent's reward converges.\", \"5. Analyze the trade-off between reward maximization and constraint satisfaction, and verify that the results align with the theoretical derivation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code implementation must match the derived equations from /math; fallback: if convergence fails in /code, escalate to /math to re-check the stability conditions of the derivation).\", \"todo_list\": [\"- [ ] /general-search: Find recent papers on Lagrangian methods for CMDPs, focusing on convergence proofs.\", \"- [ ] /math: Derive the Karush-Kuhn-Tucker (KKT) conditions for the CMDP optimization problem.\", \"- [ ] /math: Formulate the complete primal-dual update equations for the policy and the Lagrange multiplier.\", \"- [ ] /code: Implement the CMDP environment and the agent's policy network.\", \"- [ ] /code: Implement the training loop incorporating the primal-dual updates.\", \"- [ ] /math: Verify that the implemented gradient updates in the code correctly reflect the mathematical derivation.\", \"- [ ] /code: Validate the simulation results, ensuring the safety cost remains below the specified threshold.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"robotics\", \"control-theory\"], \"acceptance_criteria\": [\"The mathematical derivation must be complete and self-contained, showing the steps for both primal and dual updates.\", \"The Python code must execute without errors and demonstrate convergence of the policy reward while respecting the safety cost constraint.\", \"The final report must include plots showing reward and cost curves over training epochs and cite at least two relevant arXiv papers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 265, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive and implement a Hamiltonian Monte Carlo (HMC) sampler for a Bayesian logistic regression model. The derivation must explicitly detail the leapfrog integrator for discretizing the Hamiltonian dynamics and the Metropolis-Hastings acceptance step. The implementation should be benchmarked on a synthetic dataset, and I need a rigorous analysis of MCMC convergence using the Gelman-Rubin statistic (R-hat) across multiple chains.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org hamiltonian monte carlo bayesian logistic regression convergence leapfrog integrator\\\", mode=both)\", \"/math(Derive the Hamiltonian equations for the Bayesian logistic regression posterior, including the potential and kinetic energy terms, and formulate the discrete-time leapfrog integration steps and the Metropolis-Hastings acceptance probability.)\", \"/code(Implement the HMC sampler in Python using JAX, including the leapfrog integrator and acceptance logic. Run multiple chains on synthetic data and compute convergence diagnostics like the R-hat statistic and effective sample size using bootstrap methods.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on HMC and its application, ensuring a theoretically sound approach. This research informs the /math agent's formal derivation of the sampler's mechanics. Finally, the /code agent translates the verified mathematical formulas into a practical implementation and performs the empirical validation, completing the task from theory to practice.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the HMC sampler for the specified model.\", \"- A Python script implementing the HMC sampler and the convergence analysis.\", \"- A summary report including plots of MCMC chains (trace plots), posterior distributions, and a table of convergence diagnostics (R-hat, ESS).\", \"- A bibliography file citing at least two authoritative papers on HMC or Bayesian inference.\", \"- A verification log confirming the symplectic properties of the integrator and the correctness of the R-hat calculation.\"], \"thinking_outline\": [\"1. Formulate the Bayesian logistic regression model by defining the likelihood and prior distributions, leading to the target posterior probability density.\", \"2. Define the Hamiltonian system by identifying the potential energy (negative log-posterior) and a kinetic energy term (using auxiliary momentum variables). Derive the corresponding partial differential equations for Hamilton's dynamics.\", \"3. Discretize the continuous-time Hamiltonian dynamics using a leapfrog (St\u00f6rmer-Verlet) integrator, providing the explicit update equations for position and momentum.\", \"4. Validate the leapfrog integrator implementation by checking for approximate energy conservation and time-reversibility, which are critical properties for a valid MCMC transition kernel.\", \"5. Derive the Metropolis-Hastings acceptance probability to correct for discretization errors from the leapfrog integrator, ensuring the resulting Markov chain converges to the correct target distribution.\", \"6. Verify MCMC convergence by running multiple chains from dispersed starting points and calculating the Gelman-Rubin (R-hat) diagnostic, ensuring it is close to 1.0 for all parameters.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers (e.g., Neal's HMC review) to the math agent. The math agent hands off the finalized, verified differential and update equations to the code agent. The code agent implements these equations and returns the code, plots, and diagnostic metrics. Router QA performs a final check on the consistency between the derivation, implementation, and results. Fallback: If the code's convergence diagnostics fail (R-hat >> 1.0), the task reverts to the math agent to re-check the integrator derivation for errors.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Hamiltonian Monte Carlo, such as Neal (2011) and Betancourt (2017).\", \"- [ ] /math: Derive the gradient of the log-posterior (the force term) for the Bayesian logistic regression model.\", \"- [ ] /math: Formulate the leapfrog integrator steps and the final Metropolis-Hastings acceptance ratio.\", \"- [ ] /code: Implement the HMC sampler in a Python script.\", \"- [ ] /code: Generate synthetic data and run at least 4 MCMC chains in parallel.\", \"- [ ] /code: Compute and report the R-hat and Effective Sample Size (ESS) for all model parameters.\", \"- [ ] /math: Verify that the derived leapfrog update equations are symplectic and volume-preserving.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"mcmc\", \"stochastic-processes\"], \"acceptance_criteria\": [\"- The mathematical derivation of the leapfrog integrator and acceptance probability must be complete and correct.\", \"- The implemented HMC sampler must successfully run and produce posterior samples for a synthetic logistic regression problem.\", \"- All model parameters must achieve a Gelman-Rubin statistic (R-hat) value less than 1.05, indicating successful convergence of the MCMC chains.\", \"- The final report must include trace plots that show the chains mixing well and posterior density plots for each parameter.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 267, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the optimization objective for structured pruning in a Transformer's multi-head attention block by formulating it as a constrained minimization problem. Specifically, use the proximal gradient descent algorithm to enforce a low-rank constraint on the weight matrices, analyzing the convergence properties via the Kurdyka-\u0141ojasiewicz inequality. Then, implement this in PyTorch, validating the theoretical convergence rate against empirical measurements of spectral decay and perplexity on a downstream task.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org proximal gradient methods structured pruning transformer Kurdyka-\u0141ojasiewicz convergence analysis\\\", mode=both)\", \"/math(Derive the proximal operator for nuclear norm regularization on attention matrices and sketch the convergence proof based on the Kurdyka-\u0141ojasiewicz property identified from arXiv literature.)\", \"/code(Implement the proximal gradient update in a PyTorch optimizer for a small transformer model, tracking spectral norm for convergence analysis and perplexity as the primary performance metric., using Python)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature on non-convex optimization for transformers. The math agent then uses these sources to build the core theoretical framework and derivations. Finally, the code agent implements the theory and runs experiments to validate the mathematical claims, creating a full theory-to-practice loop.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the proximal operator and the convergence proof sketch.\", \"A standalone Python script implementing the structured pruning algorithm for a pre-trained Transformer model.\", \"A summary report in Markdown comparing the empirical convergence rate (spectral norm decay) with the theoretical rate, including plots.\", \"A metrics file in JSON format containing final perplexity scores and convergence mismatch values.\", \"A bibliography citing at least two relevant arXiv papers with IDs.\"], \"thinking_outline\": [\"1. Research literature on structured pruning for Transformers, focusing on proximal algorithms and convergence analysis using the Kurdyka-\u0141ojasiewicz (KL) inequality.\", \"2. Formulate the constrained optimization problem and derive the proximal operator for a low-rank inducing norm (e.g., nuclear norm).\", \"3. Sketch the proof of convergence for the proximal gradient algorithm, explicitly stating the assumptions required for the objective function to satisfy the KL property.\", \"4. Implement the algorithm in Python using PyTorch, applying it to the attention weight matrices of a pre-trained language model like GPT-2.\", \"5. Validate the implementation by plotting the spectral norm of the weight matrices over training epochs and check if the decay aligns with the theoretical rate derived from the KL analysis.\", \"6. Verify the final model's utility by measuring perplexity on a validation set and comparing it to a dense baseline to audit for catastrophic performance degradation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides papers to the math agent. The math agent's derivation of the convergence rate provides a target for the code agent's empirical validation. Verification: The empirical convergence curve from the code agent must qualitatively match the theoretical rate from the math agent. Fallback: If rates diverge significantly, the math agent must re-check the proof's assumptions against the implementation details, or the code agent must audit for numerical stability issues.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on proximal gradient methods for structured pruning and convergence proofs using the Kurdyka-\u0141ojasiewicz inequality on arXiv.\", \"- [ ] /math: Derive the proximal operator for nuclear norm regularization applied to the attention weight matrices.\", \"- [ ] /math: Outline the convergence proof, stating the necessary assumptions related to the KL property.\", \"- [ ] /code: Implement the proximal gradient descent update rule within a PyTorch optimization loop for a GPT-2 model.\", \"- [ ] /code: Instrument the implementation to log the spectral norm of weights and model perplexity at each step.\", \"- [ ] /math: Verify that the empirical spectral norm decay curve from the /code artifact aligns with the derived theoretical convergence rate.\", \"- [ ] /code: Validate the final pruned model's perplexity against a dense baseline to quantify the performance-sparsity trade-off.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"structured-pruning\", \"convergence-analysis\", \"transformer-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the proximal operator for the specified norm is present and correct.\", \"- The Python implementation successfully reduces the spectral norm of the target weight matrices over successive iterations.\", \"- The final report includes a plot comparing empirical convergence to the theoretical rate, with a mismatch (per the secondary metric) below 20%.\", \"- At least two relevant arXiv papers are cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 268, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a safe reinforcement learning agent for a robotic manipulator. Please formulate the problem as a Constrained Markov Decision Process (CMDP) and derive the primal-dual update rules using Lagrangian relaxation. The goal is to maximize cumulative reward while ensuring the expected cumulative safety cost remains below a predefined threshold. Can you also guide me through implementing this with a modern deep RL framework, including verification steps for both the control theory derivation and the code's convergence properties?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org constrained markov decision process lagrangian relaxation primal-dual reinforcement learning robotics\\\", mode=rag)\", \"/math(Formulate the CMDP objective with Lagrangian relaxation. Derive the Karush-Kuhn-Tucker (KKT) conditions and the corresponding gradient-based primal-dual update rules for policy and multiplier parameters, analyzing the control theory convergence guarantees.)\", \"/code(Implement the derived primal-dual algorithm in Python with PyTorch for a Safety Gym environment. Track performance metrics like cumulative reward and constraint violation rate, and include runtime validation checks for the safety budget.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative papers on Lagrangian methods for CMDPs, which will ground the theoretical work. The results are passed to /math to perform the core theoretical derivation of the primal-dual update rules. Finally, /code will translate the validated mathematical formulation into a working implementation and evaluate it empirically.\", \"expected_artifacts\": [\"- A PDF report containing the full mathematical derivation of the Lagrangian primal-dual updates for the CMDP.\", \"- Python source code implementing the safe RL agent and the simulation environment.\", \"- A `results.json` file logging the training curves for cumulative reward and constraint violation rate.\", \"- A bibliography file with citations to at least two relevant arXiv papers.\", \"- A verification summary confirming the alignment between the mathematical derivation and the code's behavior.\"], \"thinking_outline\": [\"1. Formalize the problem as a Constrained Markov Decision Process (CMDP), clearly defining state, action, transition dynamics, reward function, and the safety cost function with its threshold.\", \"2. Use Lagrangian relaxation to incorporate the safety constraint into the objective function and derive the primal-dual optimization problem.\", \"3. Derive the update rules for the policy (primal update) and the Lagrange multiplier (dual update) by applying policy gradient theorems and gradient ascent, respectively.\", \"4. Validate the derived Karush-Kuhn-Tucker (KKT) conditions for the constrained policy optimization problem to ensure theoretical soundness.\", \"5. Implement the algorithm and verify that the empirical constraint violation rate in simulation respects the theoretical safety bound established in the derivation.\", \"6. Analyze the convergence of the algorithm by plotting the reward and constraint cost over training epochs, checking for stability and oscillations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Cross-reference the implemented update rules in /code against the final derivation from /math; fallback: If implementation diverges, flag for /math to re-check the derivation's practical assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find at least two seminal papers on primal-dual methods for CMDPs (e.g., from Achiam et al. or Tessler et al.).\", \"- [ ] /math: Derive the policy gradient for the Lagrangian objective and the update rule for the dual variable.\", \"- [ ] /math: Verify the correctness of the primal-dual update rule derivation against sources from /general-search.\", \"- [ ] /code: Implement the CMDP agent using a trust-region or proximal policy optimization base.\", \"- [ ] /code: Implement unit tests to validate the constraint cost calculation and confirm it matches the mathematical formulation.\", \"- [ ] /code: Run experiments in a simulated environment (e.g., Safety Gym 'Safexp-PointGoal1-v0') and generate plots for reward and safety cost.\", \"- [ ] router: Perform final QA review, ensuring the generated plots in the report align with the acceptance criteria and metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"robotics\", \"constrained-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the primal-dual updates must be complete and self-contained.\", \"- The implemented agent must achieve a cumulative reward comparable to unconstrained baselines while maintaining the constraint violation rate below the specified threshold (e.g., < 0.05).\", \"- The final report must include plots demonstrating the agent's learning progress and constraint satisfaction over at least 1 million environment steps.\", \"- The solution must cite at least two relevant academic papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 269, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the computational complexity of solving generalized Wythoff games on graphs. I need a formal proof for the PSPACE-completeness of the problem, a Python implementation of a recursive algorithm with memoization to find Sprague-Grundy values for bounded-treewidth graphs, and an analysis of its fixed-parameter tractability. The implementation must handle adversarial scenarios.\n", "completion": "{\"route_plan\": [\"/general-search(Find foundational arXiv papers on PSPACE-completeness proofs for combinatorial games and algorithms for computing Sprague-Grundy values, focusing on adversarial complexity., mode=web)\", \"/math(Derive the formal PSPACE-completeness proof via reduction from Quantified Boolean Formulas (QBF) and formulate the recurrence for Sprague-Grundy values, ensuring the complexity analysis is rigorous., includes numpy)\", \"/code(Implement a Python solver for the game on bounded-treewidth graphs, using memoization to handle the adversarial search space and empirically validate its fixed-parameter tractability.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature, as PSPACE-completeness proofs are often non-trivial and build on existing work. This research informs the /math agent's formal proof construction and recurrence derivation. Finally, the /code agent implements the derived algorithm, providing an empirical validation of the theoretical findings.\", \"expected_artifacts\": [\"A formal proof in LaTeX/PDF detailing the reduction for PSPACE-completeness.\", \"A well-documented Python script implementing the fixed-parameter tractable algorithm for computing Grundy values.\", \"A performance analysis report comparing empirical runtime against theoretical complexity bounds.\", \"A bibliography file citing at least two relevant academic papers with their arXiv IDs.\"], \"thinking_outline\": [\"1. Research existing literature on the complexity of impartial games, particularly those known to be PSPACE-complete, and algorithms on graphs of bounded treewidth.\", \"2. Formulate a polynomial-time reduction from a known PSPACE-complete problem, such as Quantified Boolean Formulas (QBF), to the decision version of the generalized Wythoff game on graphs.\", \"3. Derive the recursive formula for the Sprague-Grundy (nim-sum) values of game positions based on the graph structure and game rules.\", \"4. Implement the algorithm in Python, incorporating memoization to manage the overlapping subproblems inherent in the recursion.\", \"5. Validate the logical soundness of the PSPACE-completeness proof by auditing each step of the reduction and the construction of the corresponding game instance.\", \"6. Verify the algorithm's complexity by profiling its runtime on graphs with varying sizes and treewidth, checking for consistency with the expected fixed-parameter theoretical bounds.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to /math. The /math agent produces a formal proof and recurrence relation, which serves as the specification for the /code agent. The /code agent implements and profiles the algorithm. Router QA performs the final review. Verification: The proof from /math must be consistent with the algorithm from /code. Fallback: If the proof is flawed, return to /math with counterexamples found during coding.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on PSPACE-completeness in combinatorial games and FPT algorithms for graph problems.\", \"- [ ] /math: Construct the formal reduction from QBF to the generalized Wythoff game on graphs.\", \"- [ ] /math: Define the recurrence relation for the Sprague-Grundy values based on the game's move set.\", \"- [ ] /code: Implement the recursive solver with memoization in Python, using an appropriate library for graph representation.\", \"- [ ] /code: Develop a test suite with known game states and graphs to validate the algorithm's correctness.\", \"- [ ] /math: Verify that the implemented algorithm's logic correctly reflects the derived recurrence for the Sprague-Grundy values.\", \"- [ ] /code: Profile the implementation to empirically validate its fixed-parameter tractability with respect to treewidth.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"computational-complexity\", \"fixed-parameter-tractability\"], \"acceptance_criteria\": [\"The PSPACE-completeness proof must be logically sound, with a clearly defined and correct polynomial-time reduction.\", \"The Python implementation must correctly compute Grundy values for all provided test cases, including edge cases like disconnected graphs or cycles.\", \"The empirical runtime analysis must demonstrate performance consistent with the theoretical fixed-parameter complexity `O(f(k) * poly(n))`, where k is treewidth.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 270, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the relationship between the Hessian eigenspectrum and generalization in deep learning. Please derive the mathematical connection between the top eigenvalue of the loss Hessian and the network's local Lipschitz constant. Then, implement a power iteration method in PyTorch using Hessian-vector products to efficiently track this dominant eigenvalue for a ResNet-18 on CIFAR-10, and analyze its correlation with the generalization gap and loss landscape sharpness during training.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org hessian eigenspectrum generalization deep learning lipschitz constant power iteration\\\", mode=rag, includes cybersecurity)\", \"/math(Derive the relationship between the Hessian's spectral norm and the local Lipschitz constant for a multi-layer perceptron, noting assumptions about activation functions and their derivatives. Outline the power iteration for finding the top eigenvalue.)\", \"/code(Implement the power iteration method in Python using PyTorch's autograd for Hessian-vector products. Train a ResNet-18 on CIFAR-10, track the top Hessian eigenvalue, training/validation loss, and accuracy, ensuring CUDA compatibility and convergence checks.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on Hessian analysis and generalization theory. This theoretical grounding is then passed to /math for a formal derivation of the core mathematical relationships. Finally, /code implements the derived algorithm to perform an empirical study, directly translating the theory into a verifiable experiment.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF with the mathematical derivation connecting the Hessian's spectral norm to the Lipschitz constant.\", \"A Python script implementing the power iteration algorithm and the training loop for ResNet-18 on CIFAR-10.\", \"A results report (.md or .pdf) containing plots of the top eigenvalue, training/validation loss, and accuracy over epochs.\", \"A JSON file containing the logged metrics from the training run.\", \"A bibliography file with citations for at least 2 relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research the theoretical foundations connecting loss landscape geometry, specifically the Hessian's eigenspectrum, to measures of model generalization and the Lipschitz constant.\", \"2. Formulate and write down the formal mathematical derivation linking the top eigenvalue (spectral norm) of the loss Hessian to the network's local Lipschitz properties.\", \"3. Design a computationally feasible algorithm (power iteration via Hessian-vector products) to estimate this top eigenvalue for a large-scale neural network without forming the full Hessian matrix.\", \"4. Implement the algorithm and training experiment in PyTorch, ensuring correct use of autograd for the Hessian-vector products.\", \"5. Validate the power iteration implementation on a small, analytically tractable model by comparing its output to the true top eigenvalue computed via direct eigendecomposition.\", \"6. Execute the main experiment, then analyze and visualize the resulting data to verify the hypothesized correlation between the eigenvalue's magnitude and the model's generalization gap.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides key papers (e.g., on sharpness-aware minimization) to the math agent. The math agent produces a formal derivation that serves as a specification for the code agent. The code agent implements this spec and produces empirical results. Router QA performs the final review. Verification: The code's eigenvalue tracking logic must match the math derivation. Fallback: If the derivation is flawed or non-convergent, the math agent must re-derive using alternative assumptions, possibly prompted by another targeted search.\", \"todo_list\": [\"- [ ] /general-search: Find at least two seminal papers on Hessian analysis for deep learning generalization (e.g., from Keskar et al., or Yao et al.).\", \"- [ ] /math: Produce a clean derivation of the relationship between the Hessian spectral norm and the local Lipschitz constant.\", \"- [ ] /math: Outline the steps of the power iteration algorithm for finding the largest eigenvalue of a symmetric matrix.\", \"- [ ] /code: Implement the power iteration algorithm using `torch.autograd.grad` to compute Hessian-vector products.\", \"- [ ] /code: Integrate the eigenvalue tracking into a standard CIFAR-10 training loop for a ResNet-18 model.\", \"- [ ] /math: Verify that the implemented power iteration logic in the code correctly reflects the mathematical algorithm.\", \"- [ ] /code: Run a validation check on a small linear model to confirm the eigenvalue calculation is correct.\", \"- [ ] router QA: Review the final plots to ensure they correctly correlate the eigenvalue dynamics with the generalization gap.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"hessian-eigenvalues\", \"matrix-calculus\"], \"acceptance_criteria\": [\"- The final report must contain a clear plot showing the top Hessian eigenvalue increasing or stabilizing over training epochs.\", \"- The Python code must execute without errors and produce a metrics file containing at least 'epoch', 'train_loss', 'val_loss', 'val_acc', and 'hessian_top_eigenvalue' keys.\", \"- The mathematical derivation must be self-contained, clearly stated, and correctly link the Hessian's spectral norm to the function's local Lipschitzness.\", \"- The implementation must use efficient Hessian-vector products and not attempt to materialize the full Hessian matrix for the ResNet model.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the Pearson correlation coefficient between the time series of the top Hessian eigenvalue (\u03bb_max) and the generalization gap (validation loss - training loss). A strong positive correlation is expected.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the number of iterations required for the eigenvalue estimate to converge within a tolerance of 1e-4 at different training checkpoints. This should remain stable.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 271, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a policy gradient algorithm, specifically Proximal Policy Optimization (PPO), but augmented with a curiosity module based on prediction error, like the Intrinsic Curiosity Module (ICM). Please provide a formal derivation of the combined objective function, including the KL-divergence constraint and the intrinsic reward term. Then, implement this in PyTorch for a continuous control environment like MuJoCo's Hopper-v4, and analyze its sample efficiency and convergence against a vanilla PPO baseline. Also, provide a theoretical analysis of the surrogate objective's first-order approximation and its impact on monotonic improvement guarantees.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org intrinsic curiosity module PPO convergence analysis continuous control theory\\\", mode=rag)\", \"/math(Derive the surrogate objective for PPO with an Intrinsic Curiosity Module, analyzing the first-order approximation of the KL-divergence constraint and its impact on monotonic convergence guarantees in control theory.)\", \"/code(Implement the derived PPO-ICM algorithm in PyTorch for the Hopper-v4 environment, tracking policy entropy and value function convergence, and ensure CUDA compatibility for accelerated training., using Python)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational papers on Proximal Policy Optimization (PPO) and Intrinsic Curiosity Modules (ICM), establishing the theoretical basis. This information is passed to /math to formally derive the combined objective function and analyze its properties. Finally, /code implements the derived algorithm to empirically validate the theoretical claims on a challenging benchmark.\", \"expected_artifacts\": [\"- A PDF report containing the complete mathematical derivation of the PPO-ICM objective function and its analysis.\", \"- A standalone Python script implementing the PPO-ICM agent and the baseline PPO agent.\", \"- A set of PNG plots visualizing the learning curves (cumulative reward vs. environment steps) for both agents.\", \"- A bibliography in the report citing at least two relevant arXiv research papers with their IDs.\", \"- A verification summary confirming that the implemented loss function matches the derived mathematical expressions.\"], \"thinking_outline\": [\"1. Research the foundational papers on PPO (Schulman et al.) and ICM (Pathak et al.) to understand the core components: the clipped surrogate objective, the value function loss, the policy entropy bonus, and the intrinsic reward generation mechanism.\", \"2. Formally derive the composite loss function for the PPO-ICM agent, carefully combining the PPO objective with the losses for the ICM's forward and inverse dynamics models.\", \"3. Validate the derivation by checking the gradient expressions and ensuring they align with the principles of monotonic policy improvement and curiosity-driven exploration.\", \"4. Implement the complete agent in PyTorch, including the actor-critic networks, the ICM module, and the training loop that collects rollouts and performs updates.\", \"5. Verify the implementation's correctness through unit tests on the ICM's prediction error calculation and by logging intermediate values like intrinsic rewards and policy entropy to check for stability and expected behavior.\", \"6. Execute the comparative experiment between PPO-ICM and baseline PPO on the Hopper-v4 environment, ensuring proper hyperparameter tuning and statistical significance in the results.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The final implementation's loss calculation in /code must be audited against the equations derived in /math. The empirical results must support the theoretical benefits outlined from /general-search. fallback: If the implementation diverges from the derivation, /math will be re-engaged to simplify or correct the formulation.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on PPO and Intrinsic Curiosity Module (ICM), focusing on convergence proofs and objective functions.\", \"- [ ] /math: Derive the combined PPO-ICM objective function, explicitly showing the policy and value loss terms, plus the forward/inverse model losses.\", \"- [ ] /code: Implement the PPO agent with the ICM module in PyTorch, targeting the Hopper-v4 environment.\", \"- [ ] /code: Implement the baseline PPO agent for comparative analysis, ensuring identical network architectures and hyperparameters where applicable.\", \"- [ ] /math: Verify that the gradients derived mathematically for the policy update correspond to the computational graph implemented in the code's backward pass.\", \"- [ ] /code: Generate plots comparing the learning curves (cumulative reward vs. timesteps) for PPO-ICM and the PPO baseline.\", \"- [ ] /router: QA review of all artifacts, ensuring the final report's analysis aligns with the empirical results and mathematical derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"control-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the PPO-ICM objective function is complete, correct, and included in the final report.\", \"- The Python implementation runs successfully and demonstrates a statistically significant improvement in sample efficiency (e.g., faster convergence to a target reward) over the baseline PPO.\", \"- The final report includes learning curve plots comparing PPO-ICM and PPO, and cites at least two relevant arXiv papers by ID.\", \"- A verification step confirms that the code's loss components directly map to the terms in the derived equations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 273, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on SE(3)-equivariant graph neural networks for molecular dynamics. I need to derive the message passing update rules for a gauge equivariant GNN based on a fiber bundle formulation. Please provide a formal proof demonstrating that the learned Hamiltonian is invariant and the resulting forces are equivariant under SE(3) transformations. Also, provide a prototype implementation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org gauge equivariant graph neural networks molecular dynamics SE(3) fiber bundle Lie algebra\\\", mode=web)\", \"/math(Derive the continuous-time Hamiltonian dynamics using a gauge equivariant formulation. Prove that the system's total energy is SE(3)-invariant and that the force vectors are SE(3)-equivariant by applying transformations from the Lie algebra se(3).)\", \"/code(Implement the derived equivariant message passing layers in Python using PyTorch Geometric. Validate the implementation with a numerical test checking for low equivariance error and minimal energy conservation drift on a small molecular system like ethanol.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on gauge equivariant GNNs. This informs the /math agent's formal derivation and proof of the SE(3) equivariance properties. Finally, the /code agent implements the verified mathematical framework and performs numerical validation.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the gauge equivariant message passing scheme and the proof of SE(3) invariance/equivariance.\", \"Python source code for the GNN model, including custom PyTorch Geometric layers.\", \"A summary report citing at least two relevant academic papers with arXiv IDs.\", \"A JSON file containing the results of the numerical equivariance and energy conservation tests.\", \"A visualization plot showing the energy drift over a short simulation trajectory.\"], \"thinking_outline\": [\"1. Research and synthesize the mathematical formalism of gauge equivariant GNNs from recent literature, focusing on the use of fiber bundles and Lie algebra for representing SE(3) transformations.\", \"2. Formulate the molecular graph problem in this geometric framework and derive the specific message-passing and update equations for the Hamiltonian and forces.\", \"3. Construct a formal proof showing that the derived Hamiltonian is invariant and the forces are equivariant with respect to global SE(3) transformations.\", \"4. Validate the mathematical proof for internal consistency and correctness, ensuring it aligns with the principles of Hamiltonian mechanics.\", \"5. Translate the derived mathematical operations into a stable and efficient PyTorch Geometric implementation, focusing on the logarithmic and exponential maps for the se(3) Lie algebra.\", \"6. Verify the implementation's correctness by designing and running a numerical test that applies random SE(3) transformations to the input coordinates and measures the equivariance error of the output forces.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code's numerical equivariance error must be below a tolerance of 1e-5 and align with the formal proof; fallback: if numerical tests fail, return to /math to check for errors in the derivation of the force calculation).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on gauge equivariant GNNs for MD.\", \"- [ ] /math: Produce a complete derivation of the gauge equivariant update equations.\", \"- [ ] /math: Write a formal proof of SE(3) invariance for the Hamiltonian and equivariance for the forces.\", \"- [ ] /code: Implement the core message passing layers in a Python script.\", \"- [ ] /code: Create a numerical test suite to validate the equivariance property.\", \"- [ ] /math: Verify that the implemented numerical tests in the code correctly reflect the conditions outlined in the mathematical proof.\", \"- [ ] /code: Verify that the model conserves energy over a short NVE simulation trajectory.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"equivariant-gnn\", \"molecular-dynamics\", \"differential-geometry\"], \"acceptance_criteria\": [\"- Pass: The submitted mathematical proof is rigorous and correctly demonstrates SE(3) invariance and equivariance.\", \"- Pass: The implemented code achieves a numerical equivariance error below 1e-5 on the test dataset.\", \"- Pass: The model demonstrates energy conservation with a drift of less than 1% over a 100-step NVE simulation.\", \"- Fail: The mathematical proof contains logical errors or is incomplete.\", \"- Fail: The code produces an equivariance error greater than 1e-5 or fails to run.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 274, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the logical error rate for a [[d, 1, d]] surface code under a phenomenological depolarizing noise model with physical error probability 'p'. I need the full mathematical derivation of the threshold theorem's scaling behavior, a numerical simulation in Python to validate the analytical bounds against a Monte Carlo estimation, and a review of recent advancements in fault-tolerant architectures that might improve the code's performance, specifically citing post-2020 arXiv literature.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org surface code logical error rate threshold theorem phenomenological depolarizing noise bayesian analysis\\\", mode=web)\", \"/math(Derive the logical error rate for a surface code under depolarizing noise, establishing the convergence criteria and complexity scaling based on the code distance 'd' and physical error 'p'.)\", \"/code(Implement a Monte Carlo simulation in Python using a bootstrap method to estimate the logical error rate for varying code distances and verify the analytical threshold. Profile for potential CUDA optimization.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather the latest theoretical foundations from arXiv. This literature then informs the /math agent's formal derivation. Finally, the /code agent implements a simulation to numerically validate the theoretical results from /math, providing a robust verification loop.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the logical error rate.\", \"- A Python script implementing the Monte Carlo simulation of the surface code.\", \"- A JSON file containing the simulated logical error rates for different physical error rates and code distances.\", \"- A plot comparing the analytical performance bounds against the simulated data points.\", \"- A bibliography with citations to at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Use search to find the most current and authoritative formulations of the surface code threshold theorem and its proof under a depolarizing noise model.\", \"2. Formulate the mathematical model for the logical error rate as a function of the physical error rate 'p' and code distance 'd', based on the literature.\", \"3. Derive the scaling laws and the analytical expression for the error threshold.\", \"4. Implement a Monte Carlo simulation of the complete error correction cycle for the surface code, including syndrome extraction and decoding.\", \"5. Validate the analytical derivation against the simulation results by plotting both and calculating the discrepancy, ensuring they fall within expected statistical bounds.\", \"6. Verify that the assumptions made in the mathematical derivation are correctly implemented in the simulation code and supported by the cited literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The analytical curve from /math must fit the simulated data from /code within a 95% confidence interval. fallback: If discrepancy is high, return to /math to re-check derivation assumptions or to /code to debug the simulation's noise model.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal and recent arXiv papers on surface code thresholds under depolarizing noise.\", \"- [ ] /math: Formulate and complete the derivation for the logical error rate scaling as a function of p and d.\", \"- [ ] /code: Implement the Monte Carlo simulation for the [[d,1,d]] surface code, including a minimum weight perfect matching decoder.\", \"- [ ] /code: Generate plots comparing the analytical curve to simulated data points with statistical error bars.\", \"- [ ] /math: Verify that the analytical derivation's assumptions (e.g., independent errors) align with the simulation's implementation.\", \"- [ ] /code: Validate the statistical convergence of the Monte Carlo results by analyzing the variance as a function of simulation runs.\", \"- [ ] Router QA: Review final report for consistency between derivation, simulation, and the cited literature.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"information-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly establishes the exponential suppression of the logical error rate below the threshold.\", \"- The simulation results for the logical error rate match the analytical bounds within a statistically acceptable margin of error (e.g., 95% confidence interval).\", \"- The final report cites at least 2 relevant post-2020 arXiv papers with their corresponding IDs.\", \"- The implemented code must be runnable and reproduce the plots and metrics submitted.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 275, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the gradient expression for the architecture parameters in Differentiable Architecture Search (DARTS), paying close attention to the bilevel optimization structure and the implicit differentiation that leads to the inverse Hessian-vector product. Then, propose and derive a regularization term based on the Hessian eigenspectrum to stabilize the search process, and implement this regularized version in PyTorch to demonstrate its superior convergence properties on the CIFAR-10 search space compared to the unregularized baseline.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org DARTS differentiable architecture search instability Hessian eigenvalue regularization\\\", mode=rag)\", \"/math(Derive the bilevel AutoML gradient for architecture parameters, analyzing the Hessian eigenvalue spectrum and its impact on convergence stability.)\", \"/code(Implement the regularized DARTS search in PyTorch, tracking validation accuracy to ensure monotonic convergence and comparing the final architecture against a simple Bayesian baseline., using Python)\"], \"route_rationale\": \"The blue general-search agent first gathers foundational literature on DARTS instability. The /math agent then formalizes the problem by deriving the regularized gradient estimator. Finally, the /code agent provides an empirical validation of the mathematical theory by implementing and testing the solution, creating a closed loop of theory and practice.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the regularized DARTS gradient.\", \"- A Python script implementing both the baseline and regularized DARTS search algorithms.\", \"- A JSON file containing final validation accuracy and stability metrics for both methods.\", \"- A plot comparing the validation accuracy curves of the regularized vs. baseline search over epochs.\", \"- A summary report citing at least two relevant arXiv papers that informed the solution.\"], \"thinking_outline\": [\"1. Formulate the bilevel optimization problem of DARTS, clearly separating the inner (network weights) and outer (architecture parameters) optimization loops.\", \"2. Derive the gradient for the architecture parameters using the implicit function theorem, identifying the inverse Hessian-vector product as the source of instability.\", \"3. Propose a regularization scheme based on perturbing or controlling the Hessian eigenvalues and derive the corresponding modified gradient update rule.\", \"4. Implement the regularized and baseline DARTS updates in Python, setting up a search experiment on a standard proxy dataset like CIFAR-10.\", \"5. Verify the correctness of the analytical gradient implementation by comparing its output against a numerical gradient computed via finite differences.\", \"6. Validate the regularizer's effectiveness by running the search and demonstrating that the regularized method achieves more stable validation accuracy during training and finds a better final architecture.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: the /code agent will use finite differences to numerically verify the analytical gradient derived by the /math agent; fallback: if the numerical check fails (error > 1e-4), the task is routed back to /math to audit the derivation for errors in applying the chain rule or implicit function theorem).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on DARTS instability and Hessian-based regularization.\", \"- [ ] /math: Formulate the DARTS bilevel optimization problem and derive the standard architecture gradient.\", \"- [ ] /math: Propose a Hessian eigenvalue-based regularizer and derive the new, stabilized architecture gradient.\", \"- [ ] /code: Implement the DARTS search space and both the baseline and regularized optimizers in PyTorch.\", \"- [ ] /code: Implement a numerical gradient checker using finite differences for verification.\", \"- [ ] /math: Verify that the analytical derivation from the math agent aligns with the numerical gradient check from the code agent.\", \"- [ ] /code: Execute the search experiment on CIFAR-10, logging all specified metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"differentiable-automl\", \"bilevel-optimization\", \"gradient-estimation\"], \"acceptance_criteria\": [\"- The mathematical derivation of the regularized gradient is self-contained, rigorous, and clearly explains the motivation for the regularizer.\", \"- The Python implementation's analytical gradient must match a finite-difference numerical gradient check with a relative error of less than 1e-4.\", \"- The final report must show that the regularized search method reduces the variance of validation accuracy by at least 20% compared to the baseline during the search process.\", \"- The found architecture from the regularized search achieves a higher final validation accuracy than the baseline.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 277, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am working with a non-parametric Structural Causal Model (SCM) and need to derive the adjustment formula for the Average Causal Effect (ACE) of a continuous intervention, Z, on an outcome, Y, confounded by a set of covariates, X. Can you derive the appropriate g-formula or an equivalent adjustment expression using Pearl's do-calculus, and then implement a simulation in Python using a kernel-based estimator to validate the formula's unbiasedness under a known data-generating process? Please research recent advancements in debiased machine learning for this estimation to ensure robustness.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org debiased machine learning continuous treatment effect causal inference econometrics\\\", mode=web)\", \"/math(Derive the g-formula for the continuous intervention using do-calculus, ensuring the identification strategy is sound for this Bayesian causal model and respects the backdoor criterion.)\", \"/code(Implement a Python simulation with a non-linear SCM to validate the derived formula; use bootstrap resampling to compute a confidence interval for the estimated Average Causal Effect.)\"], \"route_rationale\": \"The blue general-search agent is first used to survey recent literature on debiased estimators for continuous treatments, as requested. Next, the /math agent is required to perform the formal symbolic derivation of the adjustment formula using do-calculus. Finally, the /code agent implements a simulation to numerically validate the derived formula's properties, such as unbiasedness.\", \"expected_artifacts\": [\"A LaTeX document containing the full do-calculus derivation of the adjustment formula.\", \"A Python script (`simulation.py`) implementing the SCM, the derived estimator, and Monte Carlo validation.\", \"A summary report (`report.md`) detailing the simulation results, including bias and variance metrics and comparison plots.\", \"A verification log confirming the logical consistency between the mathematical derivation and the code implementation.\", \"A bibliography with citations for at least two recent arXiv papers on debiased machine learning.\"], \"thinking_outline\": [\"1. Review the foundational principles of Pearl's do-calculus and the back-door criterion for identifying causal effects from observational data.\", \"2. Search for and synthesize recent literature (past 5 years) on non-parametric and semi-parametric estimation of continuous treatment effects, focusing on doubly robust or debiased machine learning techniques.\", \"3. Formally derive the adjustment formula for P(Y|do(Z=z)) using the rules of do-calculus, expressing it as an integral over the confounding covariates X.\", \"4. Construct a Python simulation based on a non-linear Structural Causal Model (SCM) where the true Average Causal Effect (ACE) is known analytically.\", \"5. Implement the derived kernel-based estimator and a naive regression estimator in the simulation; validate the implementation against the mathematical derivation for logical consistency.\", \"6. Execute Monte Carlo runs to check if the implemented estimator is unbiased for the true ACE and quantify the bias of the naive approach.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: search results inform the derivation's choice of estimator; math derivation provides the exact formula for the code. The code's empirical bias must be statistically insignificant, validating the math. fallback: if the derivation is intractable, /general-search will be re-queried for identifiable sub-classes of the model; if the code fails verification, the math-to-code logic will be audited.)\", \"todo_list\": [\"- [ ] /general-search: Find recent arXiv papers on debiased machine learning for continuous treatment effect estimation.\", \"- [ ] /math: Derive the adjustment formula for P(Y|do(Z=z)) using do-calculus principles.\", \"- [ ] /code: Implement a non-linear SCM data-generating process in Python.\", \"- [ ] /code: Implement the derived kernel-based estimator for the Average Causal Effect (ACE).\", \"- [ ] /math: Verify that the implemented estimator in the code logically corresponds to the derived mathematical formula.\", \"- [ ] /code: Run Monte Carlo simulations to compute the bias and variance of the estimator and compare against the known true ACE.\", \"- [ ] /general-search: Cross-reference the final results with benchmarks or findings from the cited literature.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"debiased-machine-learning\"], \"acceptance_criteria\": [\"- The do-calculus derivation for the adjustment formula is mathematically sound and complete.\", \"- The Python simulation correctly implements the specified SCM and the derived estimator.\", \"- The final report shows that the estimator's bias is statistically insignificant (e.g., p > 0.05 in a t-test against zero bias) and is demonstrably lower than a naive regression baseline.\", \"- At least two relevant arXiv papers from the last 5 years are cited.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 278, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am working on Bayesian deep learning for continuous-time models. Can you derive the evidence lower bound (ELBO) for a latent variable model where the latent process is governed by a stochastic differential equation (SDE)? Please connect this to Girsanov's theorem for the change of measure. Then, implement a variational autoencoder with this Neural SDE latent space using PyTorch and `torchsde` to model a synthetic Ornstein-Uhlenbeck process, and validate that the learned drift and diffusion functions converge to the true parameters.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference for neural stochastic differential equations Girsanov theorem KL divergence\\\", mode=both)\", \"/math(Derive the ELBO for a latent SDE model using a change of measure via Girsanov's theorem. Formulate the KL divergence between the prior SDE and the approximate posterior Bayesian SDE path measures.)\", \"/code(Implement a VAE with a Neural SDE latent space in PyTorch to learn the drift and diffusion of a synthetic Ornstein-Uhlenbeck process. Use the adjoint sensitivity method for backpropagation and validate with the Euler-Maruyama scheme., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on Neural SDEs and variational inference. This informs the /math agent's formal derivation of the ELBO. Finally, the /code agent implements the derived mathematical framework and performs empirical validation, creating a tight loop between theory and practice.\", \"expected_artifacts\": [\"- A LaTeX document containing the full mathematical derivation of the SDE ELBO, referencing Girsanov's theorem.\", \"- A Python script implementing the Neural SDE VAE, including data generation, model training, and evaluation.\", \"- A results summary comparing the learned drift/diffusion functions to the ground truth parameters via plots and metrics.\", \"- A verification report confirming the consistency between the mathematical derivation and the implemented loss function.\", \"- A bibliography with at least two cited arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Research the formulation of variational inference in continuous-time latent variable models, specifically focusing on Neural SDEs and the role of Girsanov's theorem.\", \"2. Formally derive the ELBO, expressing the KL divergence between the prior and posterior path measures as an integral involving the drift functions.\", \"3. Implement a data generator for a known SDE (e.g., Ornstein-Uhlenbeck process) to serve as the ground truth for validation.\", \"4. Construct and train a VAE with a Neural SDE encoder and a simple decoder in PyTorch, ensuring the loss function correctly implements the derived ELBO.\", \"5. Validate the trained model by comparing the learned neural network approximations of the drift and diffusion functions against the true functions of the Ornstein-Uhlenbeck process.\", \"6. Audit the entire pipeline by cross-referencing the code's loss computation with the mathematical derivation to ensure correctness and stability.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to the /math agent. The /math agent produces a formal derivation that serves as a blueprint for the /code agent's implementation of the loss function. The /code agent's results are then passed to the router for final verification against the mathematical specification. Verification: Code's KL divergence term must match the derived integral form. Fallback: If derivation is incorrect or implementation diverges, the /math agent must revisit the Girsanov application and simplify assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on variational inference for Neural SDEs.\", \"- [ ] /math: Produce a step-by-step derivation of the ELBO for SDEs using Girsanov's theorem.\", \"- [ ] /code: Generate a synthetic dataset from an Ornstein-Uhlenbeck process.\", \"- [ ] /code: Implement the Neural SDE VAE model and training loop using `torchsde`.\", \"- [ ] /math: Verify that the loss function implemented in the code is a correct discretization of the derived continuous-time ELBO.\", \"- [ ] /code: Validate the final model's learned parameters against the ground truth and generate comparison plots.\", \"- [ ] /router: Perform final QA check, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-differential-equations\", \"bayesian-deep-learning\", \"variational-inference\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO must be complete and correctly apply Girsanov's theorem.\", \"- The Python implementation must successfully train and learn the drift/diffusion of the synthetic data, with the MSE of the learned drift function being less than 0.1 compared to the true function.\", \"- The final report must include plots visually confirming the convergence of the learned functions to the ground truth.\", \"- All generated artifacts, including code, derivation, plots, and citations, must be delivered and internally consistent.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 279, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to model the posterior distribution of weights in a Bayesian Neural Network using a continuous-time Langevin dynamics approach. Can you derive the Stochastic Differential Equation (SDE) corresponding to the posterior update, specifically incorporating a pre-conditioning matrix derived from the Fisher Information Matrix? Then, implement a numerical solver like the Euler-Maruyama method in PyTorch to sample from this posterior and validate the convergence of the predictive uncertainty against a known ground truth distribution on a synthetic regression task.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org \\\\\\\"stochastic gradient langevin dynamics\\\\\\\" \\\\\\\"fisher information\\\\\\\" bayesian neural network sde\\\", mode=both, context=\\\"Fetch recent arXiv papers on pre-conditioned Stochastic Gradient Langevin Dynamics (pSGLD) for Bayesian deep learning, focusing on the SDE formulation with Fisher Information pre-conditioning and its convergence properties.\\\")\", \"/math(context=\\\"Derive the continuous-time SDE for Bayesian posterior sampling using Langevin dynamics, incorporating the Fisher Information Matrix as a pre-conditioner and analyzing its effect on the convergence rate, considering potential eigenvalue issues.\\\")\", \"/code(context=\\\"Implement the Euler-Maruyama discretisation of the derived pSGLD in PyTorch, apply it to a Bayesian neural network for a regression task, and empirically validate the calibrated predictive uncertainty and confidence interval coverage against a ground truth.\\\", using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to ground the task in established literature on SDE-based Bayesian inference. The findings are passed to the math agent for a rigorous derivation of the specific pre-conditioned SDE. Finally, the code agent implements the derived mathematical formula and runs a validation experiment to check its empirical performance.\", \"expected_artifacts\": [\"- A LaTeX document detailing the derivation of the pre-conditioned SDE and its discretization.\", \"- A Python script implementing the pSGLD sampler, the Bayesian neural network, and the validation experiment.\", \"- A JSON file containing the final metrics, including Negative Log-Likelihood and Prediction Interval Coverage Probability.\", \"- A PNG plot visualizing the model's predictive mean and uncertainty against the true function.\", \"- A bibliography file citing at least two relevant research papers.\"], \"thinking_outline\": [\"1. Formulate the general SDE for posterior sampling in a Bayesian Neural Network, starting from the overdamped Langevin equation.\", \"2. Introduce the Fisher Information Matrix as a pre-conditioner and derive the modified pSGLD update equation in its continuous-time form.\", \"3. Discretize the continuous-time SDE using the Euler-Maruyama method, carefully handling the drift, diffusion, and pre-conditioning terms.\", \"4. Implement the discretized sampler in Python to train a BNN on a synthetic regression problem.\", \"5. Verify that the implemented update rule in the code correctly matches the discretized mathematical derivation, paying close attention to matrix-vector products and noise scaling.\", \"6. Validate the model's performance by checking if the predictive uncertainty (e.g., 95% confidence interval) correctly covers the true data-generating function with the expected frequency.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent provides the final, discretized equation to the code agent. The code agent provides the implementation, plots, and metrics for final review. Verification: The router will audit the code to ensure the update rule is a one-to-one match with the math derivation's final equation. Fallback: If the implementation fails to converge, the router will request the math agent to re-evaluate the stability conditions of the discretization or ask the search agent to find alternative pre-conditioners.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on pSGLD with Fisher pre-conditioning.\", \"- [ ] /math: Derive the continuous-time SDE for the pSGLD sampler.\", \"- [ ] /math: Provide the Euler-Maruyama discretized version of the SDE, ready for implementation.\", \"- [ ] /code: Implement the Bayesian neural network architecture and the pSGLD sampler in PyTorch.\", \"- [ ] /code: Execute the training and evaluation on the synthetic regression task, generating all artifacts.\", \"- [ ] /math: Verify that the numerical implementation in the Python script correctly reflects the discretized SDE derivation.\", \"- [ ] /code: Validate the predictive uncertainty calibration by computing the PICP metric.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-deep-learning\", \"stochastic-differential-equations\", \"mcmc\"], \"acceptance_criteria\": [\"- The mathematical derivation of the pre-conditioned SDE and its discretization is self-contained, rigorous, and correct.\", \"- The Python script runs without errors and correctly implements the Euler-Maruyama discretization of the derived SDE.\", \"- The final model achieves a test NLL below 0.1 and a PICP between 0.92 and 0.98 for a 95% confidence interval.\", \"- All expected artifacts are generated and meet their descriptions.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 280, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of Lagrangian-based methods for solving Constrained Markov Decision Processes (CMDPs) applied to a robotic manipulation task. Please formulate the primal and dual problems, derive the policy gradient update rules with Lagrange multipliers for safety constraints, and analyze the theoretical convergence guarantees. The goal is to implement a prototype that minimizes constraint violations while maximizing task reward, ensuring the policy respects safety bounds related to actuator torque limits.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Lagrangian methods for Constrained MDP robotics control theory convergence\\\", mode=both)\", \"/math(Derive the primal-dual update for the CMDP Lagrangian, analyzing convergence guarantees and the Hessian eigenvalue spectrum for stability in this control theory problem.)\", \"/code(Implement a Lagrangian PPO agent in JAX with CUDA acceleration for a robotics task, ensuring the implementation respects the derived convergence criteria and safety constraints., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on Lagrangian methods for safe RL. This informs the /math agent's formal derivation of the optimization problem and update rules. Finally, the /code agent implements the derived algorithm, grounding the theoretical work in a practical robotics simulation.\", \"expected_artifacts\": [\"* A formal mathematical derivation of the Lagrangian dual formulation and the policy/multiplier update rules.\", \"* A summary of at least two relevant research papers from arXiv, including their IDs.\", \"* Python source code for the safe RL agent and the simulation environment.\", \"* Plots visualizing the training process, showing cumulative reward vs. cumulative constraint violations.\", \"* A JSON file containing the final evaluation metrics.\"], \"thinking_outline\": [\"1. Research existing literature on Lagrangian relaxation and primal-dual methods for Constrained Markov Decision Processes (CMDPs), focusing on applications in robotics.\", \"2. Formulate the CMDP objective function and constraints for a representative robotic manipulation task. Derive the corresponding Lagrangian and the Karush-Kuhn-Tucker (KKT) conditions for optimality.\", \"3. Derive the policy gradient update rule for the primal objective and the gradient ascent rule for the dual variables (Lagrange multipliers).\", \"4. Implement the derived algorithm (e.g., Lagrangian-PPO) in a suitable framework like JAX or PyTorch and a robotics simulator.\", \"5. Validate the implementation by checking if the agent's behavior adheres to the safety constraints during training and evaluation, comparing empirical violation rates against the theoretical cost limit.\", \"6. Verify that the mathematical derivation for convergence is correctly reflected in the algorithmic implementation's hyperparameters and update logic.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The final code's performance metrics, particularly constraint violations, must align with the theoretical safety guarantees derived by the /math agent. The implementation details must match the derived update rules. fallback: If the implementation fails to satisfy constraints, revisit the /math derivation to check for flawed assumptions or reformulate the Lagrangian.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 seminal papers on Lagrangian methods for CMDPs from arXiv.\", \"- [ ] /math: Formulate the constrained optimization problem and derive the full primal-dual update equations.\", \"- [ ] /math: Analyze the convergence conditions for the derived algorithm.\", \"- [ ] /code: Implement the algorithm and a robotics simulation environment (e.g., in Brax/MuJoCo).\", \"- [ ] /code: Train the agent and generate plots for reward and constraint violation trajectories.\", \"- [ ] /math: Verify that the implemented update rules in the code are an exact match to the derived equations.\", \"- [ ] /code: Validate that the empirical constraint violation rate is below the specified safety threshold in the final policy.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"safe-reinforcement-learning\", \"constrained-optimization\", \"robotics\"], \"acceptance_criteria\": [\"* The mathematical derivation of the update rules must be complete and self-contained.\", \"* The implemented agent must achieve a positive cumulative reward while maintaining an average constraint violation rate below a predefined threshold (e.g., 0.05).\", \"* The final report must cite at least two relevant arXiv papers with their IDs.\", \"* The code must be runnable and include instructions to reproduce the reported metrics.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 282, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for a Bayesian Neural Network with hierarchical Gaussian priors using mean-field variational inference. Then, implement Stochastic Variational Inference (SVI) in Python to optimize the ELBO, employing the local reparameterization trick to reduce gradient variance. Validate the implementation by comparing its convergence rate and final predictive log-likelihood against a standard reparameterization approach on a synthetic regression dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'local reparameterization trick' 'Bayesian neural network' SVI variance reduction\\\", mode=rag)\", \"/math(Derive the Evidence Lower Bound (ELBO) for a Bayesian neural network with hierarchical priors, detailing the local reparameterization trick and its impact on the Monte Carlo gradient estimator's variance, checking for convergence conditions.)\", \"/code(Implement the SVI algorithm in JAX for the Bayesian model, comparing local and standard reparameterization. Track ELBO convergence and model uncertainty via bootstrap confidence intervals on the predictions., using Python)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather seminal papers on the local reparameterization trick, providing theoretical grounding. This research informs the /math agent, which will perform the formal derivation of the model's objective function and gradient estimators. Finally, the /code agent implements the derived algorithm and runs empirical validation, directly testing the theoretical claims.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the hierarchical model, the ELBO, and the stochastic gradient estimator.\", \"A Python script (JAX) implementing the Bayesian Neural Network and SVI with both reparameterization methods.\", \"A convergence plot comparing the ELBO trace for local vs. standard reparameterization over training iterations.\", \"A summary report (JSON or Markdown) containing final performance metrics (ELBO, predictive log-likelihood) and citations for at least two relevant papers.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of the local reparameterization trick and its specific application to Bayesian Neural Networks to reduce gradient variance in Stochastic Variational Inference.\", \"2. Formally define the probabilistic model, including the likelihood, the hierarchical prior structure over the weights, and the mean-field variational posterior distribution.\", \"3. Derive the Evidence Lower Bound (ELBO) for this model and then derive the unbiased stochastic gradient estimator using the local reparameterization trick.\", \"4. Implement the complete SVI optimization loop in Python using JAX, creating two versions of the gradient step: one with standard reparameterization and one with local reparameterization.\", \"5. Validate the implementation by training the model on a synthetic dataset, meticulously plotting and comparing the ELBO convergence and the variance of the gradient estimates between the two methods.\", \"6. Verify that the derived gradients in the math step are consistent with the implicit gradients computed by the automatic differentiation engine in the code by checking numerical outputs on a small, fixed batch of data.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The papers from /general-search provide the theoretical basis for the /math agent's derivation. The resulting mathematical formulas from /math are the direct specification for the /code agent's implementation. Verification: The code's ELBO calculation for a single data point and fixed random seed must numerically match a manual calculation from the math derivation. Fallback: If the ELBO diverges during training, the /code agent must halt and request a review of the /math derivation for potential errors in the gradient estimator.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on local reparameterization tricks for BNNs.\", \"- [ ] /math: Derive the complete ELBO for the specified hierarchical Bayesian model.\", \"- [ ] /math: Derive the stochastic gradient estimator using the local reparameterization trick.\", \"- [ ] /code: Implement the BNN, SVI optimizer, and both reparameterization strategies in JAX.\", \"- [ ] /code: Generate a synthetic regression dataset and run the comparative experiment.\", \"- [ ] /math: Verify that the analytical derivation of the ELBO's gradient aligns with the logic of the autograd implementation from the /code agent.\", \"- [ ] /code: Validate that the empirical results (faster convergence for local reparameterization) align with the claims in the researched literature.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"probabilistic-programming\"], \"acceptance_criteria\": [\"The mathematical derivation of the ELBO and its gradient is complete, mathematically sound, and correctly incorporates the hierarchical prior structure.\", \"The Python implementation correctly applies SVI and demonstrates a clear, replicable difference in convergence behavior between the local and standard reparameterization tricks.\", \"The final report includes a plot that quantitatively shows the superior convergence rate (steeper ELBO curve) and lower variance of the local reparameterization method.\", \"The work cites at least two relevant academic papers by their arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 285, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a GPU-accelerated solver for a large-scale, sparse, symmetric positive-definite linear system Ax=b arising from a finite element discretization. Please provide a full analysis, from theory to implementation, of an Incomplete Cholesky preconditioned conjugate gradient (IC-PCG) method. Compare its performance and convergence rate against a simpler Jacobi preconditioner using Python with CUDA acceleration, focusing on efficient sparse matrix operations and memory bandwidth considerations.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org incomplete cholesky CUDA sparse triangular solve parallel conjugate gradient convergence\\\", mode=both)\", \"/math(Derive the IC(0) preconditioned conjugate gradient algorithm. Analyze its convergence properties by bounding the condition number and its effect on the matrix eigenvalue distribution, contrasting it with the Jacobi preconditioner.)\", \"/code(Implement the IC-PCG and Jacobi-PCG solvers in Python using CuPy for CUDA operations on sparse matrices in CSR format. Profile the convergence rate and wall-clock time to a residual tolerance of 1e-8.)\"], \"route_rationale\": \"The route follows a theory-validation-justification pattern. The 'blue' /general-search agent first gathers state-of-the-art literature on GPU-specific algorithms for sparse triangular solves, which are critical for the IC preconditioner. /math then provides a rigorous theoretical foundation and derives the specific algorithm. Finally, /code implements and empirically validates the derived theory, comparing it against a baseline to quantify the performance gains.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the IC-PCG algorithm and its convergence analysis.\", \"- A Python script implementing the solver using CuPy, including comparison logic.\", \"- A performance report with plots comparing the convergence history (residual norm vs. iteration) and total solution time for both preconditioners.\", \"- A bibliography with at least two cited arXiv papers on GPU-accelerated numerical methods.\"], \"thinking_outline\": [\"1. Research efficient parallel algorithms for sparse matrix-vector multiplication (SpMV) and sparse triangular solves on GPUs, as these are the core kernels for PCG and the Incomplete Cholesky preconditioner.\", \"2. Formally derive the Incomplete Cholesky factorization (IC(0)) and the subsequent PCG algorithm. Prove that the preconditioning step improves the spectral properties (condition number) of the system matrix.\", \"3. Implement the IC-PCG and a baseline Jacobi-PCG solver in Python using a GPU-acceleration library like CuPy, ensuring correct handling of sparse matrix formats (e.g., CSR).\", \"4. Validate the implementation by testing it on a known SPD matrix and checking that the residual norm decreases monotonically at a rate consistent with theory.\", \"5. Profile and compare the two methods on a large, representative sparse matrix, measuring wall-clock time to convergence, final residual norm, and iteration count.\", \"6. Synthesize the findings into a report that connects the theoretical convergence analysis from step 2 with the empirical results from step 5, referencing the initial research from step 1.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides relevant papers and parallel algorithmic patterns to the math agent. The math agent produces a formal algorithm specification and theoretical convergence guarantees for the code agent. The code agent implements this spec and generates empirical results (plots, timings). Router QA performs the final review. Verification: code output must match theoretical convergence predictions. Fallback: If the IC factorization proves numerically unstable for the target matrix, the code agent should report the failure and proceed with only the Jacobi preconditioner results.\", \"todo_list\": [\"- [ ] /general-search: Find >=2 papers on parallel sparse triangular solves and IC-PCG for CUDA.\", \"- [ ] /math: Derive the IC(0)-PCG algorithm, including the update steps for the residual, direction, and solution vectors.\", \"- [ ] /math: Analyze the condition number of the preconditioned matrix M\u207b\u00b9A and relate it to the convergence rate.\", \"- [ ] /code: Implement the solvers using CuPy, targeting the CSR sparse matrix format.\", \"- [ ] /code: Generate a test SPD matrix (e.g., from a 2D Poisson problem) to benchmark the solvers.\", \"- [ ] /math: Verify that the implemented code's residual reduction rate aligns with the theoretically expected convergence behavior.\", \"- [ ] /code: Validate that the final solution vector x satisfies Ax=b within the specified tolerance.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"gpu-acceleration\", \"numerical-methods\", \"sparse-solvers\"], \"acceptance_criteria\": [\"- The implemented IC-PCG solver must converge to a residual norm of less than 1e-8.\", \"- The final report must show, via plots, that the IC-PCG solver converges in significantly fewer iterations than the Jacobi-PCG solver.\", \"- The mathematical derivation must correctly prove that the IC preconditioner improves the matrix condition number.\", \"- The code must be runnable and include comments explaining the CUDA-accelerated sparse operations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 286, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive and implement a linearized Alternating Direction Method of Multipliers (ADMM) variant for solving a group LASSO problem with non-convex penalty functions, specifically for a large-scale bioinformatics logistic regression model. Please analyze its convergence rate properties compared to standard proximal gradient descent and provide a proof of convergence under the relaxed assumptions found in recent literature.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org linearized ADMM non-convex group lasso convergence rate analysis bioinformatics\\\", mode=rag)\", \"/math(Derive the primal-dual update rules for linearized ADMM applied to the bioinformatics group LASSO objective. Prove convergence guarantees, referencing Chebyshev inequality and conditions found in recent arXiv literature.)\", \"/code(Implement the derived ADMM algorithm in Python using JAX. Validate against a baseline proximal gradient solver, tracking primal residual convergence and model sparsity metrics for the bioinformatics dataset.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find cutting-edge research on non-convex linearized ADMM convergence, as requested. This literature is then passed to /math to construct a formal derivation and proof. Finally, /code implements the derived algorithm for empirical validation and comparison, creating a complete theoretical and practical analysis.\", \"expected_artifacts\": [\"- A formal mathematical derivation and proof of convergence in a LaTeX document.\", \"- A Python script implementing the linearized ADMM variant and a baseline solver.\", \"- A summary report including plots of convergence metrics (primal residual, objective value) vs. iteration.\", \"- A bibliography file citing at least two relevant arXiv papers with their IDs.\", \"- A JSON file containing the final computed metrics for both algorithms.\"], \"thinking_outline\": [\"1. Conduct a literature search to identify recent theoretical analyses of linearized ADMM for non-convex, constrained optimization problems, focusing on applications like group LASSO.\", \"2. Formulate the group LASSO problem within the primal-dual framework, defining the objective function, constraints, and the augmented Lagrangian.\", \"3. Derive the iterative update steps for the primal variables and the dual variable based on the linearized ADMM scheme found in the literature.\", \"4. Verify the derived update equations by checking against the source papers and ensuring the conditions for the convergence proof (e.g., Lipschitz continuity, sufficient decrease) are correctly stated.\", \"5. Implement the algorithm and a baseline (e.g., proximal gradient descent), then execute on a synthetic bioinformatics dataset to compare performance.\", \"6. Validate the implementation by auditing the code against the mathematical derivation and checking if the empirical convergence behavior aligns with the theoretical bounds.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent will verify that the assumptions in the papers found by the search agent are applicable to the problem statement. The code agent will add assertions to check for convergence criteria. fallback: If the convergence proof is intractable, the math agent will flag it and the search agent will be re-triggered with a broader query for alternative proof techniques.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize >=2 recent arXiv papers on linearized ADMM for non-convex problems.\", \"- [ ] /math: Formulate the augmented Lagrangian for the group LASSO problem.\", \"- [ ] /math: Derive the complete update rules for the linearized ADMM algorithm.\", \"- [ ] /math: Construct the formal proof of convergence based on the literature.\", \"- [ ] /code: Implement the linearized ADMM algorithm and a proximal gradient baseline in Python.\", \"- [ ] /code: Generate synthetic data and run experiments to compare convergence.\", \"- [ ] /math: Verify that the implemented update rules in the Python code exactly match the mathematical derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"convex-optimization\", \"admm\", \"bioinformatics\"], \"acceptance_criteria\": [\"- The provided proof of convergence is mathematically sound and correctly applies theorems from the cited literature.\", \"- The Python implementation of linearized ADMM is correct and demonstrably converges on the test problem.\", \"- The final report shows that the linearized ADMM variant converges faster (in terms of wall-clock time or iterations) than the proximal gradient baseline, as measured by the primary metric.\", \"- All expected artifacts are delivered and meet the specified formats.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the L2 norm of the primal residual `r_k = Ax_k + Bz_k - c` at each iteration `k`. Convergence is achieved when `||r_k||_2 < 1e-6`.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the wall-clock time required for the primary metric to fall below the convergence threshold.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 287, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the computational complexity of determining the winning strategy for a partisan game on a directed acyclic graph (DAG) with asymmetric movesets. Please provide a formal proof of its PSPACE-completeness by reduction, devise an efficient dynamic programming algorithm for a tractable subclass like bounded treewidth graphs, and survey recent literature on approximation algorithms for generalized geography games, particularly those that might apply Bayesian optimization.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:mit.edu 'partisan games' pspace-completeness 'generalized geography' 'bounded treewidth' bayesian game theory\\\", mode=both)\", \"/math(Construct the PSPACE-completeness proof by reduction from Quantified Boolean Formulas (QBF). Derive the dynamic programming recurrence for game states on DAGs with bounded treewidth, analyzing its time complexity., includes cybersecurity)\", \"/code(Implement the dynamic programming algorithm in Python using networkx for graph manipulation. Include runtime validation checks for DAG properties and treewidth constraints, and benchmark against a small, curated game state database.)\"], \"route_rationale\": \"The plan starts with the 'blue' /general-search agent to gather foundational proofs and recent approximation techniques, which is critical for this literature-dependent problem. /math is then used for the core theoretical work: formal proof construction and algorithm derivation. Finally, /code implements and validates the derived algorithm, providing an empirical check on the theoretical findings.\", \"expected_artifacts\": [\"A LaTeX document containing the formal PSPACE-completeness proof.\", \"A Python script implementing the dynamic programming solution for bounded treewidth graphs.\", \"A summary report (PDF) of the literature review, including citations and a discussion on approximation algorithms.\", \"A verification log detailing the results of runtime checks and benchmarks for the implemented code.\"], \"thinking_outline\": [\"1. Conduct a literature review to find canonical PSPACE-completeness proofs for impartial games like Generalized Geography and survey modern approximation techniques for partisan games.\", \"2. Formulate a polynomial-time reduction from a known PSPACE-complete problem, such as Quantified Boolean Formulas (QBF), to the problem of determining the winner in the specified partisan game.\", \"3. Derive a dynamic programming recurrence relation to solve the game on the tractable subclass of graphs with bounded treewidth, and formally analyze its time and space complexity.\", \"4. Implement the algorithm and validate its correctness by testing on a set of small, hand-solved graph instances and comparing outcomes.\", \"5. Audit the formal proof for logical soundness and ensure the complexity analysis of the implemented algorithm matches the theoretical derivation.\", \"6. Synthesize all findings\u2014the proof, algorithm, complexity analysis, and literature review\u2014into a comprehensive report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results inform the mathematical proof and algorithm design. The mathematical derivation is then implemented in code. (Verification: The proof from /math must be logically sound and the implementation from /code must correctly reflect the derived recurrence. Fallback: If the proof is invalid, return to /math with a counterexample. If the code fails validation tests, return to /code for debugging.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on partisan game complexity and recent (post-2018) approximation algorithms.\", \"- [ ] /math: Construct the formal PSPACE-completeness proof by reduction from QBF.\", \"- [ ] /math: Derive the dynamic programming recurrence and analyze its complexity for bounded treewidth graphs.\", \"- [ ] /code: Implement the DP algorithm in Python with networkx.\", \"- [ ] /code: Add unit tests and property-based tests to validate the implementation against known outcomes.\", \"- [ ] /math: Verify the code's complexity matches the theoretical analysis from the math derivation.\", \"- [ ] /code: Verify the implementation passes all validation tests on a benchmark suite of DAGs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"graph-algorithms\"], \"acceptance_criteria\": [\"- The PSPACE-completeness proof must be logically sound and the reduction clearly explained.\", \"- The implemented algorithm must solve for winning states correctly on a provided test suite of 10+ graphs with bounded treewidth.\", \"- The literature review must cite at least two relevant arXiv papers from the last 5 years.\", \"- The reported time complexity of the algorithm must match its empirical performance scaling.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 290, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the computational complexity of determining the winner in a generalized impartial game defined on a dynamically generated directed acyclic graph. Specifically, I want a formal analysis of why finding the P-positions is PSPACE-complete for arbitrary rule sets, referencing the Sprague-Grundy theorem. Further, implement an efficient algorithm in Python to compute Grundy numbers for a tractable subclass of these games where the out-degree of any node is bounded by a small constant k, and analyze its convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:.edu combinatorial game theory PSPACE-completeness Sprague-Grundy theorem reduction\\\", mode=both)\", \"/math(Formally prove the PSPACE-completeness of the generalized impartial game by reduction from Generalized Geography. Derive the recurrence relation for the Grundy numbers (nim-values) in the bounded out-degree subclass, analyzing the complexity and convergence properties.)\", \"/code(Implement a Python algorithm using dynamic programming and memoization to calculate Grundy numbers for the tractable game subclass. Include adversarial test generation to validate correctness and benchmark performance against theoretical complexity bounds, tracking confidence interval metrics.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational literature on the problem's complexity class. This theoretical basis is then handed to /math for formal proof construction and derivation of the specific algorithm's properties. Finally, /code implements and empirically validates the mathematical constructs, providing a complete theory-to-practice solution.\", \"expected_artifacts\": [\"A LaTeX document detailing the PSPACE-completeness proof via reduction.\", \"A commented Python script implementing the Grundy number calculation for the specified game subclass.\", \"A performance analysis report comparing empirical runtime against theoretical complexity.\", \"A bibliography with at least two cited arXiv preprints or peer-reviewed articles.\", \"A JSON file containing metrics from the validation test suite.\"], \"thinking_outline\": [\"1. Deconstruct the user query to identify the core theoretical concepts: impartial games, Sprague-Grundy theorem, Grundy numbers (nim-values), and PSPACE-completeness.\", \"2. Locate seminal papers and authoritative academic sources that establish the PSPACE-completeness of similar games, such as Generalized Geography, to inform the reduction proof.\", \"3. Formulate a precise mathematical model for the game, defining states, moves, and terminal conditions, and derive the recurrence for computing Grundy numbers based on the mex (minimum excluded value) rule.\", \"4. Construct the formal reduction from a known PSPACE-complete problem to the provided game, proving both correctness and polynomial-time computability of the reduction.\", \"5. Validate the implemented algorithm by designing test cases with known outcomes, including edge cases and larger graphs, to check correctness.\", \"6. Audit the empirical performance of the code against the derived O(|V|+|E|) time complexity for the bounded-degree subclass and document any deviations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal specification and proof, which the code agent implements. Verification: The code agent must validate that its implementation's complexity matches the mathematical derivation. Fallback: If the derivation is flawed or non-constructive, the math agent must refine its model based on feedback from the code agent.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on the complexity of impartial games and the Sprague-Grundy theorem.\", \"- [ ] /math: Write the formal proof of PSPACE-completeness using a reduction from Generalized Geography.\", \"- [ ] /math: Derive the recurrence relation for calculating Grundy numbers for the game subclass.\", \"- [ ] /code: Implement the Grundy number calculation algorithm in Python.\", \"- [ ] /code: Develop a test suite with at least 10 varied graph instances to validate the algorithm.\", \"- [ ] /math: Verify that the implemented algorithm's logic correctly reflects the derived recurrence relation.\", \"- [ ] /code: Verify that the empirical runtime complexity aligns with the theoretical O(|V|+|E|) bound.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"computational-complexity\", \"pspace-completeness\"], \"acceptance_criteria\": [\"- The PSPACE-completeness proof must be logically sound and the reduction correctly formulated.\", \"- The Python implementation must pass all test cases, correctly calculating Grundy numbers for various game graphs.\", \"- The performance analysis must empirically support the theoretical time complexity of the algorithm.\", \"- At least two relevant academic sources must be cited correctly.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 292, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a full implementation and theoretical analysis of Tucker decomposition for learning compact representations from a third-order tensor. The implementation should use Alternating Least Squares (ALS) with non-negativity constraints on the factor matrices. Please derive the update rules for the core tensor and factors using Lagrange multipliers, analyze the convergence properties, and provide a JAX implementation that is benchmarked on a synthetic dataset for reconstruction error and eigenvalue decay of the unfolded factor matrices.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'tucker decomposition' 'alternating least squares' 'non-negativity constraints' convergence analysis\\\", mode=web)\", \"/math(Derive the Alternating Least Squares update rules for Tucker decomposition with non-negativity constraints using Karush-Kuhn-Tucker (KKT) conditions. Analyze the convergence guarantees and the impact of the core tensor's eigenvalue distribution on representation quality.)\", \"/code(Implement the derived ALS algorithm for constrained Tucker decomposition in JAX. The implementation must include functions for generating synthetic tensor data, computing reconstruction error based on the Frobenius norm, and validating the non-negativity constraints via runtime assertions, tracking convergence., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey the literature for established algorithms and convergence proofs related to constrained Tucker decomposition. This research informs the /math agent, which will formally derive the specific update rules required by the prompt using KKT conditions. Finally, the /code agent will translate the verified mathematical formulation into a high-performance JAX implementation for empirical validation and benchmarking.\", \"expected_artifacts\": [\"- A PDF report containing the full mathematical derivation of the constrained ALS update rules.\", \"- A Python script (`tucker_als.py`) with the JAX implementation of the algorithm and benchmarking suite.\", \"- A JSON file (`metrics.json`) reporting the final reconstruction error and iterations to convergence.\", \"- A bibliography file (`references.bib`) with at least two cited arXiv papers.\", \"- A verification summary (`verification.md`) confirming the implemented gradients match the derived equations.\"], \"thinking_outline\": [\"1. Conduct a literature search to identify standard formulations and convergence analyses for non-negative Tucker decomposition via ALS.\", \"2. Formulate the Tucker decomposition as a constrained optimization problem, explicitly defining the objective function (reconstruction error) and the non-negativity constraints.\", \"3. Derive the multiplicative update rules for the factor matrices and the core tensor by applying the Karush-Kuhn-Tucker (KKT) conditions to the optimization problem.\", \"4. Implement the derived ALS algorithm in JAX, paying close attention to numerical stability and computational efficiency using tensor operations.\", \"5. Verify the correctness of the implementation by comparing the numerical gradients computed by JAX's autodiff with the analytically derived update rules on a small-scale problem.\", \"6. Validate the algorithm's performance by running it on synthetic data, checking for convergence, and measuring the final reconstruction error against theoretical expectations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to the /math agent. The /math agent provides the complete, verified derivations to the /code agent. The /code agent produces the implementation and benchmark results for final review. Verification: The router will check that the JAX implementation's gradients align with the math agent's derivations. Fallback: If the code fails to converge, the task is routed back to the /math agent to re-verify the convergence proof and derivation correctness.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers and review articles on constrained Tucker decomposition and the convergence of ALS algorithms.\", \"- [ ] /math: Formulate the optimization problem and derive the KKT-based multiplicative update rules for all factor matrices and the core tensor.\", \"- [ ] /math: Analyze the conditions under which the ALS algorithm is guaranteed to converge to a stationary point.\", \"- [ ] /code: Implement the Tucker decomposition with constrained ALS in JAX, including a function to generate synthetic low-rank tensors.\", \"- [ ] /code: Develop functions to compute the reconstruction error and to plot the error as a function of iteration.\", \"- [ ] /math: Verify that the gradients implemented in the JAX code (via `jax.grad`) numerically match the analytically derived update rules.\", \"- [ ] /code: Execute the benchmark on synthetic data and save the final metrics and convergence plot.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"numerical-linear-algebra\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ALS update rules using KKT conditions for non-negativity is complete and mathematically sound.\", \"- The JAX implementation correctly executes the derived updates, passes unit tests for tensor shapes and constraints, and demonstrates monotonic decrease in reconstruction error on synthetic data.\", \"- The final reconstruction error reported in `metrics.json` is below 1e-5 for a synthetic tensor with a known low-rank structure.\", \"- The report cites at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 293, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nGiven a Structural Causal Model (SCM) represented by the directed acyclic graph Z -> X -> Y and Z -> Y, where Z is an unobserved confounder, can you formally derive the expression for the interventional distribution P(Y|do(X=x)) using Pearl's do-calculus? Then, write a Python simulation to validate that your derived formula, when applied to observational data, correctly recovers the true interventional distribution obtained by simulating a direct intervention on the SCM.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:causality.cs.ucla.edu Pearl do-calculus identifiability rules front-door criterion causal econometrics\\\", mode=both)\", \"/math(Derive the interventional distribution P(y|do(x)) for the specified Bayesian network with confounding, applying Pearl's three rules of do-calculus to achieve causal identifiability via the front-door adjustment formula.)\", \"/code(Implement the Structural Causal Model in Python using a Bayesian framework, simulate both observational and interventional data, and numerically verify the causal formula by reporting Kullback-Leibler divergence.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on do-calculus and identifiability criteria, which is crucial for the theoretical derivation. This information is passed to /math for the formal derivation. Finally, /code is used to create a simulation that provides empirical validation for the theoretical result from /math, ensuring correctness.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the interventional distribution using LaTeX.\", \"- A Python script implementing the SCM, the intervention, and the derived adjustment formula.\", \"- A summary report (.md) explaining the results, including plots comparing the estimated and true interventional distributions.\", \"- A JSON file containing the calculated metrics (e.g., KL divergence).\", \"- A list of at least two relevant citations with arXiv IDs or DOIs.\"], \"thinking_outline\": [\"1. Review the foundational rules of do-calculus and the conditions for causal identifiability, specifically the front-door and back-door criteria.\", \"2. Formally apply the rules of do-calculus step-by-step to the given causal graph to derive an expression for P(Y|do(X=x)) that only contains observational probabilities.\", \"3. Construct a specific SCM with linear-Gaussian or discrete probability functions that conforms to the given graph structure.\", \"4. Implement a Python simulation to generate a large observational dataset from the SCM and a ground-truth interventional dataset by surgically modifying the SCM.\", \"5. Validate the derived formula by applying it to the observational dataset and comparing the resulting distribution to the ground-truth interventional distribution using a statistical metric like KL divergence.\", \"6. Audit the code and the derivation to ensure the assumptions of the do-calculus are met and the implementation correctly reflects the mathematical formula.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The derived formula from /math must be correctly implemented in /code, confirmed by near-zero KL divergence between estimated and true interventional distributions; fallback: If divergence is high, /math re-evaluates the derivation for potential misapplication of do-calculus rules).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Pearl's do-calculus and the front-door criterion for causal identifiability.\", \"- [ ] /math: Derive P(Y|do(X=x)) for the Z -> {X,Y}, X -> Y graph using the rules of do-calculus.\", \"- [ ] /code: Implement the SCM with specified relationships (e.g., linear-Gaussian).\", \"- [ ] /code: Generate both observational and interventional datasets from the simulation.\", \"- [ ] /math: Verify that the final derived expression is identifiable (i.e., contains no do-operators).\", \"- [ ] /code: Implement the derived formula and compute the estimated interventional distribution from the observational data, then calculate KL divergence against the ground truth.\", \"- [ ] router: Perform final quality assurance, checking that the derivation, code, and final report are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"structural-causal-models\", \"bayesian-networks\"], \"acceptance_criteria\": [\"- The mathematical derivation must correctly apply the rules of do-calculus to arrive at the standard front-door adjustment formula.\", \"- The Python simulation must demonstrate that the KL divergence between the derived formula's estimate and the true interventional distribution is less than 0.01.\", \"- The final report must include at least two citations to authoritative papers or books on causal inference (e.g., by Pearl, Rubin).\", \"- The generated plots must clearly visualize the observational, true interventional, and estimated interventional distributions, showing a close match between the latter two.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 294, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for the Method of Optimal Directions (MOD) for dictionary learning, framing it as an alternating minimization problem. Then, implement MOD in Python using ADMM for the sparse coding step (Lasso). Validate the algorithm's convergence on synthetic data and compare its performance (reconstruction error vs. sparsity) against a fixed DCT basis for compressing biomedical signals, ensuring analysis of the learned dictionary's eigenvalue distribution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org OR site:ieeexplore.ieee.org 'Method of Optimal Directions' MOD 'dictionary learning' ADMM convergence lasso\\\", mode=rag)\", \"/math(Derive the dictionary update and sparse coding steps for MOD, proving convergence conditions for the ADMM subproblem by analyzing the Lagrangian and its eigenvalue spectrum.)\", \"/code(Implement MOD in Python/NumPy, using a custom ADMM solver for the Lasso step. Profile convergence and measure reconstruction error and dictionary mutual coherence against a fixed Fourier basis.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on MOD and ADMM for Lasso. This theoretical grounding is passed to /math to produce a formal derivation of the algorithm's update rules and convergence properties. Finally, /code implements the derived algorithm, runs the specified experiments, and generates quantitative results for validation.\", \"expected_artifacts\": [\"A PDF report containing the complete mathematical derivation of the MOD update rules and the ADMM solver for the sparse coding stage.\", \"A Python script (`mod_vs_dct.py`) that implements the MOD algorithm and the comparison experiment against the DCT basis.\", \"A JSON file (`results.json`) containing the final metrics, including MSE, PSNR, and average sparsity for both methods.\", \"A set of PNG images visualizing the learned dictionary atoms and plotting reconstruction error versus sparsity.\", \"A bibliography file with at least two citations to relevant academic papers with arXiv IDs or DOIs.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of dictionary learning, specifically the Method of Optimal Directions (MOD), and the Alternating Direction Method of Multipliers (ADMM) for solving the \u21131-regularized least squares (Lasso) problem.\", \"2. Formally derive the alternating minimization update rules for both the dictionary atoms and the sparse codes within the MOD framework, including the detailed steps for the ADMM solver.\", \"3. Implement the derived MOD algorithm in Python, including the ADMM solver, and generate synthetic signals for initial testing and verification.\", \"4. Verify the implementation's correctness by checking the ADMM solver's convergence criteria against the mathematical derivation and comparing its output to a trusted library's Lasso solver (e.g., scikit-learn) on a small, controlled problem.\", \"5. Execute the full comparison experiment on simulated biomedical signals, contrasting the learned dictionary against a fixed DCT basis. Analyze reconstruction error, PSNR, and sparsity.\", \"6. Validate the final results by confirming that the learned dictionary provides a statistically significant improvement in reconstruction error over the DCT basis at equivalent sparsity levels, as predicted by sparse representation theory.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal derivation) -> /code (implementation & validation) -> router QA (verification: check consistency between derivation, code, and results; fallback: if code fails validation against trusted library, return to /math to re-check derivation of ADMM updates).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on MOD and ADMM for Lasso, focusing on convergence analysis.\", \"- [ ] /math: Derive the MOD update rules and the full ADMM formulation for the sparse coding subproblem.\", \"- [ ] /code: Implement the MOD algorithm and the experimental comparison with a DCT basis on synthetic biomedical data.\", \"- [ ] /code: Generate plots for reconstruction error vs. sparsity and visualize the learned dictionary atoms.\", \"- [ ] /math: Verify that the implemented ADMM convergence checks (e.g., primal and dual residual norms) align with the derived theoretical conditions.\", \"- [ ] /code: Validate the sparse codes generated by the custom ADMM solver against the output of `sklearn.linear_model.Lasso` on a shared test dataset.\", \"- [ ] /general-search: Collate citations and references into a bibliography for the final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\", \"optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation for MOD and the ADMM solver is complete, self-contained, and mathematically sound.\", \"- The Python implementation successfully runs, demonstrates convergence on a test problem, and generates all specified artifacts (plots, metrics JSON).\", \"- The final report shows that the learned dictionary achieves at least a 2 dB PSNR improvement over the DCT basis at the target sparsity level.\", \"- All claims are supported by at least 2 citations to peer-reviewed papers or arXiv preprints, with identifiers included.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"Calculate the Frobenius norm squared of the residual matrix and normalize by the number of elements: ||X - D*Alpha||_F^2 / (num_signals * signal_dimension).\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"Calculate as 20 * log10(MAX_SIGNAL_VALUE) - 10 * log10(MSE), assuming signals are normalized to a max value of 1.0.\"}}", "categories": ["advanced", "metrics_computation"]}
{"index": 295, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time SDE for a variance-preserving diffusion process, starting from the corresponding Fokker-Planck equation. Then, implement a score-based generative model in PyTorch using this reverse SDE sampler, and empirically validate its convergence by tracking the evolution of the FID score on a synthetic dataset, ensuring the implementation correctly parameterizes the score network to approximate the gradient of the log-density.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse SDE Fokker-Planck equation\\\", mode=both)\", \"/math(Derive the reverse-time SDE from the Fokker-Planck equation governing the diffusion process, analyzing its convergence properties and ensuring the drift term correctly incorporates the score function.)\", \"/code(Implement a PyTorch-based SDE sampler for the reverse diffusion process using an Euler-Maruyama discretization, tracking the FID metric and CUDA utilization to monitor convergence and performance., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational papers on score-based models and SDEs from arXiv. This theoretical grounding is then passed to /math to perform the formal derivation of the reverse-time SDE. Finally, /code implements the derived mathematical object and runs experiments to validate the theory empirically by tracking convergence metrics.\", \"expected_artifacts\": [\"A formal derivation of the reverse-time SDE in LaTeX/PDF format.\", \"A documented Python script implementing the score network and the SDE sampler.\", \"A summary report (report.md) including plots of the FID score versus training epochs.\", \"A JSON file (metrics.json) containing the final FID score and other relevant metrics.\", \"A bibliography file with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Establish the theoretical link between the forward diffusion process, its corresponding Fokker-Planck partial differential equation, and the general form of a reverse-time stochastic differential equation.\", \"2. Formally derive the specific reverse-time SDE, explicitly demonstrating how the score function, \u2207_x log p_t(x), arises as the corrective drift term.\", \"3. Verify the derived SDE by checking its consistency with the time-reversal properties of stochastic processes as detailed in the sourced literature.\", \"4. Implement the score network (e.g., a U-Net) and the Euler-Maruyama solver for the reverse SDE in PyTorch.\", \"5. Train the model on a synthetic 2D dataset and validate the implementation by observing a monotonically decreasing Fr\u00e9chet Inception Distance (FID) score over training epochs.\", \"6. Audit the final code against the mathematical derivation to ensure all terms, particularly the drift and diffusion coefficients, are correctly implemented.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to the /math agent. The /math agent produces a formal derivation, which is handed off to the /code agent for implementation. The /code agent's results (code, plots, metrics) are passed to the router for final quality assurance. Verification: /math derivation is cross-referenced with sources; /code implementation is checked against the /math derivation. Fallback: If the derivation is flawed, the /math agent re-derives with guidance; if the implementation fails to converge, the /code agent debugs hyperparameters or the SDE solver.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on score-based generative modeling via SDEs, focusing on the connection to Fokker-Planck equations.\", \"- [ ] /math: Produce a step-by-step derivation of the reverse-time SDE from the Fokker-Planck equation.\", \"- [ ] /math: Verify the final derived SDE against established results in the papers found by /general-search.\", \"- [ ] /code: Implement the score network and the reverse SDE sampler in PyTorch.\", \"- [ ] /code: Train the model and generate plots showing FID score convergence over time.\", \"- [ ] /code: Verify the implementation of the SDE's drift and diffusion terms precisely matches the mathematical derivation.\", \"- [ ] /router: Perform final QA check on all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"score-based-modeling\", \"stochastic-differential-equations\", \"generative-ai\"], \"acceptance_criteria\": [\"The provided mathematical derivation of the reverse-time SDE must be complete and mathematically sound.\", \"The Python implementation must execute without errors and demonstrate a clear trend of decreasing FID score during training.\", \"The final report must cite at least two relevant arXiv papers with their IDs, as per the citation policy.\", \"The code's implementation of the SDE solver must directly correspond to the terms derived by the math agent.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 296, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nFor a generative modeling competition, derive the reverse-time SDE for a forward diffusion process governed by the Fokker-Planck equation and implement a basic solver to approximate the score function.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time SDE Fokker-Planck\\\", mode=both)\", \"/math(Derive the reverse-time SDE for a forward diffusion process, explicitly showing the connection between the score function and the probability flow ODE.)\", \"/code(Implement an Euler-Maruyama solver in JAX for the reverse SDE, calculating the empirical covariance of generated samples against a ground truth 2D Gaussian mixture., using Python)\"], \"route_rationale\": \"The plan starts with the 'blue' /general-search agent to gather authoritative papers on the theory. This theoretical foundation is then passed to the /math agent to produce a formal, step-by-step derivation of the core equation. Finally, the /code agent implements this mathematical specification and provides empirical validation.\", \"expected_artifacts\": [\"A LaTeX document (`derivation.tex`) containing the full mathematical derivation of the reverse-time SDE.\", \"A Python script (`solver.py`) implementing the Euler-Maruyama solver and a test harness.\", \"A PNG image (`sample_comparison.png`) visualizing the generated samples against the target distribution.\", \"A JSON file (`metrics.json`) reporting the quantitative evaluation metrics.\", \"A markdown report (`report.md`) summarizing the method, results, and including all required citations.\"], \"thinking_outline\": [\"1. Establish the mathematical formulation of the forward diffusion process as an Ornstein-Uhlenbeck stochastic differential equation (SDE).\", \"2. Derive the corresponding reverse-time SDE, demonstrating how the unknown drift term is related to the score of the data distribution, \u2207_x log p_t(x).\", \"3. Discretize the reverse-time SDE using the Euler-Maruyama scheme to create a practical sampling algorithm.\", \"4. Implement this algorithm to generate samples from a known target distribution (e.g., a 2D Gaussian mixture) and compare statistical moments.\", \"5. Validate that the empirical covariance of the generated samples converges to the true covariance as the number of discretization steps increases.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The code agent's output samples must statistically match the distribution implied by the math agent's derivation. The primary metric from the code must meet the acceptance criteria; fallback: If metrics fail, the code agent must debug the solver implementation. If derivation is suspect, the math agent must re-verify against sources from /general-search.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers (e.g., by Song, Ermon) on score-based generative modeling and reverse-time SDEs.\", \"- [ ] /math: Produce a step-by-step derivation of the reverse-time SDE from the forward process SDE and its associated Fokker-Planck equation.\", \"- [ ] /code: Implement the Euler-Maruyama solver for the derived SDE in JAX.\", \"- [ ] /code: Create a visualization comparing generated samples to a target 2D Gaussian mixture distribution.\", \"- [ ] /code: Validate outputs by computing the Frobenius norm difference between the empirical and true covariance matrices.\", \"- [ ] /router: Perform final QA check on the consistency of the derivation, implementation, and visualized results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"diffusion-models\", \"generative-modeling\", \"stochastic-calculus\"], \"acceptance_criteria\": [\"The mathematical derivation in the LaTeX document must be self-contained, correct, and clearly link the forward and reverse processes.\", \"The implemented solver must generate samples that visually match the target 2D Gaussian mixture distribution.\", \"The Frobenius norm of the difference between the empirical and true covariance matrices of the generated samples must be less than 0.1.\", \"The final report must cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute ||\u03a3_empirical - \u03a3_true||_F. A lower value indicates more accurate capture of the distribution's second-order moments.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate W_2(P_generated, P_true) between the empirical distribution of generated points and the true distribution. A lower value indicates better overall distributional similarity.\"}}", "categories": ["metrics_guidance"]}
{"index": 297, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the logical error rate for a distance-d surface code under a depolarizing noise model with physical error probability p. Model this using stabilizer measurements and a minimum-weight perfect matching decoder. Then, write a Python simulation using Stim and Pymatching to validate the analytical threshold theorem scaling, specifically investigating the relationship between p_logical and p for d=3, 5, and 7. Analyze the convergence behavior and cite foundational literature on fault-tolerant thresholds.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'surface code' 'threshold theorem' 'depolarizing noise' 'minimum weight perfect matching'\\\", mode=rag, context=\\\"Gather foundational papers to analyze the problem's computational complexity and find the key arxiv literature on surface code thresholds.\\\")\", \"/math(context=\\\"Derive the logical error rate p_L as a function of physical error p and distance d, using Bayesian inference principles to model stabilizer measurement outcomes and analyzing the convergence properties of the series expansion.\\\")\", \"/code(context=\\\"Implement a Monte Carlo simulation in Python with Stim and Pymatching, using bootstrap resampling for confidence intervals on the logical error rate. The implementation must be efficient enough to run on standard CPU or optional CUDA hardware.\\\")\"], \"route_rationale\": \"The route follows a standard theory-to-practice workflow. The blue /general-search agent first gathers foundational theory and established results. This informs the /math agent's formal derivation of the logical error rate. Finally, the /code agent implements a simulation to empirically validate the mathematical model and estimate the error threshold, providing a complete, verified analysis.\", \"expected_artifacts\": [\"- A Markdown or LaTeX document containing the full mathematical derivation of the logical error rate scaling.\", \"- A Python script (`simulation.py`) using Stim and Pymatching to simulate the surface code and its decoder.\", \"- A plot (`logical_error_rate.png`) showing logical error rate vs. physical error rate for code distances d=3, 5, and 7.\", \"- A `metrics.json` file containing the calculated logical error rates for each parameter set and the estimated error threshold.\", \"- A bibliography section in the report citing at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Review the stabilizer formalism for the planar surface code, defining the X and Z stabilizers and the logical operators.\", \"2. Derive the leading-order analytical expression for the logical error rate P_L as a function of the physical error rate p and code distance d, assuming a low-p regime.\", \"3. Formulate the decoding problem as finding a minimum-weight perfect matching on a graph constructed from the error syndrome.\", \"4. Implement a numerical simulation using the Stim library to generate circuits and error syndromes for the surface code under depolarizing noise, and Pymatching for decoding.\", \"5. Validate the simulation's output by plotting the results and checking that the scaling behavior for different distances qualitatively matches the analytical derivation and exhibits a clear error threshold.\", \"6. Verify that the numerically estimated error threshold is consistent with values reported in the literature found during the search phase.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers (e.g., arXiv preprints) to the math agent. The math agent produces a formal derivation, which the code agent uses as a specification for the simulation's model and expected behavior. The code agent's simulation results are then compared against the math agent's derivation for verification. Fallback: If the simulation results drastically diverge from the analytical model, the task reverts to the math agent to check for incorrect assumptions in the low-p expansion.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on surface code error thresholds under depolarizing noise.\", \"- [ ] /math: Derive the analytical scaling of the logical error rate P_L \u2248 c(p/p_th)^(d+1)/2.\", \"- [ ] /code: Set up a Stim circuit for a distance-d surface code with memory experiment logic.\", \"- [ ] /code: Implement the Monte Carlo loop to collect statistics on decoding failure rates for d=3,5,7 across a range of physical error rates p.\", \"- [ ] /code: Generate a plot of P_L vs. p on a log-log scale to visualize the threshold.\", \"- [ ] /math: Verify that the functional form of the simulated curves in the low-p regime aligns with the derived analytical expression.\", \"- [ ] /general-search: Audit the final report to ensure the estimated threshold from the simulation is compared against values from the sourced literature.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction\", \"fault-tolerance\"], \"acceptance_criteria\": [\"- The mathematical derivation for the logical error rate scaling must be provided and logically sound.\", \"- The Python simulation must be executable and generate a plot showing a clear intersection of the curves for d=3, 5, and 7, indicating an error threshold.\", \"- The estimated error threshold must be within a reasonable range (e.g., 15-16%) of established values for this model.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 300, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to design a reinforcement learning agent for a robotic manipulator that guarantees safety during operation. Specifically, I want to formulate this as a Constrained Markov Decision Process (CMDP) and solve it using a Lagrangian relaxation approach with primal-dual updates. The safety constraint should be enforced using a Lyapunov function to ensure the system state remains within a safe region. Can you derive the update rules and implement a prototype in Python to validate the approach, analyzing its convergence properties?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Lyapunov-based safe reinforcement learning primal-dual CMDP\\\", mode=rag)\", \"/math(Derive the Lagrangian for the CMDP objective with Lyapunov safety constraints, formulate the primal-dual update rules, and analyze the theoretical convergence guarantees based on control-theory principles and potential Hessian eigenvalue analysis.)\", \"/code(Implement the derived primal-dual algorithm in Python with JAX/PyTorch for a simulated robotics environment, incorporating Bayesian uncertainty estimates in the safety critic and tracking constraint violation metrics to validate convergence.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on Lyapunov-based safe RL, which is essential for grounding the theoretical work. This is handed to /math to derive the specific algorithm's update rules and theoretical guarantees. Finally, /code implements and empirically validates the derived mathematics, providing a concrete demonstration of the method's efficacy.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Lagrangian formulation and the primal-dual update rules.\", \"A Python script implementing the safe RL algorithm in a simulated environment (e.g., using PyBullet).\", \"A summary report including plots of cumulative reward and constraint cost over training epochs.\", \"A bibliography file with citations and summaries for at least two relevant arXiv papers.\", \"A verification log confirming that the code's update logic matches the mathematical derivation.\"], \"thinking_outline\": [\"1. Research state-of-the-art literature on Lyapunov-based safe RL and primal-dual methods for CMDPs to establish a theoretical foundation.\", \"2. Formulate the robotics task as a CMDP, defining the state space, action space, reward function, and the safety constraint function based on a Lyapunov candidate.\", \"3. Derive the Lagrangian dual of the CMDP and the corresponding stochastic gradient-based primal-dual update rules for the policy (primal) and the Lagrange multiplier (dual).\", \"4. Implement the algorithm in a suitable simulation environment, ensuring the code for the update rules and constraint checking is correct.\", \"5. Validate the implementation by cross-referencing the code logic with the mathematical derivations from step 3.\", \"6. Run experiments to empirically check for algorithm convergence and verify that the safety constraints are satisfied throughout the learning process by monitoring the cost function.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers. The /math agent uses these papers to produce a formal derivation. The /code agent implements this derivation. Verification occurs at each handoff: /math must verify its derivation is consistent with the literature; /code must verify its implementation logic matches the /math agent's derivation. Fallback: If the /code implementation fails verification, it is returned to the /math agent to clarify the derivation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on primal-dual methods for CMDPs with Lyapunov constraints.\", \"- [ ] /math: Derive the complete set of primal-dual update equations based on the sourced literature.\", \"- [ ] /code: Set up the robotic simulation environment and the CMDP problem definition.\", \"- [ ] /code: Implement the policy network, value critic, and safety critic, along with the derived update rules.\", \"- [ ] /math: Verify that the implemented gradient updates in the code correspond exactly to the derived equations.\", \"- [ ] /code: Add runtime assertions to halt execution if the safety constraint violation exceeds a critical threshold during training.\", \"- [ ] /code: Execute the training loop, log metrics (reward, cost), and generate final performance plots.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"constrained-optimization\", \"control-theory\"], \"acceptance_criteria\": [\"- The final report must show via plots that the agent's average constraint cost per episode remains below the predefined safety threshold 'd' after an initial exploration phase.\", \"- The implemented code must contain functions that directly correspond to the primal (policy) and dual (Lagrange multiplier) update rules derived in the math document.\", \"- The submission must cite and briefly explain the relevance of at least two arXiv papers on safe RL or CMDPs.\", \"- The mathematical derivation must be self-contained and logically sound.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 301, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Bayesian neural network with structured variational inference. Instead of a standard mean-field approximation, I want to use a matrix-variate normal posterior over the weights of each layer to capture correlations. Please derive the evidence lower bound (ELBO) for this setup, paying close attention to the entropy term for the matrix-variate normal and the reparameterization trick for gradient estimation. Then, implement this in PyTorch using the Pyro library and validate its convergence against a mean-field baseline on a regression benchmark.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org matrix-variate normal variational inference bayesian neural network kronecker\\\", mode=rag)\", \"/math(Derive the evidence lower bound (ELBO) for a Bayesian neural network with a matrix-variate normal posterior, including the entropy term and the necessary reparameterization for Monte Carlo estimation. Analyze its computational complexity.)\", \"/code(Implement the derived Bayesian model in Python using Pyro, ensuring proper sampling from the matrix-variate posterior via Kronecker factorization. Track convergence of the ELBO and predictive log-likelihood metrics.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational papers on structured VI with matrix-variate posteriors. This informs the /math agent's formal derivation of the ELBO. Finally, the /code agent translates the verified mathematical formulation into a working Pyro implementation for empirical validation.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the ELBO for the matrix-variate normal posterior.\", \"A Python script implementing the specified Bayesian neural network and training loop in Pyro.\", \"A verification report comparing the ELBO's convergence against a standard mean-field VI implementation.\", \"A metrics JSON file containing final ELBO, predictive log-likelihood, and RMSE values.\", \"A bibliography with at least two cited arXiv papers on structured variational inference.\"], \"thinking_outline\": [\"1. Research the mathematical properties of the matrix-variate normal distribution, focusing on its entropy, covariance structure (e.g., Kronecker product), and sampling procedures.\", \"2. Formulate the evidence lower bound (ELBO) for a Bayesian neural network model where the variational posterior over each layer's weights is a matrix-variate normal distribution.\", \"3. Derive the analytical expression for the KL divergence between the matrix-variate normal posterior and a standard normal prior, and formulate the reparameterization trick for low-variance gradient estimation.\", \"4. Implement the variational family and the ELBO objective in a probabilistic programming framework like Pyro, and verify the implementation's numerical stability and gradient correctness using finite differences on a small-scale model.\", \"5. Train the model on a standard regression dataset, comparing its convergence rate and final performance (ELBO, RMSE) against a simpler mean-field (diagonal Gaussian) variational approximation to validate the benefits of capturing weight correlations.\", \"6. Audit the final report to ensure the mathematical derivations in the write-up perfectly match the code implementation and that the experimental results are correctly interpreted.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent verifies that the code's objective function in /code correctly implements the derived ELBO; fallback: if discrepancies arise, /code agent must refactor the loss function and resubmit for verification by /math agent).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on structured VI using matrix-variate normal or Kronecker-factored posteriors.\", \"- [ ] /math: Derive the complete ELBO, including the entropy of the matrix-variate normal and the KL divergence term.\", \"- [ ] /math: Formulate the reparameterization gradient estimator for the specified posterior.\", \"- [ ] /code: Implement the matrix-variate normal variational family in Pyro, using Kronecker products for efficiency.\", \"- [ ] /code: Build the training loop, logging ELBO and predictive log-likelihood metrics per epoch.\", \"- [ ] /math: Verify that the implemented loss function in the code matches the derived ELBO equation from the mathematical derivation.\", \"- [ ] /code: Run benchmark comparison against a mean-field baseline and generate performance plots.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"probabilistic-programming\"], \"acceptance_criteria\": [\"The provided LaTeX derivation of the ELBO must be mathematically sound and complete.\", \"The Python code must successfully train the BNN without numerical instability (e.g., NaN losses) and show ELBO convergence.\", \"The final report must demonstrate that the structured VI model achieves a higher or comparable final ELBO than a mean-field baseline on the test dataset.\", \"The implementation must correctly use the reparameterization trick, as verified by examining the computation graph.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 302, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating second-order optimization methods for non-convex problems, specifically escaping strict saddle points in Generative Adversarial Networks. Can you derive the conditions under which a point is a strict saddle based on the Hessian's eigenvalue spectrum for a GAN's generator loss? Then, implement a Newton-CG (Newton-Conjugate Gradient) algorithm in Python to demonstrate its superior convergence properties over Adam by exploiting negative curvature, and visualize the optimization trajectory on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org saddle point analysis GAN hessian eigenvalue newton-cg convergence\\\", mode=both)\", \"/math(Derive the Hessian of the generator's loss for a simple adversarial GAN setup. Formalize the strict saddle point condition using its eigenvalue spectrum and derive the Newton-CG update rule, ensuring guaranteed convergence by handling negative curvature.)\", \"/code(Implement a toy GAN in PyTorch. Code the Newton-CG optimizer and Adam. Train the GAN with both optimizers, log the gradient norm and minimum Hessian eigenvalue, and plot the convergence comparison to validate the escape from saddle points., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on Hessian analysis in GANs. This informs the /math agent's formal derivation of saddle point conditions and the Newton-CG algorithm. Finally, the /code agent implements and empirically validates the theoretical findings, providing a concrete demonstration.\", \"expected_artifacts\": [\"* A LaTeX document containing the full mathematical derivation of the generator's Hessian and the formal proof for saddle point escape conditions.\", \"* A Python script implementing the GAN and both Newton-CG and Adam optimizers, with clear comments and visualizations.\", \"* A summary report comparing the empirical convergence rates and saddle point escape times, referencing at least two relevant papers.\", \"* A JSON file containing the logged metrics (loss, gradient norm, minimum eigenvalue) for both optimization runs.\", \"* A verification log confirming that the code's update rule correctly implements the derived mathematical formula.\"], \"thinking_outline\": [\"1. Formulate the minimax optimization objective for a standard Generative Adversarial Network.\", \"2. Derive the gradient and the Hessian matrix of the generator's loss function with respect to its parameters.\", \"3. Analyze the Hessian's spectral properties to define a strict saddle point, i.e., a point with a small gradient norm and at least one negative eigenvalue.\", \"4. Formulate the Newton-CG update step, including a mechanism (e.g., trust region or damping) to handle the indefinite Hessian and exploit directions of negative curvature for saddle point escape.\", \"5. Verify that the mathematical derivation for the Newton-CG update rule is correctly translated into the Python implementation by comparing numerical outputs on a test case.\", \"6. Check the empirical results to validate that the Newton-CG optimizer shows faster convergence or reaches a lower loss value compared to Adam, specifically in regions identified as potential saddle points.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The core verification step is ensuring the Newton-CG implementation in /code exactly matches the update rule derived by /math, which was informed by literature from /general-search. The final plots must clearly show Newton-CG escaping a plateau where Adam stagnates. fallback: If the full Hessian is too costly, /math will propose a Hessian-vector product approximation for /code to implement.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on second-order methods for GAN training, focusing on Hessian analysis.\", \"- [ ] /math: Derive the generator's Hessian matrix for a simple GAN with a standard cross-entropy loss.\", \"- [ ] /math: Define the condition for a strict saddle point and derive the Newton-CG update rule.\", \"- [ ] /code: Implement the toy GAN, the Adam optimizer, and the Newton-CG optimizer in PyTorch.\", \"- [ ] /code: Instrument the training loop to log metrics: loss, gradient norm, and minimum Hessian eigenvalue per epoch.\", \"- [ ] /math: Verify that the implemented update rule in the /code agent's script aligns with the derived equations.\", \"- [ ] /code: Generate plots comparing the convergence of Newton-CG and Adam, and a visualization of the parameter trajectory.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-analysis\"], \"acceptance_criteria\": [\"* The mathematical derivation must correctly identify the role of negative eigenvalues in defining strict saddle points.\", \"* The Python implementation must run without errors and produce visualizations that show a clear performance difference between the Newton-CG and Adam optimizers.\", \"* The final report must correctly link the theoretical findings (saddle points) to the empirical results (Adam stalling vs. Newton-CG escaping).\", \"* The primary metric (Convergence Rate) for Newton-CG must be at least 20% better (fewer iterations to threshold) than for Adam.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 304, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nExplain the computational complexity of the self-attention mechanism in Transformers, derive its scaling law, and then code a simple Python simulation to verify this O(n^2) behavior for different sequence lengths, including a basic CUDA check.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org \\\\\\\"Attention Is All You Need\\\\\\\" Vaswani et al. transformer complexity\\\", mode=both, context=\\\"Locate the original arxiv paper to establish the canonical definition of self-attention for the subsequent mathematical derivation.\\\")\", \"/math(Derive the O(n^2 * d) computational complexity of the self-attention mechanism, explaining the Bayesian interpretation of attention weights based on the Query, Key, and Value matrix multiplications.)\", \"/code(Implement a basic self-attention layer in Python using PyTorch, measure execution time against sequence length, and include a simple check for CUDA device availability to ensure reproducibility on different hardware.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find the foundational paper for context. It then uses /math for the theoretical derivation of complexity. Finally, /code is used for empirical verification of the derived theory, providing a complete and validated explanation.\", \"expected_artifacts\": [\"A markdown document containing the step-by-step mathematical derivation of self-attention's computational complexity.\", \"A Python script (`attention_simulation.py`) that implements a simple self-attention layer and times its execution over various sequence lengths.\", \"A PNG image (`complexity_plot.png`) plotting sequence length vs. execution time, visually confirming the quadratic relationship.\", \"A metrics file (`performance_metrics.json`) reporting the R-squared value of the curve fit and peak VRAM usage.\", \"A verification summary in the main report that confirms the empirical results from the code align with the mathematical derivation.\"], \"thinking_outline\": [\"1. Locate and review the original 'Attention Is All You Need' paper to understand the canonical definition of scaled dot-product attention.\", \"2. Formulate the self-attention mechanism as a series of matrix operations involving Query (Q), Key (K), and Value (V) matrices, with dimensions (n, d_k), (n, d_k), and (n, d_v) respectively.\", \"3. Analyze the dimensions of these matrices to derive the computational complexity of their multiplications (specifically QK^T), showing it scales as O(n^2 * d).\", \"4. Implement the derived operations in a Python script and create a profiling loop to measure wall-clock time against increasing sequence lengths (n).\", \"5. Validate that the empirical timing data from the code fits a quadratic curve by calculating the R-squared value, thereby verifying the mathematical derivation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides the canonical paper reference to the math agent. The math agent provides the theoretical complexity formula (O(n^2*d)) to the code agent. The code agent generates a plot and metrics which are checked against the theoretical formula during router QA. Fallback: If the code's performance plot is not quadratic, the implementation and the derivation must be re-audited for errors.\", \"todo_list\": [\"- [ ] /general-search: Find and provide the link to the 'Attention Is All You Need' paper (arXiv:1706.03762).\", \"- [ ] /math: Derive the O(n^2 * d) complexity of self-attention, detailing each matrix multiplication step.\", \"- [ ] /code: Write a Python script to implement and time the self-attention mechanism, generating a performance plot and metrics JSON.\", \"- [ ] Verification: Compare the plot from the /code agent to the quadratic complexity derived by the /math agent. Ensure the R-squared metric exceeds the acceptance threshold.\", \"- [ ] QA Review: Submit all artifacts (derivation, code, plot, citation, verification report) for final router approval.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"transformer-architecture\", \"computational-complexity\", \"llm-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly concludes that self-attention complexity is O(n^2 * d).\", \"- The Python code successfully runs and generates a plot clearly showing an upward-curving, quadratic-like relationship.\", \"- The primary metric (Curve Fit R-squared) must be greater than 0.98, confirming the visual trend.\", \"- The final report correctly cites 'Vaswani, A., et al. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.'\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Fit a quadratic curve (y=ax^2+bx+c) to the (sequence length, execution time) data points. The R-squared value measures goodness of fit and should be > 0.98.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the peak GPU memory allocated during the run with the largest sequence length, reported in MiB. Use `torch.cuda.max_memory_allocated()`.\"}}", "categories": ["metrics_guidance"]}
{"index": 305, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to understand the core concept behind differentiable architecture search (DARTS). Can you explain the continuous relaxation of the architecture representation and derive the bilevel optimization objective for this AutoML method?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org \\\\\\\"Differentiable Architecture Search\\\\\\\" \\\\\\\"continuous relaxation\\\\\\\" bilevel optimization\\\", mode=rag)\", \"/math(Derive the bilevel optimization objective for DARTS, explaining the continuous relaxation of the search space using a softmax over operations, and contrast this with Bayesian AutoML approaches.)\", \"/code(Implement a minimal DARTS search cell in Python using PyTorch, demonstrating the gradient-based architecture update step. Verify the computation graph and ensure it's runnable on a CUDA device.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational theory and papers on DARTS. This information is then passed to /math to formally derive the core equations. Finally, /code is used to create a practical implementation that validates the theoretical concepts, making the abstract ideas concrete and verifiable.\", \"expected_artifacts\": [\"A markdown document with the mathematical derivation of the DARTS bilevel optimization objective.\", \"A Python script implementing a simplified DARTS search cell using PyTorch.\", \"A list of at least 2 citations to foundational DARTS papers with their arXiv IDs.\", \"A verification report in JSON format showing that the implementation correctly computes gradients for the architecture parameters.\"], \"thinking_outline\": [\"1. Establish the foundational theory of Neural Architecture Search (NAS) and the limitations of discrete search methods that motivate differentiable approaches.\", \"2. Formulate the concept of continuous relaxation in DARTS, where the discrete choice of operations is replaced by a softmax-weighted sum over a set of possible operations.\", \"3. Derive the bilevel optimization problem, separating the inner loop (training network weights with fixed architecture parameters) from the outer loop (updating architecture parameters on validation data).\", \"4. Implement a small-scale version of the DARTS search cell to practically demonstrate the gradient flow from the validation loss back to the architecture parameters.\", \"5. Verify the implementation by programmatically inspecting the gradients of the architecture parameters ('alphas') and checking that they are non-zero after a full optimization step.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code implementation is checked against the mathematical derivation for correctness, and gradient magnitudes are audited; fallback: if gradients are zero, re-run /code with added debugging hooks to trace the computation graph).\", \"todo_list\": [\"- [ ] Use /general-search to find the original DARTS paper and tutorials on its core concepts of continuous relaxation and bilevel optimization.\", \"- [ ] Use /math to formally derive the continuous relaxation and the bilevel optimization equations based on the sourced materials.\", \"- [ ] Use /code to write a minimal PyTorch implementation of a DARTS cell that allows for gradient-based updates.\", \"- [ ] Verify that the gradients for the architecture parameters in the code are non-zero and flow correctly by logging their magnitude.\", \"- [ ] Submit all artifacts (derivation, code, citations, verification metrics) for final router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"automl\", \"neural-architecture-search\", \"optimization\"], \"acceptance_criteria\": [\"The mathematical derivation correctly formulates the bilevel optimization problem for DARTS, defining both inner and outer loop objectives.\", \"The Python code implements a searchable cell and successfully executes at least one full training step without errors.\", \"The verification metrics confirm that non-zero gradients are flowing to the architecture parameters.\", \"At least 2 relevant arXiv papers, including the original DARTS paper, are cited correctly.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"guidance\": \"Primary: Calculate the L2 norm of the gradients with respect to the architecture parameters (`alphas`) after one optimization step; a non-zero value is required. Secondary: Track the training loss of the network weights during the inner loop optimization; it should show a decreasing trend.\"}}", "categories": ["metrics_guidance"]}
{"index": 307, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a detailed theoretical analysis of second-order Model-Agnostic Meta-Learning (MAML) convergence. Specifically, derive the convergence rate bounds for a non-convex objective landscape, assuming a federated learning context. The analysis must explicitly account for the Hessian-vector product approximation and its impact on the Bayesian regret bound. Compare this theoretical rate with an empirical simulation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org second-order MAML convergence federated bayesian regret analysis\\\", mode=both)\", \"/math(Derive the convergence proof for second-order MAML, establishing the relationship between the inner-loop learning rate, Hessian eigenvalue spectrum, and the overall Bayesian regret.)\", \"/code(Implement the second-order MAML algorithm in JAX using CUDA for a synthetic few-shot regression task; empirically measure the convergence rate and bootstrap confidence intervals., using Python)\", \"/math(Verify that the gradient and Hessian-vector product updates in the JAX code align with the derived mathematical expressions, auditing for potential sources of numerical instability or complexity blowup.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational theorems from recent literature. This informs the core theoretical work in /math. Then, /code is used for empirical validation of the theory. A final /math step is crucial for cross-verification, ensuring the implementation accurately reflects the derived mathematical principles.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the convergence bound.\", \"A Python script implementing the MAML simulation and plotting empirical vs. theoretical rates.\", \"A bibliography file citing at least two relevant arXiv papers with their IDs.\", \"A verification report confirming the alignment between the mathematical proof and the code implementation logic.\"], \"thinking_outline\": [\"1. Review current literature on MAML, specifically focusing on second-order methods, convergence proofs in non-convex settings, and connections to Bayesian regret.\", \"2. Formulate the optimization problem and formally derive the convergence rate, clearly stating all assumptions about the loss landscape (e.g., L-smoothness, Hessian Lipschitz continuity).\", \"3. Implement the algorithm and a synthetic testbed to empirically measure the convergence rate across different hyperparameters.\", \"4. Validate the mathematical derivation by comparing the theoretical bounds against the empirical results from the simulation.\", \"5. Audit the implementation to verify that the core update rules (e.g., Hessian-vector products) are a correct translation of the derived mathematical formulas.\", \"6. Synthesize all findings into a final report, including the proof, code, plots, and a discussion on any discrepancies.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (verification) -> router QA (verification: Compare theoretical bound from math with empirical rate from code; fallback: If mismatch > 15%, relaunch /math to re-check assumptions in derivation).\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 key papers on second-order MAML convergence in federated or Bayesian contexts.\", \"- [ ] /math: Produce a step-by-step derivation of the convergence rate based on findings.\", \"- [ ] /code: Implement the MAML variant and the simulation environment for a synthetic task.\", \"- [ ] /code: Generate plots comparing the empirical convergence rate to the theoretical bound.\", \"- [ ] /math: Verify derivation aligns with the JAX implementation's gradient and Hessian logic.\", \"- [ ] /math: Produce a formal verification statement of the code's mathematical correctness.\", \"- [ ] router: Perform final QA, ensuring the empirical plot in the report is supported by the LaTeX proof.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"optimization-theory\", \"federated-learning\"], \"acceptance_criteria\": [\"The final report must contain a complete, mathematically sound proof of convergence for second-order MAML under the specified conditions.\", \"The provided code must be executable and reproduce the plots showing empirical convergence rates.\", \"The theoretical convergence bound must match the empirical rate within a 15% margin of error.\", \"At least two relevant arXiv pre-prints must be cited correctly in a bibliography section.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 308, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the analytical conditions for avoiding barren plateaus in Variational Quantum Eigensolvers by analyzing the Hamiltonian's eigenvalue distribution and the expressivity of hardware-efficient ansatzes. Please implement a VQE simulation in Python for the 4-qubit transverse-field Ising model, benchmarking a proposed quantum natural gradient optimizer against SPSA, and cite recent arXiv preprints on the topic.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational quantum eigensolver 'barren plateau' 'quantum natural gradient' hamiltonian eigenvalue\\\", mode=both)\", \"/math(Derive the gradient variance for a hardware-efficient ansatz applied to the transverse-field Ising Hamiltonian, connecting the barren plateau condition to the Hamiltonian's eigenvalue spectrum and circuit depth.)\", \"/code(Implement a VQE simulation using Python's PennyLane to find the ground state energy of a 4-qubit transverse-field Ising model, comparing the convergence of a custom quantum natural gradient optimizer against SPSA.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art research on VQE barren plateaus and quantum natural gradient methods from arXiv. This literature informs the /math agent's formal derivation of gradient variance. Finally, the /code agent implements the VQE simulation and benchmarks the optimizers based on the theoretical groundwork, providing empirical validation.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the barren plateau conditions.\", \"- A Python script implementing the VQE simulation and optimizer comparison.\", \"- A Jupyter notebook plotting the convergence of ground state energy for each optimizer.\", \"- A summary report in Markdown that interprets the results and includes citations for at least two relevant arXiv preprints.\", \"- A JSON file containing the final ground state energy error and convergence metrics.\"], \"thinking_outline\": [\"1. Conduct a literature review to identify key papers on VQE barren plateaus, particularly focusing on the role of cost function locality, ansatz expressivity, and quantum natural gradient optimizers.\", \"2. Formulate the mathematical problem, defining the Hamiltonian for the transverse-field Ising model, the structure of a hardware-efficient ansatz, and the expression for the cost function gradient.\", \"3. Derive an analytical expression for the variance of the cost function gradient, showing its exponential decay with the number of qubits, which is the signature of a barren plateau.\", \"4. Validate the derivation by cross-referencing it with established results from the papers found during the literature search.\", \"5. Implement the VQE algorithm, the specified Hamiltonians, and both the SPSA and quantum natural gradient optimizers in a quantum simulation framework.\", \"6. Verify the implementation by comparing the VQE-computed ground state energy for a small system (e.g., 2 qubits) against the value obtained from exact diagonalization.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent's derivation of gradient variance must be consistent with the literature found by search. The code agent's implementation must numerically reflect the analytical model from the math agent. fallback: If derivation is inconsistent, /math agent must refine its model using additional search results. If implementation fails verification, /code agent must debug by checking numerical gradients against the analytical formula.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent arXiv papers on barren plateaus and quantum natural gradient methods.\", \"- [ ] /math: Derive the expression for the gradient variance of the VQE cost function for the specified problem.\", \"- [ ] /code: Implement the VQE simulation for the 4-qubit transverse-field Ising model.\", \"- [ ] /code: Implement both the SPSA and quantum natural gradient optimizers and the benchmarking logic.\", \"- [ ] /math: Verify that the analytical derivation aligns with the theoretical results presented in the sourced arXiv papers.\", \"- [ ] /code: Validate that the simulated ground state energy for a 2-qubit case matches the result from exact diagonalization within a 1e-4 tolerance.\", \"- [ ] /router: QA review of all generated artifacts for consistency, correctness, and adherence to the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-machine-learning\", \"variational-quantum-eigensolver\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly links gradient variance to system size and is consistent with established literature.\", \"- The Python code runs without errors and produces a plot clearly comparing the convergence rates of the SPSA and quantum natural gradient optimizers.\", \"- The final report accurately interprets the results and cites at least two relevant arXiv papers with their IDs.\", \"- The primary metric (ground state energy error) for the best optimizer is below 0.01 for the 4-qubit simulation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as |E_vqe - E_exact|, where E_vqe is the final energy from the VQE run and E_exact is the ground state energy computed via exact diagonalization of the 4-qubit Hamiltonian.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure as the number of optimization iterations required to reach an energy within 1% of the final converged energy. A lower number is better.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 309, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement the K-SVD algorithm for dictionary learning to solve an underdetermined system for sparse signal recovery. Please derive the alternating minimization steps for both the sparse coding stage using Orthogonal Matching Pursuit (OMP) and the dictionary update stage involving Singular Value Decomposition (SVD). Analyze the algorithm's convergence properties and computational complexity, then implement it in Python and validate its performance on a synthetic dataset by measuring signal reconstruction SNR versus dictionary coherence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD dictionary learning algorithm convergence analysis SVD update\\\", mode=rag)\", \"/math(Derive the K-SVD objective function and its alternating minimization update rules for both the sparse coding and dictionary update stages. Analyze the algorithm's convergence guarantees and the computational complexity of its SVD-based eigenvalue calculations.)\", \"/code(Implement the K-SVD algorithm in Python with NumPy/SciPy, including an OMP subroutine. Generate a synthetic dataset, run the algorithm to learn a dictionary, and evaluate signal recovery performance by plotting reconstruction SNR against dictionary coherence.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on K-SVD, focusing on its mathematical underpinnings. This research informs the /math agent's task of formally deriving the update equations and analyzing complexity. Finally, the /code agent implements the derived algorithm, using the mathematical specification as a blueprint and performing the required empirical validation.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the K-SVD update rules, including complexity analysis.\", \"- A Python script implementing the K-SVD algorithm and the experimental validation procedure.\", \"- A PDF report summarizing the derivation, implementation details, and plots showing SNR vs. dictionary coherence.\", \"- A list of citations for the core K-SVD algorithm and any variants explored.\"], \"thinking_outline\": [\"1. Research the K-SVD algorithm, focusing on the objective function and the two-stage optimization process: sparse coding and dictionary update.\", \"2. Formally derive the update rule for the sparse coefficients, typically solved via a pursuit algorithm like OMP.\", \"3. Derive the dictionary atom update rule, showing how it reduces to a rank-1 approximation problem solved efficiently via SVD.\", \"4. Verify the implementation of the OMP sparse coding stage by checking if it correctly identifies the support for a known sparse signal.\", \"5. Validate the complete K-SVD implementation by confirming that the reconstruction error on the training data is non-increasing across iterations.\", \"6. Generate final plots and metrics, ensuring they accurately reflect the trade-offs between dictionary properties and signal recovery fidelity.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: the Python implementation in /code must directly correspond to the update equations derived by /math, which in turn must be grounded in the literature found by /general-search; fallback: if derivation is ambiguous, /math flags it and requests /general-search to find alternative explanations or proofs).\", \"todo_list\": [\"- [ ] /general-search: Find the seminal K-SVD paper (Aharon, Elad, Bruckstein) and at least one follow-up paper discussing convergence.\", \"- [ ] /math: Derive the update rules for sparse coefficients (OMP) and dictionary atoms (SVD).\", \"- [ ] /math: Analyze the computational complexity per iteration of the full K-SVD algorithm.\", \"- [ ] /code: Implement the full K-SVD algorithm, including a functional OMP implementation.\", \"- [ ] /code: Create a synthetic data generation function and a script to run the experiment and produce the required plots.\", \"- [ ] /code: Verify numerical stability and correctness of the SVD update step with unit tests.\", \"- [ ] /math: Verify that the code's stopping criteria align with the convergence properties discussed in the derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The mathematical derivation of both the sparse coding and dictionary update steps is present and correct.\", \"- The Python implementation successfully runs and generates a plot of Signal-to-Noise Ratio (SNR) vs. dictionary coherence.\", \"- The final report includes the derivation, plots, and cites at least two relevant academic papers with their arXiv IDs or DOIs.\", \"- The reconstruction error of the algorithm on the training set is shown to be monotonically non-increasing over iterations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 311, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis comparing persistent homology with manifold learning techniques like UMAP for feature extraction on high-dimensional biomedical datasets, specifically single-cell RNA-seq. The analysis should formalize the stability theorem for persistence diagrams under the bottleneck distance, provide a proof sketch, and then implement a computational pipeline to demonstrate this stability empirically by perturbing input data. Compare the topological features (Betti numbers) against the geometric clustering quality produced by UMAP.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org persistent homology stability theorem bottleneck distance biomedical data manifold learning\\\", mode=rag)\", \"/math(Derive the stability theorem for persistence diagrams under Gromov-Hausdorff perturbations, focusing on the convergence of simplicial complexes and the induced bottleneck distance bounds. Analyze the eigenvalue spectrum of the graph Laplacian as an alternative stability measure.)\", \"/code(Implement a Python pipeline using giotto-tda to compute persistence diagrams for a perturbed point cloud dataset. Use a bootstrap method to estimate confidence intervals for Betti numbers and compare against UMAP embedding quality using silhouette scores.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative papers on TDA stability, which is a foundational theoretical requirement. This research grounds the formal derivation in /math, which will prove the key stability theorem. Finally, /code implements the theory to provide an empirical validation and comparison, creating a full theoretical-to-practical workflow.\", \"expected_artifacts\": [\"- A LaTeX-formatted technical report detailing the mathematical proof of the persistence stability theorem.\", \"- A commented Python script implementing the persistence diagram computation and UMAP comparison pipeline.\", \"- A visualization plot comparing the persistence diagram of original vs. perturbed data and the corresponding UMAP embeddings.\", \"- A bibliography with at least 3 citations to relevant arXiv preprints or peer-reviewed articles.\", \"- A JSON file containing the computed bottleneck distances and silhouette scores for different perturbation levels.\"], \"thinking_outline\": [\"1. Survey the foundational literature on persistent homology, specifically the stability theorem, and its application in bioinformatics.\", \"2. Formalize the mathematical concepts: define simplicial complexes, filtrations, persistence modules, and the bottleneck distance for persistence diagrams.\", \"3. Construct a proof sketch for the stability theorem, showing that small perturbations in the input data lead to small changes in the output persistence diagram.\", \"4. Validate the mathematical derivation by cross-referencing it with established results from the literature search.\", \"5. Develop a computational experiment to empirically verify the stability theorem by adding controlled noise to a dataset and measuring the bottleneck distance between the resulting diagrams.\", \"6. Audit the code implementation to ensure the distance metrics and TDA computations are correctly implemented according to the mathematical formulation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent verifies that its derivation of the stability theorem is supported by the literature found by search. The code agent verifies that its implementation of the bottleneck distance matches the mathematical definition. router QA checks that empirical results from the code align with theoretical expectations from the math proof; fallback: If the code's empirical results contradict the theorem, escalate to the math agent to check for assumptions violated in the proof sketch.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on TDA stability (e.g., by Cohen-Steiner, Edelsbrunner, Harer) and applications in bioinformatics.\", \"- [ ] /math: Formulate the definitions of Vietoris-Rips complexes, filtrations, and persistence diagrams.\", \"- [ ] /math: Provide a detailed proof sketch of the stability theorem for persistence diagrams.\", \"- [ ] /code: Implement the generation of a sample dataset (e.g., a noisy point cloud on a torus) and a perturbation function.\", \"- [ ] /code: Write functions to compute persistence diagrams and the bottleneck distance using a library like `giotto-tda` or `ripser`.\", \"- [ ] /math: Verify that the code agent's implementation of the bottleneck distance metric is consistent with the mathematical definition from the literature.\", \"- [ ] /code: Validate the implementation by comparing the output on a known example from a cited paper.\", \"- [ ] router QA: Review the final report, code, and visualizations for consistency and correctness.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"computational-topology\"], \"acceptance_criteria\": [\"- The mathematical proof sketch for the stability theorem must be logically sound and consistent with established literature.\", \"- The Python implementation must correctly compute persistence diagrams and demonstrate that the bottleneck distance between diagrams of original and perturbed data is small.\", \"- The comparative analysis must clearly articulate the trade-offs between topological features from persistence homology and geometric clusters from UMAP.\", \"- All generated artifacts (report, code, plots) must be complete, well-documented, and internally consistent.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 312, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the computational complexity of determining the nim-sum (Grundy value) for a variant of the game of Nim played on a directed acyclic graph (DAG) where moves are restricted by vertex coloring constraints. Derive the recurrence for the Grundy values, analyze its worst-case complexity, and implement a dynamic programming solution in Python to validate the theoretical complexity bounds for sparse, randomly generated DAGs with high chromatic numbers. Compare this against a naive recursive implementation to demonstrate the effects of memoization.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org \\\\\\\"combinatorial game theory\\\\\\\" \\\\\\\"computational complexity\\\\\\\" \\\\\\\"Sprague-Grundy\\\\\\\" \\\\\\\"games on graphs\\\\\\\" PSPACE\\\", mode=both)\", \"/math(Derive the recurrence for Grundy numbers in this adversarial game on a colored DAG. Formally analyze the time complexity, discussing its relation to PSPACE-completeness and the impact of the graph's topological structure.)\", \"/code(Implement in Python both a naive recursive solver and a memoized dynamic programming solution for the Grundy values. Validate the implementation by comparing runtimes on sparse, random DAGs, tracking convergence and demonstrating the complexity difference.)\"], \"route_rationale\": \"The blue general-search agent is used first to gather state-of-the-art literature on the complexity of impartial games. The /math agent is then tasked with the formal derivation and theoretical complexity analysis based on these foundations. Finally, the /code agent implements the algorithms and performs empirical validation to confirm the mathematical findings.\", \"expected_artifacts\": [\"- A formal report in PDF format detailing the mathematical derivation and complexity analysis.\", \"- Python source code (`.py`) containing the graph generation, naive solver, and dynamic programming solver.\", \"- A plot (`.png`) comparing the runtime performance of the naive and DP algorithms as a function of graph size.\", \"- A `citations.bib` file with at least two relevant academic papers.\", \"- A `metrics.json` file reporting the measured algorithmic speedup and empirical fit.\"], \"thinking_outline\": [\"1. Research the Sprague-Grundy theorem and its complexity implications for impartial games on graphs, particularly PSPACE-completeness.\", \"2. Formulate the game rules precisely for the vertex-colored DAG and derive the mathematical recurrence for the Grundy values (g-numbers).\", \"3. Analyze the worst-case time complexity of computing these values, relating it to graph properties like the number of vertices and edges.\", \"4. Implement both a naive exponential-time recursive algorithm and an efficient dynamic programming (memoization) solution in Python.\", \"5. Validate the theoretical complexity analysis by running experiments on generated graphs and comparing the empirical performance of the two algorithms.\", \"6. Check that the experimental runtime data aligns with the theoretical bounds derived in the mathematical analysis.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results provide theoretical context for the /math agent's derivation. The formal derivation from /math provides the specification for the /code agent's implementation. The /code agent's empirical results are then used to verify the /math agent's theoretical claims. Fallback: If the empirical results from /code contradict the theory from /math, the task is routed back to /math for re-evaluation of the complexity proof.\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on Sprague-Grundy theorem complexity and games on graphs.\", \"- [ ] /math: Derive the recurrence for Grundy values on the specified colored DAG.\", \"- [ ] /math: Formally prove the time complexity bounds for the computation.\", \"- [ ] /code: Implement the DAG data structure and the two solver algorithms (naive and DP).\", \"- [ ] /code: Generate test graphs and run empirical performance comparisons, plotting the results.\", \"- [ ] /math: Verify that the implemented logic in /code correctly reflects the derived recurrence relation.\", \"- [ ] /code: Validate that the empirical runtime plots align with the theoretical complexity bounds from /math.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"dynamic-programming\"], \"acceptance_criteria\": [\"- The submitted report contains a correct mathematical derivation of the Grundy value recurrence and its O(V+E) time complexity for the DP solution.\", \"- The Python code correctly implements both the naive exponential and the memoized polynomial-time algorithms without errors on test inputs.\", \"- The generated plot clearly shows an exponential vs. polynomial runtime divergence, empirically supporting the theoretical analysis.\", \"- At least 2 relevant academic papers (e.g., from arXiv) are cited in a `.bib` file.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 314, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the g-formula for estimating the causal effect of a time-varying treatment on a survival outcome, considering time-dependent confounders affected by prior treatment. Formulate the identifiability conditions using do-calculus and provide a proof sketch for how the backdoor criterion is generalized in this longitudinal, causal setting. Discuss potential violations of the sequential exchangeability assumption and how they impact the estimator's bias.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'g-formula' 'do-calculus' 'time-dependent confounders' causal inference\\\", mode=both)\", \"/math(Derive the longitudinal g-formula using do-calculus, proving identifiability from observational data under sequential exchangeability. Analyze the causal Bayesian network and apply backdoor adjustment criteria iteratively.)\", \"/code(Implement a Python simulation of a time-varying treatment scenario with known causal ground truth. Apply the derived g-formula to estimate the average treatment effect and compare it to the true effect to validate the derivation. Use bootstrapping for confidence interval estimation.)\", \"/math(Audit the Python code's implementation of the g-formula estimator. Verify that the iterative expectation and conditioning steps correctly match the derived causal identification formula and that the bootstrap confidence interval logic is sound.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on the g-formula and do-calculus. This informs the core theoretical work performed by /math, which will produce the formal derivation and proof. To ground the theory, /code implements a simulation to validate the formula empirically. A final /math step provides a rigorous audit, verifying that the code's logic faithfully represents the derived mathematics, ensuring correctness.\", \"expected_artifacts\": [\"A LaTeX document containing the full derivation of the g-formula and the do-calculus proof of identifiability.\", \"A Python script implementing the simulation and g-formula estimator, with comments linking code to the mathematical derivation.\", \"A summary report in Markdown explaining the results, the estimated causal effect, and the impact of assumption violations.\", \"A list of cited arXiv papers with their corresponding IDs.\", \"A verification log detailing the audit of the code against the mathematical derivation.\"], \"thinking_outline\": [\"1. Establish the formal causal model using a Directed Acyclic Graph (DAG) for a time-varying treatment, outcome, and time-dependent confounders.\", \"2. Apply Pearl's do-calculus rules to the DAG to derive an expression for the post-intervention distribution P(Y|do(A=a)) in terms of observed data probabilities, leading to the g-formula.\", \"3. Prove the identifiability of the causal effect under the assumptions of sequential exchangeability, positivity, and consistency.\", \"4. Validate the derived g-formula by implementing a simulation in Python. Generate data from a known structural causal model and check if the estimator recovers the true parameter.\", \"5. Verify the mathematical consistency between the derived formula and the Python implementation's logic for calculating the nested expectations.\", \"6. Analyze and document potential sources of bias, particularly violations of sequential exchangeability, and discuss their qualitative effect on the estimate.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation handed to the code agent for implementation and simulation. The code and its output are returned to the math agent for a formal audit. All verified artifacts are then passed to the router for final quality assurance. Verification: The second /math call audits the /code implementation against the first /math derivation. Fallback: If the code audit fails, the task is returned to /code with specific correction requests from /math.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on g-formula and do-calculus for longitudinal data on arXiv.\", \"- [ ] /math: Formulate the causal model (DAG) and derive the g-formula for a time-varying treatment.\", \"- [ ] /math: Prove the identifiability conditions using do-calculus, explicitly stating all assumptions.\", \"- [ ] /code: Implement a Python simulation based on the provided causal model to generate synthetic data.\", \"- [ ] /code: Implement the g-formula estimator in Python and apply it to the synthetic data.\", \"- [ ] /math: Verify the Python implementation by comparing its logical flow against the derived mathematical formula for the nested expectations.\", \"- [ ] /code: Generate a report summarizing the simulation results, including the estimated effect, confidence interval, and a comparison to the true effect.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"advanced-statistics\"], \"acceptance_criteria\": [\"The mathematical derivation correctly applies do-calculus to arrive at the standard longitudinal g-formula.\", \"The Python simulation successfully recovers the true causal effect from the synthetic data, with the true value falling within the 95% bootstrap confidence interval.\", \"The final report clearly explains the assumptions, derivation, simulation setup, and results, including citations for at least 2 relevant papers.\", \"The verification log from the math agent confirms alignment between the formal proof and the code's implementation logic.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 315, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and analyze a post-training quantization (PTQ) scheme for a Vision Transformer (ViT) that uses second-order information. Specifically, can you derive and code a method based on Optimal Brain Quantization (OBQ), which uses the inverse Hessian to minimize quantization error? I need a PyTorch implementation that calculates the Hessian saliency, applies non-uniform quantization, and measures the impact on ImageNet top-1 accuracy, ensuring CUDA optimization for the Hessian-vector products.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Optimal Brain Quantization inverse Hessian transformer quantization theory\\\", mode=web)\", \"/math(Derive the update rule for quantizing a weight tensor using the inverse Hessian to minimize reconstruction error. Formulate the quadratic proxy for the loss function and detail the Hessian eigenvalue constraints.)\", \"/code(Implement the OBQ algorithm in PyTorch with CUDA support for Hessian-vector products. Apply it to a pre-trained ViT model, track top-1 accuracy degradation, and include validation checks for numerical stability., using Python)\", \"/math(Verify that the empirical quantization error from the code implementation scales as predicted by the theoretical derivation involving the Hessian spectral properties. Check for convergence of the quantization updates.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational papers on Optimal Brain Quantization (OBQ) and Hessian-based methods. This theoretical grounding is then passed to /math to derive the specific optimization problem. The resulting algorithm is implemented by /code, which requires performance-critical CUDA optimizations. A final /math step is crucial for verification, ensuring the empirical results from the code align with the theoretical predictions, closing the loop between theory and practice.\", \"expected_artifacts\": [\"- A markdown document with the mathematical derivation of the Hessian-based quantization update rule.\", \"- A Python script (`quantize_vit.py`) implementing the OBQ algorithm for a Vision Transformer.\", \"- A JSON file (`metrics.json`) reporting pre- and post-quantization top-1 accuracy, model size, and Hessian spectral norm.\", \"- A verification report (`verification.log`) comparing empirical quantization error against theoretical predictions.\", \"- A list of at least 2 cited arXiv papers on second-order optimization methods in quantization.\"], \"thinking_outline\": [\"1. Survey existing literature on second-order methods for neural network quantization, focusing on Optimal Brain Quantization and its variants.\", \"2. Formulate the layer-wise quantization problem as minimizing a quadratic approximation of the task loss, parameterized by the Hessian matrix.\", \"3. Derive the closed-form solution for optimal weight updates and the order in which weights should be quantized based on Hessian saliency.\", \"4. Implement the algorithm in PyTorch, leveraging its autograd engine and CUDA capabilities to efficiently compute Hessian-vector products without materializing the full Hessian.\", \"5. Validate the implementation by applying it to a pre-trained Vision Transformer and measuring the top-1 accuracy degradation on the ImageNet validation set.\", \"6. Audit the final results to confirm that the weights with the smallest Hessian saliency (i.e., in directions of low curvature) were indeed pruned or quantized with minimal impact on accuracy.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: The final math step verifies that the code's empirical error aligns with the initial derivation's predictions; fallback: if verification fails, /code will be tasked to simplify the Hessian approximation, e.g., using a diagonal or K-FAC approximation, and re-run the experiment).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Optimal Brain Quantization (OBQ) or Hessian-aware quantization.\", \"- [ ] /math: Derive the weight update rule for OBQ, clearly defining the objective function and the role of the inverse Hessian.\", \"- [ ] /code: Implement the OBQ algorithm for a `timm`-based Vision Transformer model.\", \"- [ ] /code: Profile and optimize the Hessian-vector product computation using CUDA.\", \"- [ ] /math: Formulate a verification test comparing the empirical error from the coded quantization against the theoretical error bound derived from the Hessian.\", \"- [ ] /math: Execute the verification test using outputs from the /code agent.\", \"- [ ] /router: Perform final QA check on all artifacts, ensuring consistency between the derivation, code, and reported metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"transformer-optimization\", \"quantization\", \"hessian-analysis\"], \"acceptance_criteria\": [\"- The provided Python script successfully quantizes a pre-trained ViT model and runs without error.\", \"- The final report includes a valid mathematical derivation of the OBQ update rule.\", \"- The `metrics.json` file shows a top-1 accuracy degradation of less than 2% for 4-bit quantization.\", \"- The verification report confirms that the quantization error correlates with the Hessian-based saliency scores.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 316, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the complete primal-dual update rules for a consensus Alternating Direction Method of Multipliers (ADMM) algorithm tailored for a high-dimensional Group Lasso regression problem. Please provide a formal analysis of its convergence conditions, specifically how the penalty parameter \u03c1 affects the spectral properties of the iteration matrix. Subsequently, implement this ADMM variant in Python and empirically validate its convergence rate against your theoretical findings on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org consensus ADMM convergence analysis group lasso operator splitting\\\", mode=both)\", \"/math(Formulate the augmented Lagrangian for the consensus Group Lasso problem. Derive the primal-dual update steps using proximal operators and analyze the convergence conditions based on the spectral radius of the iteration matrix, referencing ADMM literature.)\", \"/code(Implement the derived consensus ADMM algorithm in Python using NumPy/SciPy. Generate a synthetic sparse dataset and verify the primal and dual residual convergence, comparing the final solution against a centralized CVXPY solver.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey authoritative literature on ADMM for Group Lasso, providing theoretical grounding. The /math tool is then used for the core derivation and convergence analysis based on these findings. Finally, /code is employed to implement the algorithm and empirically validate the theoretical results, creating a closed loop of theory and practice.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the ADMM updates and convergence analysis, formatted in LaTeX.\", \"A Python script (.py) with the complete implementation of the consensus ADMM solver and the simulation harness.\", \"A JSON file summarizing the final metrics, including convergence rates and solution accuracy.\", \"A set of plots (e.g., .png files) showing the primal/dual residuals and objective value over iterations.\", \"A bibliography file with citations to at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the global Group Lasso objective function and decompose it into a consensus optimization form suitable for ADMM.\", \"2. Construct the augmented Lagrangian for the consensus problem and derive the iterative update equations for each primal variable (local coefficients, global consensus variable) and the dual variable (scaled Lagrange multiplier).\", \"3. Analyze the convergence criteria by examining the fixed-point iteration, relating the penalty parameter \u03c1 to the properties of the problem data (e.g., Lipschitz constants) based on foundational operator splitting theory.\", \"4. Implement the algorithm in Python, including a function to generate synthetic data with a known group-sparse structure, and a benchmark solver using a standard library like CVXPY.\", \"5. Validate the implementation by ensuring the primal and dual residuals converge towards zero and that the final ADMM solution vector is close to the one obtained by the centralized benchmark solver.\", \"6. Run experiments to plot the convergence behavior and check if the observed rate is consistent with the theoretical analysis from step 3.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results from /general-search provide foundational papers for the /math agent's derivation. The complete mathematical formulation from /math is the direct specification for the /code agent's implementation. Verification: The /code agent must check that its empirical convergence plots do not contradict the theoretical conditions derived by /math. Fallback: If the implementation fails to converge, the task is returned to /math to re-check the derivation and convergence analysis, possibly triggering a new search query.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on consensus ADMM and its convergence theory, particularly for problems with non-smooth regularizers like Group Lasso.\", \"- [ ] /math: Derive the complete ADMM update equations (x-update, z-update, u-update) for the consensus Group Lasso formulation.\", \"- [ ] /math: Formally analyze the convergence conditions, specifying the valid range for the penalty parameter \u03c1.\", \"- [ ] /code: Implement the ADMM solver, the proximal operator for the group lasso penalty, and a synthetic data generator.\", \"- [ ] /code: Run simulations, plot the convergence of primal/dual residuals, and compute the final solution error against a CVXPY benchmark.\", \"- [ ] /math: Verify that the empirical convergence rate observed in the code's output aligns with the theoretical bounds derived earlier.\", \"- [ ] /code: Validate that the primal and dual residuals fall below a tolerance (e.g., 1e-6) and that the algorithm terminates correctly.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"convex-optimization\", \"admm\", \"operator-splitting\"], \"acceptance_criteria\": [\"- The LaTeX document must present a clear, step-by-step derivation of the ADMM updates and a formal proof or rigorous argument for its convergence conditions.\", \"- The Python implementation must converge on a synthetic dataset, with primal and dual residuals decreasing monotonically to a value below 1e-5.\", \"- The final solution vector from the ADMM implementation must be within an L2 distance of 1e-4 of the solution found by a centralized CVXPY solver on the same data.\", \"- The final report must cite at least two relevant arXiv pre-prints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 319, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to understand how graph neural networks can replace classical force fields in molecular dynamics. Please explain the concept of SE(3) equivariance and demonstrate how it ensures energy conservation in a basic simulation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org graph neural networks for molecular dynamics SE(3) equivariance energy conservation\\\", mode=both)\", \"/math(Formally define SE(3) equivariance for a scalar potential energy function and derive the corresponding force vector field via the negative gradient. Discuss the properties of the potential's Hessian and its relationship to vibrational frequencies, touching on the eigenvalue problem for normal modes in chemistry.)\", \"/code(Implement a simple equivariant GNN potential using PyTorch Geometric for a toy atomic system. Train the model and run a short NVE simulation, validating on a GPU with CUDA, and plotting the total energy over time to check for conservation., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on GNN potentials. This informs the /math agent, which will formalize the core concept of equivariance and the derivation of forces. Finally, the /code agent provides a practical implementation and empirical validation of the theory, demonstrating energy conservation.\", \"expected_artifacts\": [\"A markdown report explaining SE(3) equivariance in GNNs for molecular dynamics.\", \"Mathematical derivations for the force calculation from the GNN's potential energy output.\", \"A Python notebook with a working implementation of a GNN-based MD simulation.\", \"A plot showing total energy conservation over the simulation trajectory.\", \"A list of at least two cited arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Research the fundamentals of using graph neural networks to model interatomic potentials, focusing on the concepts of equivariance and invariance required for physical systems.\", \"2. Formally define SE(3) equivariance and mathematically derive how applying it to a potential energy function guarantees that the resulting forces transform correctly under rotation and translation.\", \"3. Implement a basic equivariant GNN architecture (e.g., using relative distances and vector differences as features) to learn a potential for a simple, multi-atom system.\", \"4. Integrate the learned GNN potential into a simple velocity-Verlet integrator to run a microcanonical (NVE) ensemble simulation.\", \"5. Validate the model by plotting the kinetic, potential, and total energy over time to verify that the total energy is conserved within a small tolerance, which is a critical check for a valid physical model.\"], \"handoff_plan\": \"/general-search -> /math (verification: check that mathematical definitions of equivariance align with authoritative sources from the search; fallback: refine search queries for clearer formalisms) -> /code (verification: ensure the implementation correctly reflects the derived force equations and validate that the simulation conserves energy; fallback: debug the GNN architecture or training hyperparameters if energy drift is high) -> router QA\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers and tutorials on equivariant GNNs for MD.\", \"- [ ] /math: Formally derive the force expression from the GNN energy potential, explaining the role of equivariance.\", \"- [ ] /code: Implement the toy GNN potential and integrate it into a molecular dynamics simulation loop.\", \"- [ ] /code: [Verification] Plot the total system energy over time and confirm it remains nearly constant.\", \"- [ ] router QA: Review all generated artifacts for correctness, clarity, and consistency.\"], \"difficulty\": \"introductory\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"physics-informed-ml\", \"chemistry\"], \"acceptance_criteria\": [\"- The mathematical derivation of forces from the equivariant GNN potential is correct and clearly explained.\", \"- The Python code successfully runs a molecular dynamics simulation using the trained GNN without crashing.\", \"- The generated plot must show a total energy drift of less than 1% over the course of the simulation.\", \"- The final report cites at least two relevant arXiv papers by their ID.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as `(|E_final - E_initial| / |E_initial|) * 100` where E is the total energy (kinetic + potential) at the start and end of the simulation.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Compute the Root Mean Square Error between the GNN-predicted forces and forces from a reference calculation (e.g., DFT) on a held-out test set.\"}}", "categories": ["metrics_guidance"]}
{"index": 320, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the synthesis of certified programs for manipulating heap-based data structures. Can you develop a neural-symbolic framework that combines a graph neural network (GNN) for learning spatial predicates in separation logic with a symbolic solver for Hoare-style verification? The goal is to automatically synthesize a verified implementation of a concurrent skip list insertion algorithm, providing a formal proof trace and benchmarking its convergence rate against state-of-the-art deductive verifiers.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org graph neural networks separation logic program synthesis concurrent data structures\\\", mode=web)\", \"/math(Formalize the separation logic fragment for concurrent skip lists, defining the inductive spatial predicates and the Hoare triples for the insertion operation, ensuring convergence properties and causal consistency are captured.)\", \"/code(Implement a prototype in PyTorch using a Graph Attention Network (GAT) to predict separation logic frames from heap snapshots, and integrate it with a Z3-based symbolic solver to guide the proof search for the skip list insertion. Track proof convergence and AUPRC metrics., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey recent literature on neural approaches to separation logic, which is crucial for this research-level task. The findings inform the /math agent's formalization of the specific logic required for concurrent skip lists. Finally, the /code agent implements the proposed GNN-guided solver based on the mathematical specification, creating a concrete artifact for evaluation.\", \"expected_artifacts\": [\"A LaTeX document detailing the formal specification of the separation logic fragment and Hoare logic rules.\", \"A Python codebase implementing the GNN-guided neural-symbolic solver using PyTorch and Z3.\", \"A formal proof trace for the verified skip list insertion algorithm, generated by the tool.\", \"A performance report in JSON format comparing proof convergence rates and synthesis time against a baseline.\", \"A bibliography with at least two cited arXiv papers with their corresponding IDs.\"], \"thinking_outline\": [\"1. Survey existing literature on combining GNNs with formal methods, specifically for program verification using separation logic for concurrent programs.\", \"2. Develop the formal mathematical framework: define the syntax and semantics of the separation logic fragment tailored for concurrent heap-manipulating programs like skip lists.\", \"3. Implement the neural component (GNN) to learn and predict spatial predicates and the symbolic component (SMT solver) for deductive reasoning.\", \"4. Integrate the neural and symbolic components, ensuring the GNN's predictions correctly guide the solver's proof search.\", \"5. Validate the synthesized program and its proof trace by checking it against the formal specification defined in step 2. Verify that the Hoare triples hold for all paths.\", \"6. Benchmark the system's performance (proof success rate, synthesis time) and check for convergence issues or adversarial examples.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The literature review from /general-search provides foundational models and formalisms for /math. The formal specification from /math defines the precise logic and proof obligations for the /code agent to implement. The /code agent's implementation and generated proofs are then passed to the router for final QA. Verification: The /code agent must validate that its output satisfies the Hoare triples defined by /math. Fallback: If the symbolic solver fails to find a proof, the /code agent should log the failing state and backtrack, requesting refined guidance from the GNN.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 key papers on neural program synthesis for concurrent data structures using separation logic.\", \"- [ ] /math: Define the specific separation logic predicates and Hoare logic rules for a concurrent skip list insertion.\", \"- [ ] /code: Implement the GNN model in PyTorch for learning heap representations and predicting logical frames.\", \"- [ ] /code: Integrate the GNN with the Z3 SMT solver to construct the full neural-symbolic prover.\", \"- [ ] /math: Verify that the generated proof trace from the code correctly satisfies the formalized Hoare triples.\", \"- [ ] /code: Benchmark the system's proof success rate and synthesis time on a set of skip list variants and report AUPRC.\", \"- [ ] /general-search: Compile a bibliography of all referenced literature with their DOIs/arXiv IDs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic-methods\", \"program-synthesis\", \"formal-verification\"], \"acceptance_criteria\": [\"A complete, verifiable proof trace for the concurrent skip list insertion algorithm is generated.\", \"The implemented Python code is well-documented, includes unit tests, and successfully synthesizes the target program.\", \"The final report includes a formal definition of the separation logic fragment and cites at least 2 relevant arXiv papers.\", \"Performance metrics are reported, with a proof success rate exceeding 85% on the test suite.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 322, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a safe reinforcement learning agent for a 7-DOF robotic arm. The task involves formulating the problem as a Constrained Markov Decision Process (CMDP) and applying Lagrangian relaxation to enforce safety constraints on joint velocities. Please derive the primal-dual update rule for a policy gradient method like PPO, analyze the convergence guarantees, and implement the resulting algorithm in a simulated environment, ensuring the constraint violation rate remains below 1%.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Lagrangian relaxation constrained Markov decision process robotics control theory\\\", mode=rag)\", \"/math(Derive the Lagrangian dual for the CMDP objective, formulate the Karush-Kuhn-Tucker conditions for optimality, and derive the primal-dual gradient update rules for the policy and Lagrange multiplier. Analyze the convergence criteria.)\", \"/code(Implement the Lagrangian-PPO agent in Python with PyTorch and MuJoCo. Integrate runtime validation checks for joint velocity constraints and track key metrics like the safety violation rate and Lagrange multiplier convergence using CUDA for acceleration.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on safe RL and CMDPs, providing a theoretical foundation. This informs the /math agent's formal derivation of the optimization problem and update rules. Finally, the /code agent implements the derived algorithm, translating theory into a practical, verifiable simulation.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Lagrangian-PPO update rules as a PDF.\", \"- Python source code for the agent and simulation environment.\", \"- A final report summarizing the method, results, and citations (including arXiv IDs).\", \"- Plots showing cumulative reward, constraint violation rate, and Lagrange multiplier values over training epochs.\", \"- A metrics JSON file containing final performance and safety validation results.\"], \"thinking_outline\": [\"1. Formalize the robotics task as a Constrained Markov Decision Process (CMDP), clearly defining states, actions, rewards, and the cost function associated with the joint velocity safety constraints.\", \"2. Derive the Lagrangian relaxation of the CMDP objective function and formulate the corresponding saddle-point (min-max) optimization problem.\", \"3. Derive the gradient-based update rules for both the policy parameters (primal update) and the Lagrange multiplier (dual update).\", \"4. Implement the complete Lagrangian-PPO algorithm and the robotic arm simulation environment.\", \"5. Verify that the implemented gradient updates in the code correctly match the mathematical derivations from step 3.\", \"6. Validate the agent's performance by running multiple simulation trials, ensuring the average constraint violation rate is below the 1% threshold while maximizing task reward.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent's derivations must provide a clear blueprint for the code agent. The code agent's final metrics must be cross-referenced with the acceptance criteria; fallback: If the math derivation is intractable, the /general-search agent will be re-tasked to find alternative, simpler constrained RL algorithms. If the code fails verification, it will be returned to the /code agent with specific failing test cases.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Lagrangian methods for CMDPs in robotics.\", \"- [ ] /math: Produce a self-contained PDF with the full derivation of the primal-dual update rules.\", \"- [ ] /code: Implement the PPO agent with the Lagrangian dual update mechanism.\", \"- [ ] /code: Set up the MuJoCo simulation environment and instrument it for metric collection.\", \"- [ ] /math: Verify that the symbolic gradients derived align with the numerical implementation planned by /code.\", \"- [ ] /code: Validate the final trained agent against the safety constraint, ensuring the violation rate is below 1% over 100 evaluation episodes.\", \"- [ ] /router: Perform final QA check on all artifacts, ensuring consistency between the derivation, implementation, and final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"robotics\", \"control-theory\"], \"acceptance_criteria\": [\"- The agent must achieve a constraint violation rate of less than 1% on average over 100 evaluation rollouts.\", \"- The submitted code must be executable and reproduce the reported metrics.\", \"- The final report must clearly present the mathematical derivation and cite at least two relevant arXiv papers.\", \"- The plot of the Lagrange multiplier must show convergence or stable oscillation, indicating the constraint is being actively managed.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 323, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to formalize structured pruning for a transformer block as a constrained optimization problem. Specifically, I want to minimize a cross-entropy loss function subject to an L0-norm constraint on neuron groups, which is non-differentiable. I need to formulate this using a Lagrangian relaxation with Karush-Kuhn-Tucker (KKT) conditions and explore how recent research uses the Alternating Direction Method of Multipliers (ADMM) to solve this. Can you derive the full update rules, demonstrate a numerical implementation, and verify the convergence properties by analyzing the Hessian for negative eigenvalues?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'structured pruning' ADMM 'L0 norm' transformer convergence analysis\\\", mode=rag)\", \"/math(Formulate the L0-constrained pruning problem using a Lagrangian and derive the full ADMM update steps based on recent arXiv literature. Analyze the Karush-Kuhn-Tucker conditions for optimality and discuss the Hessian eigenvalue distribution of the augmented Lagrangian for convergence guarantees.)\", \"/code(Implement the derived ADMM algorithm in PyTorch with CUDA support to prune a pre-trained transformer block. Validate the implementation by tracking primal and dual residuals to check for convergence and compare final model sparsity against theoretical expectations., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art ADMM formulations for L0 pruning from recent literature. This context is then passed to /math for a rigorous formal derivation of the optimization steps and theoretical analysis. Finally, /code implements and validates the derived algorithm numerically, providing an empirical check on the theoretical findings.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the ADMM update rules for L0-constrained pruning, including the Lagrangian and KKT conditions.\", \"- A Python script implementing the pruning algorithm, with functions for tracking convergence metrics.\", \"- A final report (report.md) summarizing the method, results, and citing at least two relevant arXiv papers.\", \"- A JSON file (metrics.json) containing the primal/dual residual convergence data and final model sparsity.\", \"- Verification logs confirming the consistency between the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Research current state-of-the-art approaches for L0-norm constrained optimization in neural networks, specifically using ADMM for structured pruning.\", \"2. Formulate the specific problem of pruning a transformer block as a constrained optimization problem, defining the objective function and the L0 constraint precisely.\", \"3. Derive the augmented Lagrangian and the corresponding iterative update rules for the primal variables and dual variables based on the ADMM framework.\", \"4. Verify the derived KKT optimality conditions to ensure the theoretical formulation is sound and will lead to a valid solution.\", \"5. Implement the derived algorithm in code and test it on a small-scale transformer model, instrumenting the code to track convergence diagnostics.\", \"6. Validate that the code's output (e.g., convergence of residuals) aligns with the theoretical guarantees derived in the mathematical analysis.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Math agent verifies derivations against sources; Code agent validates implementation against math derivation; router performs final QA on artifacts. fallback: If convergence is not achieved, /math re-evaluates the penalty parameters or formulation based on /code's empirical results.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key arXiv papers on ADMM for L0-norm neural network pruning.\", \"- [ ] /math: Formulate the augmented Lagrangian for the transformer pruning problem.\", \"- [ ] /math: Derive the complete set of ADMM update equations and state the KKT conditions for optimality.\", \"- [ ] /code: Implement the ADMM algorithm in PyTorch, targeting a single transformer block.\", \"- [ ] /code: Add logging to track primal and dual residuals over iterations and plot the results.\", \"- [ ] /math: verify that the implemented update rules in the code exactly match the derived equations.\", \"- [ ] /code: validate that the empirical convergence behavior observed matches theoretical expectations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"deep-learning-theory\", \"lagrangian-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ADMM updates must be complete and self-contained.\", \"- The Python code must execute, demonstrate convergence of residuals below a threshold of 1e-4, and achieve a pre-defined sparsity target (e.g., 50%).\", \"- The final report must correctly cite at least two relevant arXiv papers with IDs and links.\", \"- The primal residual convergence rate metric must be correctly computed and reported in metrics.json.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 324, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive and implement the Natural Policy Gradient algorithm using a Kronecker-factored approximation for the Fisher Information Matrix (FIM). Your implementation must be benchmarked on a continuous control environment from OpenAI Gym, specifically focusing on the convergence rate and sample complexity compared to vanilla REINFORCE. Also, analyze the computational overhead introduced by the K-FAC approximation and its impact on wall-clock time.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Natural Policy Gradient Kronecker-factored approximation Fisher Information Matrix convergence\\\", mode=web)\", \"/math(Derive the Natural Policy Gradient update rule using the Kronecker-factored (K-FAC) approximation of the Fisher Information Matrix, ensuring the derivation handles the continuous control theory context and respects convergence properties for stochastic optimization.)\", \"/code(Implement the K-FAC Natural Policy Gradient agent in Python using PyTorch for a continuous control task like Pendulum-v1, ensuring proper tracking of the reward convergence and including CUDA optimizations for matrix inversions.)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature on K-FAC for Natural Policy Gradients. This informs the /math agent's formal derivation of the update rule. Finally, the /code agent implements the derived algorithm and performs the required benchmarking.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the K-FAC NPG update rule.\", \"A standalone Python script implementing the K-FAC NPG agent and the benchmarking logic.\", \"A summary report in Markdown or JSON format containing the final performance metrics and convergence plots.\", \"A list of cited arXiv papers with their corresponding IDs.\", \"A verification log comparing the code's matrix operations against the mathematical derivation.\"], \"thinking_outline\": [\"1. Research the state-of-the-art literature on Natural Policy Gradients and Kronecker-factored Approximate Curvature (K-FAC) to understand the theoretical underpinnings and common implementation patterns.\", \"2. Formally derive the NPG update rule incorporating the K-FAC approximation for the Fisher Information Matrix, specializing the derivation for a Gaussian policy used in continuous control.\", \"3. Implement the agent in PyTorch, focusing on the efficient computation of the Kronecker factors and their inverses for each layer of the policy network.\", \"4. Validate the K-FAC implementation by checking the dimensions and numerical stability of the computed factor matrices and their inverses against theoretical expectations on a small, toy network.\", \"5. Design and run a comparative experiment on a continuous control environment (e.g., Pendulum-v1) to benchmark the sample efficiency and convergence speed of K-FAC NPG against a vanilla REINFORCE baseline.\", \"6. Audit the final results to verify that observed performance improvements are statistically significant and that the computational overhead analysis aligns with the complexity of the K-FAC updates.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: router will cross-validate the core K-FAC inverse logic in the Python code against the formal derivation from the /math agent; fallback: if the implementation diverges, the /math agent will be asked to provide a step-by-step numerical example for a 2-layer network).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on K-FAC for Natural Policy Gradients.\", \"- [ ] /math: Produce a full, self-contained derivation of the K-FAC approximated natural gradient.\", \"- [ ] /code: Implement the K-FAC NPG agent and the REINFORCE baseline agent in PyTorch.\", \"- [ ] /code: Set up the benchmarking experiment on the specified OpenAI Gym environment.\", \"- [ ] /math: Verify that the code's implementation of Kronecker factor inversion aligns with the derived mathematical formulas.\", \"- [ ] /code: Run the benchmark, collect training logs, and generate convergence plots comparing the two agents.\", \"- [ ] /code: Analyze the wall-clock time per update step for both agents and report the overhead.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"approximate-second-order-optimization\"], \"acceptance_criteria\": [\"- The implemented K-FAC NPG agent must achieve a higher average return or converge in significantly fewer episodes than the REINFORCE baseline on the Pendulum-v1 task.\", \"- The mathematical derivation must be complete, clearly defining all terms and steps from the basic policy gradient theorem to the final K-FAC update rule.\", \"- The final report must include plots showing the learning curves (average return vs. episodes) for both agents, with appropriately labeled axes.\", \"- The submission must cite at least two relevant arXiv papers with IDs, as specified in the citation policy.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 325, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the information-theoretic lower bound on the bit-rate required for robustly watermarking a generative language model's output distribution against adversarial paraphrasing attacks. Formulate this using rate-distortion theory with a Kullback-Leibler divergence distortion measure, and structure an optimization problem to maximize the watermark's mutual information while constraining the KL divergence and ensuring the model's perplexity degradation remains below a specified threshold. Then, implement a prototype to estimate this bound.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org rate-distortion theory llm watermarking adversarial robustness Kullback-Leibler divergence mutual information\\\", mode=rag)\", \"/math(Derive the rate-distortion function R(D) for the LLM watermarking problem, defining the source distribution, the adversarial channel, and the KL divergence as the distortion metric. Formulate the Lagrangian for the constrained optimization of mutual information.)\", \"/code(Implement a JAX-based simulation to estimate the derived rate-distortion lower bound for a small transformer. Use Monte Carlo integration to approximate mutual information and KL divergence, tracking convergence and validating against a Bayesian model., using Python)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature on information-theoretic AI safety. The math agent then formalizes the problem by deriving the core theoretical bounds. Finally, the code agent implements a simulation to empirically validate the derived theory and estimate the bounds for a practical model.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the rate-distortion lower bound in a PDF document.\", \"- A Python script implementing the numerical estimation of the bound.\", \"- A final report summarizing the methodology, results, and including plots of the Rate vs. Distortion curve.\", \"- A bibliography file with at least two cited arXiv papers.\", \"- A JSON file containing the computed metrics.\"], \"thinking_outline\": [\"1. Survey recent literature on using information theory, specifically rate-distortion theory, for robust watermarking in generative models.\", \"2. Formulate the watermarking process as a communication channel problem, defining the source (original model distribution), encoder (watermark inserter), channel (adversarial perturbation), decoder (watermark detector), and distortion measure (KL divergence).\", \"3. Derive the rate-distortion function R(D) which provides the minimum achievable bit-rate for a given maximum tolerable distortion D. This will involve setting up and solving a constrained optimization problem.\", \"4. Implement a numerical simulation to estimate R(D) for a pre-trained model like GPT-2-small. This requires careful implementation of estimators for high-dimensional mutual information and KL divergence.\", \"5. Validate the numerical estimators for stability and convergence. Check that the resulting R(D) curve is convex and non-increasing as theoretically expected.\", \"6. Audit the final mathematical derivation against the code's logic to ensure the implementation faithfully represents the theory.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The derivation from /math must provide the exact objective function and constraints implemented by /code. The code must output metrics that directly correspond to the variables in the derivation; fallback: If derivation is not implementable, /math must simplify the model, e.g., by assuming a Gaussian mixture model approximation, and re-submit).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent arXiv papers on rate-distortion theory applied to generative model watermarking or steganography.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the rate-distortion function R(D) lower bound for the specified watermarking problem.\", \"- [ ] /code: Write a Python script using JAX or PyTorch to simulate the watermarking process and estimate the R(D) curve.\", \"- [ ] /code: Implement and run unit tests for the mutual information and KL divergence estimators to ensure numerical stability.\", \"- [ ] /math: Verify that the objective function and constraints implemented in the code from /code are mathematically equivalent to the final derived expressions.\", \"- [ ] /code: Generate a plot of the estimated rate-distortion curve and a table of perplexity values for different distortion levels.\", \"- [ ] /router: Perform final QA, checking for consistency between the derivation, code, report, and acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"rate-distortion-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the rate-distortion lower bound is self-contained, rigorous, and clearly defines all terms.\", \"- The provided Python code is executable and successfully generates a plot showing a convex, non-increasing R(D) curve.\", \"- The final report cites at least two relevant arXiv papers with IDs and provides a clear interpretation of the results in the context of AI safety.\", \"- The computed KL divergence in the metrics output is non-negative.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 327, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a comprehensive analysis of differentiable architecture search (DARTS), focusing on the mathematical justification for its bilevel optimization framework. Please derive the implicit gradient computation for the architecture parameters, which involves the inverse Hessian-vector product. Then, implement a version of the DARTS search space and algorithm in PyTorch, using a finite difference approximation for the Hessian-vector product, and validate its convergence properties on a benchmark dataset like CIFAR-10. The goal is to critically examine the stability and performance implications of this second-order approximation in the context of AutoML pipelines.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org DARTS bilevel optimization AutoML Hessian approximation convergence\\\", mode=both)\", \"/math(Derive the architecture gradient update rule for the DARTS bilevel optimization problem. Detail the chain rule application and the resulting inverse Hessian-vector product, analyzing potential eigenvalue impacts on convergence.)\", \"/code(Implement the DARTS supernet and search loop in PyTorch with CUDA support. Use a finite difference method to approximate the Hessian-vector product needed for the architecture update, targeting a competitive AUPRC on a downstream validation task., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the work in seminal papers. Next, /math is used to formally derive the core optimization logic, which is a prerequisite for a correct implementation. Finally, /code translates the mathematical derivation into a working, verifiable model.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the DARTS gradient approximation in LaTeX.\", \"- A Python script implementing the DARTS search algorithm.\", \"- A final report (PDF) summarizing the derivation, implementation, results, and including at least two arXiv citations.\", \"- A JSON file containing final validation metrics and plots of search-phase convergence.\", \"- A verification log confirming alignment between the mathematical derivation and the code's gradient computation.\"], \"thinking_outline\": [\"1. Use /general-search to locate the original DARTS paper and subsequent analyses on its convergence issues.\", \"2. Formulate the bilevel optimization problem and use /math to derive the expression for the architecture gradient, highlighting the inverse Hessian-vector product term.\", \"3. Implement the supernet and the two-stage training process in PyTorch, paying close attention to the numerical approximation of the Hessian-vector product.\", \"4. Validate the implementation by running the search process and observing if the architecture parameters converge towards a stable, discrete architecture.\", \"5. Train the discovered architecture from scratch and verify its performance against published DARTS results on CIFAR-10.\", \"6. Audit the final report to ensure the mathematical derivation is clear, the code is well-documented, and the experimental results support the conclusions.\"], \"handoff_plan\": \"/general-search -> /math (verification: key papers provide the foundational equations) -> /code (verification: implementation must match the derived gradient formula; fallback: if numerical instability occurs, revert to /math to analyze Hessian properties) -> router QA (verification: final metrics and artifacts align with acceptance criteria).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize the key contributions of the original DARTS paper (Liu et al., 2018) and one follow-up work on its stability.\", \"- [ ] /math: Derive the complete formula for the architecture parameter gradient using implicit differentiation.\", \"- [ ] /code: Implement the CIFAR-10 supernet structure and the bilevel optimization training loop.\", \"- [ ] /code: Implement the finite difference approximation for the Hessian-vector product.\", \"- [ ] /math: Verify that the logic in the /code agent's gradient computation function correctly implements the derived mathematical formula.\", \"- [ ] /code: Run the architecture search for 50 epochs, then train the discovered architecture and report its final test accuracy.\", \"- [ ] /code: Generate plots showing the evolution of architecture parameters (alphas) during the search phase.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-architecture-search\", \"automl\", \"bilevel-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the architecture gradient is complete and formally correct.\", \"- The implemented search algorithm produces a valid, discrete architecture that achieves >95% test accuracy on CIFAR-10 after retraining.\", \"- The final report correctly cites at least two relevant arXiv papers with their IDs and analyzes the search convergence.\", \"- The code includes unit tests for the gradient approximation module.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 329, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a post-training quantization (PTQ) strategy for a large language model, specifically focusing on Hessian-aware methods like OBS (Optimal Brain Surgeon). Derive the second-order Taylor approximation for the loss function, formulate the weight quantization as a constrained quadratic optimization problem, and implement a JAX/CUDA-accelerated version that leverages the inverse Hessian to determine weight saliency. The goal is to maximize the compression ratio while keeping the perplexity degradation on the WikiText-103 benchmark below a 5% threshold.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian-aware quantization transformer 'Optimal Brain Surgeon' 'second-order methods'\\\", mode=web, context=\\\"Investigate foundational papers on Optimal Brain Surgeon (OBS) and Hessian-based quantization to establish the theoretical framework, focusing on the derivation of the inverse Hessian update rule and its connection to Bayesian learning principles.\\\")\", \"/math(context=\\\"Derive the quadratic approximation of the loss function for a transformer layer's weights, formulate the quantization problem using this approximation, and show how the inverse Hessian eigenvalue spectrum informs weight saliency for minimal perplexity impact.\\\")\", \"/code(context=\\\"Implement the derived Hessian-aware quantization algorithm in JAX with CUDA acceleration. The implementation must include a function to compute or approximate the Hessian inverse for a given layer and apply the quantization map. Validate against a pre-trained GPT-2 model on WikiText-103, measuring perplexity and spectral norm.\\\", using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative literature on Hessian-aware quantization, ensuring the subsequent steps are grounded in state-of-the-art research. Then, /math is used to formally derive the optimization problem, which is a prerequisite for a correct implementation. Finally, /code implements and validates the derived algorithm, providing empirical evidence of its efficacy.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the Hessian-based weight saliency metric.\", \"A Python script in JAX implementing the post-training quantization algorithm.\", \"A results report in Markdown comparing the perplexity and model size of the quantized vs. original model.\", \"A bibliography with at least 2 cited arXiv papers on second-order optimization methods for neural networks.\", \"A verification log confirming the alignment between the mathematical derivation and the code's logic.\"], \"thinking_outline\": [\"1. Survey seminal literature on second-order methods for model compression, specifically Optimal Brain Surgeon (OBS) and related Hessian-aware techniques.\", \"2. Formally derive the second-order Taylor expansion of the loss function with respect to the model weights.\", \"3. Formulate the weight quantization problem as a constrained optimization problem, aiming to minimize the loss increase subject to a sparsity or quantization bit-width constraint.\", \"4. Implement the algorithm in JAX, paying close attention to numerical stability when computing or approximating the inverse Hessian.\", \"5. Validate the implementation by applying it to a pre-trained transformer model and evaluating the trade-off between perplexity and compression on a standard benchmark.\", \"6. Audit the results to check that the perplexity degradation is within the specified 5% tolerance and that the code correctly reflects the derived mathematical formulas.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent will verify that the search results provide a sufficient theoretical basis for the derivation. The code agent will then validate that its implementation matches the formal derivation from the math agent. The final report and code will be reviewed by the router for alignment with the acceptance criteria. Fallback: If the derivation is not computationally tractable, the math agent must propose a low-rank approximation to the Hessian and re-derive.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Hessian-aware quantization (e.g., OBS, OBD) from arXiv.\", \"- [ ] /math: Derive the second-order approximation for the LLM loss function and formulate the constrained optimization problem.\", \"- [ ] /math: Explicitly show the update rule for weights based on the inverse Hessian saliency measure.\", \"- [ ] /code: Implement the quantization algorithm in JAX, including a module for Hessian inverse approximation.\", \"- [ ] /code: Benchmark the quantized model's perplexity on WikiText-103 and report the compression ratio.\", \"- [ ] /math: Verify that the JAX implementation of the Hessian update rule is a correct translation of the derived mathematical formula.\", \"- [ ] /code: Validate that the reported perplexity degradation is below the 5% threshold.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"llm-quantization\", \"hessian-methods\", \"numerical-optimization\"], \"acceptance_criteria\": [\"The mathematical derivation of the second-order approximation and weight saliency must be complete and self-contained.\", \"The JAX implementation must successfully quantize a pre-trained GPT-2 model and run without numerical errors.\", \"The final report must show a perplexity degradation of less than 5% on the WikiText-103 validation set compared to the baseline model.\", \"The model compression ratio must be explicitly reported and be at least 2x.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 331, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Variational Quantum Eigensolver (VQE) to find the ground state energy of the Lithium Hydride (LiH) molecule. Please derive the qubit Hamiltonian using a Jordan-Wigner transformation, construct a Unitary Coupled Cluster Singles and Doubles (UCCSD) ansatz, and then detail the implementation using the parameter-shift rule for analytic gradient calculation to optimize the circuit parameters, ensuring the final energy convergence is below 1 milliHartree of the FCI value.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org VQE UCCSD ansatz LiH Hamiltonian 'parameter shift rule' chemistry\\\", mode=rag)\", \"/math(Derive the qubit Hamiltonian for LiH via Jordan-Wigner mapping and formalize the energy expectation value and its analytic gradient using the parameter-shift rule for the UCCSD ansatz, checking for eigenvalue consistency.)\", \"/code(Implement the VQE algorithm in Python with Pennylane, defining the UCCSD quantum circuit, the derived LiH Hamiltonian, and an Adam optimizer that uses analytic gradients. Track energy convergence and final eigenvalue.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative papers on VQE for quantum chemistry, focusing on the UCCSD ansatz and gradient methods. This research grounds the subsequent /math agent's task of deriving the specific Hamiltonian and gradient equations for LiH. Finally, the /code agent implements the derived mathematical framework in a quantum computing library, leveraging the formulas for an efficient and accurate simulation.\", \"expected_artifacts\": [\"- A LaTeX document detailing the derivation of the LiH qubit Hamiltonian and the analytic gradient expressions for the VQE cost function.\", \"- A Python script implementing the full VQE simulation for LiH, including the UCCSD ansatz and optimizer.\", \"- A results summary containing a plot of energy vs. optimization iteration and a JSON file with the final ground state energy and convergence metrics.\", \"- A list of at least two cited arXiv preprints that informed the ansatz construction and optimization strategy.\"], \"thinking_outline\": [\"1. Research established VQE implementations for small molecules like LiH, focusing on the choice of ansatz (UCCSD) and efficient gradient computation techniques to avoid barren plateaus.\", \"2. Formally derive the electronic structure Hamiltonian for LiH and map it to a qubit Hamiltonian using the Jordan-Wigner transformation.\", \"3. Derive the analytical expression for the gradient of the energy expectation value with respect to the ansatz parameters using the parameter-shift rule.\", \"4. Validate the mathematical derivations by cross-referencing with established literature and checking the commutation relations of the Hamiltonian terms.\", \"5. Implement the VQE algorithm, including the quantum circuit for the UCCSD ansatz and the classical optimization loop, and run the simulation to find the ground state energy.\", \"6. Verify the final computed ground state energy against known Full Configuration Interaction (FCI) benchmark values for LiH to ensure accuracy is within the required chemical precision.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal derivation) -> /code (implementation & simulation) -> router QA (verification: check if code's final energy matches theoretical benchmarks from search and math derivation is correctly implemented; fallback: if energy is inaccurate, revisit math derivation for errors or refine search for more specific ansatz parameters).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on VQE for quantum chemistry, focusing on UCCSD ansatz and barren plateau mitigation.\", \"- [ ] /math: Derive the LiH qubit Hamiltonian and the parameter-shift rule expressions for the cost function gradient.\", \"- [ ] /code: Implement the VQE simulation using Pennylane, including the quantum circuit, Hamiltonian, and optimizer.\", \"- [ ] /code: Plot the energy convergence curve over optimization iterations.\", \"- [ ] /math: Verify that the implemented Hamiltonian's matrix representation is Hermitian.\", \"- [ ] /code: Verify that the final ground state energy is within 1.6e-3 Hartree of the FCI benchmark value (~ -7.882 Hartree).\", \"- [ ] /router: Review all generated artifacts for consistency, accuracy, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"quantum-machine-learning\", \"variational-algorithms\"], \"acceptance_criteria\": [\"- The final computed ground state energy for LiH must be within 1 milliHartree of the accepted FCI value.\", \"- The provided Python code must be executable and reproduce the reported energy value.\", \"- The mathematical derivation must clearly show the mapping from the fermionic Hamiltonian to the qubit Hamiltonian and the formulation of the analytic gradient.\", \"- At least two relevant research papers from arXiv must be cited.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 332, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to explore neural-symbolic program synthesis for verifying inductive properties of functional programs. Please derive the mathematical formulation for a synthesis-via-sketching approach that uses a neural model to fill holes ('??') in a program sketch, constrained by Hoare logic pre/post-conditions. Then, implement a prototype in Python that attempts to synthesize a recursive list reversal function by searching over a combinatorial space of operators, using a simple Bayesian model as a heuristic guide and the Z3 SMT solver for verification of each candidate program. Analyze the convergence properties and search space complexity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:acm.org neural program synthesis sketching Hoare logic SMT solver\\\", mode=rag, includes cybersecurity)\", \"/math(Formalize the program synthesis problem as a search over a discrete space of Abstract Syntax Trees, deriving the Hoare logic verification conditions for inductive proofs and analyzing the worst-case complexity of the search given a finite operator set., includes cybersecurity)\", \"/code(Implement a Python prototype using the Z3 SMT solver to check verification conditions. The synthesizer should iteratively propose program sketches, using a simple Bayesian heuristic to guide the search, and validate correctness, reporting convergence metrics.)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art techniques for neural-symbolic synthesis to ground the approach. The /math agent then formalizes the problem, deriving the necessary logic and complexity analysis. Finally, the /code agent implements the synthesizer to provide an empirical proof-of-concept and performance validation.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical formulation of the synthesis problem, including the Hoare logic verification conditions.\", \"A self-contained Python script implementing the program synthesizer with Z3 solver integration.\", \"A summary report (PDF or Markdown) presenting the experimental results, including synthesis success rate and performance metrics.\", \"A BibTeX file (`references.bib`) containing citations for the surveyed literature.\", \"A log file of the Z3 solver interactions during a successful synthesis run.\"], \"thinking_outline\": [\"1. Research existing neural-symbolic synthesis frameworks, focusing on sketch-based methods and SMT solver integration to establish a theoretical foundation.\", \"2. Formulate the synthesis problem mathematically, defining the program sketch representation, the search space, and the Hoare logic-based verification objective function.\", \"3. Implement the core synthesis loop in Python, integrating the Z3 SMT solver to check the validity of generated program candidates against the derived verification conditions.\", \"4. Validate the implementation by testing its ability to synthesize a known simple recursive function, like list reversal, and verify that the generated program is provably correct.\", \"5. Analyze the performance of the synthesizer, measuring metrics like synthesis time, number of SMT queries, and the size of the explorable search space.\", \"6. Audit the final mathematical derivation against the code's logic to ensure the implementation correctly reflects the theory.\"], \"handoff_plan\": \"/general-search results provide foundational papers -> /math uses these papers to formalize the problem. The derivation is then handed to -> /code for implementation. The code's output is sent to -> router QA for final review. (Verification: /code's implementation must pass unit tests derived from the /math specification; Fallback: If SMT solver fails to prove a correct program, /math must revisit the verification conditions or constraints).\", \"todo_list\": [\"- [ ] /general-search: Survey recent literature (arXiv, ACM) on neural program synthesis via sketching.\", \"- [ ] /math: Derive the formal specification for the synthesis problem and the Hoare logic verification conditions.\", \"- [ ] /code: Implement the Python-based synthesizer, including the search loop and Z3 SMT solver integration.\", \"- [ ] /code: Develop unit tests for the synthesizer using the list reversal target function.\", \"- [ ] /math: Verify that the logical formulas passed to the Z3 solver in the code correctly represent the derived verification conditions.\", \"- [ ] /code: Validate that the synthesized program is correct and passes all test cases and formal proofs.\", \"- [ ] /router: Review final report, code, and derivation for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"program-synthesis\", \"theorem-proving\", \"formal-methods\"], \"acceptance_criteria\": [\"The mathematical derivation of the verification conditions based on Hoare logic is sound and complete.\", \"The Python implementation successfully synthesizes the target recursive list reversal function and proves its correctness using the Z3 solver.\", \"The final report accurately presents the methodology, results, and an analysis of the synthesizer's performance, including synthesis time and SMT query count.\", \"At least two relevant academic papers are cited correctly.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 334, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the connection between the flatness of a neural network's loss landscape and its generalization capabilities. Specifically, derive a formal relationship between the spectral norm of the Hessian matrix (maximum eigenvalue \u03bb_max) and the generalization gap. Then, empirically validate this by implementing an algorithm to estimate \u03bb_max for a pre-trained ResNet on CIFAR-10 and correlating it with out-of-sample performance, referencing techniques from recent literature on Sharpness-Aware Minimization (SAM).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian spectral analysis generalization gap deep learning eigenvalue sharpness\\\", mode=rag)\", \"/math(Derive a PAC-Bayesian or similar generalization bound that explicitly incorporates the Hessian's spectral norm, leveraging Chebyshev inequality and convergence properties of stochastic optimizers.)\", \"/code(Implement a power iteration method in PyTorch with CUDA support to estimate the top Hessian eigenvalue for a ResNet model, and plot its correlation with test set accuracy under varying Bayesian regularization strengths., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey current literature on Hessian analysis, as requested. This research informs the formal derivation by /math. Finally, /code provides empirical validation for the derived theory, a critical step for robust scientific inquiry.\", \"expected_artifacts\": [\"- A LaTeX document containing the full mathematical derivation of the generalization bound.\", \"- A Python script implementing the Hessian eigenvalue estimation and the correlation analysis.\", \"- A summary report in Markdown that synthesizes the theoretical and empirical findings, including plots and at least 3 citations.\", \"- A verification log detailing the cross-checks between the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Conduct a literature review to identify state-of-the-art theoretical frameworks connecting Hessian spectra to generalization bounds in deep learning.\", \"2. Formulate a precise mathematical theorem and derive its proof, showing how the maximum eigenvalue of the Hessian bounds the generalization error.\", \"3. Implement a numerically stable algorithm, like the power method or Lanczos algorithm, to estimate the Hessian's top eigenvalue for a large-scale neural network.\", \"4. Verify the mathematical derivation's logical consistency and check for alignment with established theorems from the literature search.\", \"5. Execute the code to gather empirical data, plotting the estimated maximum eigenvalue against the model's test accuracy to visualize the theoretical relationship.\", \"6. Audit the code's implementation of the power iteration method against its mathematical definition and validate its convergence properties on a known test case.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: search results provide the theoretical basis for the /math derivation, which in turn specifies the quantity that /code must compute; fallback: if the derivation is inconsistent with literature, /math must re-evaluate its approach based on alternative papers found by /general-search).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least 3 recent arXiv papers on Hessian eigenvalues and generalization.\", \"- [ ] /math: Derive the generalization bound incorporating the Hessian's spectral norm, clearly stating all assumptions.\", \"- [ ] /code: Implement the power iteration method to estimate the top eigenvalue of the Hessian-vector product without materializing the full Hessian.\", \"- [ ] /code: Train several models or use checkpoints to create a spectrum of test accuracies and corresponding max eigenvalues.\", \"- [ ] /math: Verify the final derived inequality aligns with the theoretical claims in the sourced literature.\", \"- [ ] /code: Validate the eigenvalue estimation code by comparing its output on a small, known matrix to the result from `torch.linalg.eigvalsh`.\", \"- [ ] /router: Perform a final QA check, ensuring the report's conclusion is supported by both the derivation and the empirical plot.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The submitted mathematical derivation must be logically sound and self-contained.\", \"- The Python code must execute without errors and produce a plot showing a clear correlation (or lack thereof) between the max eigenvalue and test accuracy.\", \"- The final report must correctly cite at least 3 relevant papers and accurately connect the theoretical and empirical results.\", \"- The verification log must confirm that the code's logic matches the mathematical formulation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 335, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the properties of strict saddle points in non-convex optimization landscapes. Specifically, derive the conditions under which a perturbed gradient descent algorithm can escape such points, focusing on the role of the Hessian's minimum eigenvalue. Can you formalize a proof for the convergence rate and implement a numerical simulation to validate the theoretical escape time, considering a high-dimensional Rosenbrock-like function with tunable saddle point structures?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org perturbed gradient descent strict saddle point Hessian minimum eigenvalue convergence\\\", mode=web)\", \"/math(Derive the theoretical escape time for perturbed gradient descent from a strict saddle point, establishing the dependence on the minimum Hessian eigenvalue and the noise variance for a guaranteed convergence rate.)\", \"/code(Implement a Python simulation using JAX to visualize the escape trajectory of perturbed gradient descent on a non-convex function, numerically validate the Hessian eigenvalue condition at the saddle, and plot empirical vs. theoretical convergence.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational literature on escaping saddle points. This research informs the /math agent's formal derivation of the escape time theorem. Finally, the /code agent implements a simulation to numerically validate the theoretical findings from the /math agent, providing empirical evidence.\", \"expected_artifacts\": [\"A formal proof in LaTeX detailing the escape dynamics of perturbed gradient descent from strict saddle points.\", \"A Python script implementing the simulation, including the non-convex test function and Hessian analysis.\", \"Generated plots comparing the empirical escape times from the simulation against the theoretical predictions.\", \"A summary report including the derivation, simulation results, and at least 2 citations from arXiv.\"], \"thinking_outline\": [\"1. Review the literature on non-convex optimization to formalize the definition of a strict saddle point and the role of the Hessian matrix.\", \"2. Formulate a theorem for the escape time of perturbed gradient descent, deriving the relationship between the Hessian's minimum negative eigenvalue, noise magnitude, and problem dimensions.\", \"3. Implement the perturbed gradient descent algorithm and a suitable non-convex test function with controllable saddle points.\", \"4. Numerically compute the Hessian and its eigenvalues at the saddle point to validate that it meets the 'strict' criteria defined in the derivation.\", \"5. Run simulations to measure the empirical escape time and verify that it aligns with the theoretical bounds derived in step 2.\", \"6. Synthesize the findings into a report, cross-checking the mathematical derivation against the simulation's empirical results for consistency.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code output matches theoretical bounds; fallback: if mismatch, /math re-audits assumptions and /code checks implementation for bugs).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers (e.g., Jin et al., 2017) on escaping saddle points.\", \"- [ ] /math: Formally derive the escape time theorem based on findings, clearly stating all assumptions about the Hessian and noise.\", \"- [ ] /code: Implement the perturbed gradient descent algorithm and a non-convex test function in Python/JAX.\", \"- [ ] /code: Add functionality to numerically calculate the Hessian and its minimum eigenvalue at saddle points.\", \"- [ ] /math: Verify that the symbolic derivation of the Hessian in the proof matches the numerical implementation in the code.\", \"- [ ] /code: Run simulation, collect escape time data, and generate plots comparing empirical results with the mathematical theorem.\", \"- [ ] /router: Perform final QA review, ensuring the proof, code, and plots are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-free-methods\"], \"acceptance_criteria\": [\"The submitted mathematical proof for the escape rate is logically sound and complete.\", \"The Python simulation correctly implements perturbed gradient descent and the specified non-convex function.\", \"The generated plots show a clear comparison between theoretical bounds and empirical results, with a discrepancy of less than 15%.\", \"The final report includes citations for at least 2 relevant arXiv preprints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 337, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the equivariant message passing update rule for a steerable E(3) graph neural network intended for molecular dynamics simulations, ensuring the formulation explicitly conserves Hamiltonian energy. Provide a formal proof of its rotational equivariance using tensor calculus and concepts from differential geometry. Then, implement a prototype to simulate a simple N-body system, empirically validating the energy conservation property against a baseline simulation's energy drift over time.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org E(3) equivariant GNN energy conservation Hamiltonian molecular dynamics chemistry\\\", mode=web)\", \"/math(Derive the continuous-time Hamiltonian dynamics formulation for an E(3)-equivariant Graph Neural Network, ensuring the model conserves total system energy and is consistent with Bayesian interpretations of the force field.)\", \"/code(Implement the derived equivariant message passing layers and a symplectic integrator using PyTorch and CUDA. Validate the model by checking for energy drift on a small molecular system, ensuring convergence to a stable trajectory., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey foundational literature on equivariant GNNs and Hamiltonian mechanics. This informs the /math agent's formal derivation and proof of the energy-conserving update rule. Finally, the /code agent implements the derived mathematics and performs empirical validation, which is crucial for verifying the theoretical claims in a practical setting.\", \"expected_artifacts\": [\"- A PDF report containing the full mathematical derivation of the energy-conserving update rule and the proof of E(3) equivariance, written in LaTeX.\", \"- Python source code for the GNN model, integrator, and simulation script.\", \"- A plot (`energy_drift.png`) visualizing the system's total energy over time, comparing the GNN model to a baseline.\", \"- A `metrics.json` file reporting the final mean energy drift and equivariance error.\", \"- A bibliography with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Research state-of-the-art E(3) equivariant graph networks and their application to Hamiltonian systems and molecular dynamics, focusing on energy conservation principles.\", \"2. Formulate the mathematical framework, defining the Hamiltonian for the molecular system and expressing the GNN architecture's constraints required to create a symplectic map.\", \"3. Derive the specific message-passing functions using tensor calculus to ensure they are E(3) equivariant and that the resulting force field is conservative.\", \"4. Validate the derivation by formally proving that a rotation applied to the input coordinates results in an identical rotation of the output force vectors.\", \"5. Implement the derived equations in a deep learning framework, including a symplectic integrator (e.g., Velocity Verlet) to evolve the system state.\", \"6. Verify the implementation's physical correctness by running a simulation and measuring the energy drift, ensuring it remains within acceptable numerical error bounds.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The final report's derivations must match the Python implementation's logic; fallback: If energy drift is high, the /code agent re-checks the integrator implementation and numerical precision before escalating back to /math to audit the derivation for potential errors).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Hamiltonian/Lagrangian GNNs for physical simulations.\", \"- [ ] /math: Derive the energy-conserving message passing update rule and document it using LaTeX.\", \"- [ ] /math: Formulate and write the formal proof of rotational equivariance for the derived rule.\", \"- [ ] /code: Implement the equivariant layers and a Velocity Verlet symplectic integrator in PyTorch.\", \"- [ ] /code: Run the N-body simulation and generate a plot of energy drift vs. time.\", \"- [ ] /math: Verify that the mathematical derivation in the final report is consistent with the code's logic and tensor operations.\", \"- [ ] /code: Audit the implementation for numerical stability and correctness against the derived equations, especially the force calculation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"equivariant-networks\", \"differential-geometry\"], \"acceptance_criteria\": [\"- The provided proof of E(3) equivariance must be mathematically rigorous and self-contained.\", \"- The implemented model must demonstrate a mean energy drift at least one order of magnitude smaller than a non-conservative baseline integrator over a 1000-step simulation.\", \"- The final code must be runnable, well-documented, and reproduce the energy drift plot presented in the report.\", \"- The report correctly cites at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 338, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the optimization landscape of deep neural networks. Can you derive the relationship between the Hessian's top eigenvalue and training stability, specifically in the context of Sharpness-Aware Minimization (SAM)? Please provide a formal proof connecting the spectral norm of the Hessian matrix to the convergence rate analysis, and then implement a method in JAX to empirically track the Hessian's spectral density during training of a ResNet on CIFAR-10. Also, how does this relate to the generalization gap?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian spectral analysis deep learning Sharpness-Aware Minimization convergence rate\\\", mode=rag)\", \"/math(Derive the formal proof linking the Hessian's maximum eigenvalue to the Lipschitz constant of the gradient, and analyze its impact on the convergence guarantees for gradient-based optimizers like SAM. Reference at least one key theorem from the arxiv search.)\", \"/code(Implement a JAX-based routine using Lanczos iteration to estimate the top eigenvalue of the loss Hessian for a ResNet model during training. Track this metric alongside training loss and validation accuracy to check for correlation with convergence behavior., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to establish a theoretical foundation from recent arXiv literature on Hessian analysis and SAM. This context is then handed to the /math agent for a rigorous formal derivation. Finally, the /code agent implements the concepts to empirically validate the derived theory, providing a complete theoretical-to-practical workflow.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical proof connecting the Hessian's spectral norm to optimizer convergence.\", \"A Python script (JAX) implementing the Hessian eigenvalue tracking for a ResNet model.\", \"A summary report (.md) that interprets the empirical results, connects them to the theory, and includes plots of eigenvalue evolution vs. training loss.\", \"A verification log file confirming that the mathematical derivation is correctly implemented in the code.\", \"Citations for at least two relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Formulate the optimization problem, defining the loss function and the role of the Hessian matrix.\", \"2. Research existing literature on spectral analysis of the Hessian in deep learning, focusing on its connection to generalization, sharpness, and convergence rates.\", \"3. Derive the mathematical relationship between the largest eigenvalue (spectral norm) of the Hessian and the local curvature of the loss landscape, proving its impact on the step size constraints for stable convergence.\", \"4. Implement a computationally efficient algorithm (e.g., Lanczos or power iteration) to estimate the Hessian's top eigenvalue during training.\", \"5. Validate the implementation by comparing its output on a simple quadratic function against the analytical solution.\", \"6. Execute the experiment, plot the evolution of the top eigenvalue against training/validation metrics, and interpret the results in the context of the derived theory, verifying the predicted correlation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /math agent uses the papers from /general-search to ground its proof. The /code agent implements the formula derived by /math. The router performs a final QA check, verifying that the code's eigenvalue estimation method is consistent with the derived mathematical principles. Fallback: If the derivation is flawed, the router re-engages /math with feedback from the /code agent's failed implementation tests.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Hessian spectral analysis, sharpness-aware minimization, and convergence theory on arXiv.\", \"- [ ] /math: Derive the proof connecting the Hessian's maximum eigenvalue (\u03bb_max) to the gradient's Lipschitz constant.\", \"- [ ] /math: Formulate the convergence rate inequality for a generic gradient-based optimizer as a function of \u03bb_max.\", \"- [ ] /code: Implement Lanczos algorithm in JAX to efficiently estimate the Hessian's top eigenvalue for a ResNet-18.\", \"- [ ] /code: Set up a training loop on CIFAR-10 to track \u03bb_max, training loss, and validation accuracy over epochs.\", \"- [ ] /math: Verify the code implementation of the Lanczos iteration correctly reflects the mathematical algorithm.\", \"- [ ] /code: Verify the empirical results by plotting \u03bb_max against the loss and checking for the expected correlation, adding sanity checks for numerical stability.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"matrix-calculus\", \"spectral-analysis\", \"optimization-theory\"], \"acceptance_criteria\": [\"The provided LaTeX proof must be mathematically sound and clearly link the Hessian's spectral norm to convergence criteria.\", \"The JAX code must execute without errors and produce visualizations showing the evolution of the Hessian's top eigenvalue over training epochs.\", \"The final report must correctly interpret the results, citing at least two relevant arXiv papers with their IDs.\", \"The tracked eigenvalue must show a plausible correlation with training dynamics (e.g., spiking during early training phases or before overfitting).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 339, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the local saddle point properties for a Wasserstein GAN with gradient penalty (WGAN-GP). Derive the full Hessian of the objective function with respect to both generator (theta) and discriminator (phi) parameters. I need to understand the conditions under which the eigenvalues of this game Hessian indicate a stable saddle point versus a pathological one, such as a local minimum for the discriminator. Please research recent literature from arXiv for state-of-the-art GAN convergence diagnostics related to spectral analysis and control theory.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Wasserstein GAN Hessian spectral analysis convergence stability\\\", mode=both)\", \"/math(Derive the symbolic Hessian of the WGAN-GP objective function, considering both generator and discriminator parameters. Formulate the block matrix structure and define the analytical conditions for saddle point convergence based on its eigenvalue spectrum, referencing findings from the literature search.)\", \"/code(Implement a toy WGAN-GP in JAX to numerically compute the game Hessian's eigenvalues at various training stages. Track the max eigenvalue and verify if the empirical spectral distribution aligns with the theoretical saddle point conditions derived by the math agent., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey current literature on GAN Hessian analysis, which is essential for this advanced topic. The findings are then passed to /math for the core symbolic derivation. Finally, /code is used for numerical verification of the derived theory, providing an empirical check on the mathematical model.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the WGAN-GP Hessian and its spectral properties in a LaTeX-formatted PDF.\", \"- A Python script (JAX) that implements a toy WGAN-GP and computes the Hessian eigenvalues.\", \"- A summary report (.md) synthesizing the theoretical findings, numerical results, and citing at least two relevant papers.\", \"- A JSON file containing the computed eigenvalues at different training iterations.\"], \"thinking_outline\": [\"1. Formulate the complete WGAN-GP min-max objective function, including the critic loss, generator loss, and the gradient penalty term.\", \"2. Conduct a literature search to identify key papers on the spectral analysis of the Jacobian and Hessian for GANs, focusing on second-order conditions for saddle point stability.\", \"3. Symbolically derive the full Hessian matrix of the WGAN-GP objective with respect to the concatenated parameter vector [theta, phi], and analyze its block structure.\", \"4. From the derived Hessian, establish the theoretical conditions on its eigenvalues that correspond to a stable local saddle point suitable for convergence.\", \"5. Validate the symbolic derivation by implementing a numerical gradient and Hessian computation in JAX for a small-scale problem and comparing the results.\", \"6. Check if the numerically computed eigenvalues from the toy experiment satisfy the theoretically derived conditions for stability, and document any discrepancies.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: numerical eigenvalues from /code must match the properties predicted by /math's derivation; fallback: if a mismatch occurs, /math re-derives the relevant Hessian block and /code re-runs computation).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 key arXiv papers on WGAN Hessian analysis and convergence diagnostics.\", \"- [ ] /math: Derive the symbolic block Hessian of the WGAN-GP objective function.\", \"- [ ] /math: Define the theoretical conditions on the Hessian's eigenvalues for a stable saddle point.\", \"- [ ] /code: Implement a toy WGAN-GP model in JAX with functionality to compute the full game Hessian.\", \"- [ ] /code: Numerically compute and log the eigenvalues of the Hessian during a short training run.\", \"- [ ] /math: Verify that the symbolic derivation is consistent with a finite-difference approximation of the Hessian from the /code agent's implementation.\", \"- [ ] /code: Validate that the computed eigenvalue spectrum matches theoretical predictions for saddle points from the math derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"saddle-point-analysis\", \"non-convex-optimization\", \"gan-stability\"], \"acceptance_criteria\": [\"- The submitted report includes a complete and correct symbolic derivation of the WGAN-GP game Hessian.\", \"- The Python script successfully executes and produces a set of eigenvalues for a toy problem.\", \"- The final report's conclusion on saddle point stability is supported by both the theoretical derivation and the numerical results.\", \"- At least two relevant arXiv papers are correctly cited with their IDs in the report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Binary metric (1/0). 1 if the real parts of the top k eigenvalues from the numerical experiment are close to zero as predicted by theory for a stable saddle, 0 otherwise.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Float. The maximum real part of all computed Hessian eigenvalues. Large positive values indicate instability or movement away from a saddle point.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 341, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time SDE for a variance-preserving (VP) diffusion process as detailed in seminal score-based generative modeling papers. Then, implement a simple PyTorch solver using the Euler-Maruyama method to sample from a 2D toy distribution, ensuring the implementation correctly uses the score function approximation via a neural network. Validate the convergence of the learned score function to the true data score.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models variance preserving SDE reverse process derivation\\\", mode=rag)\", \"/math(Derive the reverse-time SDE for a VP diffusion process, starting from the forward process dxt = f(xt, t)dt + g(t)dwt. Show how the drift term depends on the score of the marginal probability density, referencing the Fokker-Planck equation and Bayesian inversion principles for stochastic convergence.)\", \"/code(Implement a PyTorch model to learn the score function for a 2D Swiss roll dataset under a VP diffusion process. Implement an Euler-Maruyama SDE solver for the reverse-time process to generate samples, and track FID convergence against the true data distribution using CUDA., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find foundational papers on VP-SDEs and score matching. This theoretical grounding is then passed to /math for a rigorous derivation of the reverse-time SDE. Finally, /code implements the derived theory in PyTorch, leveraging CUDA for acceleration, and validates the approach by tracking FID convergence, a key generative modeling metric.\", \"expected_artifacts\": [\"A LaTeX/PDF document containing the full derivation of the reverse-time SDE.\", \"A Python script (`score_sde_solver.py`) implementing the score network and the Euler-Maruyama sampler.\", \"A plot (`convergence.png`) showing the FID score over training epochs.\", \"A summary report (`report.md`) explaining the results and citing at least two relevant arXiv papers.\", \"A JSON file (`metrics.json`) with the final FID score and training duration.\"], \"thinking_outline\": [\"1. Research the formulation of continuous-time diffusion models, specifically the Variance Preserving (VP) SDE framework.\", \"2. Formally derive the corresponding reverse-time SDE, clearly showing its dependence on the score function, \u2207x log p_t(x).\", \"3. Implement a neural network in PyTorch to approximate this time-dependent score function using denoising score matching.\", \"4. Implement a numerical SDE solver (e.g., Euler-Maruyama) for the derived reverse-time SDE to generate samples.\", \"5. Validate the implementation by training on a 2D toy dataset and checking if the distribution of generated samples converges to the data distribution, as measured by FID.\", \"6. Audit the code to verify that the numerical solver's discretization correctly aligns with the derived continuous-time SDE.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The /math agent's derivation of the drift term must match the implementation in the /code agent's SDE solver. The final FID metric reported by /code must be below a pre-defined threshold. fallback: If derivation is flawed, /math must revisit the Fokker-Planck equation. If implementation fails to converge, /code must debug the score network architecture or training stability.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on score-based generative modeling with SDEs (e.g., by Song & Ermon).\", \"- [ ] /math: Derive the reverse-time SDE for a variance-preserving diffusion process.\", \"- [ ] /math: Verify the derived drift term is consistent with the literature found by /general-search.\", \"- [ ] /code: Implement the denoising score matching objective and a time-conditional U-Net for the score model in PyTorch.\", \"- [ ] /code: Implement the Euler-Maruyama solver for the reverse-time SDE sampling.\", \"- [ ] /code: Validate model convergence by plotting the FID score over training time against a 2D toy dataset.\", \"- [ ] /math: Post-hoc, check that the discretization scheme in the /code solver is a valid first-order approximation of the continuous SDE derived.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"score-based-generation\"], \"acceptance_criteria\": [\"- The mathematical derivation in the PDF must be complete and correctly link the forward and reverse SDEs via the score function.\", \"- The Python code must successfully train and generate visually plausible samples from the 2D toy distribution.\", \"- The final reported FID score in `metrics.json` must be below 50.\", \"- The final report must cite at least two relevant arXiv papers with correct formatting and IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 342, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm researching inverse problems for the 2D incompressible Navier-Stokes equations. I need to develop a physics-informed neural network (PINN) that can reconstruct the velocity field from sparse sensor data. Standard PINNs struggle with this, so I want to explore a variational energy-based formulation for the loss function, potentially augmented with adversarial training for regularization, as suggested by recent literature. Please outline the derivation, implementation, and validation process.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational physics-informed neural network navier-stokes inverse problem adversarial regularization energy methods\\\", mode=both)\", \"/math(Derive the Euler-Lagrange equations for the energy functional of the Navier-Stokes flow, incorporating Bayesian uncertainty quantification constraints and analyzing the convergence criteria for the composite loss.)\", \"/code(Implement the variational PINN using JAX with CUDA acceleration, applying bootstrap resampling for confidence interval estimation on the reconstructed flow field and tracking convergence of the PDE residual., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on variational PINNs for fluid dynamics. This informs the /math agent, which will derive the specific variational formulation and composite loss function. Finally, the /code agent implements and validates the derived model based on the mathematical specification.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the variational loss function for the Navier-Stokes equations.\", \"- A documented Python implementation of the PINN model and training loop using JAX.\", \"- A technical report (report.md) summarizing the method, results, and including plots of the reconstructed velocity field and loss convergence.\", \"- A bibtex file with citations for at least 3 relevant arXiv papers.\", \"- A JSON file containing final evaluation metrics (metrics.json).\"], \"thinking_outline\": [\"1. Conduct a literature survey to identify state-of-the-art variational principles and adversarial regularization techniques for PINNs applied to inverse fluid dynamics problems.\", \"2. Formulate the weak (variational) form of the incompressible Navier-Stokes equations and derive the corresponding energy functional that will serve as the physics-based loss component.\", \"3. Construct the full composite loss function, combining the data fidelity term (from sparse sensors), the energy functional, and any boundary condition penalties.\", \"4. Implement the neural network architecture and the custom variational loss function in Python using JAX, ensuring automatic differentiation is correctly applied.\", \"5. Verify the correctness of the loss implementation by numerically checking the gradients against analytical solutions for a simplified problem, such as Stokes flow.\", \"6. Validate the trained model's accuracy on a hold-out dataset by computing the relative L2 error of the velocity and pressure fields and auditing the convergence behavior of the physical energy functional.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal derivation) -> /code (implementation & validation) -> router QA (final review). The handoff from /math to /code is critical; verification involves checking if the coded loss function's gradients match the derived Euler-Lagrange equations. Fallback: if gradients mismatch, /math must re-check the derivation for potential errors in functional derivatives.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 3+ recent papers on variational or energy-based PINNs for Navier-Stokes equations.\", \"- [ ] /math: Derive the weak form and energy functional for the 2D incompressible Navier-Stokes equations.\", \"- [ ] /math: Formulate the composite loss function, including data, energy, and boundary terms.\", \"- [ ] /code: Implement the PINN architecture and the custom variational loss in JAX.\", \"- [ ] /code: Set up a training pipeline for a test case (e.g., lid-driven cavity) with sparse data.\", \"- [ ] /math: Verify that the implemented loss function in the code correctly reflects the derived variational principle.\", \"- [ ] /code: Run experiments, generate convergence plots, and compute final L2 error metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"navier-stokes\", \"variational-methods\"], \"acceptance_criteria\": [\"- The submitted code must be executable and reproduce the reported metrics with the provided seed.\", \"- The final report must show a relative L2 error for the velocity field below 5% on the test case.\", \"- The mathematical derivation must be self-contained, clearly define all terms, and directly correspond to the implemented loss function.\", \"- The solution must cite at least 3 relevant arXiv preprints on PINNs published in the last 3 years.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 343, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement structured pruning for a Transformer model using the Alternating Direction Method of Multipliers (ADMM). Please provide a full derivation of the ADMM updates for block-sparsity on the weight matrices, analyze the convergence guarantees in terms of the spectral radius of the iteration matrix, and develop a well-tested JAX implementation to validate the approach on a pre-trained model, measuring the trade-off between perplexity and sparsity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org ADMM structured pruning transformers convergence analysis\\\", mode=both)\", \"/math(Derive the augmented Lagrangian and proximal updates for ADMM-based structured pruning of Transformer attention heads, analyzing the convergence criteria based on the problem's dual feasibility and eigenvalue spectrum.)\", \"/code(Implement the derived ADMM optimization loop in JAX/Flax, targeting CUDA execution. Integrate it with a pre-trained BERT model, track convergence via perplexity on a validation set, and log sparsity metrics., using Python)\", \"/math(Verify the JAX implementation's correctness by auditing the code against the formal derivation. Analyze the empirical convergence plots and check for consistency with the theoretical rate derived from the spectral analysis.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on ADMM for pruning. This informs the /math agent's formal derivation of the optimization problem. The /code agent then translates this mathematical framework into a practical JAX implementation. A final /math step is crucial for verification, ensuring the code's logic faithfully represents the derived theory and that empirical results align with theoretical predictions.\", \"expected_artifacts\": [\"- A PDF report containing the complete mathematical derivation of the ADMM updates and convergence analysis.\", \"- A well-documented Python script (`admm_pruning.py`) implementing the algorithm in JAX.\", \"- A JSON file (`results.json`) containing the final perplexity, sparsity ratio, and convergence logs.\", \"- A `CITATIONS.md` file with at least two cited arXiv papers, including their IDs.\"], \"thinking_outline\": [\"1. Research state-of-the-art literature on ADMM-based structured pruning for neural networks, focusing on applications to Transformers.\", \"2. Formulate the constrained optimization problem for block sparsity and derive the complete ADMM update equations, including the proximal operators for the sparsity-inducing norm and the model weights.\", \"3. Implement the derived algorithm in JAX, creating a training wrapper that applies the ADMM updates iteratively to a pre-trained Transformer model like DistilBERT.\", \"4. Validate the correctness of the JAX implementation by cross-referencing the update logic directly with the equations derived in step 2.\", \"5. Execute the pruning experiment, collecting metrics on perplexity degradation versus the achieved sparsity ratio across multiple ADMM iterations.\", \"6. Audit the empirical convergence data from the experiment to verify that it behaves consistently with the theoretical convergence analysis.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (verification) -> router QA. The /general-search agent provides foundational papers to /math. /math produces a formal derivation that serves as a specification for /code. /code generates an implementation and empirical results, which are handed back to the second /math agent for a verification audit. Verification failure at the final step (e.g., implementation does not match derivation) triggers a fallback to the /code agent with specific correction requests.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on ADMM for Transformer pruning.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the ADMM update rules for the specified structured pruning problem.\", \"- [ ] /code: Implement the ADMM pruning algorithm in a JAX script capable of loading a pre-trained model and running the optimization.\", \"- [ ] /math: Verify the mathematical correctness of the JAX implementation by comparing its update functions line-by-line against the derived equations.\", \"- [ ] /code: Run the experiment to achieve at least 50% structured sparsity and verify that perplexity and sparsity metrics are logged correctly to a JSON file.\", \"- [ ] /math: Analyze the logged convergence data and confirm it does not violate the theoretical bounds derived earlier.\", \"- [ ] /router: Perform final QA review of all generated artifacts for coherence, correctness, and completeness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"transformer-optimization\", \"admm\", \"structured-pruning\"], \"acceptance_criteria\": [\"- The submitted report must contain a mathematically sound derivation of the ADMM updates.\", \"- The JAX script must execute without errors and successfully prune a pre-trained Transformer model, generating a `results.json` file.\", \"- The final model must achieve at least 50% block sparsity while maintaining a perplexity score no more than 15% higher than the dense model's baseline.\", \"- The `CITATIONS.md` file must list at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"index": 344, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the convergence instability in Differentiable Architecture Search (DARTS). Please formulate the bilevel optimization problem and derive the second-order approximation for the inner loop's response using the inverse Hessian-vector product. Then, implement this second-order DARTS variant in JAX, compare its discovered architecture's performance on CIFAR-10 against a first-order baseline, and analyze the Hessian eigenvalue spectrum of the final architecture to assess stability.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org differentiable architecture search second-order optimization Hessian convergence instability\\\", mode=both)\", \"/math(Formulate the bilevel optimization for AutoML and derive the gradient update using the inverse Hessian-vector product, analyzing convergence criteria and the role of the architecture's Hessian eigenvalue.)\", \"/code(Implement the second-order DARTS algorithm in JAX, including a baseline, run the search on CIFAR-10 within the compute budget, and plot the final architecture's Hessian eigenvalue distribution., using Python)\"], \"route_rationale\": \"The task requires a literature survey, mathematical derivation, and a complex implementation. The 'blue' /general-search agent will first find state-of-the-art approaches. Then, /math will formalize the algorithm based on these findings. Finally, /code will implement and validate the derived method.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the second-order DARTS update rule.\", \"- A Python script in JAX implementing the NAS pipeline, including data loaders and training loops.\", \"- A results summary in JSON format containing test accuracy, search time, and computed Hessian eigenvalues for both baseline and second-order models.\", \"- A PDF report visualizing the discovered cell architecture and plotting the eigenvalue spectrum, with citations for at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research existing literature on the instabilities of DARTS, focusing on second-order optimization methods and Hessian approximations.\", \"2. Formulate the nested bilevel optimization problem where architecture parameters are outer variables and network weights are inner variables.\", \"3. Derive the hypergradient for the architecture parameters using the implicit function theorem, resulting in an update rule that involves an inverse Hessian-vector product.\", \"4. Validate the mathematical derivation against the approximation techniques found in the literature to ensure correctness.\", \"5. Implement the derived algorithm and a first-order baseline in JAX, paying close attention to efficient computation of the Hessian-vector products.\", \"6. Verify the implementation's correctness through numerical gradient checks and by auditing the final architecture's performance against published results on CIFAR-10.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Cross-check if the implemented Hessian approximation in the /code artifact matches the /math derivation; fallback: If implementation fails, instruct /code to use a simpler finite-difference approximation and flag the discrepancy).\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 key papers on second-order optimization for DARTS.\", \"- [ ] /math: Produce a formal derivation of the architecture gradient using the implicit function theorem and Hessian-vector products.\", \"- [ ] /math: Verify the final update rule derivation against approximations cited in the source papers.\", \"- [ ] /code: Implement the complete DARTS search pipeline in JAX for CIFAR-10.\", \"- [ ] /code: Add functionality to compute the Hessian eigenvalue spectrum for a trained network.\", \"- [ ] /code: Implement unit tests and numerical gradient checks for the AutoML architecture update logic.\", \"- [ ] /code: Train both first-order and second-order models and generate all specified artifacts (plots, metrics).\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"automl\", \"neural-architecture-search\", \"differentiable-programming\"], \"acceptance_criteria\": [\"- The JAX implementation must run without errors and complete the architecture search within the allocated compute budget.\", \"- The final report must show a statistically significant difference in test accuracy or a demonstrably more stable Hessian eigenvalue spectrum for the second-order method.\", \"- The mathematical derivation must be self-contained, clearly referencing steps from the cited papers.\", \"- All expected artifacts (derivation, code, results JSON, report PDF) must be generated and correctly formatted.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 346, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a neural-symbolic framework in Lean 4 to synthesize a program that proves the constructive version of the Brouwer Fixed-Point Theorem. The system must leverage a pre-trained language model to guide a Monte Carlo Tree Search over the proof space, optimizing for proof length and minimizing backtracking steps. The primary challenge is integrating the symbolic reasoning of the Lean kernel with the probabilistic guidance from the neural network, ensuring formal soundness at every step.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'neural theorem proving' 'Lean 4' MCTS 'Bayesian program synthesis'\\\", mode=web)\", \"/math(Formalize the Monte Carlo Tree Search over the Lean tactic state space, defining a reward function that promotes convergence and accounts for the problem's high combinatorial complexity.)\", \"/code(Implement the Python-Lean 4 interface and MCTS loop using PyTorch for the Bayesian policy network, ensuring runtime validation of each tactic against the Lean kernel to maintain soundness and track performance metrics.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey the latest literature on neural theorem provers. /math is then used to formally define the algorithm and search space based on the findings. Finally, /code implements the system, integrating the mathematical specification with the practicalities of interfacing with the Lean 4 proof assistant.\", \"expected_artifacts\": [\"A formal mathematical specification of the neural-guided MCTS algorithm for theorem proving.\", \"A Python codebase implementing the theorem prover, capable of interfacing with Lean 4.\", \"A generated Lean 4 proof script for the constructive Brouwer Fixed-Point Theorem.\", \"A summary report detailing the methodology, performance metrics (success rate, search time), and at least two relevant citations.\", \"A verification log confirming that all generated proof steps were validated by the Lean kernel.\"], \"thinking_outline\": [\"1. Review recent literature on neural-symbolic methods for theorem proving in assistants like Lean or Coq, focusing on Monte Carlo Tree Search and neural-guided strategies.\", \"2. Formulate the constructive proof of the Brouwer Fixed-Point Theorem as a search problem over a state-action space defined by Lean tactics and proof states.\", \"3. Design the MCTS algorithm, specifying the reward function and the neural network architecture for the policy and value functions, ensuring Bayesian convergence criteria are considered.\", \"4. Implement the framework, connecting a Python-based MCTS agent to the Lean 4 proof environment and validate the soundness of each generated tactic application against the Lean kernel.\", \"5. Execute the prover on the target theorem, analyze its performance (proof length, search time), and verify the final synthesized proof script is accepted by the Lean compiler.\", \"6. Synthesize the findings into a report, citing the foundational research and detailing the experimental results and limitations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: the Python implementation must correctly instantiate the MCTS formulation from the /math agent, and the final proof must be valid in Lean; fallback: if the MCTS formulation is not feasible, revert to /math to simplify the search algorithm, potentially to a guided best-first search).\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 state-of-the-art papers on integrating MCTS with proof assistants.\", \"- [ ] /math: Define the state space, action space, and reward function for the MCTS-based proof search.\", \"- [ ] /math: Specify the architecture of the policy and value networks for guiding the search.\", \"- [ ] /code: Implement the communication bridge between Python and the Lean 4 server.\", \"- [ ] /code: Implement the MCTS search loop guided by the specified neural network.\", \"- [ ] /math: Verify that the reward function formulation is guaranteed to terminate and encourages minimal proofs.\", \"- [ ] /code: Validate that the generated Lean 4 proof script is fully accepted by the Lean kernel without axioms.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic\", \"theorem-proving\", \"program-synthesis\"], \"acceptance_criteria\": [\"- The synthesized Lean 4 proof script must be successfully verified by the Lean compiler without any 'sorry' placeholders or added axioms.\", \"- The implementation must include a functional MCTS loop whose decisions are guided by a trained or pre-trained neural network.\", \"- The final report must cite at least 2 relevant arXiv papers by ID and present the proof success rate and search time metrics.\", \"- The system must demonstrate successful synthesis of at least one non-trivial lemma required for the main theorem.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 348, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the optimization instability in Model-Agnostic Meta-Learning (MAML). Can you derive the complete second-order MAML update rule involving Hessian-vector products, analyze its convergence properties by characterizing the meta-objective's Hessian eigenvalues, and then implement it in JAX to benchmark its few-shot accuracy and training stability against the first-order approximation on the mini-ImageNet dataset? I'm also curious about connections to Bayesian meta-learning.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org MAML second-order optimization Hessian analysis Bayesian meta-learning convergence\\\", mode=web)\", \"/math(Derive the full second-order MAML gradient using multivariable calculus, analyzing the Hessian of the meta-objective to prove conditions for convergence and relate them to the inner-loop learning rate.)\", \"/code(Implement both first-order and second-order MAML in JAX for the mini-ImageNet benchmark, tracking few-shot accuracy and meta-gradient norm. Use CUDA for acceleration and include assertions to check for gradient explosion., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational arXiv papers on MAML and its Hessian. This research grounds the theoretical work. Next, the /math agent performs the formal derivation and convergence analysis based on the sourced literature. Finally, the /code agent implements the derived algorithm and runs empirical validation, which is the ultimate test of the theoretical model.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the second-order MAML update rule.\", \"- A JAX implementation of both first-order and second-order MAML, including a training script for mini-ImageNet.\", \"- A summary report comparing the few-shot accuracy, convergence speed, and stability of both methods, supported by plots.\", \"- A bibliography with at least two cited arXiv preprints on meta-learning optimization.\", \"- A verification log confirming the alignment between the mathematical derivation and the code's gradient computation.\"], \"thinking_outline\": [\"1. Systematically review the literature on MAML, specifically focusing on analyses of its optimization dynamics and second-order methods.\", \"2. Formally derive the gradient of the meta-objective without the first-order approximation, expressing the update in terms of Hessian-vector products.\", \"3. Analyze the derived update rule to establish theoretical conditions for convergence, linking stability to the spectral properties of the meta-Hessian.\", \"4. Implement the derived algorithm in JAX, alongside a baseline first-order MAML, and prepare the mini-ImageNet data pipeline.\", \"5. Verify that the implemented Hessian-vector product computation in the JAX code numerically matches the analytical derivation for a small-scale problem.\", \"6. Validate the final benchmark results by comparing the performance of the first-order implementation against established open-source results to ensure the experimental setup is correct.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers (e.g., the original MAML paper) to the math agent. The math agent produces a formal derivation that the code agent must implement precisely. A unit test will verify the code's gradient calculation against the math agent's equations. Fallback: If the second-order implementation proves computationally intractable, the code agent will report the bottleneck and focus on profiling a single update step. The final report is then reviewed by the router for coherence between theory and empirical results.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on MAML optimization and second-order methods, providing at least two arXiv links.\", \"- [ ] /math: Produce a full derivation of the second-order MAML update rule.\", \"- [ ] /math: Analyze the convergence properties based on the Hessian of the meta-loss.\", \"- [ ] /code: Implement first-order and second-order MAML variants in JAX.\", \"- [ ] /code: Run benchmarks on mini-ImageNet, generating plots for accuracy and gradient norm variance.\", \"- [ ] /math: Verify that the symbolic gradient derived matches the JAX implementation's automatic differentiation output on a toy example.\", \"- [ ] /code: Validate the first-order MAML implementation's accuracy against published results to confirm the test harness is correct.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"few-shot-learning\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The submitted code must be executable and reproduce the reported accuracy metrics on a provided random seed.\", \"- The mathematical derivation must be complete, self-contained, and free of logical errors.\", \"- The final report must show a clear performance comparison (accuracy and stability) between first-order and second-order MAML, with results aligning with the theoretical analysis.\", \"- The solution must cite at least two relevant arXiv papers with IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 349, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to construct a robust molecular dynamics simulation for a solvated protein system using a graph neural network potential. The model must be SE(3)-equivariant to respect physical symmetries, handle long-range electrostatic interactions beyond a simple cutoff, and crucially, conserve the total Hamiltonian energy over microsecond-scale trajectories to avoid unphysical energy drift. How can I formulate the potential, derive a corresponding symplectic integrator, and verify its stability and accuracy?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org SE(3)-equivariant graph neural networks molecular dynamics long-range electrostatics energy conservation\\\", mode=rag)\", \"/math(Derive the symplectic integrator for the learned Hamiltonian, analyzing the Fourier spectrum of the trajectory and bounding the maximum eigenvalue of the system's Hessian to ensure stability.)\", \"/code(Implement the GNN potential in PyTorch using CUDA for acceleration, run a simulation for a water box system from chemistry, and validate energy conservation and structural properties against benchmark data., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art equivariant GNN architectures and long-range interaction methods from arXiv. The findings inform the /math agent, which will formulate the underlying physics and numerical integration scheme. Finally, the /code agent implements the GNN and simulation, performing empirical validation of the theoretical model.\", \"expected_artifacts\": [\"A literature review summary citing at least three authoritative papers on equivariant GNNs for MD.\", \"A formal mathematical derivation of the GNN-based force field and the chosen symplectic integration algorithm (e.g., Velocity Verlet).\", \"A Python implementation in a Jupyter Notebook demonstrating the GNN-MD simulation.\", \"A verification report with plots showing total energy conservation over time and a comparison of the model's radial distribution function against a known benchmark.\", \"A final JSON file containing the computed metrics (`energy_drift_rate`, `rdf_js_divergence`).\"], \"thinking_outline\": [\"1. Research state-of-the-art SE(3)-equivariant GNN architectures (e.g., E(n)-GNN, NequIP) and methods for incorporating long-range electrostatics in neural network potentials, such as particle-mesh Ewald (PME) adaptations.\", \"2. Formulate the system's Hamiltonian using the GNN as the potential energy function. Derive the analytical expression for the force on each atom as the negative gradient of this potential.\", \"3. Define a symplectic integrator, like the Velocity Verlet algorithm, that uses the GNN-derived forces to propagate the system's phase space coordinates (positions and momenta).\", \"4. Implement the GNN architecture and the MD simulation loop in a suitable framework like PyTorch Geometric.\", \"5. Validate the implementation by simulating a benchmark system (e.g., a TIP3P water box) and verifying that the total energy is conserved within a strict tolerance over a nanosecond-scale trajectory.\", \"6. Audit the physical realism of the simulation by computing the radial distribution function (RDF) and comparing it against established results for the benchmark system.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The literature review from /general-search provides candidate models for /math to formalize. The derivations from /math serve as the specification for the /code agent's implementation. The final code and its output plots are passed to the router for a final QA review. Verification: The energy drift reported by /code must match the stability criteria established in the /math derivation. Fallback: If energy drift is unacceptably high, the /code agent must revisit hyperparameters or flag the integration scheme for the /math agent to refine.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22653 key papers on SE(3)-equivariant GNNs for MD, focusing on energy conservation.\", \"- [ ] /math: Derive the force F = -\u2207U_GNN(R) where U_GNN is the GNN potential over atomic coordinates R.\", \"- [ ] /math: Formulate the complete Velocity Verlet integration algorithm using the derived GNN forces.\", \"- [ ] /code: Implement the selected equivariant GNN architecture using PyTorch Geometric.\", \"- [ ] /code: Build the MD simulation loop incorporating the symplectic integrator.\", \"- [ ] /math: Verify that the force implementation in the code is a correct and numerically stable gradient of the potential model.\", \"- [ ] /code: Verify energy conservation by plotting E_total vs. time for a 1ns simulation and compute the energy drift rate.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"molecular-dynamics\", \"gnn\", \"equivariant-networks\"], \"acceptance_criteria\": [\"- The implementation must demonstrate energy conservation over a 1 nanosecond NVE simulation with a mean energy drift rate below 1e-5 kT/atom/ns.\", \"- The final report must cite at least 3 relevant arXiv papers on equivariant GNNs, including their arXiv IDs.\", \"- The computed radial distribution function (RDF) for a water box benchmark must have a Jensen-Shannon divergence below 0.05 when compared to a ground truth simulation.\", \"- The provided code must be runnable and reproduce the metrics reported in the verification artifacts.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 350, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the physics-informed loss function for the 2D incompressible Navier-Stokes equations, specifically the momentum and continuity equations. Then, implement a Physics-Informed Neural Network (PINN) in JAX to model lid-driven cavity flow, using automatic differentiation to compute the PDE residuals. I need to see how the Bayesian framework informs the loss terms and a final validation against benchmark data, analyzing the Fourier modes of the velocity field for convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org PINN Navier-Stokes bayesian loss lid-driven cavity benchmark\\\", mode=both)\", \"/math(Derive the PINN loss for 2D Navier-Stokes, incorporating Bayesian inference principles for the residual terms and Fourier features for the input encoding to improve convergence.)\", \"/code(Implement a JAX PINN solver for the lid-driven cavity, using the derived Bayesian loss. Track L2 error and total system energy as a diagnostic metric during training., using Python)\"], \"route_rationale\": \"The blue /general-search agent is used first to gather state-of-the-art architectures and find appropriate benchmark datasets for validation. /math is then needed for the rigorous derivation of the composite loss function, combining PDE residuals and boundary conditions from a Bayesian perspective. Finally, /code is required to translate the mathematical model into a functional JAX implementation, leveraging its automatic differentiation capabilities for the complex differential operators and enabling GPU acceleration for training.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the Navier-Stokes PINN loss function.\", \"A fully commented JAX/Flax Python script implementing the PINN solver for the lid-driven cavity problem.\", \"A set of plots comparing the PINN's predicted velocity and pressure fields against benchmark data.\", \"A JSON file containing the final L2 relative error metrics and convergence diagnostics.\", \"A summary report with citations to at least 2 relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research contemporary PINN architectures and loss formulations for solving the Navier-Stokes equations, focusing on the lid-driven cavity benchmark.\", \"2. Formally derive the composite loss function, expressing the momentum and continuity equations as residuals. Frame the data and physics loss terms within a Bayesian inference context.\", \"3. Implement the neural network architecture and the derived loss function in JAX, using `jax.grad` and `jax.vmap` to efficiently compute the PDE residuals over collocation points.\", \"4. Train the PINN model and validate its output by computing the L2 relative error of the velocity and pressure fields against a known high-fidelity numerical solution.\", \"5. Perform a final verification by analyzing the Fourier spectrum of the predicted velocity field to check for physical consistency and convergence.\", \"6. Synthesize all findings, including the derivation, code, plots, and metrics, into a comprehensive final report.\"], \"handoff_plan\": \"/general-search (benchmark data/papers) -> /math (loss derivation) -> /code (JAX implementation & training). The /code agent will verify that the implemented differential operators match the ones in the /math derivation. Fallback: If the L2 error metric is above the acceptance threshold, the /code agent will flag the issue for the router, potentially triggering a new /math task to reconsider the loss term weighting.\", \"todo_list\": [\"- [ ] /general-search: Find \u22652 recent arXiv papers on PINNs for fluid dynamics and a reference dataset for the 2D lid-driven cavity flow (e.g., from Ghia et al.).\", \"- [ ] /math: Derive the steady-state, 2D incompressible Navier-Stokes equations in their residual form for a PINN loss.\", \"- [ ] /math: Formulate the complete loss function, including terms for the u-momentum, v-momentum, continuity residuals, and Dirichlet boundary conditions.\", \"- [ ] /code: Implement the PINN architecture (e.g., a multi-layer perceptron) in JAX/Flax.\", \"- [ ] /code: Implement the custom loss function using automatic differentiation to compute gradients representing the PDE operators.\", \"- [ ] /code: Train the model and generate comparison plots (velocity contour, streamlines) against the benchmark data.\", \"- [ ] /math: Verify that the symbolic derivatives in the derivation match the computational graph implemented in the JAX code.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-neural-networks\", \"pde-solvers\", \"navier-stokes\"], \"acceptance_criteria\": [\"The final L2 relative error for the velocity field (u and v components) must be less than 5e-2 when compared to the benchmark data.\", \"The mathematical derivation must correctly represent the Navier-Stokes partial differential operators and boundary conditions.\", \"The generated plots must qualitatively match the expected vortex structures in the lid-driven cavity flow.\", \"The implementation must successfully run and produce the specified artifacts (plots, metrics JSON).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 352, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the local convergence properties of Generative Adversarial Networks (GANs) trained with the extragradient method. Specifically, derive the conditions on the Hessian of the generator-discriminator game's value function that guarantee convergence to a saddle point. Compare this with standard gradient descent-ascent and implement a toy GAN in PyTorch to numerically validate the theoretical Hessian eigenvalue conditions for a simple 2D data distribution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org extragradient method GAN convergence local saddle point analysis hessian\\\", mode=both)\", \"/math(Derive the local convergence conditions for the extragradient method in a two-player zero-sum game by analyzing the Jacobian of the update operator, relating its eigenvalue spectrum to the Hessian of the value function near a saddle point.)\", \"/code(Implement a toy GAN in PyTorch for a 2D mixture of Gaussians, apply the extragradient optimizer, and write a utility to compute the Hessian's eigenvalue distribution to empirically verify convergence criteria., using Python)\"], \"route_rationale\": \"The blue /general-search agent is needed first to gather foundational papers on extragradient methods in GANs. The /math agent will then perform the formal theoretical derivation based on this literature. Finally, the /code agent will implement the algorithm and perform numerical experiments to validate the mathematical theory.\", \"expected_artifacts\": [\"A formal mathematical derivation of the convergence conditions for the extragradient method based on the Hessian's eigenvalues.\", \"A link to a public Git repository containing the PyTorch implementation of the toy GAN, extragradient optimizer, and Hessian analysis script.\", \"A summary report (.md) citing at least two relevant arXiv papers with IDs, explaining the theoretical results and presenting plots of the Hessian eigenvalue spectrum during training.\", \"A JSON file containing the computed eigenvalues at various training steps, demonstrating the satisfaction of the derived conditions near convergence.\"], \"thinking_outline\": [\"1. Research the application of extragradient methods to zero-sum games, focusing on seminal papers that analyze local convergence guarantees for GANs.\", \"2. Formulate the GAN training as a continuous-time dynamical system and derive the Jacobian of the gradient vector field, showing its connection to the value function's Hessian.\", \"3. Derive the sufficient conditions on the eigenvalues of the Hessian for the extragradient method to converge locally to a saddle point.\", \"4. Implement the toy GAN and the extragradient update rule in PyTorch. Validate the implementation by comparing its behavior on a simple quadratic saddle point problem f(x,y) = xy with the known analytical solution.\", \"5. Numerically compute the eigenvalues of the game's Hessian at different stages of training and check if they satisfy the derived theoretical conditions as the model approaches the saddle point.\", \"6. Synthesize all findings into a report, comparing the theoretical predictions with the empirical results from the simulation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent verifies that the eigenvalue conditions implemented in /code match the derivation; /code agent verifies that the model converges as predicted by theory. fallback: if derivation is intractable, /math agent should simplify the problem to a quadratic case; if code fails to converge, /code agent should debug optimizer hyperparameters and report stability issues.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers (\u22652) on extragradient methods for GAN convergence and Hessian analysis.\", \"- [ ] /math: Derive the eigenvalue conditions for local convergence of the extragradient method applied to the GAN objective.\", \"- [ ] /code: Implement the toy GAN, extragradient optimizer, and a function to compute the Hessian of the game dynamics.\", \"- [ ] /math: Verify that the symbolic Hessian derivation aligns with the numerical Hessian computation logic planned for the code.\", \"- [ ] /code: Run the simulation, log the Hessian eigenvalues over time, and generate plots showing their evolution.\", \"- [ ] /code: Validate that the empirical convergence behavior aligns with the theoretical predictions from the /math agent.\", \"- [ ] router: Review all generated artifacts (derivation, code, plots, report) for consistency and correctness before finalizing.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"saddle-point-analysis\", \"generative-adversarial-networks\", \"game-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly identifies the conditions on the Hessian's eigenvalues for local convergence and is supported by citations.\", \"- The PyTorch implementation correctly implements the extragradient method and successfully trains the toy GAN to generate the target distribution.\", \"- The final report includes plots showing that the empirically measured Hessian eigenvalues satisfy the derived theoretical conditions as training converges.\", \"- The provided code repository is runnable and reproduces the results presented in the report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 353, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the continuous-time update rule for a neural network's weights by treating training as a variational problem. Formulate an energy functional incorporating both a loss term and a regularization term based on the network's Jacobian spectral norm. Then, apply the Euler-Lagrange equation to find the optimal weight trajectory, and implement this optimizer in PyTorch, verifying its convergence properties on a synthetic dataset against standard methods like Adam, while ensuring the Lipschitz continuity constraint is respected.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org calculus of variations deep learning optimizer Euler-Lagrange equation spectral norm\\\", mode=rag)\", \"/math(Derive the Euler-Lagrange equation for the energy functional. Formulate the Lagrangian and compute partial derivatives to establish the continuous-time update rule, referencing foundational control theory papers found.)\", \"/code(Implement the derived update rule as a PyTorch optimizer, including a power iteration method for tracking the Jacobian's spectral norm and benchmark its convergence against Adam on a synthetic regression task., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find foundational literature on this advanced topic. The findings are then passed to /math for the core theoretical derivation. Finally, /code implements the derived mathematical formula and runs empirical validation, a standard theory-to-practice workflow.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the Euler-Lagrange optimizer.\", \"A Python script implementing the custom PyTorch optimizer and the benchmarking experiment.\", \"A summary report with plots comparing the custom optimizer's convergence and final loss against Adam.\", \"A list of at least two cited arXiv papers that informed the derivation.\", \"A verification log confirming the spectral norm constraint was satisfied during training.\"], \"thinking_outline\": [\"1. Research foundational papers on variational principles and optimal control applied to deep learning optimizers.\", \"2. Formulate the energy functional and constraints precisely based on the user query and literature.\", \"3. Derive the Euler-Lagrange equation for this functional to obtain the optimal weight update differential equation.\", \"4. Discretize the continuous-time update rule to make it computationally feasible for a practical optimizer implementation.\", \"5. Validate the mathematical derivation by checking for consistency with established principles and verifying the dimensional correctness of all terms.\", \"6. Implement the discretized optimizer and empirically verify its convergence behavior against standard optimizers, auditing the spectral norm constraint enforcement.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses them for the formal derivation -> /code implements the derived equations. Verification: The final discretized equation from /math serves as the specification for the /code implementation. Fallback: If the /code implementation shows poor convergence or instability, the discretization step in the /math derivation will be re-evaluated for numerical stability.\", \"todo_list\": [\"- [ ] /general-search: Find authoritative papers on variational optimizers and Euler-Lagrange equations in ML.\", \"- [ ] /math: Formulate the energy functional and derive the continuous-time update rule using the calculus of variations.\", \"- [ ] /math: Discretize the continuous-time rule into a practical, implementable algorithm.\", \"- [ ] /code: Implement the discretized update rule as a custom PyTorch optimizer class.\", \"- [ ] /code: Implement a benchmarking suite on a synthetic regression task to compare against Adam.\", \"- [ ] /math: Verify that the implemented logic in the /code step correctly matches the discretized mathematical derivation.\", \"- [ ] /code: Verify at runtime that the spectral norm constraint on the model's Jacobian is being respected within a given tolerance.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"calculus-of-variations\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"The mathematical derivation in the LaTeX document is complete, self-contained, and mathematically sound.\", \"The Python code runs without errors and produces convergence plots for both the custom optimizer and the Adam baseline.\", \"The final report demonstrates that the custom optimizer successfully trains a model, and the spectral norm constraint is verifiably maintained throughout training.\", \"The solution cites at least two relevant arXiv pre-prints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 355, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Knill-Laflamme conditions for quantum error correction starting from the operator-sum representation of a general quantum channel. Then, implement a Python simulation using the stabilizer formalism for the 5-qubit perfect code, subjecting it to a depolarizing noise model, and analyze the logical error rate as a function of the physical error probability to verify its corrective capabilities.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org quantum error correction Knill-Laflamme conditions stabilizer formalism\\\", mode=both)\", \"/math(Derive the Knill-Laflamme conditions for a completely positive trace-preserving map using the Kraus operator-sum representation, analyzing the computational complexity of checking the conditions and the resulting subspace's eigenvalue structure.)\", \"/code(Implement the 5-qubit perfect code in Python, simulating its performance against a depolarizing channel. Calculate the logical error rate and its confidence interval using Bayesian inference over repeated trials to validate the code's fault-tolerant properties.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative literature on the core mathematical concepts (Knill-Laflamme, stabilizers). This research grounds the formal derivation performed by /math. Finally, /code provides empirical validation by simulating a specific code described in the literature, checking if the mathematical theory holds in practice.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full derivation of the Knill-Laflamme conditions.\", \"- A Python script (`simulation.py`) implementing the 5-qubit code and depolarizing noise simulation.\", \"- A plot (`fidelity_vs_error.png`) showing the logical error rate versus the physical error probability.\", \"- A JSON file (`metrics.json`) reporting the final logical error rate at a specific physical error probability.\", \"- A markdown report summarizing the findings and citing at least two arXiv papers.\"], \"thinking_outline\": [\"1. Research the foundational theory of quantum error correction, focusing on quantum channels, Kraus operators, and the stabilizer formalism.\", \"2. Formulate and execute the step-by-step mathematical derivation of the Knill-Laflamme conditions from first principles.\", \"3. Validate the derivation against established results found in the literature (e.g., Nielsen & Chuang) to ensure correctness.\", \"4. Design and implement a numerical simulation of the 5-qubit perfect code, including encoding, applying a depolarizing noise channel, syndrome measurement, and recovery operations.\", \"5. Execute the simulation across a range of physical error probabilities, collecting statistics on the logical error rate.\", \"6. Verify that the simulation results demonstrate a crossover point where the logical error rate becomes lower than the physical error rate, confirming the code's corrective power.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses these sources to produce a formal derivation -> /code implements the theory from the derivation for a specific code and generates empirical data -> router QA (verification: code's fidelity curve matches theoretical expectations for the 5-qubit code; fallback: if simulation fails to show error correction, re-audit the /math derivation and /code implementation for errors).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two high-authority papers on Knill-Laflamme conditions and the 5-qubit code.\", \"- [ ] /math: Produce a self-contained, step-by-step derivation of the error correction conditions in LaTeX.\", \"- [ ] /code: Develop a Python simulation using a quantum computing framework (e.g., Qiskit) for the 5-qubit code.\", \"- [ ] /code: Implement the depolarizing noise model and the complete error correction circuit (encoding, noise, decoding).\", \"- [ ] /math: Verify that the matrix representations of the stabilizer generators in the code align with the mathematical derivation's assumptions.\", \"- [ ] /code: Generate a plot of logical vs. physical error rate and a final metrics report.\", \"- [ ] /code: Validate that the simulation's output is statistically sound and the results are reproducible.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction\", \"stabilizer-codes\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Knill-Laflamme conditions must be rigorous and complete.\", \"- The Python simulation must correctly implement the 5-qubit code's stabilizer generators and recovery operations.\", \"- The final plot must clearly show that for a non-zero range of physical error probabilities 'p', the logical error rate 'p_L' is less than 'p'.\", \"- The final report must cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 358, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on an inverse problem for the 2D incompressible Navier-Stokes equations and need to develop a robust Physics-Informed Neural Network (PINN). Standard PINNs are failing due to noisy boundary data. Can you formulate and implement an adversarial training scheme to regularize the solution space, prove its convergence properties within a Sobolev space framework, and demonstrate its superiority over a baseline L2-regularized PINN for estimating the viscosity parameter from sparse velocity field measurements?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org adversarial physics-informed neural networks Navier-Stokes Sobolev training\\\", mode=rag)\", \"/math(Derive the minimax objective for an adversarial PINN solving the inverse Navier-Stokes problem. Formulate the loss in terms of Sobolev norms and provide a proof sketch for convergence, analyzing the Hessian eigenvalue spectrum for stability.)\", \"/code(Implement the adversarial PINN in JAX for the 2D Navier-Stokes inverse problem. Use Bayesian-inspired dropout for uncertainty quantification and track convergence metrics, ensuring the implementation matches the math derivation., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on adversarial PINNs for fluid dynamics. This foundational knowledge is then passed to /math to create a rigorous theoretical framework, including the specific loss formulation and convergence proof. Finally, /code implements this mathematical specification to produce a working model and validate the approach empirically.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the adversarial loss function and a proof sketch for its convergence.\", \"- A documented Python script implementing the adversarial PINN for the Navier-Stokes inverse problem using JAX.\", \"- A summary report including a literature review with citations, plots comparing the adversarial PINN vs. baseline, and analysis of convergence metrics.\", \"- A JSON file containing the final computed metrics (L2 error, PDE residual, etc.).\", \"- Verification output confirming the symbolic gradient derivation matches the backpropagation implementation.\"], \"thinking_outline\": [\"1. Deconstruct the inverse problem for the Navier-Stokes equations, identifying the roles of the PDE residual, boundary conditions, sparse data, and the adversarial regularizer.\", \"2. Survey recent literature via /general-search to find established architectures and training schemes for adversarial PINNs, particularly those referencing Sobolev training or fluid dynamics.\", \"3. Formulate the complete minimax optimization problem, defining the generator (the PINN) and the discriminator networks, and derive the gradient update rules for both players.\", \"4. Verify the analytical gradients of the PDE residual with respect to the network inputs and parameters against a symbolic math engine's output.\", \"5. Implement the full model and training loop in JAX, ensuring numerical stability and correct handling of the alternating gradient updates.\", \"6. Validate the final model by comparing the estimated viscosity and reconstructed velocity field against a known ground-truth solution, auditing the L2 relative error and the final PDE residual.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: research findings from search inform the mathematical derivation in /math, which in turn provides the exact specification for the /code implementation. The code's performance is checked against theoretical expectations; fallback: if the convergence proof in /math is intractable, pivot to an empirical stability analysis in /code and note the limitation).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on adversarial or generative models for solving ill-posed inverse PDE problems.\", \"- [ ] /math: Derive the complete loss function, including PDE, boundary, data, and adversarial terms.\", \"- [ ] /math: Provide a formal sketch of the convergence proof under suitable assumptions on the solution space.\", \"- [ ] /code: Implement the PINN, discriminator network, and the minimax optimization loop in JAX.\", \"- [ ] /code: Generate comparison plots of the predicted vs. true velocity fields and loss convergence curves.\", \"- [ ] /math: Verify that the PDE residual implementation in the code is a correct discretization of the continuous form derived.\", \"- [ ] /code: Validate that the trained model produces physically plausible results and achieves a lower error than a non-adversarial baseline.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"tensor-calculus\", \"inverse-problems\"], \"acceptance_criteria\": [\"- The mathematical derivation of the adversarial loss and its gradients must be complete and self-consistent.\", \"- The JAX code must execute without errors and successfully train the model, showing a decrease in both generator and discriminator losses over time.\", \"- The final report must show the adversarial PINN achieves at least a 15% reduction in the primary metric (L2 relative error) compared to a baseline PINN.\", \"- All required artifacts (derivation, code, report, metrics) must be submitted and internally consistent.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 360, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am tasked with implementing Trust Region Policy Optimization (TRPO) from first principles. My primary goal is to formally derive the natural policy gradient update, specifically focusing on how the Fisher Information Matrix approximates the second-order Taylor expansion of the KL divergence constraint. The implementation must use the conjugate gradient algorithm to efficiently compute the Fisher-vector product without explicitly forming the FIM and demonstrate stable convergence in a high-dimensional, adversarial continuous control theory environment.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:jmlr.org 'Trust Region Policy Optimization' 'natural policy gradient' 'conjugate gradient for Fisher-vector product'\\\", mode=web)\", \"/math(Derive the TRPO surrogate objective function with the KL divergence constraint. Formulate the conjugate gradient method to solve for the policy update direction, analyzing the Hessian's positive-definiteness and potential eigenvalue issues for convergence.)\", \"/code(Implement the TRPO agent in Python with PyTorch. Build the conjugate gradient solver and a backtracking line search to enforce the trust region constraint. Validate the agent's convergence on a continuous control theory benchmark, tracking adversarial robustness.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational papers on TRPO and natural gradients. This research informs the /math agent's formal derivation of the optimization problem. Finally, the /code agent implements the derived algorithm, directly translating the mathematical specification into a verifiable, high-performance program.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the TRPO update step, including the conjugate gradient optimization.\", \"A Python script containing the complete, commented TRPO agent implementation.\", \"A performance report with plots showing mean episode reward and average KL divergence per epoch.\", \"A bibliography file citing at least two seminal papers on policy gradients with their arXiv IDs.\", \"A verification log confirming that the conjugate gradient solver converges and the KL divergence constraint is met on >95% of updates.\"], \"thinking_outline\": [\"1. Research the seminal papers on Trust Region Policy Optimization and natural policy gradients to understand the theoretical foundations.\", \"2. Formally derive the surrogate advantage objective and the quadratic approximation of the KL divergence constraint using the Fisher Information Matrix.\", \"3. Implement the TRPO agent, paying close attention to the conjugate gradient algorithm for solving Ax=b where A is the FIM and b is the gradient, and the backtracking line search to satisfy the constraint.\", \"4. Verify the mathematical derivation against the implementation logic, ensuring the Fisher-vector product is computed correctly without instantiating the full FIM.\", \"5. Train the agent in a suitable continuous control environment (e.g., MuJoCo's Hopper) and validate its performance by checking for monotonic reward improvement and consistent adherence to the KL divergence trust region.\", \"6. Collate all derivations, code, and experimental results into a comprehensive final report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The final code's performance metrics must align with published TRPO benchmarks, and the KL divergence must stay within the specified trust region delta. fallback: If convergence fails, /code will revert to /math to re-verify the derivation's assumptions or simplify the conjugate gradient implementation.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize the original TRPO paper (Schulman et al.) and a key paper on natural policy gradients.\", \"- [ ] /math: Produce a step-by-step derivation of the conjugate gradient update for the natural policy gradient.\", \"- [ ] /code: Implement the TRPO agent, including policy and value networks, in PyTorch.\", \"- [ ] /code: Implement the conjugate gradient solver and backtracking line search logic.\", \"- [ ] /math: Verify that the code's implementation of the Fisher-vector product matches the mathematical derivation.\", \"- [ ] /code: Validate that the average KL divergence per update remains below the configured delta during training.\", \"- [ ] /router: Review all generated artifacts for correctness, clarity, and consistency before final submission.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"control-theory\"], \"acceptance_criteria\": [\"- The submitted code must successfully train an agent that achieves a mean episodic reward of at least 2000 on the Hopper-v4 environment.\", \"- The formal mathematical derivation must be complete, self-contained, and consistent with the cited literature.\", \"- The performance report must clearly show that the average KL divergence per policy update was consistently held below the specified delta (e.g., 0.01).\", \"- The project must cite at least two relevant arXiv papers, including the original TRPO publication.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 361, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating few-shot learning and need to move beyond standard MAML. Please provide a full theoretical derivation for a probabilistic or Bayesian variant of MAML, treating the meta-parameters as a distribution. Then, implement this Bayesian MAML algorithm, validate its gradient formulation, and empirically evaluate its uncertainty quantification capabilities on the mini-ImageNet benchmark compared to the baseline point-estimate MAML.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Bayesian MAML variational inference hierarchical models\\\", mode=both)\", \"/math(Derive the variational objective (ELBO) for a hierarchical Bayesian interpretation of MAML, detailing the gradient estimators for both task-specific adaptation and meta-level updates to ensure theoretical convergence.)\", \"/code(Implement Bayesian MAML in PyTorch for few-shot classification on mini-ImageNet, incorporating metrics like AUPRC and Expected Calibration Error, and validate gradient computations against the formal Bayesian derivation., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational papers on Bayesian MAML. This research informs the /math agent's formal derivation of the ELBO and update rules. Finally, the /code agent implements the derived algorithm, running experiments and performing numerical validation based on the mathematical proofs.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Bayesian MAML objective function and gradient estimators in LaTeX.\", \"- A complete, commented Python implementation of the algorithm and evaluation harness.\", \"- A summary report including plots of training convergence, few-shot accuracy, and uncertainty calibration metrics (e.g., reliability diagrams).\", \"- A bibliography with at least two cited arXiv preprints.\"], \"thinking_outline\": [\"1. Frame Model-Agnostic Meta-Learning (MAML) within a hierarchical Bayesian model, where meta-parameters are latent variables with a prior distribution.\", \"2. Derive the Evidence Lower Bound (ELBO) for this model using variational inference to create a tractable meta-objective function.\", \"3. Formulate the stochastic gradient estimators for the meta-parameters, carefully handling the nested differentiation required by the MAML inner loop.\", \"4. Implement the algorithm and validate the correctness of the analytical gradients by comparing them with numerical gradients computed via finite differences on a toy problem.\", \"5. Conduct experiments on a standard few-shot benchmark (e.g., 5-way 1-shot mini-ImageNet) to compare accuracy and uncertainty calibration against a standard MAML implementation.\", \"6. Verify that the experimental results, particularly the improved calibration scores, align with the theoretical motivation and claims from the literature review.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation based on papers) -> /code (implementation of derivation). /math must provide a clear, step-by-step derivation for /code to implement. /code must verify its gradient implementation against the analytical formulas from /math. Router QA verifies final report against both derivation and code. Fallback: If derivation is intractable, /math agent should propose a simplified or alternative Bayesian formulation and notify the router for plan adjustment.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Bayesian MAML or probabilistic meta-learning.\", \"- [ ] /math: Produce a self-contained derivation of the variational objective and its gradients for Bayesian MAML.\", \"- [ ] /code: Implement the Bayesian MAML algorithm, the data loader for mini-ImageNet, and the training loop.\", \"- [ ] /code: Implement the evaluation logic, including accuracy and Expected Calibration Error (ECE) metrics.\", \"- [ ] /math: Verify that the final gradient update rules derived are consistent with the principles of variational inference and reparameterization tricks.\", \"- [ ] /code: Audit the implementation by adding a numerical gradient check to confirm the analytical gradients are correct.\", \"- [ ] /code: Run the final experiments and generate all plots and result tables for the report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"bayesian-inference\", \"few-shot-learning\"], \"acceptance_criteria\": [\"- The provided LaTeX derivation of the Bayesian MAML objective must be mathematically sound and self-contained.\", \"- The Python code must successfully run and reproduce the reported few-shot classification accuracy on a validation set within +/- 2%.\", \"- The final report must show a quantitative improvement in uncertainty calibration (lower ECE) for Bayesian MAML over a baseline MAML implementation.\", \"- The code must include a passing numerical gradient check that compares the analytical implementation to finite differences.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 362, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the connection between the spectral properties of a molecular Hamiltonian, specifically its eigenvalue distribution, and the emergence of barren plateaus in Variational Quantum Eigensolvers (VQE). Derive the formal expression for the gradient variance decay, then implement a simulation in Python to demonstrate this phenomenon for the LiH molecule, and finally, test a layer-wise learning strategy to mitigate the issue.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational quantum eigensolver 'barren plateau' gradient variance Hamiltonian eigenvalue gap layerwise learning\\\", mode=both)\", \"/math(Derive the analytical expression for the variance of the cost function gradient for a deep parameterized quantum circuit, connecting it to the Hamiltonian's eigenvalue properties and showing its exponential decay with the number of qubits.)\", \"/code(Implement a VQE simulation for the LiH molecular Hamiltonian using Pennylane, ensuring the ansatz architecture is susceptible to barren plateaus. Track gradient variance and energy convergence metrics carefully., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather the latest theoretical foundations on barren plateaus and mitigation techniques from authoritative sources like arXiv. This literature informs the /math agent, which will perform a formal derivation of the gradient variance. Finally, the /code agent implements the VQE simulation based on the theoretical work, providing an empirical validation of the derived principles and testing a mitigation strategy.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the gradient variance decay.\", \"A Python script implementing the VQE simulation, the barren plateau measurement, and the layer-wise learning mitigation.\", \"A JSON file reporting the final ground state energy error and the measured gradient variance scaling.\", \"A plot visualizing the exponential decay of the gradient variance against the number of qubits.\", \"A bibliography file with citations to at least two relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Research the theoretical basis of barren plateaus in quantum neural networks, focusing on the role of 2-designs and the concentration of measure.\", \"2. Formulate the mathematical derivation for the variance of the partial derivative of the VQE cost function, expressing it in terms of the Hamiltonian operator and circuit properties.\", \"3. Implement the VQE algorithm for a specific molecular Hamiltonian (LiH), ensuring the simulation can reproduce the barren plateau phenomenon.\", \"4. Validate that the numerically computed gradient variance from the simulation decays exponentially, consistent with the mathematical derivation.\", \"5. Implement and evaluate a mitigation strategy, such as a curriculum-based or layer-wise training approach, and quantify its effectiveness.\", \"6. Check the final energy convergence against known results for LiH to verify the simulation's physical accuracy.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent hands off key theoretical papers (arXiv IDs) to /math. The /math agent provides a final, verified derivation to /code. The /code agent implements this, and its empirical results must match the theoretical predictions from /math. Verification: The router will check that the gradient variance plot from /code aligns with the scaling law from /math. Fallback: If there's a mismatch, /code must revisit ansatz construction and /math must re-check the derivation's assumptions.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize \u22652 seminal papers on barren plateaus and mitigation strategies like layer-wise learning.\", \"- [ ] /math: Formally derive the expression for Var[\u2202C/\u2202\u03b8_k] for a global cost function, linking it to the Hamiltonian's eigenvalue spectrum.\", \"- [ ] /code: Set up the Pennylane environment, define the LiH molecular Hamiltonian, and construct a hardware-efficient ansatz.\", \"- [ ] /code: Implement the VQE loop and a function to explicitly calculate the gradient variance across multiple random initializations.\", \"- [ ] /math: Verify that the symbolic gradient implemented in the code matches the formal mathematical derivation.\", \"- [ ] /code: Implement the layer-wise learning protocol and compare its convergence and gradient variance to the baseline VQE.\", \"- [ ] /code: Generate plots of energy convergence and gradient variance vs. number of qubits/layers.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"variational-algorithms\", \"barren-plateaus\"], \"acceptance_criteria\": [\"- The mathematical derivation must correctly show that gradient variance vanishes exponentially with the number of qubits for a global cost function.\", \"- The Python simulation must successfully reproduce a barren plateau, evidenced by a plot showing gradient variance decaying as O(1/2^n).\", \"- The implemented mitigation strategy must demonstrate a statistically significant improvement in convergence speed or final energy accuracy compared to the naive VQE.\", \"- The final computed ground state energy for LiH must be within 5% of the known value from classical quantum chemistry calculations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 364, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for a Hamiltonian Graph Neural Network (HGN) that preserves symplectic structure for long-term energy conservation in molecular dynamics simulations. Detail the tensor calculus required for the E(3)-equivariant message passing layers and formally prove that the learned Hamiltonian flow remains time-reversible and conserves the symplectic 2-form after discretization.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hamiltonian graph neural network symplectic integrator E(3)-equivariant molecular dynamics\\\", mode=both)\", \"/math(Derive the continuous-time Hamiltonian dynamics using tensor calculus and prove that its discretization via a symplectic Euler-Lagrange integrator preserves the system's total energy and phase space volume.)\", \"/code(Implement the derived Hamiltonian GNN in PyTorch Geometric, using CUDA for acceleration. Verify energy conservation over a long simulation trajectory and check for numerical drift against a baseline Runge-Kutta integrator., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on equivariant Hamiltonian GNNs. These papers are then handed to /math for the core theoretical task of deriving the dynamics and proving conservation laws. Finally, the verified mathematical formulation is passed to /code for implementation and empirical validation of the theoretical claims.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the equivariant Hamiltonian dynamics and the proof of symplectic preservation.\", \"A Python script implementing the Hamiltonian GNN and a simulation environment for a simple molecular system.\", \"A summary report citing at least two relevant arXiv pre-prints with their IDs.\", \"A PNG plot comparing the energy drift of the symplectic HGN against a non-symplectic baseline integrator over time.\", \"A JSON file containing the final computed `mean_energy_drift` metric.\"], \"thinking_outline\": [\"1. Research and synthesize foundational concepts from literature on Hamiltonian mechanics, symplectic geometry, and E(3)-equivariant neural networks for molecular systems.\", \"2. Formulate the GNN architecture to learn a Hamiltonian function that is invariant to translations, rotations, and reflections (E(3) group transformations).\", \"3. Derive the partial derivatives of the learned Hamiltonian with respect to generalized coordinates (positions) and momenta using tensor calculus to define the equations of motion.\", \"4. Formally prove that discretizing these equations of motion using a specific symplectic integrator (e.g., Leapfrog/Verlet) preserves the symplectic 2-form, ensuring long-term energy conservation.\", \"5. Validate the implementation by simulating a small N-body system and measuring the energy drift over thousands of timesteps, comparing it against a non-conservative baseline.\", \"6. Audit the final mathematical proof for rigor and verify its direct correspondence to the update rules implemented in the code.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: the implemented update rule in /code must be a direct, term-for-term discretization of the final equations derived in /math; fallback: if the code's energy drift is high, re-audit the symplectic integration derivation in /math for potential errors).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers and recent pre-prints on Hamiltonian GNNs and E(3)-equivariant networks on arXiv.\", \"- [ ] /math: Derive the Hamiltonian equations of motion for a graph-represented molecular system, ensuring equivariance.\", \"- [ ] /math: Formally prove that the chosen symplectic integrator preserves the system's energy and phase-space structure.\", \"- [ ] /code: Implement the equivariant layers and the symplectic integrator update rule in a Python framework.\", \"- [ ] /code: Run a simulation and generate a plot comparing energy drift against a non-symplectic baseline.\", \"- [ ] /math: Verify that the implemented update rule in the /code artifact is a correct discretization of the derived continuous-time dynamics.\", \"- [ ] /code: Validate that the empirical energy drift from the simulation is below the 1e-5 tolerance threshold.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"symplectic-geometry\", \"equivariant-networks\"], \"acceptance_criteria\": [\"- The mathematical proof correctly and rigorously demonstrates the preservation of the symplectic 2-form under the discretized dynamics.\", \"- The implemented HGN model exhibits a `mean_energy_drift` of less than 1e-5 over a 10,000-step simulation, outperforming a 4th-order Runge-Kutta baseline by at least two orders of magnitude.\", \"- The final report includes the complete derivation, the comparative energy plot, and citations for at least 2 relevant arXiv pre-prints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 366, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a robust implementation of Bayesian Model-Agnostic Meta-Learning (BMAML) for few-shot sinusoidal regression. Please derive the meta-objective using a variational approximation for the meta-posterior, employ a Laplace approximation for the task-specific parameter updates, and provide a JAX implementation that quantifies epistemic uncertainty. The focus is on the rigorous mathematical formulation and its verified translation to code.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian model-agnostic meta-learning laplace approximation posterior\\\", mode=both)\", \"/math(Derive the Bayesian MAML objective and gradient estimators based on the Laplace approximation for the task-specific posterior, ensuring theoretical convergence properties and analyzing the Hessian eigenvalue spectrum.)\", \"/code(Implement the BMAML algorithm in JAX with CUDA acceleration, using bootstrap sampling for meta-batching and including runtime checks to validate the Hessian computation against the mathematical derivation., using Python)\"], \"route_rationale\": \"The blue /general-search agent will first gather foundational literature on Bayesian MAML and Laplace approximations. This theoretical grounding is then passed to /math to produce a formal, verifiable derivation. Finally, /code will translate the mathematical specification into an efficient, validated JAX implementation.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the BMAML update rules and the Laplace approximation.\", \"A well-documented JAX implementation of the BMAML algorithm, including meta-training and evaluation scripts.\", \"A results report summarizing the model's few-shot regression performance (MSE) and uncertainty calibration (NLL).\", \"A bibliography file citing at least two relevant arXiv papers with full IDs.\", \"A verification log cross-referencing equations from the derivation with specific functions in the code.\"], \"thinking_outline\": [\"1. Review seminal papers on Bayesian meta-learning, focusing on the probabilistic interpretation of MAML and the use of Laplace approximations for posteriors.\", \"2. Formally derive the evidence lower bound (ELBO) for the meta-learning objective, clearly defining the prior and variational distributions over meta-parameters.\", \"3. Derive the gradient update for the meta-parameters, detailing the application of the Laplace approximation to the inner-loop adaptation step for each task.\", \"4. Implement the derived algorithm in JAX, paying close attention to the numerical stability of the Hessian inversion required for the Laplace approximation.\", \"5. Verify the implementation by creating unit tests that check the dimensions and values of the computed gradients against manually calculated examples for a simple model.\", \"6. Audit the final model's performance on a standardized few-shot regression benchmark, ensuring the reported uncertainty estimates are well-calibrated.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. /general-search provides key papers to /math. /math produces a formal derivation which serves as a blueprint for /code. /code's implementation is then cross-verified against the derivation from /math. Fallback: If /math finds ambiguities in the literature, it will request a more targeted query from /general-search. If the /code implementation fails verification against the math, it is returned to the /code agent for correction.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Bayesian MAML, specifically those using Laplace approximations.\", \"- [ ] /math: Derive the meta-objective function and its gradients, specifying the exact form of the Laplace approximation for the task-level posteriors.\", \"- [ ] /math: Verify the analytical consistency of the derived gradients, for instance by checking limiting cases.\", \"- [ ] /code: Implement the BMAML meta-training loop in JAX, including the Hessian calculation and inversion.\", \"- [ ] /code: Implement an evaluation harness for the few-shot sinusoidal regression task.\", \"- [ ] /code: Add assertions to validate the shape and positive-definiteness of the Hessian matrix during runtime.\", \"- [ ] /router: Review the generated derivation, code, and report to ensure they align with the user query and acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"bayesian-methods\", \"few-shot-learning\"], \"acceptance_criteria\": [\"- The provided mathematical derivation must clearly show the steps for deriving the meta-gradient using the Laplace approximation.\", \"- The JAX code must successfully run and demonstrate meta-learning (i.e., test MSE decreases over meta-training epochs) on a sinusoidal regression task.\", \"- The final report must include a plot showing the 5-shot test MSE and NLL on held-out tasks, with uncertainty bands visualized.\", \"- The code must pass a verification check where key computational functions are manually cross-referenced with the equations in the derivation document.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 367, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the computational complexity of determining winning strategies in a partisan combinatorial game defined on a directed acyclic graph where players Red and Blue can only move tokens of their own color. The game follows the misere play convention. Specifically, I need a formal proof of its complexity class, contrasting it with the normal play equivalent and the inapplicability of the Sprague-Grundy theorem. How does the game state's Kolmogorov complexity relate to finding optimal adversarial moves?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org partisan combinatorial game misere play complexity PSPACE\\\", mode=both)\", \"/math(Formalize the game's P-positions and N-positions under the misere convention and construct a proof, likely via reduction, to determine the game's computational complexity class. Analyze convergence properties of the game value.)\", \"/code(Implement a Python solver for the partisan game using NetworkX for graph representation. The implementation must handle adversarial logic and validate the derived complexity on generated DAGs, storing results in a local database for analysis.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey existing literature on partisan game complexity, ensuring our approach is state-of-the-art. This research informs the /math agent's formal proof construction. Finally, the /code agent implements the derived theory to provide empirical validation, a crucial step for verifying theoretical computer science claims.\", \"expected_artifacts\": [\"A LaTeX document containing the formal proof of the game's computational complexity.\", \"A commented Python script implementing the game solver and complexity analysis.\", \"A summary report (report.md) integrating the theoretical findings, empirical results, and literature review.\", \"A log file (verification.log) detailing the cross-validation between the mathematical proof and the code's output on test cases.\", \"A JSON file (metrics.json) containing the computed primary and secondary metrics.\"], \"thinking_outline\": [\"1. Formalize the rules of the specified partisan, misere-play game on a directed acyclic graph, defining terminal positions and player moves.\", \"2. Derive the recursive definitions for P-positions (previous player winning) and N-positions (next player winning), accounting for the complexities introduced by the misere convention in partisan games.\", \"3. Construct a formal proof, likely by creating a polynomial-time reduction from a known PSPACE-complete problem like Quantified Boolean Formulas (QBF), to classify the game's complexity.\", \"4. Validate the proof's logic by implementing a solver and verifying its output against manually-solved small game instances.\", \"5. Analyze the asymptotic runtime of the solver on procedurally generated graphs to empirically check for consistency with the proven complexity class.\", \"6. Audit the final proof for any logical gaps and ensure the summary report correctly synthesizes the theoretical and empirical evidence.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent's proof provides the theoretical foundation and recurrence relations for the /code agent's solver; the /code agent's empirical results are used to validate the /math agent's complexity claims. fallback: if a PSPACE-completeness proof is intractable, pivot to proving membership in PSPACE and establishing P-hardness).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on partisan game complexity under the misere play convention.\", \"- [ ] /math: Define the game formally and derive the win/loss state recurrence relations.\", \"- [ ] /math: Construct the full complexity proof, including the reduction from a known PSPACE-complete problem.\", \"- [ ] /code: Implement the game solver in Python, including functions for graph generation and state evaluation.\", \"- [ ] /code: Instrument the solver to measure runtime and compare its scaling against the theoretical complexity.\", \"- [ ] /math: Verify that the implemented logic in the code correctly reflects the derived mathematical recurrences.\", \"- [ ] /code: Validate the solver's output against a set of manually computed small-graph examples.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"Combinatorial Game Theory\", \"Algorithmic Complexity\", \"PSPACE-completeness\"], \"acceptance_criteria\": [\"A logically sound, step-by-step proof classifying the game's complexity as PSPACE-complete is provided.\", \"The Python implementation correctly identifies the winning player for all test graphs with up to 15 vertices.\", \"The final report cites at least 2 relevant arXiv preprints with their IDs, as per the citation policy.\", \"The empirical runtime analysis is consistent with the proven theoretical complexity class.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 368, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze a federated learning scheme that uses Ring-LWE based fully homomorphic encryption for privacy-preserving gradient aggregation. Please provide a formal proof of its semantic security under a semi-honest adversarial model, analyze the impact of cryptographic noise accumulation on the convergence rate, and develop a simulation to validate the theoretical bounds.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning 'ring learning with errors' homomorphic encryption convergence analysis\\\", mode=rag)\", \"/math(Derive the epsilon-delta privacy proof for federated averaging under Ring-LWE encryption, analyzing noise propagation and its impact on the algorithm's convergence rate.)\", \"/code(Implement a Python simulation of the secure aggregation protocol using the SEAL or TenSEAL library, including adversarial checks and a Bayesian analysis of parameter uncertainty.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on Ring-LWE in federated learning from arXiv. This research informs the /math agent, which will construct the formal security proof and convergence analysis. Finally, the /code agent implements the scheme to empirically validate the theoretical findings from the math agent, providing a concrete demonstration.\", \"expected_artifacts\": [\"A LaTeX document containing the formal security proof and convergence analysis.\", \"A Python script implementing the simulated federated learning environment with homomorphic aggregation.\", \"A JSON file reporting the final model accuracy, epsilon-delta privacy budget, and convergence rate.\", \"A summary of verification steps, confirming the alignment between the theoretical proof and the empirical simulation results.\", \"A bibliography with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Survey existing literature on Ring-LWE based homomorphic encryption schemes tailored for federated learning gradient aggregation.\", \"2. Formulate the precise security game under a semi-honest adversarial model and derive the formal proof of semantic security, quantifying the privacy leakage in terms of an epsilon-delta budget.\", \"3. Model the noise growth in the ciphertexts over successive aggregation rounds and derive its impact on the convergence guarantees of the underlying optimization algorithm (e.g., FedAvg).\", \"4. Implement the federated learning protocol and the homomorphic encryption scheme in a simulation environment.\", \"5. Verify that the implementation correctly reflects the mathematical operations and security parameters specified in the proof.\", \"6. Validate the theoretical convergence bounds against the empirical performance observed in the simulation, analyzing any discrepancies.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The formal proof from /math must be directly implemented by /code, with security parameters and noise models matching. /code's empirical convergence results must be checked against /math's theoretical bounds; fallback: If a discrepancy is found, re-engage /math to refine the theoretical model or /code to debug the implementation.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Ring-LWE for federated learning.\", \"- [ ] /math: Formulate the security definition and derive the privacy proof as a function of the cryptographic parameters.\", \"- [ ] /math: Analyze the noise growth and derive the theoretical convergence rate.\", \"- [ ] /code: Implement the client-server simulation for federated averaging.\", \"- [ ] /code: Integrate a Python HE library to simulate the secure aggregation protocol.\", \"- [ ] /math: Verify that the noise model in the proof aligns with the cryptographic operations implemented in the code.\", \"- [ ] /code: Run experiments and generate plots comparing empirical vs. theoretical convergence.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"homomorphic-encryption\", \"cryptography\"], \"acceptance_criteria\": [\"The submitted LaTeX proof for epsilon-delta privacy must be mathematically rigorous and complete.\", \"The Python simulation must execute without errors and reproduce the claimed privacy/utility trade-off.\", \"The final report must show that the empirical convergence rate is within a 15% margin of the derived theoretical bound.\", \"The solution must cite at least two relevant academic papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 369, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and analyze the Augmented Lagrangian Method (ALM) to enforce a 1-Lipschitz constraint on a convolutional neural network by constraining the spectral norm of its weight matrices. Please provide a full derivation of the primal-dual updates, a corresponding PyTorch implementation, and an empirical analysis of its convergence behavior and its effect on adversarial robustness against PGD attacks, including a discussion of the Karush-Kuhn-Tucker conditions.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org augmented lagrangian method ADMM lipschitz neural network adversarial convergence\\\", mode=both)\", \"/math(Derive the primal-dual update steps for the Augmented Lagrangian targeting spectral norm constraints on a deep network; analyze convergence based on the Hessian eigenvalue properties and its connection to Bayesian methods.)\", \"/code(Implement the derived ADMM optimizer in Python/PyTorch, training a model for adversarial robustness; track AUPRC and the constraint violation, ensuring the CUDA implementation is deterministic for reproducibility.)\"], \"route_rationale\": \"The blue general-search agent is first used to gather state-of-the-art literature on ALM for Lipschitz networks, which is a rapidly evolving area. The /math agent then takes this context to produce a formal, rigorous derivation of the optimization algorithm. Finally, the /code agent implements this mathematical specification to produce a verifiable, working artifact and run experiments.\", \"expected_artifacts\": [\"A formal mathematical derivation of the ALM update rules for spectral norm constraints, formatted in LaTeX.\", \"A Python script containing the PyTorch implementation of the ALM optimizer and training loop.\", \"A summary report (PDF) containing convergence plots (loss, constraint violation) and a table of adversarial robustness results.\", \"A bibtex file citing at least two relevant arXiv papers.\", \"A log file from the gradient verification check.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem for a 1-Lipschitz neural network by constraining the spectral norm of each weight matrix to be less than or equal to one.\", \"2. Construct the Augmented Lagrangian for this problem and formally derive the primal (weight) and dual (Lagrange multiplier) update steps.\", \"3. Implement the optimization procedure in PyTorch, using the power iteration method to estimate spectral norms and their gradients during training.\", \"4. Validate the correctness of the implementation by writing a unit test that compares the analytical gradients from the derivation against numerical gradients computed via finite differences.\", \"5. Train a network on CIFAR-10, evaluating its standard accuracy, adversarial robustness under a PGD attack, and tracking the maximum constraint violation over training epochs.\", \"6. Audit the final trained model to verify that all weight matrices satisfy the spectral norm constraint up to a specified tolerance (e.g., 1e-5).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code implementation must pass numerical gradient check against the math derivation; fallback: if gradient check fails, return to /math to re-verify the derivation before debugging the code).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on ALM/ADMM for enforcing Lipschitz constraints in deep learning, focusing on convergence guarantees.\", \"- [ ] /math: Formulate the optimization problem and derive the primal-dual update rules for the Augmented Lagrangian.\", \"- [ ] /math: Analyze the Karush-Kuhn-Tucker (KKT) conditions for the derived problem to characterize the optimal solution.\", \"- [ ] /code: Implement the ALM optimizer in PyTorch, including a power iteration module for spectral norm estimation.\", \"- [ ] /code: Train a small ResNet on CIFAR-10 and generate plots for the primary and secondary metrics over training epochs.\", \"- [ ] /math: Verify that the implemented gradient updates in the Python code are mathematically consistent with the formal derivation.\", \"- [ ] /code: Validate the implementation by implementing and passing a numerical gradient check against the analytical gradient.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"lipschitz-networks\", \"adversarial-robustness\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ALM updates must be complete, correct, and self-contained.\", \"- The provided code must run without errors and successfully train a model, with the max constraint violation metric decreasing to below 1e-4.\", \"- The final report must include plots demonstrating convergence and show a measurable improvement in adversarial accuracy over a non-constrained baseline.\", \"- The implementation must pass a gradient check unit test with a tolerance of 1e-6.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 370, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the convergence properties of AdamW for training large-scale Transformers. Can you derive the conditions under which the self-attention mechanism's Jacobian matrix remains spectrally bounded? Please start by reviewing recent literature on non-convex optimization for Transformers, particularly focusing on theorems that connect Lipschitz constants of the attention gradients to convergence guarantees. Then, provide a formal mathematical derivation for the spectral norm bound of the attention Jacobian. Finally, implement a Python script using JAX to numerically verify this bound for a small-scale Transformer model and track the eigenvalue distribution during training under different learning rates.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org transformer optimization jacobian spectral norm attention convergence analysis\\\", mode=rag, context=\\\"Find recent theorems on causal attention complexity and Lipschitz constants for proving AdamW convergence.\\\")\", \"/math(Derive the spectral norm bound for the self-attention Jacobian, incorporating findings on Lipschitz continuity and Hessian eigenvalue properties from the search results. Formulate the proof using matrix calculus.)\", \"/code(Implement a JAX script to train a small Transformer, compute the attention Jacobian's spectral norm at each step, and verify the derived bound. The implementation must include CUDA-accelerated eigenvalue decomposition and track convergence metrics., using Python)\"], \"route_rationale\": \"The task requires synthesizing cutting-edge research with formal proof and numerical verification. The blue /general-search agent is first tasked with a literature review on arXiv to ground the mathematical work in the latest findings. The /math agent then takes these theoretical foundations to construct a formal derivation. Finally, the /code agent implements the derived theory in a JAX environment to provide empirical validation, which is crucial for computational science.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the spectral norm bound for the attention Jacobian.\", \"A list of citations with at least 3 arXiv pre-prints published after 2021.\", \"A well-commented Python script (JAX) for the numerical verification experiment.\", \"A JSON file reporting the maximum observed spectral norm vs. the derived theoretical bound across several training runs.\", \"A plot visualizing the evolution of the largest eigenvalue of the attention Jacobian during training.\"], \"thinking_outline\": [\"1. Systematically search for and synthesize state-of-the-art literature on the convergence analysis of optimizers for Transformers, focusing on spectral properties of attention layers.\", \"2. Formulate a rigorous mathematical proof for the upper bound of the self-attention Jacobian's spectral norm, clearly stating all assumptions and referencing theorems from the literature review.\", \"3. Implement a numerical experiment in JAX to compute the Jacobian and its spectral norm for a running Transformer model, ensuring the computation is numerically stable.\", \"4. Validate the implementation by comparing the numerical results against the derived theoretical bound. The script must log any discrepancies and parameter settings that lead to violations.\", \"5. Analyze and visualize the results, plotting the empirical spectral norm against the theoretical bound over training iterations to confirm the derivation's correctness.\", \"6. Check that all claims in the final report are supported by either the mathematical proof or the numerical evidence, and that all citations are correctly formatted.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent will verify the theorems from search are applicable. The code agent will validate that the numerical results do not violate the mathematical bounds derived by the math agent. The router will perform a final audit on the consistency between the proof and the code's output; fallback: If the bound is violated in the code, escalate to the math agent to re-check assumptions in the derivation.)\", \"todo_list\": [\"- [ ] /general-search: Execute query for recent literature on Transformer Jacobian spectral norms and convergence proofs.\", \"- [ ] /math: Based on search results, derive the formal proof for the spectral norm bound.\", \"- [ ] /code: Develop the JAX implementation for a small Transformer model.\", \"- [ ] /code: Implement the function to compute the attention Jacobian and its spectral norm using `jax.jacfwd` and `jnp.linalg.svd`.\", \"- [ ] /math: Verify that the final code implementation correctly reflects the mathematical formulation of the Jacobian.\", \"- [ ] /code: Run the experiment, log the spectral norm values, and generate the comparison plot and JSON report.\", \"- [ ] /general-search: Cross-reference the final report's claims against the initial papers to ensure no misinterpretation and gather arXiv IDs for citations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"transformer-optimization\", \"spectral-theory\", \"convergence-analysis\"], \"acceptance_criteria\": [\"The submitted PDF contains a complete, mathematically sound derivation for the spectral norm bound.\", \"The JAX script runs without errors and reproduces the results shown in the report, including the plot.\", \"The final report includes a table or JSON file where the empirically measured spectral norm is consistently less than or equal to the derived theoretical bound.\", \"At least 3 relevant arXiv papers published after 2021 are cited correctly.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 371, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to model high-frequency financial time series and need to properly quantify posterior uncertainty. Can you help me derive and implement a Bayesian Neural Stochastic Differential Equation (SDE)? Specifically, I want to use variational inference with a recognition network to approximate the posterior path measure. Please detail the derivation of the ELBO using Girsanov's theorem for the change of measure and the adjoint sensitivity method for efficient gradient computation. Also, implement this in JAX, showing how to use an Euler-Maruyama solver and evaluate the model's calibration using CRPS and NLL metrics.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural stochastic differential equations bayesian variational inference Girsanov theorem adjoint sensitivity method\\\", mode=both)\", \"/math(Derive the Evidence Lower Bound for the Neural SDE posterior, applying Girsanov's theorem to handle the Radon-Nikodym derivative between the prior and variational posterior SDEs, focusing on convergence properties.)\", \"/code(Implement the Bayesian Neural SDE in JAX using an Euler-Maruyama solver. The implementation must include the variational objective derived by /math, track CRPS metrics, and handle the score-matching diffusion dynamics., using Python)\", \"/math(Verify the correctness of the coded objective function by deriving the analytical gradients for a simplified drift network component and establishing the convergence criteria for the Bayesian model.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find authoritative arXiv papers on Neural SDEs and variational inference. /math then uses these sources to construct the core mathematical derivation. /code translates this complex derivation into a JAX implementation. A final /math step is included for verification, ensuring the implemented gradients match the formal derivation, which is critical for such complex models.\", \"expected_artifacts\": [\"- A literature review summary citing at least two foundational arXiv papers on Neural SDEs.\", \"- A LaTeX document containing the full mathematical derivation of the variational inference objective.\", \"- A Python script (`neural_sde.py`) with a JAX/Equinox implementation of the model and training loop.\", \"- A verification report comparing analytical and auto-differentiated gradients for a model sub-component.\", \"- A JSON file (`metrics.json`) reporting the final CRPS and NLL scores on a test dataset.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation by surveying literature on Neural SDEs, particularly focusing on variational inference approaches using the adjoint sensitivity method and Girsanov's theorem.\", \"2. Formally derive the Evidence Lower Bound (ELBO) for a continuous-time latent variable model defined by an SDE, explicitly showing the application of Girsanov's theorem for the change of measure.\", \"3. Implement the Neural SDE model in JAX, including a numerical SDE solver (e.g., Euler-Maruyama), the derived variational objective function, and data loaders for a synthetic time-series task.\", \"4. Verify the implementation's correctness by performing a gradient check, comparing analytical gradients derived in the math step with auto-differentiated gradients from the code for a key parameter block.\", \"5. Train the model and evaluate its ability to quantify predictive uncertainty using appropriate metrics like Continuous Ranked Probability Score (CRPS) and Negative Log-Likelihood (NLL).\", \"6. Validate the final results by analyzing the posterior predictive distributions to ensure the model produces well-calibrated and physically plausible uncertainty estimates.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The first /math agent consumes arXiv papers to produce a derivation. /code implements this derivation. The second /math agent verifies the implementation by comparing its analytical gradients against the code's autodiff output. Verification: Gradient mismatch < 1e-5. Fallback: If mismatch is large, return to the first /math agent to re-audit the derivation for errors.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Bayesian Neural SDEs and the adjoint method.\", \"- [ ] /math: Derive the ELBO using Girsanov's theorem, referencing the search results.\", \"- [ ] /code: Implement the Neural SDE, solver, and variational objective in JAX.\", \"- [ ] /math: Derive the analytical gradients for the drift network's final layer weights.\", \"- [ ] /code: Implement a numerical gradient check comparing the analytical and autodiff gradients.\", \"- [ ] /code: Train the model on a synthetic dataset and generate evaluation metrics.\", \"- [ ] /router: Perform final QA review, ensuring derivation, code, and metrics are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-differential-equations\", \"bayesian-deep-learning\", \"variational-inference\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO is complete, self-contained, and correctly applies Girsanov's theorem.\", \"- The JAX implementation trains successfully and achieves a CRPS score below 0.1 on the specified synthetic dataset.\", \"- The verification report shows a relative error between analytical and numerical gradients of less than 1e-5.\", \"- The submission includes citations for at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"Compute as the integral of the squared difference between the model's predictive cumulative distribution function (CDF) and the empirical CDF of the ground truth. Average over the test set.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"Calculate the average negative log probability density of the test data points under the model's predictive distribution.\"}}", "categories": ["advanced", "four_step", "math_backstop", "math_multi_pass", "metrics_computation", "non_canonical_route"]}
{"index": 372, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to construct a Physics-Informed Neural Network (PINN) to solve the time-dependent Schr\u00f6dinger equation for a 1D quantum system. How do I correctly formulate the complex-valued loss function to minimize the PDE residual, and how can I incorporate a soft constraint to enforce the conservation of probability, i.e., ensuring the total probability \u222b|\u03c8(x,t)|\u00b2 dx remains constant? The network should handle complex-valued outputs \u03c8(x,t) and be validated against an analytical solution like the quantum harmonic oscillator.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org complex-valued neural networks PINN Schr\u00f6dinger equation conservation law loss\\\", mode=both, includes cybersecurity)\", \"/math(Derive the Euler-Lagrange-based loss function for the Schr\u00f6dinger PDE residual, separating real and imaginary parts. Also, formulate a penalty term based on Fourier analysis to enforce the conservation of total probability, considering potential Bayesian interpretations of the constraint.)\", \"/code(Implement a complex-valued PINN in PyTorch with CUDA support, including the custom loss function. Validate the model's convergence and accuracy against the quantum harmonic oscillator's analytical solution, tracking the L2 relative error., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art literature on complex-valued PINNs, which is a specialized topic. This informs the /math agent's formal derivation of the specific loss function required. Finally, the /code agent implements and validates the derived theory, providing an end-to-end verified solution.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the complex-valued PDE residual loss and the probability conservation penalty term.\", \"- A complete, executable Python script implementing the PINN for the Schr\u00f6dinger equation.\", \"- A set of plots comparing the PINN's predicted real and imaginary parts of the wavefunction against the analytical solution over space and time.\", \"- A summary report including the L2 relative error metric, the conservation law residual metric, and citations to at least two relevant arXiv papers.\", \"- A JSON file containing the final computed metrics for error and conservation residual.\"], \"thinking_outline\": [\"1. Research existing methodologies for handling complex-valued PDEs with PINNs, focusing on loss function construction for the Schr\u00f6dinger equation.\", \"2. Formulate the mathematical loss function by taking the norm of the PDE residual. Derive an additional penalty term to enforce the physical constraint of probability conservation.\", \"3. Implement the PINN architecture in Python, ensuring it can output complex numbers or pairs of real numbers representing the wavefunction.\", \"4. Code the custom, multi-term loss function and the training loop, using automatic differentiation to compute gradients.\", \"5. Validate the trained model by comparing its output to the known analytical solution for a test case like the quantum harmonic oscillator. Compute the L2 relative error.\", \"6. Verify that the implemented model successfully conserves the total probability norm over the simulation time, checking the secondary metric.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results from /general-search provide foundational papers for /math's derivation. The rigorous derivation from /math serves as the blueprint for the /code agent's implementation. The /code agent's output (plots, metrics) is then passed to the router for final verification against the acceptance criteria. Fallback: If the /code implementation fails to converge, the router will ask the /math agent to re-check the loss function's formulation for potential numerical instabilities.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 recent arXiv papers on PINNs for complex-valued PDEs like the Schr\u00f6dinger or Navier-Stokes equations.\", \"- [ ] /math: Provide a step-by-step derivation of the loss function L = L_pde + \u03bb * L_conserv, where L_pde is the mean squared residual of the Schr\u00f6dinger equation.\", \"- [ ] /code: Implement the PINN architecture using a suitable framework (PyTorch or JAX) to model \u03c8(x,t).\", \"- [ ] /code: Implement the training logic, sampling collocation points, and applying the custom loss function from /math.\", \"- [ ] /math: Verify that the Python code for the loss function is a correct translation of the derived mathematical expressions.\", \"- [ ] /code: Train the model and generate comparison plots and metrics against the analytical solution for the quantum harmonic oscillator.\", \"- [ ] /code: Validate that the conservation law is upheld by plotting the integrated probability over time.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"partial-differential-equations\", \"scientific-computing\"], \"acceptance_criteria\": [\"- The derived loss function correctly separates and minimizes the real and imaginary parts of the Schr\u00f6dinger PDE residual.\", \"- The L2 relative error between the PINN prediction and the analytical solution must be less than 1e-3.\", \"- The total probability integral \u222b|\u03c8|\u00b2 dx must remain constant over the simulation time with a deviation of less than 1%.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 373, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the second-order Taylor expansion for the MAML objective function with respect to the meta-parameters, interpreting the Hessian as a pullback of the Fisher-Rao metric tensor on the statistical manifold of the inner-loop model. Then, implement a prototype in JAX to empirically validate the derived curvature properties, focusing on convergence rates and task-family complexity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org information geometry in meta-learning MAML Fisher-Rao metric\\\", mode=web, constraints=['find seminal papers on the topic to establish theoretical grounding for the derivation', 'assess task complexity metrics'])\", \"/math(Derive the MAML meta-objective Hessian, showing its structure as a pullback of the Fisher-Rao metric from the inner-loop parameter space, analyzing its eigenvalue spectrum for convergence properties.)\", \"/code(Implement MAML in JAX, compute the Hessian's top eigenvalue via power iteration using Hessian-vector products, and correlate this with empirical convergence on a Bayesian regression task under a limited CUDA budget., using Python)\"], \"route_rationale\": \"The blue general-search agent is used first to gather foundational literature on information geometry and MAML, providing the necessary theoretical context. The math agent then takes these formalisms to perform the core tensor calculus derivation. Finally, the code agent implements the derived theory in JAX to provide empirical validation, connecting the mathematical abstraction to measurable performance.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the MAML Hessian as a pullback of the Fisher-Rao metric tensor.\", \"A Python script using JAX to implement MAML and compute the Hessian's top eigenvalue on a benchmark task.\", \"A summary report with plots correlating the Hessian's top eigenvalue with the meta-learning convergence rate.\", \"A bibliography file citing at least two relevant arXiv pre-prints.\", \"Verification logs demonstrating that the analytical gradients from the derivation match the numerical gradients from JAX's autodiff.\"], \"thinking_outline\": [\"1. Formulate the MAML objective function precisely, defining the inner-loop adaptation process and the outer-loop meta-objective.\", \"2. Derive the first and second-order gradients (Hessian) of the meta-objective using the multivariable chain rule, carefully tracking terms from the inner-loop updates.\", \"3. Re-interpret the derived Hessian in the language of differential geometry, formally demonstrating its relationship to the pullback of the Fisher-Rao metric tensor from the task-specific parameter manifold to the meta-parameter manifold.\", \"4. Implement the MAML algorithm and a method to compute Hessian-vector products efficiently in JAX to avoid materializing the full, memory-intensive Hessian.\", \"5. Validate the implementation by comparing the analytical gradients derived in step 2 with numerical gradients computed by JAX's autodifferentiation for a simple toy problem.\", \"6. Verify that the empirical convergence behavior on a few-shot regression benchmark qualitatively aligns with the predictions from the Hessian's spectral properties (i.e., its largest eigenvalue).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code implementation of the Hessian-vector product must align with the equations derived by the math agent; fallback: if a mismatch is found, flag for the math agent to re-audit the derivation, specifically the application of the chain rule across optimization steps).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on MAML (Finn et al.) and Information Geometry in Machine Learning (Amari).\", \"- [ ] /math: Derive the MAML Hessian, explicitly expressing it as a pullback of the Fisher-Rao metric.\", \"- [ ] /math: Formulate the theoretical relationship between the Hessian's largest eigenvalue and the local convergence rate of the meta-optimization.\", \"- [ ] /code: Implement a basic MAML training loop in JAX for a sinusoidal regression benchmark task.\", \"- [ ] /code: Implement a function to compute the top eigenvalue of the MAML Hessian using the power iteration method with Hessian-vector products.\", \"- [ ] /math: Verify that the analytical gradient derived matches the numerical gradient computed by the code agent's JAX implementation on a test vector.\", \"- [ ] /code: Run experiments to plot the measured top eigenvalue against the observed convergence speed and document the correlation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"information-geometry\", \"tensor-calculus\"], \"acceptance_criteria\": [\"The mathematical derivation correctly expresses the MAML Hessian as a pullback of the Fisher-Rao metric, with all steps shown.\", \"The JAX implementation passes a numerical gradient check against the analytical derivation with a tolerance of 1e-5.\", \"The final report presents empirical results (plots and tables) that support the theoretical connection between Hessian curvature and convergence speed, citing at least two relevant papers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Mean Squared Error on held-out regression tasks after 5-shot adaptation. Lower is better.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Largest eigenvalue of the meta-objective Hessian, computed via the power iteration algorithm. Measures the sharpness of the loss landscape.\"}}", "categories": ["advanced", "metrics_guidance"]}
{"index": 374, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to unify Bayesian neural networks with continuous-time models using stochastic differential equations. Can you derive the variational inference objective, specifically the Evidence Lower Bound (ELBO), for a Neural SDE where both the prior and the approximate posterior are defined by SDEs? Please provide a full mathematical derivation involving the Girsanov theorem and Fokker-Planck equation, and then implement this framework in Python to model a synthetic time-series dataset, showing how to handle the drift and diffusion terms as neural networks.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian neural stochastic differential equations variational inference girsanov theorem\\\", mode=both)\", \"/math(Derive the continuous-time ELBO for a Bayesian Neural SDE. Formulate the KL-divergence between the posterior and prior processes, defined by SDEs with neural network drift/diffusion terms, ensuring proper application of Girsanov's theorem for the change of measure.)\", \"/code(Implement the derived Bayesian SDE model using Python with torchsde and PyTorch. The implementation must include the SDE solver, the neural network architectures for drift and diffusion, and an optimization loop that maximizes the ELBO to track convergence.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on Neural SDEs and Bayesian methods. This research informs the /math agent's formal derivation of the ELBO. Finally, the /code agent implements the derived theory for empirical validation, creating a complete theory-to-practice workflow.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the ELBO for Bayesian Neural SDEs in LaTeX/PDF format.\", \"- A Python script implementing the model, training loop, and evaluation on a synthetic dataset.\", \"- A summary report including plots of the training loss (ELBO), the model's predictive distribution, and uncertainty estimates on test data.\", \"- A bibliography file citing at least two relevant academic papers with their arXiv IDs.\"], \"thinking_outline\": [\"1. Formalize the Bayesian Neural SDE model, defining the prior and variational posterior processes via SDEs whose drift and diffusion coefficients are parameterized by neural networks.\", \"2. Use the Girsanov theorem to derive the Radon-Nikodym derivative needed to compute the KL-divergence between the two path measures, forming the core of the ELBO.\", \"3. Verify the mathematical derivation, ensuring all terms in the final ELBO are computationally tractable and correctly handle the stochastic integrals.\", \"4. Architect a Python implementation using PyTorch and the `torchsde` library to simulate SDE paths and compute gradients via the adjoint sensitivity method.\", \"5. Validate the implementation's correctness by monitoring the ELBO for convergence on a known synthetic dataset and checking the plausibility of the learned uncertainty intervals.\", \"6. Conclude by analyzing the model's ability to capture the underlying dynamics and quantify uncertainty, comparing the predictive posterior to the ground truth.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers (e.g., on Neural SDEs) to the /math agent. The /math agent produces a formal derivation of the ELBO, which becomes the specification for the /code agent. The /code agent implements this specification and produces empirical results (plots, metrics). Router QA performs final verification by cross-referencing the derivation with the code's implementation logic and checking if the empirical results are consistent with theory. Fallback: If the /code agent cannot achieve convergence, it will flag the issue for the /math agent to re-check the tractability of the derived loss function.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize at least two key arXiv papers on Bayesian inference for Neural SDEs.\", \"- [ ] /math: Produce a step-by-step derivation of the ELBO, explicitly stating all assumptions and theorems used.\", \"- [ ] /code: Implement the SDE model with neural network components for drift and diffusion.\", \"- [ ] /code: Write the training loop to optimize the ELBO using an SDE solver and automatic differentiation.\", \"- [ ] /math: Verify that the loss function implemented in the code correctly matches the derived mathematical formula for the ELBO.\", \"- [ ] /code: Validate the trained model on a hold-out portion of the synthetic dataset, generating plots for predictive mean and uncertainty bounds.\", \"- [ ] /router: Perform final QA review of all generated artifacts for coherence, correctness, and completeness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-deep-learning\", \"stochastic-differential-equations\", \"variational-inference\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO must be complete, rigorous, and directly translatable to a computational algorithm.\", \"- The Python implementation must run without errors, and the training loss (ELBO) must show a clear trend of convergence over epochs.\", \"- The final report must contain plots showing the model's learned function and its uncertainty estimates, which should correctly envelop the true data-generating process.\", \"- The solution must cite at least two relevant arXiv papers with valid IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 375, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to solve a problem that involves finding the maximum weight independent set on a graph with bounded treewidth, a common pattern in bioinformatics and IOI finals. I need a robust method for constructing a tree decomposition and then formulating the dynamic programming states and transitions over the bags, ensuring the overall complexity remains polynomial in the number of vertices but exponential only in the treewidth.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:*.edu OR site:*.ac.* tree decomposition algorithms heuristics dynamic programming independent set bioinformatics\\\", mode=both)\", \"/math(Formulate the DP state representation for each bag in the tree decomposition. Define the base cases for leaf bags and the recurrence relations for join, forget, and introduce nodes, analyzing the computational complexity and convergence properties.)\", \"/code(Implement the tree decomposition algorithm and the dynamic programming solution in Python. Include runtime validation checks for tree decomposition properties and unit tests for the DP transitions on small graphs., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative algorithms for tree decomposition and DP on trees, as these are non-trivial. The /math agent then formalizes the specific DP recurrence, which is a complex theoretical step. Finally, /code implements the validated mathematical model, separating theory from implementation.\", \"expected_artifacts\": [\"A formal mathematical write-up of the tree decomposition properties and the dynamic programming recurrence relations.\", \"A Python implementation of the complete algorithm, including tree decomposition and the DP solver.\", \"A summary of authoritative sources and benchmarks found, with at least 2 citations to academic papers or contest editorials.\", \"A verification report containing the output of unit tests on corner-case graphs (e.g., paths, cliques, stars).\"], \"thinking_outline\": [\"1. Research and select an appropriate algorithm for constructing a reasonably optimal tree decomposition, such as a greedy degree-based heuristic or one based on graph triangulation.\", \"2. Formally define the dynamic programming state for a bag in the tree decomposition. This state must encode information about independent sets within the subgraph corresponding to the bag's subtree, restricted to the vertices in the bag itself.\", \"3. Derive the transition functions for the three types of nodes in a normalized tree decomposition: leaf, introduce, forget, and join nodes. This involves combining solutions from child bags.\", \"4. Verify the correctness of the recurrence relations by proving that they correctly compute the desired values at each node of the decomposition tree.\", \"5. Implement the algorithm and validate its correctness against known solutions on small, manually constructed graphs and its performance against established benchmarks.\", \"6. Analyze the final time and space complexity as a function of the number of vertices (n) and the treewidth (k).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: router will cross-reference the implemented DP logic in /code against the formal recurrence from /math and run it against a hidden test suite; fallback: if the complexity is too high, the router will request /math to investigate a simplified state representation).\", \"todo_list\": [\"- [ ] /general-search: Curate 2-3 key academic papers or contest editorials on DP over tree decompositions for independent set.\", \"- [ ] /math: Define the DP state using bitmasking for subsets of bag vertices and formulate the recurrence for join/forget/introduce nodes.\", \"- [ ] /math: Verify the mathematical derivation of the recurrence relation ensures all valid independent set configurations are considered.\", \"- [ ] /code: Implement a function to generate a tree decomposition for a given graph.\", \"- [ ] /code: Implement the dynamic programming solver that traverses the tree decomposition and applies the derived recurrences.\", \"- [ ] /code: Create a suite of unit tests with small graphs to validate the DP logic at each type of tree node.\", \"- [ ] /router: Final QA review, checking code correctness against the math spec and performance on benchmark graphs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"dynamic-programming\", \"graph-theory\", \"tree-decomposition\"], \"acceptance_criteria\": [\"- The Python code must correctly solve the maximum weight independent set problem for provided test graphs with treewidth up to 10.\", \"- The mathematical derivation must clearly define the DP state and provide correct recurrence relations for all node types in a tree decomposition.\", \"- The solution must cite at least 2 authoritative external resources (e.g., IOI scientific committee papers, university course notes, or journal articles).\", \"- The time complexity of the DP portion must be O(c^k * n) where k is treewidth, n is number of vertices, and c is a small constant (e.g., 2 or 3).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 377, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Alternating Least Squares (ALS) update rules for the Canonical Polyadic (CP) tensor decomposition. Implement the algorithm for a third-order tensor using Python and TensorLy, and analyze its convergence guarantees. Specifically, provide a formal proof sketch connecting solution uniqueness to Kruskal's rank condition and its implications for learning disentangled latent factors in biomedical knowledge graphs, considering potential eigenvalue collapse.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org CANDECOMP/PARAFAC ALS convergence kruskal rank uniqueness biomedical representation learning\\\", mode=web)\", \"/math(Derive the Alternating Least Squares update rules for a third-order tensor via optimization of the Frobenius norm loss. Formally state Kruskal's theorem and analyze its preconditions regarding the tensor's rank and the k-rank of the factor matrices, ensuring convergence is properly contextualized.)\", \"/code(Implement the CP-ALS algorithm in Python with TensorLy. Validate the implementation on a synthetic tensor with a known low rank, tracking reconstruction error and factor match score. Ensure the implementation avoids common numerical stability issues like eigenvalue decay during normalization.)\"], \"route_rationale\": \"The blue /general-search agent will first gather authoritative literature on CP-ALS theory from arXiv to ground the subsequent steps. Then, /math will perform the formal derivation and theoretical analysis based on these sources. Finally, /code will provide a concrete implementation and empirical validation of the derived algorithm, testing its convergence properties.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the ALS update rules.\", \"A Python script implementing the CP-ALS algorithm and a validation routine on synthetic data.\", \"A summary report analyzing Kruskal's theorem and its implications, citing at least 2 relevant research papers from arXiv.\", \"A JSON file containing the convergence plot data (iteration vs. reconstruction error).\", \"Verification outputs confirming the correspondence between the mathematical derivation and the code's logic.\"], \"thinking_outline\": [\"1. Research the foundational theory of Canonical Polyadic (CP) decomposition and the Alternating Least Squares (ALS) algorithm, focusing on convergence proofs and uniqueness conditions like Kruskal's theorem.\", \"2. Formulate the CP decomposition as an optimization problem to minimize the squared Frobenius norm of the residual tensor and derive the gradient-based update rules for each factor matrix.\", \"3. Implement the derived ALS algorithm, paying close attention to numerical stability, initialization strategies, and convergence criteria.\", \"4. Validate the implementation by applying it to a synthetic tensor with a known ground-truth factorization and measure the Factor Match Score (FMS) and reconstruction error.\", \"5. Verify that the implemented update logic exactly matches the mathematical derivation from step 2, auditing the matrix operations (e.g., Khatri-Rao product, pseudoinverse).\", \"6. Synthesize the findings into a report that connects the theoretical uniqueness conditions (Kruskal's rank) to the practical ability to recover meaningful factors in representation learning.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Cross-validate the derived ALS update equations in the /math artifact against the Python implementation in the /code artifact, ensuring all matrix operations like the Khatri-Rao product are correctly mapped; fallback: If mismatch, return to /math for correction.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on CP-ALS convergence and Kruskal's rank.\", \"- [ ] /math: Derive the ALS update rules for a 3rd-order tensor, minimizing the Frobenius norm.\", \"- [ ] /math: Formally state Kruskal's theorem and explain the k-rank condition for uniqueness.\", \"- [ ] /code: Implement the CP-ALS algorithm using TensorLy/NumPy, including a function for generating synthetic tensors.\", \"- [ ] /code: Create a validation script that tracks reconstruction error and computes the Factor Match Score against a known ground truth.\", \"- [ ] /math: Verify that the implemented Khatri-Rao product and pseudoinverse operations in the code correctly reflect the derived update equations.\", \"- [ ] /router: Perform final QA check, ensuring all acceptance criteria are met and artifacts are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"numerical-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ALS update rules must be complete and correct.\", \"- The Python implementation must converge on a synthetic tensor of rank R, achieving a final reconstruction error below 1e-6 and a Factor Match Score > 0.99.\", \"- The final report must correctly explain Kruskal's theorem and cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 378, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous comparison of Persistent Homology (PH) via Vietoris-Rips complexes against Isomap for intrinsic dimension estimation on a noisy Swiss roll manifold. Analyze the theoretical guarantees for topological feature stability under Gromov-Hausdorff perturbations and compare the computational complexity, specifically the eigenvalue decomposition bottleneck in Isomap versus the boundary matrix reduction in PH. Provide a JAX implementation to benchmark this.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org persistent homology stability Gromov-Hausdorff vs Isomap eigenvalue complexity manifold learning\\\", mode=rag)\", \"/math(Derive the computational complexity bounds for Vietoris-Rips persistent homology and Isomap's eigenvalue decomposition, analyzing their convergence properties and stability under noise from the sourced arXiv literature.)\", \"/code(Implement a JAX-based benchmark comparing Persistent Homology and Isomap on a noisy Swiss roll manifold, tracking estimation error and runtime complexity. Use bootstrap methods for confidence intervals., using Python)\"], \"route_rationale\": \"The blue general-search agent is used first to ground the analysis in authoritative literature from arXiv. /math is then used to formalize the theoretical comparison and derive complexity expressions based on the research. Finally, /code implements an empirical benchmark to validate the theoretical findings.\", \"expected_artifacts\": [\"A formal mathematical derivation of complexity bounds for both algorithms.\", \"A Python script (JAX/Giotto-TDA) implementing the comparative benchmark.\", \"A summary report including persistence diagrams, Isomap embeddings, and performance plots (error vs. noise, runtime vs. data size).\", \"A bibliography citing at least 2 relevant arXiv preprints on TDA or manifold learning.\", \"A verification log detailing the consistency check between the mathematical derivation and the code's empirical complexity scaling.\"], \"thinking_outline\": [\"1. Retrieve seminal papers on the stability of persistent homology and the computational analysis of Isomap.\", \"2. Formulate the mathematical problem: define the noisy Swiss roll manifold, the objective of intrinsic dimension estimation, and the metrics for comparison (accuracy, stability, runtime).\", \"3. Derive the asymptotic computational complexity for both algorithms, highlighting the dependence on the number of points and ambient dimension.\", \"4. Implement the benchmark in Python, ensuring correct parameterization of the Vietoris-Rips filtration and the neighborhood graph for Isomap.\", \"5. Validate the implementation by checking if the empirical runtime scaling matches the derived theoretical complexity bounds for increasing data sizes.\", \"6. Verify that the interpretation of the persistence diagram correctly identifies the manifold's topological features (e.g., the Betti-1 number for the 'hole').\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: mathematical complexity derivation from /math must match empirical scaling observed in /code's output; fallback: if a mismatch occurs, /math re-evaluates asymptotic assumptions, and /code refines its timing methodology).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on TDA stability (e.g., Gromov-Hausdorff) and Isomap complexity.\", \"- [ ] /math: Derive and formalize the O(N^k) complexity for Vietoris-Rips and the O(N^3) or eigenvalue-based complexity for Isomap.\", \"- [ ] /code: Implement the noisy Swiss roll data generator.\", \"- [ ] /code: Implement the comparison benchmark using Giotto-TDA/Scikit-learn within a JAX-compatible workflow.\", \"- [ ] /math: Verify that the code's parameterization of the algorithms (e.g., neighborhood size, filtration limit) aligns with the assumptions in the theoretical derivation.\", \"- [ ] /code: Validate empirical runtime scaling against the theoretical big-O notation derived by /math.\", \"- [ ] router QA: Review final report, plots, and code for consistency and correctness against acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"computational-geometry\"], \"acceptance_criteria\": [\"- The final report must correctly state the theoretical computational complexity of both algorithms, supported by a formal derivation.\", \"- The provided code must be executable and reproduce the plots comparing estimation error and runtime for both methods across varying noise levels.\", \"- The empirical runtime plot must show a scaling behavior consistent with the theoretical complexity derived in the math section.\", \"- The analysis must cite at least two relevant academic papers (arXiv or peer-reviewed) on TDA or manifold learning.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 379, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a neural-symbolic system for program synthesis and automated theorem proving. Specifically, I want to synthesize recursive functional programs from formal specifications in higher-order logic and simultaneously prove their termination. The system should learn a ranking function using a Graph Neural Network over the program's Abstract Syntax Tree to establish a well-founded ordering, ensuring convergence and preventing non-termination.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural program synthesis termination proof 'ranking function' GNN convergence\\\", mode=web)\", \"/math(Formalize the termination proof as a search for a ranking function over a simply-typed lambda calculus, deriving a Bayesian loss function and constraints on its gradient to ensure strict decrease and convergence.)\", \"/code(Implement in Python/PyTorch the GNN-guided synthesizer and termination prover; include CUDA optimizations and a validation module to check proof correctness and program equivalence against formal specifications.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on neural termination proofs. /math then formalizes the problem by deriving the necessary mathematical framework and loss functions. Finally, /code implements and validates the complete system based on the formal theory.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the ranking function learning problem and the associated loss function.\", \"- A Python implementation of the neural-symbolic synthesizer and termination prover.\", \"- A summary report detailing the synthesis success rate and proof verification rate on a benchmark set.\", \"- A bibliography with at least two cited arXiv papers on the topic.\", \"- Verification logs showing the output of checks for both terminating and non-terminating example programs.\"], \"thinking_outline\": [\"1. Research existing literature on neural-guided program synthesis, specifically focusing on methods for proving termination via learned ranking functions.\", \"2. Formulate the problem mathematically. Define the language syntax, program semantics, and the properties of a valid ranking function (strict decrease into a well-founded domain).\", \"3. Derive a suitable loss function for training a Graph Neural Network to approximate a ranking function that satisfies the derived properties.\", \"4. Implement the complete system, including the program synthesizer, the GNN module, and the symbolic verifier for checking the generated proofs.\", \"5. Validate the mathematical derivation against the implementation logic to ensure they are perfectly aligned.\", \"6. Verify the system's performance on a curated set of benchmark problems, including both terminating and known non-terminating recursive functions, and analyze failure modes.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search results inform the mathematical model. The formal model from /math provides the exact specification for the /code agent. The code's output is then checked against the formal model during router QA. Verification: The synthesized programs must pass all formal test cases. Fallback: If the GNN fails to find valid ranking functions, the task reverts to /math to reformulate the loss constraints.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on using GNNs for program termination analysis.\", \"- [ ] /math: Derive the loss function and constraints for training the ranking function approximator.\", \"- [ ] /math: Verify that the derived loss function correctly enforces the properties of a ranking function.\", \"- [ ] /code: Implement the GNN architecture for processing program ASTs.\", \"- [ ] /code: Implement the synthesis loop and the verifier that checks the learned ranking function.\", \"- [ ] /code: Validate the synthesizer's output against a hold-out set of formal specifications and known correct programs.\", \"- [ ] /router: Final QA review of all artifacts, checking consistency between the math, code, and final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"program-synthesis\", \"theorem-proving\", \"neural-symbolic-ai\"], \"acceptance_criteria\": [\"- The system must achieve a synthesis success rate of >70% on a provided benchmark of recursive function specifications.\", \"- The learned ranking function must be successfully verified as a valid termination proof for >90% of the successfully synthesized programs.\", \"- The final report must cite at least two relevant arXiv papers with IDs and correctly explain how they informed the approach.\", \"- The Python code must be runnable and include a test suite that demonstrates correctness on at least five examples (3 terminating, 2 non-terminating).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 380, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the evidence lower bound (ELBO) for a Bayesian Prototypical Network using amortized variational inference over prototype embeddings. Implement this model in JAX, evaluating its few-shot classification accuracy and calibration under federated learning constraints. Compare its uncertainty quantification (AUPRC for OOD detection) against a baseline Euclidean Prototypical Network.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian prototypical networks variational inference ELBO federated learning\\\", mode=rag)\", \"/math(Derive the evidence lower bound (ELBO) for the amortized Bayesian posterior over class prototypes, assuming a diagonal Gaussian prior and posterior, incorporating federated learning constraints.)\", \"/code(Implement the Bayesian Prototypical Network in JAX/Flax, including the reparameterization trick for the ELBO gradient. Evaluate few-shot accuracy and OOD AUPRC metric, ensuring convergence., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find foundational literature on Bayesian meta-learning. /math then formally derives the required objective function (ELBO). Finally, /code implements and validates the derived model, comparing its performance on key metrics.\", \"expected_artifacts\": [\"A LaTeX document with the full ELBO derivation.\", \"A Python script (`main.py`) containing the JAX implementation of the Bayesian Prototypical Network and evaluation harness.\", \"A JSON file (`results.json`) with evaluation metrics (accuracy, ECE, AUPRC).\", \"A verification report (`verification.md`) confirming the alignment between the mathematical derivation and the code's gradient computation.\", \"A `references.bib` file with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Research existing literature on Bayesian Prototypical Networks to understand the standard formulation of the probabilistic model and the ELBO objective.\", \"2. Formally derive the ELBO using principles of variational inference, clearly stating all assumptions about the prior and approximate posterior distributions.\", \"3. Implement the derived model in JAX, paying close attention to the reparameterization trick for stochastic gradient estimation and the overall federated averaging logic.\", \"4. Validate the implementation by numerically checking the gradients of the ELBO implementation against a finite-difference approximation to ensure correctness.\", \"5. Conduct experiments to measure few-shot accuracy, calibration error (ECE), and out-of-distribution detection performance (AUPRC), comparing against a non-Bayesian baseline.\", \"6. Audit the final results and code for clarity, reproducibility, and alignment with the initial mathematical derivation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Cross-reference the implemented loss function in the code against the final equation in the math derivation; fallback: If mismatch, return to /math for correction based on implementation constraints).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Bayesian Prototypical Networks and their ELBO.\", \"- [ ] /math: Derive the ELBO for the specified probabilistic model.\", \"- [ ] /code: Implement the Bayesian and baseline models in JAX.\", \"- [ ] /code: Implement the evaluation harness for accuracy, ECE, and AUPRC metrics.\", \"- [ ] /math: Analytically derive the gradient of the ELBO to assist code verification.\", \"- [ ] /code: Verify the JAX automatic differentiation output against the analytical gradient or numerical approximations.\", \"- [ ] /router: Perform final QA check on artifacts, ensuring consistency between derivation, code, and results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"bayesian-deep-learning\", \"few-shot-learning\"], \"acceptance_criteria\": [\"The ELBO derivation in the LaTeX document must be mathematically sound and self-contained.\", \"The JAX code must be runnable and reproduce the metrics reported in `results.json` within a 5% tolerance.\", \"The Bayesian model must demonstrate a statistically significant improvement in AUPRC for OOD detection over the Euclidean baseline.\", \"The verification report must confirm the correctness of the gradient implementation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 381, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the stability theorem for persistence diagrams under Gromov-Hausdorff distance for a Vietoris-Rips complex built on a point cloud sampled from a Riemannian manifold. Then, implement an algorithm in JAX to compute the 0th and 1st persistent homology groups for a synthetic manifold dataset, verifying convergence guarantees and analyzing the computational complexity with respect to the number of simplices.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org stability theorem persistent homology Gromov-Hausdorff Vietoris-Rips filtration\\\", mode=rag, includes cybersecurity)\", \"/math(Derive the proof for the stability theorem of persistence diagrams for Vietoris-Rips complexes, referencing the Gromov-Hausdorff metric and bottleneck distance. Address the computational complexity and convergence properties for Bayesian inference on the underlying manifold.)\", \"/code(Implement a JAX/NumPy pipeline to construct a Vietoris-Rips complex and compute the 0th and 1st persistence diagrams for a point cloud sampled from a torus. Validate the implementation against known topological features and analyze its CUDA performance and memory complexity., using Python)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather authoritative literature on the stability theorem, providing a solid theoretical foundation. This is handed off to the /math agent to formalize the proof based on these sources. Finally, the /code agent translates the validated mathematical framework into a concrete, verifiable implementation.\", \"expected_artifacts\": [\"A LaTeX document containing the formal proof of the stability theorem.\", \"A Python script (.py) implementing the persistent homology computation using JAX/NumPy.\", \"A Jupyter notebook (.ipynb) demonstrating the code on a synthetic dataset, visualizing the persistence diagrams, and reporting on performance analysis.\", \"A CITATIONS.md file listing at least two relevant arXiv papers with their IDs.\", \"A VERIFICATION.log file showing the output of validation checks against known manifold properties.\"], \"thinking_outline\": [\"1. Research the foundational stability theorem for persistence diagrams, focusing on the relationship between Gromov-Hausdorff distance for metric spaces and bottleneck distance for diagrams.\", \"2. Formulate the step-by-step proof of the stability theorem, clearly defining all terms like Vietoris-Rips complexes, filtrations, and interleaving distances.\", \"3. Validate the logical coherence of the proof, ensuring all assumptions are stated and lemmas are correctly applied.\", \"4. Design a data structure and algorithm for constructing the Vietoris-Rips filtration and computing the boundary matrices for the chain complex.\", \"5. Implement this algorithm in JAX or NumPy, ensuring the code for reducing the boundary matrix is correct and computationally efficient. Verify the output for a simple case (e.g., a circle) where the Betti numbers are known.\", \"6. Analyze the algorithm's time and space complexity and benchmark its performance on a synthetic torus dataset.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code output matches theoretical Betti numbers for known manifolds; fallback: if verification fails, /math agent re-audits the boundary matrix derivation and /code agent debugs the reduction algorithm).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on persistence diagram stability.\", \"- [ ] /math: Write the formal proof for the stability theorem, using LaTeX for all mathematical notation.\", \"- [ ] /math: Analyze the computational complexity of the persistence algorithm based on the number of simplices.\", \"- [ ] /code: Implement the Vietoris-Rips complex construction and persistence algorithm in Python.\", \"- [ ] /code: Generate a synthetic torus dataset and compute its persistence diagram.\", \"- [ ] /math: Verify that the boundary matrix construction in the code logically aligns with the chain complex definition from the proof.\", \"- [ ] /code: Validate that the computed Betti numbers from the persistence diagram match the known topology of the torus (Betti_0=1, Betti_1=2).\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"persistent-homology\"], \"acceptance_criteria\": [\"- The provided mathematical proof of the stability theorem is complete, logically sound, and correctly uses concepts from algebraic topology.\", \"- The Python implementation correctly computes the 0th and 1st Betti numbers for a synthetic torus point cloud, matching the expected values of 1 and 2, respectively.\", \"- The performance analysis correctly identifies the computational complexity and provides empirical benchmarks.\", \"- All artifacts are delivered, and citations meet the specified policy.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 383, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous analysis of the Hessian eigenspectrum for a Vision Transformer (ViT) at different training checkpoints to understand the loss landscape geometry. Please derive the matrix calculus formulation for the Hessian-vector product (HVP) for self-attention blocks, implement a computationally efficient Lanczos algorithm to approximate the top eigenvalues, and empirically connect the spectral density to the model's generalization gap and training convergence behavior, referencing foundational papers on sharpness.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org vision transformer hessian eigenspectrum lanczos sharpness-aware minimization generalization\\\", mode=web)\", \"/math(Derive the Hessian-vector product for the multi-head self-attention mechanism, detailing the matrix calculus steps. Formulate the Lanczos iteration for approximating the top eigenvalue and discuss its convergence properties.)\", \"/code(Implement the HVP and Lanczos algorithm in PyTorch with CUDA support. Apply it to a pretrained ViT model at early, mid, and late training checkpoints, validating the numerical complexity and tracking eigenvalue changes., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on Hessian analysis in transformers, which is essential for grounding the work. Next, the /math tool is used to perform the formal derivations required for implementation. Finally, the /code tool translates the mathematical theory into an empirical experiment, running the analysis and generating the required plots and metrics.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the Hessian-vector product for the self-attention layer.\", \"- A Python script implementing the Lanczos algorithm to compute the top-k eigenvalues of a ViT model's Hessian.\", \"- A report in Markdown format summarizing the findings, including plots of the eigenvalue distribution over training and a discussion connecting spectral properties to generalization, supported by citations.\", \"- A verification log comparing the implemented HVP against a numerical finite-difference approximation to prove correctness.\"], \"thinking_outline\": [\"1. Survey existing literature to identify established methods and theoretical frameworks for analyzing the Hessian of large neural networks, particularly transformers.\", \"2. Derive the exact mathematical expression for the Hessian-vector product for a ViT's core components using matrix calculus, paying close attention to the chain rule through complex operations like softmax.\", \"3. Implement the derived HVP and the Lanczos algorithm in a deep learning framework, ensuring computational efficiency for large models.\", \"4. Verify the correctness of the HVP implementation by comparing its output to a numerical approximation via finite differences on a smaller, tractable model instance.\", \"5. Execute the analysis on a full-scale ViT model at various training stages, collect the top eigenvalues, and visualize the spectral density.\", \"6. Validate that the observed trends in the eigenspectrum align with theoretical claims from the cited literature regarding loss landscape sharpness and model generalization.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The formal derivation from /math must provide a clear, step-by-step formula that /code can implement directly. The code's numerical stability and correctness must be validated against a finite-difference check. fallback: If the derivation is numerically unstable, /math must revisit it to find an alternative formulation. If the code fails verification, it returns to /math for a derivation audit.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Hessian spectral analysis in transformers.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the Hessian-vector product for the self-attention layer.\", \"- [ ] /math: Formulate the Lanczos algorithm and its application for finding top eigenvalues of the Hessian.\", \"- [ ] /code: Implement the HVP and Lanczos algorithm in Python using PyTorch.\", \"- [ ] /code: Create a script to load a ViT model at different checkpoints and run the spectral analysis.\", \"- [ ] /math: Verify that the symbolic derivation aligns with the numerical logic in the Python implementation from /code.\", \"- [ ] /code: Generate plots showing the top eigenvalue distribution over training epochs and create a summary report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"deep-learning-theory\", \"matrix-calculus\"], \"acceptance_criteria\": [\"- The mathematical derivation of the HVP must be complete and correct.\", \"- The Python implementation must pass a numerical verification test against a finite-difference approximation with a tolerance of less than 1e-4.\", \"- The final report must contain at least one plot visualizing the change in the top Hessian eigenvalue over training and cite at least two relevant arXiv papers.\", \"- The analysis correctly connects increasing spectral norms (sharpness) with potential degradation in generalization performance, as supported by the literature.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 385, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a high-performance solver for large, sparse, symmetric positive-definite linear systems arising from finite element method discretizations. Please architect and implement a Preconditioned Conjugate Gradient (PCG) method optimized for NVIDIA GPUs using CUDA. The focus should be on selecting and justifying an effective preconditioner, like an incomplete Cholesky factorization, analyzing its impact on the system's eigenvalue distribution, and benchmarking the final implementation's convergence against a standard unpreconditioned CG solver.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org GPU incomplete cholesky preconditioner sparse linear system CUDA convergence\\\", mode=rag)\", \"/math(Derive the Preconditioned Conjugate Gradient algorithm and analyze how an incomplete Cholesky preconditioner affects the system matrix's eigenvalue spectrum to accelerate convergence for our CUDA-based solver.)\", \"/code(Implement the PCG algorithm in Python with CuPy, ensuring all vector operations and sparse matrix products are on the GPU. Benchmark performance, tracking convergence and wall-clock time to validate the theoretical eigenvalue analysis.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on GPU-native preconditioners. This informs the /math agent's theoretical derivation and analysis of the algorithm. Finally, the /code agent implements the theoretically grounded algorithm and performs empirical validation, providing a complete theory-to-practice workflow.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Preconditioned Conjugate Gradient algorithm and an analysis of the preconditioner's effect on the matrix condition number.\", \"- A well-documented Python script implementing the solver using a CUDA-enabled library like CuPy.\", \"- A performance report in Markdown format, including plots comparing the convergence (residual vs. iteration) of the preconditioned and standard CG methods.\", \"- A JSON file containing the final performance metrics (e.g., iterations to convergence, wall-clock time).\", \"- A bibliography with citations for at least two relevant research papers.\"], \"thinking_outline\": [\"1. Research state-of-the-art GPU-accelerated preconditioners for sparse, symmetric positive-definite systems, focusing on methods like Incomplete Cholesky (IC) or algebraic multigrid.\", \"2. Formulate the complete Preconditioned Conjugate Gradient (PCG) algorithm, clearly defining the role of the preconditioning matrix M.\", \"3. Implement the PCG solver in Python using a CUDA-accelerated library (e.g., CuPy) to ensure all computationally intensive operations occur on the GPU.\", \"4. Validate the implementation's numerical correctness on a small, well-conditioned test problem by comparing its output to a trusted library solver (e.g., scipy.sparse.linalg.cg).\", \"5. Benchmark the solver on a large-scale problem, and verify that the preconditioner improves the convergence rate (i.e., reduces iterations) as predicted by the mathematical analysis of the eigenvalue distribution.\", \"6. Synthesize all findings into a comprehensive report including the derivation, performance plots, and concluding analysis.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. Search results guide the mathematical derivation. The completed derivation provides the exact algorithm for the code agent to implement. The code agent's benchmark results are then used to verify the theoretical claims from the math agent. Fallback: if the chosen preconditioner (e.g., IC(0)) proves ineffective or difficult to implement efficiently on the GPU, the code agent will fall back to a simpler Jacobi (diagonal) preconditioner and document the performance trade-offs.\", \"todo_list\": [\"- [ ] /general-search: Identify at least two key papers from arXiv detailing efficient GPU implementations of preconditioners for conjugate gradient.\", \"- [ ] /math: Write out the step-by-step derivation of the PCG algorithm.\", \"- [ ] /math: Provide a brief analysis explaining how preconditioning improves the condition number and clusters eigenvalues.\", \"- [ ] /code: Implement the PCG solver using CuPy for sparse matrices and vectors.\", \"- [ ] /code: Create a validation test to check the solver's output against a known solution for a small matrix.\", \"- [ ] /math: Verify that the empirical convergence plots from the code agent align with the theoretical speedup expected from the preconditioner analysis.\", \"- [ ] /code: Generate benchmark plots comparing iteration count and wall time for PCG vs. standard CG.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"scientific-computing\"], \"acceptance_criteria\": [\"- The implemented solver must converge to the correct solution for a known test case within a specified tolerance (e.g., 1e-8).\", \"- The final report must contain a plot showing that the preconditioned solver requires significantly fewer iterations to converge than the unpreconditioned version.\", \"- The provided code must use GPU-native data structures (e.g., cupy.ndarray) for all major computational steps, confirmed via code review.\", \"- The bibliography must cite at least two relevant arXiv papers with valid identifiers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 386, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a comprehensive solution for a compressed sensing problem. Please derive the update rules for dictionary learning using the Method of Optimal Directions (MOD), analyze its convergence, and then implement it in Python. The goal is to recover sparse signals from underdetermined linear measurements. The implementation should be validated on synthetic data by demonstrating successful recovery of the ground-truth dictionary and achieving a low signal reconstruction error compared to a baseline like simple least squares.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Method of Optimal Directions dictionary learning convergence analysis ADMM\\\", mode=web)\", \"/math(Derive the iterative update rules for the Method of Optimal Directions (MOD) algorithm, ensuring dictionary atom normalization, and analyze the convergence guarantees by examining the objective function's behavior across iterations., includes numpy)\", \"/code(Implement the full MOD algorithm and the Orthogonal Matching Pursuit (OMP) sparse coding stage in Python. Validate on synthetic data, tracking reconstruction MSE and dictionary recovery error, and consider a Bayesian baseline for performance comparison.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather current theoretical formulations and proofs from arXiv. This informs the /math agent's derivation of the specific MOD algorithm's update rules and convergence analysis. Finally, the /code agent implements the derived math and performs empirical validation, which serves as a practical check on the theoretical work.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Method of Optimal Directions (MOD) update rules and a brief analysis of its convergence properties.\", \"- A Python implementation (`.py` script) of the MOD algorithm, including sparse coding via OMP.\", \"- A summary report (`report.md`) detailing the experimental setup, results (including plots of reconstruction error vs. iteration), and citations to at least two relevant papers.\", \"- A JSON file (`metrics.json`) containing the final reconstruction Signal-to-Noise Ratio (SNR) and dictionary recovery error.\", \"- Verification logs confirming the alignment between the mathematical derivation and the code's computational steps.\"], \"thinking_outline\": [\"1. Formulate the dictionary learning problem as an optimization task seeking to minimize a reconstruction error term subject to a sparsity constraint on the signal representation.\", \"2. Derive the iterative update rules for the dictionary (the MOD step) and the sparse coefficients (the OMP step), ensuring the dictionary columns are constrained (e.g., unit L2-norm).\", \"3. Implement the algorithm in Python, creating functions for generating synthetic sparse signals, the sensing matrix, the OMP solver, and the main MOD update loop.\", \"4. Validate the implementation by running it on a problem with a known ground-truth dictionary and verifying that the learned dictionary converges to the true one.\", \"5. Check the correctness of the sparse coding step by ensuring the recovered sparse codes meet the desired sparsity level.\", \"6. Evaluate the algorithm's performance by plotting the reconstruction error over iterations and measuring the final Signal-to-Noise Ratio (SNR) of the recovered signals.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code implementation of update rules must match the mathematical derivation from /math; fallback: if convergence fails in /code, revisit the derivation in /math for potential errors in the optimization steps or constraints).\", \"todo_list\": [\"- [ ] /general-search: Find two key papers on MOD or related dictionary learning techniques, focusing on convergence proofs.\", \"- [ ] /math: Derive the update equations for the dictionary atoms and sparse coefficients based on the MOD formulation.\", \"- [ ] /math: Analyze the conditions under which the objective function is guaranteed to decrease or converge.\", \"- [ ] /code: Implement the MOD algorithm, including a functional OMP solver for the sparse coding stage.\", \"- [ ] /code: Generate synthetic data and run experiments to measure reconstruction SNR and dictionary recovery error.\", \"- [ ] /math: Verify that the implemented update equations in the Python code from /code are an exact match to the derived formulas.\", \"- [ ] /code: Validate that the algorithm converges on test data by plotting the objective function value per iteration.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\", \"convex-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of MOD update rules must be complete and self-consistent.\", \"- The Python implementation must successfully recover a synthetic ground-truth dictionary with a recovery error (Frobenius norm difference) of less than 0.1.\", \"- The final report must show a signal reconstruction SNR of over 20 dB on the specified synthetic dataset.\", \"- The solution must cite at least two relevant academic papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 387, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a formal derivation of the policy gradient theorem that incorporates a general state-dependent baseline b(s). Please prove rigorously that this baseline does not introduce bias to the gradient estimator. Subsequently, derive the optimal baseline that minimizes the estimator's variance, and show its connection to the state-value function V(s). Finally, demonstrate this variance reduction empirically in a classic control theory environment.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org policy gradient theorem variance reduction baseline 'control theory' sutton barto\\\", mode=web)\", \"/math(Derive the policy gradient theorem with a state-dependent baseline, proving the resulting estimator is unbiased. Analyze the variance reduction and its connection to the advantage function, ensuring the convergence properties for control theory problems are discussed.)\", \"/code(Implement REINFORCE with the derived state-dependent baseline in PyTorch for a classic control theory environment like CartPole. Compare its learning stability and convergence rate against a vanilla REINFORCE agent, plotting gradient variance diagnostics., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on policy gradient theory. This informs the /math agent's formal derivation and proof of the variance reduction properties. Finally, the /code agent implements the derived algorithm to provide empirical validation, creating a full loop from theory to practice.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF containing the full mathematical derivation and proofs.\", \"- A Python script implementing both vanilla REINFORCE and REINFORCE with the optimal baseline.\", \"- A plot comparing the learning curves (episodic return vs. training steps) and gradient variance for both algorithms.\", \"- A bibliography file with at least two cited papers on policy gradient methods.\", \"- A verification report confirming that the code's baseline implementation matches the derived optimal baseline.\"], \"thinking_outline\": [\"1. Review the foundational Policy Gradient Theorem and the REINFORCE algorithm.\", \"2. Formulate the policy gradient estimator with an arbitrary state-dependent baseline b(s_t).\", \"3. Derive the expectation of the modified gradient estimator and prove that the baseline term's expectation is zero, thus confirming the estimator is unbiased.\", \"4. Formulate the variance of the estimator and solve the optimization problem to find the baseline b(s_t) that minimizes this variance, showing it is the state-value function V^{\\\\pi}(s_t).\", \"5. Validate the mathematical derivation by cross-referencing with authoritative sources found during the search phase.\", \"6. Implement both algorithms and verify that the empirical variance of the gradient estimator is lower for the agent using the learned value function as a baseline.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search agent provides foundational papers to the /math agent. The /math agent's formal derivation serves as a precise specification for the /code agent. The /code agent's implementation must pass unit tests based on the derivation. Verification: The router will check that the implemented baseline function in the code directly corresponds to the optimal baseline derived by the /math agent. Fallback: If the code fails to show variance reduction, the task is routed back to /math to check for errors in the variance analysis.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on policy gradient baselines, such as Sutton & Barto.\", \"- [ ] /math: Produce a formal derivation of the unbiasedness of the policy gradient with a baseline.\", \"- [ ] /math: Derive the optimal variance-reducing baseline.\", \"- [ ] /code: Implement REINFORCE and REINFORCE-with-baseline agents in PyTorch.\", \"- [ ] /code: Instrument the code to log episodic returns and gradient variance.\", \"- [ ] /math: Verify that the implemented baseline update rule in the code correctly approximates the state-value function as per the derivation.\", \"- [ ] /code: Verify that the empirical results demonstrate statistically significant variance reduction and improved learning stability.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradients\", \"variance-reduction\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly proves that the policy gradient estimator with a state-dependent baseline is unbiased.\", \"- The implementation runs without errors and produces a plot clearly showing the baseline agent achieves lower variance and/or faster convergence than the vanilla agent.\", \"- The final report cites at least two authoritative sources (e.g., from arXiv, NeurIPS, ICML) with proper identifiers.\", \"- The secondary metric (gradient variance) for the baseline agent must be demonstrably lower than the vanilla agent's after an initial learning period.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 388, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the evidence lower bound (ELBO) for a sparse Gaussian Process model utilizing a Horseshoe prior on the inducing point variances to encourage sparsity. Then, implement this model using Hamiltonian Monte Carlo in Python to analyze its convergence properties and predictive posterior on a high-frequency finance dataset, specifically focusing on the kernel's spectral properties.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org sparse gaussian process horseshoe prior inducing points variational inference\\\", mode=rag)\", \"/math(Derive the full Evidence Lower Bound (ELBO) for a Bayesian sparse Gaussian Process with a Horseshoe prior on inducing inputs. Detail the reparameterization trick required for the stochastic gradients and analyze the kernel eigenvalue behavior.)\", \"/code(Implement the derived sparse GP model in Python with JAX/NumPyro, using Hamiltonian Monte Carlo for inference. Validate gradient calculations, monitor ELBO convergence, and benchmark predictive log-likelihood on a canonical finance dataset.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather seminal papers on sparse GPs and Horseshoe priors, providing a theoretical foundation. Next, /math is used to perform the complex Bayesian derivation required for the model's objective function. Finally, /code implements and validates this mathematical theory, testing its practical performance and convergence on a real-world finance problem.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the ELBO and gradient estimators.\", \"A Python script implementing the specified sparse Gaussian Process model with HMC.\", \"A summary report (report.md) analyzing the model's convergence, sparsity, and predictive performance, including plots of the ELBO trace and posterior predictions.\", \"A JSON file (metrics.json) containing the final predictive log-likelihood and kernel spectral norm.\", \"A bibliography file with at least two cited arXiv preprints.\"], \"thinking_outline\": [\"1. Research the theoretical underpinnings of sparse Gaussian Processes (e.g., FITC/VFE approximations) and the properties of the Horseshoe prior for inducing sparsity.\", \"2. Formulate the joint probability model and derive the complete Evidence Lower Bound (ELBO), clearly defining the variational distribution and the prior terms.\", \"3. Implement the model and the Hamiltonian Monte Carlo sampler, paying close attention to numerical stability and efficient kernel computations.\", \"4. Validate the implementation by comparing the numerical gradients computed by the framework against analytical gradients for a small-scale problem.\", \"5. Execute the model on a suitable finance dataset, analyze the convergence of the ELBO and model parameters, and evaluate the predictive posterior distribution.\", \"6. Audit the final report to verify that the derived theory in the PDF matches the Python implementation and that the reported metrics are reproducible.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers (PDFs/arXiv IDs) to the math agent. The math agent produces a formal derivation (LaTeX) which serves as the blueprint for the code agent. The code agent implements this blueprint and generates performance metrics and plots. The router performs a final QA check to ensure the code's logic faithfully implements the derivation and that the results are consistent. Fallback: If the derivation is intractable, the math agent will flag it and suggest a simpler prior (e.g., standard Normal), triggering a revised plan.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on sparse variational GPs and the Horseshoe prior.\", \"- [ ] /math: Derive the ELBO, specifying all terms including the KL divergence to the Horseshoe prior.\", \"- [ ] /math: Formulate the gradient estimators needed for the Hamiltonian dynamics.\", \"- [ ] /code: Implement the sparse GP model, kernel, and HMC sampler in JAX/NumPyro.\", \"- [ ] /code: Create a validation suite to check gradient correctness and monitor ELBO on a synthetic dataset.\", \"- [ ] /code: Run the final experiment on the specified finance dataset and generate all required artifacts (plots, metrics).\", \"- [ ] /math: Verify that the implemented loss function in the code correctly reflects the derived ELBO.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"gaussian-processes\", \"bayesian-inference\", \"stochastic-calculus\"], \"acceptance_criteria\": [\"The provided PDF must contain a mathematically sound derivation of the ELBO for the specified model.\", \"The Python code must execute without errors and reproduce the metrics reported in the summary.\", \"The final ELBO must show stable convergence in the generated plots.\", \"The analysis must cite at least two relevant academic papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 389, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the instability in Differentiable Architecture Search (DARTS) by analyzing the Hessian of the validation loss with respect to the architecture parameters. Formulate the bilevel optimization problem, derive the gradient using the implicit function theorem, and implement a second-order approximation in PyTorch to regularize the search space. Compare its convergence against a baseline Gumbel-Softmax relaxation approach, reporting both validation AUPRC and final architecture complexity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org differentiable architecture search 'bilevel optimization' Hessian instability\\\", mode=both. Gather foundational arxiv papers on AutoML and Bayesian optimization to contextualize the search problem.)\", \"/math(Derive the hypergradient for the DARTS bilevel problem using the implicit function theorem. Analyze the dominant eigenvalue of the Hessian to justify the regularization term and characterize its effect on convergence.)\", \"/code(Implement the DARTS search process in PyTorch, comparing a Gumbel-Softmax baseline against the derived Hessian-based regularizer. Track validation AUPRC and architecture complexity as primary automl metrics., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on DARTS instability. This research informs the /math agent's formal derivation of the second-order gradient. Finally, the /code agent implements the derived theory for empirical validation, creating a full theory-to-practice workflow.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the regularized hypergradient for DARTS using the implicit function theorem.\", \"- A Python script implementing the DARTS search loop with both the baseline and the proposed second-order regularizer.\", \"- A JSON file containing the final validation AUPRC, architecture complexity, and convergence logs for both methods.\", \"- A summary report (.md) explaining the methodology, results, and including at least two citations to relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research the bilevel optimization formulation of DARTS and common criticisms regarding its optimization landscape and the dominance of skip-connections.\", \"2. Formally derive the hypergradient for the architecture parameters using the implicit function theorem, explicitly showing the inverse Hessian-vector product term.\", \"3. Develop a numerically stable approximation for the inverse Hessian-vector product to be used as a regularizer in the search algorithm.\", \"4. Implement the complete DARTS search cell in PyTorch, ensuring the baseline and the regularized version share the same weight-training inner loop.\", \"5. Validate the gradient implementation by comparing numerical differentiation (finite differences) with the analytical gradient on a small, tractable version of the problem.\", \"6. Audit the final experimental results to ensure the reported metrics (AUPRC, complexity) are consistent with the generated logs and support the theoretical claims about improved stability.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent's final derivation is the formal specification for the code agent's implementation. The code agent's output (metrics, logs) is then cross-referenced against the math derivation for verification by the router. Fallback: If the code implementation diverges from the derivation, the task is returned to the code agent with specific inconsistencies flagged.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on DARTS instability and second-order optimization in NAS.\", \"- [ ] /math: Produce a full, step-by-step derivation of the architecture gradient using the implicit function theorem.\", \"- [ ] /math: Analyze the role of the Hessian's eigenvalues in the optimization landscape's curvature.\", \"- [ ] /code: Implement the DARTS search algorithm in PyTorch, including the Gumbel-Softmax baseline.\", \"- [ ] /code: Integrate the Hessian-based regularizer derived by the math agent into the search algorithm.\", \"- [ ] /math: verify that the gradient derivation is correctly and efficiently implemented in the Python code's backward pass.\", \"- [ ] /code: validate the numerical stability of the implementation and generate comparison plots for convergence and final performance.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"automl\", \"neural-architecture-search\", \"differentiable-programming\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies the implicit function theorem to the specified bilevel optimization problem.\", \"- The Python code runs without error and generates logs and a metrics file comparing the baseline and regularized methods.\", \"- The final report includes plots showing improved convergence or stability for the regularized method, cites at least two relevant arXiv papers with IDs, and correctly interprets the AUPRC and complexity metrics.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 390, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a robust implementation of Trust Region Policy Optimization (TRPO). My focus is on the theoretical correctness of the conjugate gradient algorithm used to solve the trust region subproblem. Please provide a formal derivation of the update rule, relating it to the Fisher Information Matrix as a local quadratic approximation of the KL-divergence constraint. Then, implement the algorithm in Python and benchmark its convergence and stability on a continuous control theory environment like the Pendulum-v1, analyzing the impact of the backtracking line search.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org trust region policy optimization conjugate gradient fisher information matrix\\\", mode=rag)\", \"/math(Derive the TRPO objective function with the KL-divergence constraint. Formulate the quadratic approximation using the Fisher Information Matrix and derive the full conjugate gradient algorithm for solving Fx=g, incorporating concepts from control theory and Bayesian inference.)\", \"/code(Implement the TRPO agent in Python using PyTorch, including a module for the conjugate gradient solver and a backtracking line search. Validate the implementation against the MuJoCo Pendulum-v1 environment, tracking average reward and KL divergence metrics to ensure convergence.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational papers on TRPO's theory. Then, /math is used to formally derive the complex optimization steps. Finally, /code implements and validates the derived algorithm, providing empirical evidence.\", \"expected_artifacts\": [\"A LaTeX document with the full mathematical derivation of the TRPO update and the conjugate gradient method.\", \"A Python script containing the complete, commented TRPO implementation for the Pendulum-v1 environment.\", \"A performance report in PDF format, including plots of the average episodic reward vs. training steps and the mean KL divergence per policy update.\", \"A citations.bib file with at least two relevant arXiv papers.\", \"A verification log detailing the checks between the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Review the foundational literature on TRPO, focusing on the constrained optimization problem and the role of the Fisher Information Matrix.\", \"2. Formally derive the TRPO objective and the local quadratic approximation that leads to the Fx=g system for the search direction.\", \"3. Derive the conjugate gradient algorithm step-by-step as an efficient solver for this system, avoiding explicit matrix inversion.\", \"4. Implement the complete TRPO agent, paying close attention to the numerical stability of the conjugate gradient solver and the backtracking line search implementation.\", \"5. Validate the implementation by running experiments on a standard continuous control benchmark and analyzing the agent's learning curve and policy stability.\", \"6. Verify that the key mathematical expressions in the code (e.g., Fisher-vector product computation) directly correspond to the derived formulas.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: /math agent cross-references its derivation against the Python implementation from /code, checking for discrepancies in the Fisher-vector product logic; fallback: if verification fails, /code agent must refactor the implementation and add unit tests for the conjugate gradient solver).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on TRPO, focusing on the conjugate gradient solver.\", \"- [ ] /math: Produce a full derivation of the TRPO objective's quadratic approximation and the conjugate gradient update rule.\", \"- [ ] /code: Implement the TRPO agent in PyTorch, including the GAE estimator and the conjugate gradient solver.\", \"- [ ] /code: Benchmark the agent on the Pendulum-v1 environment, generating reward curves and KL divergence plots.\", \"- [ ] /math: Verify that the `code` agent's implementation of the Fisher-vector product matches the derived mathematical formula.\", \"- [ ] /code: Add comments and docstrings to the code, explaining the connection to the mathematical derivation.\", \"- [ ] /router: Perform final QA, ensuring all acceptance criteria are met and artifacts are generated correctly.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradients\", \"optimization\", \"control-theory\"], \"acceptance_criteria\": [\"The mathematical derivation must correctly formulate the TRPO optimization problem and the conjugate gradient solution.\", \"The implemented agent must achieve a stable learning curve and an average episodic reward of at least -250 on Pendulum-v1 within 100,000 timesteps.\", \"The final report must clearly visualize the learning progress and analyze the KL divergence to confirm the trust region constraint is being respected.\", \"The code must be well-documented, with specific comments linking back to the equations in the derivation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 391, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to solve a large, sparse, symmetric positive-definite linear system arising from a finite element discretization of a Poisson equation. Develop the theory for the Preconditioned Conjugate Gradient (PCG) method using an Incomplete Cholesky (IC0) preconditioner. Then, implement this solver in Python using CUDA via CuPy, focusing on optimizing the sparse matrix-vector product (SpMV) and the preconditioner application. Analyze the method's convergence rate and computational complexity, specifically how the preconditioner affects the eigenvalue clustering of the system matrix.\n", "completion": "{\"route_plan\": [\"/math(Derive the Preconditioned Conjugate Gradient (PCG) algorithm for a sparse symmetric positive-definite system. Analyze the theoretical impact of an Incomplete Cholesky (IC0) preconditioner on the system's eigenvalue distribution and expected CUDA convergence rate.)\", \"/general-search(query=\\\"site:arxiv.org efficient CUDA incomplete cholesky factorization sparse conjugate gradient\\\", mode=rag)\", \"/code(Implement the PCG solver with an IC0 preconditioner using Python with CuPy for CUDA acceleration. Profile the SpMV and preconditioner solve steps, ensuring correctness against a known solution and tracking convergence residuals.)\", \"/math(Validate the empirical convergence rate observed in the CUDA implementation against the theoretical bounds derived from the eigenvalue analysis. Quantify the improvement in condition number and justify any discrepancies.)\"], \"route_rationale\": \"/math first establishes the theoretical foundation for PCG and IC0 preconditioning. The blue /general-search agent then gathers state-of-the-art implementation techniques from arXiv to inform the practical coding phase. /code translates the theory into a high-performance CUDA implementation, focusing on profiling and correctness. The final /math call provides a critical verification step, linking empirical results back to the initial theoretical analysis.\", \"expected_artifacts\": [\"A LaTeX document with the full mathematical derivation of the PCG algorithm and the theoretical analysis of the IC0 preconditioner's effect on eigenvalues.\", \"A commented Python script implementing the GPU-accelerated PCG-IC0 solver using CuPy, including a test case with a sparse matrix.\", \"A performance report comparing the convergence of standard CG vs. PCG, including plots of residual norms vs. iteration and a table of execution times.\", \"A summary of findings from the literature search, with at least two cited arXiv papers on GPU-accelerated iterative solvers.\", \"A final verification report confirming the alignment between theoretical convergence rates and empirical results.\"], \"thinking_outline\": [\"1. Formulate the Preconditioned Conjugate Gradient (PCG) algorithm mathematically, detailing each step: initialization, matrix-vector products, dot products, and updates.\", \"2. Theoretically analyze the role of a preconditioner, specifically the Incomplete Cholesky (IC0) factorization, in improving the condition number of the system matrix A by analyzing the eigenvalues of M\u207b\u00b9A.\", \"3. Research and synthesize best practices for implementing sparse linear algebra operations (SpMV, triangular solves) on GPUs using CUDA, leveraging findings from authoritative sources.\", \"4. Implement the PCG-IC0 algorithm in Python with CuPy, ensuring numerical stability and correctness by testing against a small, well-conditioned problem with a known solution.\", \"5. Validate the performance of the implementation on a larger sparse system, plotting the convergence history (residual norm vs. iteration) and comparing it to the unpreconditioned CG method.\", \"6. Audit the empirical convergence results against the theoretical predictions from step 2, explaining any discrepancies and concluding on the effectiveness of the IC0 preconditioner.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides literature to the math agent for theoretical grounding. The math agent's derivation serves as a spec for the code agent. The code agent produces an implementation and empirical results, which are handed back to the math agent for final verification. Fallback: If verification fails, the code agent will re-audit its preconditioner implementation for precision issues before final router QA.\", \"todo_list\": [\"- [ ] /math: Derive the PCG algorithm and analyze the spectral properties of the IC0-preconditioned system matrix.\", \"- [ ] /general-search: Find >=2 recent arXiv papers on high-performance CUDA implementations of sparse iterative solvers like PCG with IC0.\", \"- [ ] /code: Implement the PCG-IC0 solver in Python/CuPy for a sparse matrix in CSR format.\", \"- [ ] /code: Profile the CUDA kernel execution times for SpMV and the forward/backward substitution steps of the preconditioner.\", \"- [ ] /math: Verify the correctness of the code's output against a reference solution from a library like SciPy.sparse.linalg.\", \"- [ ] /math: Validate that the empirical convergence plot from the code aligns with the theoretical rate predicted by the condition number analysis.\", \"- [ ] /router: Review all generated artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"krylov-subspace-methods\"], \"acceptance_criteria\": [\"The mathematical derivation of PCG and the analysis of the IC0 preconditioner must be complete and correct.\", \"The Python/CuPy implementation must successfully solve a sparse linear system and demonstrate faster convergence (fewer iterations) than the standard Conjugate Gradient method.\", \"The final report must include convergence plots (residual vs. iteration) and a performance table, and cite at least two relevant arXiv papers with IDs.\", \"The verification step must explicitly show the comparison between the theoretical and observed convergence rates.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced", "four_step", "math_backstop", "math_first", "math_multi_pass", "non_canonical_route"]}
{"index": 392, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the rate-distortion function for embedding a robust, adversarially-resilient watermark into the latent space of a pretrained diffusion model. Analyze the information-theoretic trade-off between watermark bit-rate (channel capacity) and the perceptual fidelity of the generated images, measured by the Frechet Inception Distance (FID). Implement a proof-of-concept using JAX to demonstrate this trade-off, ensuring the watermark survives common adversarial transformations like JPEG compression and Gaussian noise.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org rate-distortion theory diffusion model watermarking adversarial robustness\\\", mode=rag)\", \"/math(Derive the rate-distortion function for adversarial watermarking in a diffusion model latent space, referencing key arxiv papers on information bottleneck theory and defining distortion based on expected FID degradation.)\", \"/code(Implement a JAX-based proof-of-concept for the derived watermarking scheme in a pretrained diffusion model, evaluating the trade-off between watermark bit-error rate under adversarial attacks and generation quality using FID., using Python)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on rate-distortion theory for adversarial watermarking. This theoretical grounding is then handed to /math to produce a formal, problem-specific derivation. Finally, /code implements this mathematical framework to provide empirical validation of the theory.\", \"expected_artifacts\": [\"A formal mathematical derivation of the rate-distortion function R(D) for the specified problem.\", \"A Python script using JAX that implements the watermarking algorithm and evaluation pipeline.\", \"A JSON file containing evaluation metrics, specifically the trade-off data points for FID vs. Bit-Error Rate (BER).\", \"A summary report citing at least 3 relevant arXiv pre-prints on the topic.\", \"A verification log confirming the alignment between the mathematical derivation and the code's objective function.\"], \"thinking_outline\": [\"1. Survey foundational literature on rate-distortion theory, information bottlenecks, and generative model watermarking to establish a theoretical basis.\", \"2. Formulate the problem mathematically, defining the source (watermark), channel (diffusion latent space), distortion measure (related to FID), and the adversarial threat model (e.g., specific image transformations).\", \"3. Derive the specific rate-distortion function R(D) that characterizes the minimal achievable rate for a given level of expected distortion.\", \"4. Implement the watermarking algorithm and the evaluation framework in code; validate that the implemented loss function correctly reflects the derived distortion measure from the mathematical model.\", \"5. Conduct experiments to empirically trace the rate-distortion curve and verify that the results are consistent with the theoretical predictions under various adversarial perturbations.\", \"6. Synthesize the findings into a comprehensive report including the derivation, code, experimental results, and citations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: code's loss function must match the derived distortion metric from the math agent; fallback: if FID scores are anomalous or the trade-off curve is non-monotonic, re-consult /general-search for alternative distortion measures or implementation techniques from recent literature).\", \"todo_list\": [\"- [ ] /general-search: Curate seminal papers from arXiv on rate-distortion theory for robust watermarking in diffusion models.\", \"- [ ] /math: Based on search results, formally derive the rate-distortion function R(D) for embedding an adversarial watermark.\", \"- [ ] /code: Implement the watermarking encoder/decoder and evaluation harness in JAX for a pretrained diffusion model.\", \"- [ ] /code: Implement metric computation for FID and watermark Bit-Error Rate (BER) under adversarial attacks like JPEG compression and noise.\", \"- [ ] /math: Verify that the loss function implemented in the code accurately represents the derived distortion metric.\", \"- [ ] /code: Validate the implementation by running a small-scale experiment and checking for expected behavior in FID/BER trade-offs.\", \"- [ ] router: Perform final QA, ensuring the empirical results in the report align with the theoretical derivation and all acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"rate-distortion\"], \"acceptance_criteria\": [\"The submission includes a complete and mathematically sound derivation of the rate-distortion function.\", \"The provided JAX code successfully runs and reproduces the reported FID vs. BER trade-off curve to within a 5% tolerance.\", \"The final report cites at least 3 relevant papers from arXiv or top-tier conferences published within the last 5 years.\", \"The implemented watermark demonstrates robustness (BER < 0.1) against both 90% quality JPEG compression and Gaussian noise with sigma=0.05.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 394, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm exploring neural-guided program synthesis. I want to build a Python prototype that synthesizes a correct-by-construction sorting algorithm for integer lists based on formal specifications in higher-order logic. The system should use a Transformer-based model to guide a search over a simple functional language grammar, and then use an SMT solver backend to formally verify two properties: that the synthesized program's output is ordered and that it is a permutation of the input. I need the full formal derivation, the implementation, and a complexity analysis.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org transformer guided program synthesis SMT solver verification\\\", mode=rag)\", \"/math(Formulate the sorting program's correctness properties, sortedness and permutation, in first-order logic with quantifiers suitable for an SMT solver. Define the formal grammar and semantic rules for the target functional language, considering its computational complexity., includes cybersecurity)\", \"/code(Implement the synthesis loop in Python using Hugging Face Transformers for heuristic search and the z3-solver library to verify program candidates against the formal specification. Ensure the code checks for search convergence and logs all SMT queries.)\"], \"route_rationale\": \"/general-search is needed first to ground the project in current SOTA research from arXiv, as requested by the blue agent. This informs the /math agent, which will define the formal logic and program grammar. The formal specification from /math is then passed to the /code agent to implement the Python-based neural-guided search and SMT verification loop, ensuring the implementation matches the theory.\", \"expected_artifacts\": [\"A formal specification document (`spec.md`) detailing the logic predicates for sortedness and permutation.\", \"A Python script (`synthesis_engine.py`) implementing the neural-guided search and SMT-based verification.\", \"A results summary (`results.json`) containing the synthesized program, its proof trace, and performance metrics.\", \"A literature review (`literature.md`) with citations for at least 2 relevant arXiv papers.\", \"A verification report (`verification_log.txt`) showing the SMT solver's output for the correctness proofs.\"], \"thinking_outline\": [\"1. Research state-of-the-art approaches for neural program synthesis with formal verification, focusing on transformer-guided search and SMT solver integration.\", \"2. Formalize the synthesis problem: define the target functional language grammar, and express the correctness properties (sortedness, permutation) as SMT-checkable assertions in a suitable logic (e.g., quantifier-free logic with uninterpreted functions).\", \"3. Implement the core synthesis loop in Python, integrating a pre-trained language model to predict promising program constructs and the Z3 SMT solver to validate them against the formal specification.\", \"4. Validate the implementation by testing its ability to synthesize a known sorting algorithm (e.g., insertion sort) and successfully prove its correctness.\", \"5. Analyze the computational complexity and convergence properties of the search process, comparing them against theoretical expectations.\", \"6. Audit the final synthesized program and its formal proof trace to ensure they are logically sound and complete.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal specification) -> /code (implementation & execution) -> router QA (verification: SMT proof traces match the mathematical specification and code output; fallback: if verification fails, return to /code to debug solver integration or to /math to refine logical constraints).\", \"todo_list\": [\"- [ ] /general-search: Find >=2 arXiv papers on Transformer-based program synthesis with formal verification.\", \"- [ ] /math: Define the formal grammar and the SMT-LIB assertions for sortedness and permutation properties.\", \"- [ ] /code: Implement the main Python search loop using Hugging Face Transformers and the `z3-solver` library.\", \"- [ ] /code: Instrument the code to log synthesis time, number of SMT queries, and proof success rate.\", \"- [ ] /math: Verify that the SMT assertions correctly and completely capture the mathematical definition of a sorted permutation.\", \"- [ ] /code: Validate the Python implementation of the SMT queries against the formal specification from the /math agent.\", \"- [ ] /router: Review all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neuro-symbolic\", \"program-synthesis\", \"theorem-proving\"], \"acceptance_criteria\": [\"- The system must successfully synthesize a functional program that correctly sorts a list of 10 integers.\", \"- The SMT solver's output must provide a conclusive proof trace for the correctness of the synthesized program.\", \"- The final report must include the synthesized program, its formal proof trace, and citations to at least two relevant academic papers.\", \"- The implementation must pass static analysis and have unit tests for the SMT integration.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 395, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the dynamics of the loss landscape's sharpness during neural network training. Specifically, I need a formal derivation for the evolution of the Hessian's maximal eigenvalue under stochastic gradient descent for a simplified model. Then, validate this theoretical model by empirically tracking the Hessian's spectral norm in a PyTorch implementation, and analyze how it correlates with the generalization gap and training convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian eigenvalue dynamics neural network generalization sharpness stochastic gradient descent convergence\\\", mode=rag)\", \"/math(Derive the continuous-time dynamics of the Hessian's maximal eigenvalue for a two-layer linear network under gradient flow, explicitly showing the dependency on the data covariance matrix and Jacobian structure. Analyze the conditions for eigenvalue explosion or decay.)\", \"/code(Implement a PyTorch routine using power iteration to track the Hessian's top eigenvalue during training of a ResNet-18 on CIFAR-10. Plot the empirical eigenvalue trajectory versus the theoretical prediction under simplifying assumptions and measure the final generalization gap., using Python)\"], \"route_rationale\": \"The plan follows a theory-validation-synthesis structure. The 'blue' /general-search agent first gathers foundational literature on Hessian analysis. Then, /math formalizes the theory by deriving the core differential equations. Finally, /code provides empirical validation by implementing the tracking algorithm and comparing it against the theory, which is essential for grounding the mathematical claims.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the Hessian eigenvalue dynamics.\", \"A documented Python script for training a neural network while tracking the Hessian's spectral norm.\", \"A Jupyter notebook or PDF report presenting plots of the empirical vs. theoretical eigenvalue evolution and analyzing the correlation with the generalization gap.\", \"A bibliography with at least two cited arXiv papers on the topic.\"], \"thinking_outline\": [\"1. Survey existing literature to understand the theoretical link between the Hessian spectrum, loss landscape sharpness, and generalization in deep learning.\", \"2. Formulate and derive the ordinary differential equation for the evolution of the Hessian's maximal eigenvalue under gradient flow for a simplified but non-trivial network architecture.\", \"3. Validate the correctness of the matrix calculus derivation by checking for dimensional consistency and analyzing behavior in asymptotic cases (e.g., initialization, convergence).\", \"4. Implement a numerically stable power iteration method integrated into a standard PyTorch training loop to estimate the top eigenvalue of the Hessian without materializing the full matrix.\", \"5. Execute experiments to compare the empirically observed eigenvalue trajectory with the predictions from the simplified theoretical model, documenting and explaining any significant deviations.\", \"6. Audit the final report to verify that the empirical findings are correctly interpreted in the context of the mathematical derivation and cited literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The 'blue' agent provides foundational papers to the /math agent. The /math agent's derivation provides a formal equation that the /code agent must implement and test. The /code agent's empirical plots and data are then passed to the final review stage. Verification: The router will check if the Python implementation correctly reflects the derived ODE's structure. Fallback: If the derivation is intractable, the /math agent will be prompted to use a simpler model (e.g., a single neuron). If the code's eigenvalue tracking is unstable, the /code agent will implement Lanczos iteration as an alternative to power iteration.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Hessian spectral analysis and its connection to generalization.\", \"- [ ] /math: Produce the formal derivation for the Hessian's top eigenvalue dynamics, clearly stating all assumptions.\", \"- [ ] /code: Write a Python script to train a ResNet on CIFAR-10.\", \"- [ ] /code: Implement the power iteration or Lanczos method to track the top Hessian eigenvalue throughout training.\", \"- [ ] /math: Verify that the code's numerical approximation aligns conceptually with the derived differential equation.\", \"- [ ] /code: Generate plots comparing the empirical eigenvalue trajectory with the theoretical model's predictions and calculate the generalization gap.\", \"- [ ] /router: Perform final QA, ensuring the report's conclusions are supported by both the derivation and the empirical evidence.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-theory\", \"matrix-calculus\", \"generalization-bounds\"], \"acceptance_criteria\": [\"- The mathematical derivation for the Hessian eigenvalue dynamics must be self-contained and mathematically sound.\", \"- The Python script must successfully train the model and generate plots showing the evolution of the Hessian's top eigenvalue over epochs.\", \"- The final report must correctly correlate the observed eigenvalue (sharpness) with the model's final generalization gap, referencing at least two relevant papers.\", \"- The mean squared error between the simplified theoretical prediction and the smoothed empirical eigenvalue curve must be reported.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 399, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for score-based generative models as formulated by Song et al. Then, implement a simple Ornstein-Uhlenbeck SDE solver in Python to sample from a Gaussian mixture model, using a neural network to approximate the score function (\u2207_x log p_t(x)). Validate the implementation by comparing the empirical distribution of generated samples against the true distribution using the Wasserstein-1 distance and track convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models stochastic differential equations reverse-time SDE\\\", mode=both)\", \"/math(Derive the reverse-time SDE for a forward diffusion process, explicitly showing how the score function \u2207_x log p_t(x) arises from Kolmogorov's forward equation and Bayesian principles.)\", \"/code(Implement a JAX-based Euler-Maruyama solver for the reverse-time SDE to sample from a Gaussian mixture, tracking convergence with the Wasserstein distance and ensuring FID score compatibility., using Python)\"], \"route_rationale\": \"The blue /general-search agent is required first to locate the seminal arXiv papers on score-based SDEs. The /math agent will then formalize the derivation based on these authoritative sources. Finally, the /code agent will implement the derived equations and validation metrics for an empirical test.\", \"expected_artifacts\": [\"A LaTeX document providing the full derivation of the reverse-time SDE.\", \"A Python script containing the JAX implementation of the SDE solver and score network.\", \"A plot visualizing the Wasserstein-1 distance between generated and true samples over training iterations.\", \"A JSON file reporting the final Wasserstein distance and estimated log-likelihood.\", \"A markdown file summarizing the cited papers with their corresponding arXiv IDs.\"], \"thinking_outline\": [\"1. Review foundational literature on score matching and its connection to stochastic differential equations (SDEs), focusing on the reverse-time process.\", \"2. Formally derive the reverse-time SDE, likely involving concepts from It\u00f4 calculus and the Fokker-Planck equation, referencing the sourced papers.\", \"3. Implement the numerical SDE solver (e.g., Euler-Maruyama) and the time-dependent score-approximating neural network.\", \"4. Validate the mathematical derivation against reference implementations or known theoretical results from the sourced papers to ensure correctness before proceeding.\", \"5. Train the score network and check for convergence by monitoring the Wasserstein distance metric; investigate any instabilities or lack of convergence.\", \"6. Analyze the final generated sample distribution against the ground truth mixture model, both qualitatively (via plots) and quantitatively (via metrics).\"], \"handoff_plan\": \"/general-search (sources) -> /math (derivation) -> /code (implementation) -> router QA (verification: derivation matches code logic, metrics are correctly computed; fallback: if derivation is flawed, return to /math with specific error notes from /code's unit tests)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on score-based generative models via SDEs (e.g., Song et al., 'Score-Based Generative Modeling through Stochastic Differential Equations').\", \"- [ ] /math: Derive the reverse-time SDE based on the gathered literature, clearly stating all assumptions.\", \"- [ ] /code: Implement the Euler-Maruyama solver for the reverse SDE in JAX.\", \"- [ ] /code: Implement the score-approximating neural network and the corresponding training loop.\", \"- [ ] /code: Add metric calculation for Wasserstein-1 distance between generated and target samples.\", \"- [ ] /math: Verify that the discrete update rule in the /code implementation is a valid first-order discretization of the continuous-time SDE derived.\", \"- [ ] /code: Validate the numerical stability of the SDE solver by checking for NaNs or exploding sample values during a short test run.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-differential-equations\", \"generative-modeling\"], \"acceptance_criteria\": [\"The formal derivation of the reverse-time SDE is mathematically correct and self-contained.\", \"The Python code successfully generates samples and the produced plot shows a decreasing Wasserstein-1 distance over training time.\", \"The final report includes at least 2 citations to relevant arXiv preprints with their full IDs.\", \"The empirical distribution of generated samples qualitatively matches the target Gaussian mixture model in a 2D scatter plot.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"computation_guidance\": {\"Wasserstein-1 Distance\": \"Compute using `scipy.stats.wasserstein_distance` between a large batch of generated samples and an equally large batch of true samples from the target distribution at regular intervals.\", \"Final Log-Likelihood Estimate\": \"After training, estimate the log-likelihood of a holdout dataset using the instantaneous change of variables formula derived from the SDE, as a proxy for model fit.\"}}}", "categories": ["advanced", "metrics_computation", "metrics_guidance"]}
{"index": 400, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rule for preconditioned Stochastic Gradient Langevin Dynamics (pSGLD) for posterior sampling in a Bayesian Neural Network. Implement this in Python using JAX, and validate its convergence properties against a standard Metropolis-Hastings adjusted Langevin algorithm (MALA) on a synthetic regression task. Specifically, analyze the impact of the preconditioner on the effective sample size (ESS) and potential scale reduction factor (R-hat).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org preconditioned stochastic gradient langevin dynamics bayesian neural network preconditioner\\\", mode=rag)\", \"/math(Derive the discrete-time update rule for pSGLD from the continuous Langevin diffusion, analyzing the convergence criteria and the role of the preconditioner matrix.)\", \"/code(Implement pSGLD and a MALA baseline in JAX for a Bayesian regression model, comparing their performance using Effective Sample Size and R-hat metrics, noting potential finance applications., using Python)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather foundational theory and recent advancements in pSGLD from authoritative sources like arXiv. This theoretical grounding is passed to /math for a rigorous derivation of the algorithm's update equations. Finally, /code implements the derived math to provide an empirical validation of the theory, comparing the advanced pSGLD sampler against a well-known baseline.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the pSGLD update rule from the continuous-time SDE.\", \"A commented Python script (`main.py`) implementing pSGLD and MALA samplers for a BNN using JAX.\", \"A results report in Markdown (`report.md`) with tables and plots comparing ESS and R-hat metrics between the two samplers.\", \"A `metrics.json` file containing the final computed primary and secondary metrics.\", \"A bibliography file (`references.bib`) with at least two cited arXiv preprints.\"], \"thinking_outline\": [\"1. Research preconditioned SGLD, focusing on its connection to Riemannian manifold Langevin dynamics and the theoretical justification for using a preconditioner like the Fisher information matrix.\", \"2. Formally derive the discretized update rule for pSGLD from the continuous-time Langevin diffusion, explicitly stating all assumptions about the step size, noise term, and preconditioner.\", \"3. Implement both the pSGLD sampler and a baseline MALA sampler in JAX for a simple Bayesian neural network trained on a synthetic dataset with known properties.\", \"4. Verify the correctness of the implementation by comparing the gradient and preconditioner application logic in the code directly against the terms in the mathematical derivation.\", \"5. Execute multiple MCMC chains to collect posterior samples and compute key convergence diagnostics, primarily Effective Sample Size (ESS) and the R-hat statistic.\", \"6. Validate the core hypothesis by quantitatively demonstrating that the pSGLD sampler achieves a higher ESS per unit of computation compared to the MALA baseline, confirming improved sampling efficiency.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation that serves as a specification for the code agent. The code agent's output (metrics, plots) is then compared against the theoretical expectations from the derivation. Verification: router checks if the code's core update loop matches the math agent's final equation. Fallback: if metrics from /code contradict theory, the task is routed back to /math to check for errors in the derivation or assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on pSGLD or Riemannian Langevin methods from arXiv.\", \"- [ ] /math: Produce a step-by-step derivation of the pSGLD update rule, defining all terms.\", \"- [ ] /code: Implement the BNN model structure and the MALA baseline sampler in JAX.\", \"- [ ] /code: Implement the pSGLD sampler, including logic for the preconditioner matrix.\", \"- [ ] /math: Verify that the JAX implementation of the pSGLD update step is a one-to-one match with the derived equation.\", \"- [ ] /code: Run simulation experiments and generate plots and a summary table for ESS and R-hat metrics.\", \"- [ ] /code: Verify that the computed R-hat values for converged chains are close to 1.0 (e.g., < 1.1).\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-processes\", \"mcmc\"], \"acceptance_criteria\": [\"- The provided LaTeX derivation must correctly discretize the continuous-time Langevin SDE into a computable update rule.\", \"- The Python implementation must run without error and produce posterior samples for a Bayesian neural network.\", \"- The final report must show a clear, quantitative comparison of ESS and R-hat values, demonstrating pSGLD's improved sampling efficiency over the baseline.\", \"- The bibliography must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 403, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a robust implementation of the Fast Causal Inference (FCI) algorithm to learn a Partial Ancestral Graph (PAG) from a dataset containing both observational and interventional data. Please start by formalizing the logic for d-separation in PAGs, then implement the algorithm in Python, ensuring it can handle latent confounders. The implementation must be validated by simulating data from a known Structural Causal Model (SCM), applying specific 'do' interventions, and then evaluating the recovered graph's correctness against the ground truth using Structural Hamming Distance.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'fast causal inference' algorithm implementation 'partial ancestral graph' latent confounders interventional bayesian data\\\", mode=rag)\", \"/math(Derive the key adjacency and orientation rules for the FCI algorithm, including the logic for m-separation in PAGs and how 'do' interventions break edges in the causal graph., includes numpy)\", \"/code(Implement FCI in Python using pgmpy and networkx. Include a data simulation module for a ground-truth SCM with confounders, apply interventions, and report Structural Hamming Distance.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on the FCI algorithm and its nuances with interventional data. This research informs the /math agent's formal derivation of the algorithm's rules. Finally, the /code agent implements the algorithm based on the precise mathematical specification, including a validation suite.\", \"expected_artifacts\": [\"A formal mathematical derivation of the FCI algorithm's orientation rules for PAGs.\", \"A Python script implementing the FCI algorithm and a simulation environment for validation.\", \"A JSON file containing evaluation metrics (SHD, adjacency precision/recall) comparing the learned PAG to the ground truth.\", \"A summary report citing at least 2 relevant papers on causal discovery with their arXiv IDs.\", \"Plots visualizing the ground-truth SCM and the recovered PAG.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of the FCI algorithm, focusing on its assumptions (e.g., Causal Markov, Faithfulness) and its extension to handle latent confounders and selection bias via PAGs.\", \"2. Formally derive the conditional independence tests and the ten orientation rules that govern the FCI algorithm, paying close attention to the logic of m-separation.\", \"3. Implement a data generation process from a known Structural Causal Model (SCM), including functionality to apply hard interventions (do-calculus).\", \"4. Code the main FCI algorithm, including the skeleton discovery phase and the edge orientation phase based on the derived rules.\", \"5. Validate the implementation by running it on the simulated data and computing the Structural Hamming Distance (SHD) between the estimated PAG and the true PAG.\", \"6. Verify that the SHD metric improves with increased sample size and that interventions are correctly handled, leading to a more accurate graph structure.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The literature review from /general-search provides the foundational papers for /math's derivation. The formal derivation from /math acts as a precise pseudocode for the /code agent. The code's output artifacts are then reviewed by the router. Verification: check if the SHD is reasonably low. Fallback: if SHD is high, revisit the conditional independence test implementation in /code or the orientation rule logic in /math.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on the FCI algorithm, focusing on handling interventional data.\", \"- [ ] /math: Write down the formal definitions for d-separation, m-separation, and the complete set of FCI orientation rules.\", \"- [ ] /code: Implement the skeleton discovery phase using conditional independence tests.\", \"- [ ] /code: Implement the iterative orientation rule application phase.\", \"- [ ] /code: Create a simulation harness with a known SCM to generate validation data.\", \"- [ ] /math: Verify that the implemented orientation rules in the code match the formal derivation.\", \"- [ ] /code: Validate the full algorithm by computing SHD, precision, and recall metrics against the known SCM.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"structural-causal-modeling\", \"bayesian-networks\"], \"acceptance_criteria\": [\"- The Python implementation must correctly recover the PAG from simulated data, achieving an SHD of less than 5 on a 10-node graph with 5000 samples.\", \"- The final report must cite at least two relevant arXiv papers with IDs.\", \"- All expected artifacts (derivation, code, metrics JSON, plots, report) must be present and well-documented.\", \"- The mathematical derivation must clearly explain the logic behind at least 5 of the 10 FCI orientation rules.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 404, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating barren plateaus in Variational Quantum Eigensolvers. I need to analyze the performance of a hardware-efficient ansatz versus the Unitary Coupled Cluster Singles and Doubles (UCCSD) ansatz for finding the ground state energy of the H2 molecule. Please derive the gradient of the energy expectation value for the UCCSD ansatz, implement a VQE simulation comparing the convergence properties and final energy accuracy for both ansaetze, and research recent arXiv papers on gradient-based methods that mitigate barren plateaus in chemistry simulations.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational quantum eigensolver barren plateau mitigation uccsd ansatz H2\\\", mode=rag)\", \"/math(Derive the energy expectation value <H> for the H2 molecular Hamiltonian under the UCCSD ansatz. Formulate the partial derivatives \u2202<H>/\u2202\u03b8_i for the variational parameters to enable gradient-based optimization, ensuring the resulting eigenvalue computation is sound.)\", \"/code(Implement a VQE in Python with Qiskit for the H2 Hamiltonian. Compare the convergence rate and final ground state energy between a hardware-efficient ansatz and the UCCSD ansatz derived by /math. Validate the implementation against known H2 energy values and track convergence metrics.)\"], \"route_rationale\": \"/general-search (the blue agent) is required first to survey the latest research on VQE barren plateaus and specific ansatz constructions from authoritative arXiv preprints. The findings will inform the theoretical work done by /math, which will derive the specific gradients for the UCCSD ansatz. Finally, /code will implement and simulate the VQE algorithm based on the mathematical derivations, providing an empirical comparison.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the UCCSD ansatz energy expectation value and its gradients for the H2 Hamiltonian.\", \"A documented Python script implementing the VQE simulation comparing the two ansaetze, including setup for the molecular Hamiltonian.\", \"A summary report in Markdown format analyzing the simulation results, discussing the barren plateau phenomenon in context, and citing at least two recent arXiv papers.\", \"A plot comparing the energy convergence (energy vs. optimizer iteration) for both ansaetze.\", \"A JSON file containing the final computed ground state energies and number of function evaluations for each ansatz.\"], \"thinking_outline\": [\"1. Research the theory of barren plateaus in VQEs and the structure of the UCCSD ansatz, focusing on applications in quantum chemistry via literature search.\", \"2. Formulate the second-quantized Hamiltonian for the H2 molecule and map it to a qubit Hamiltonian using a suitable transformation like Jordan-Wigner.\", \"3. Derive the analytical expression for the energy expectation value and the parameter gradients for the UCCSD ansatz applied to the H2 qubit Hamiltonian.\", \"4. Implement the VQE algorithm, including the Hamiltonian, ansaetze, and a classical optimizer (e.g., SPSA, ADAM), and run simulations.\", \"5. Validate the implemented UCCSD circuit and Hamiltonian representation by checking against established results from Qiskit's quantum chemistry libraries.\", \"6. Analyze and compare the simulation outputs, verifying that the observed convergence behavior aligns with the theoretical understanding of barren plateaus for the chosen ansaetze.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The blue agent's literature review on ansatz design and barren plateau theory provides the necessary context and formalism for /math. /math then produces the specific mathematical derivations (gradients, expectation values) required by /code for implementation. /code implements the simulation based on these derivations. Verification: The router will cross-reference the gradient formula in the /math derivation with the gradient computation logic in the /code implementation. Fallback: If the derivation and implementation disagree, route back to /math for correction, providing the code and error trace as context.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize >=2 recent arXiv papers on barren plateau mitigation strategies in VQEs, specifically mentioning ansatz design.\", \"- [ ] /math: Derive the gradient of the energy expectation value for the H2 Hamiltonian under the UCCSD ansatz.\", \"- [ ] /code: Implement the VQE for H2 with both a hardware-efficient and the UCCSD ansatz.\", \"- [ ] /code: Plot the energy convergence curves for both ansaetze on the same graph for comparison.\", \"- [ ] /math: Verify that the implemented gradient logic in the /code script correctly reflects the derived mathematical formula.\", \"- [ ] /code: Validate the final ground state energy from the UCCSD simulation against known FCI (Full Configuration Interaction) results for the H2 molecule.\", \"- [ ] router: Perform final QA check, ensuring all artifacts are present and the summary report correctly interprets the results in the context of the cited literature.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"variational-algorithms\", \"hamiltonian-simulation\"], \"acceptance_criteria\": [\"- The final ground state energy for the H2 molecule computed using the UCCSD ansatz must be within 1e-3 Hartrees of the known FCI value.\", \"- The provided Python code must execute without errors and generate a convergence plot showing energy as a function of optimization iterations.\", \"- The final report must correctly cite at least two relevant arXiv papers with IDs and explain why one ansatz shows slower convergence, linking it to the theory of barren plateaus.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 405, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the connection between the sharpness of local minima and the generalization gap in deep neural networks. Please provide a formal derivation linking the spectral norm of the Hessian matrix at a converged solution to a Rademacher complexity-based generalization bound. Furthermore, implement a numerical experiment in JAX or PyTorch to empirically validate this theoretical relationship by tracking the evolution of the top Hessian eigenvalue and the train-test accuracy gap throughout the training process for a ResNet model on CIFAR-10.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian eigenvalue generalization gap deep learning 'Rademacher complexity'\\\", mode=both)\", \"/math(Derive a bound on the generalization gap using the Hessian's spectral norm at convergence, considering Bayesian perspectives on model complexity and its relation to information-theoretic measures.)\", \"/code(Implement a PyTorch model to track the top Hessian eigenvalue during training using the Lanczos algorithm, validating the theoretical convergence rate and its impact on generalization gap metrics on the CIFAR-10 dataset., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature from arXiv, ensuring the theoretical work is current. The /math agent then uses these sources to construct a rigorous mathematical derivation. Finally, the /code agent implements a numerical simulation to empirically verify the derived theory, providing a complete and validated analysis.\", \"expected_artifacts\": [\"- A LaTeX document containing the formal mathematical proof connecting Hessian spectral properties to generalization bounds.\", \"- A Python script implementing the neural network training, Hessian eigenvalue computation, and data logging.\", \"- A summary report in Markdown format, featuring plots of the top eigenvalue vs. generalization gap over training epochs.\", \"- A BibTeX file with citations for at least two relevant academic papers.\", \"- A validation log confirming the numerical stability and correctness of the eigenvalue computation method.\"], \"thinking_outline\": [\"1. Conduct a literature review on recent work connecting the Hessian spectrum, local minima sharpness, and generalization bounds in deep learning.\", \"2. Formulate and derive a mathematical proof that bounds the generalization gap as a function of the Hessian's spectral norm, clearly stating all assumptions.\", \"3. Implement a numerically stable method, such as the Lanczos or power iteration algorithm, to efficiently compute the top eigenvalue of the Hessian for a deep neural network.\", \"4. Design and execute a computational experiment to train a network while periodically calculating the top Hessian eigenvalue and the generalization gap.\", \"5. Verify that the experimental results (e.g., the correlation between the largest eigenvalue and the generalization gap) are consistent with the predictions from the mathematical derivation.\", \"6. Audit the final proof and its assumptions against the findings from the initial literature search to ensure correctness and novelty.\"], \"handoff_plan\": \"/general-search provides relevant arXiv papers -> /math uses papers to build derivation -> /code implements and validates the derivation empirically. At each step, results are cross-checked. For instance, /math's derivation assumptions must match /code's experimental setup. The final package is handed to the router for QA. Verification: Mathematical proof is sound and aligns with experimental data. Fallback: If derivation is flawed, return to /math with feedback from the /code agent's empirical findings.\", \"todo_list\": [\"- [ ] /general-search: Find at least two key papers from arXiv published in the last 5 years on Hessian spectra and generalization.\", \"- [ ] /math: Derive the generalization bound based on the spectral norm of the Hessian, citing the sourced papers.\", \"- [ ] /code: Implement the ResNet training on CIFAR-10 and the Lanczos algorithm for top eigenvalue estimation.\", \"- [ ] /code: Generate plots showing the top eigenvalue and generalization gap vs. training epochs.\", \"- [ ] /math: Verify that the assumptions made in the derivation (e.g., about network architecture or loss landscape) are consistent with the implementation.\", \"- [ ] /code: Validate the Lanczos implementation's output against a full eigendecomposition on a smaller proxy model to ensure correctness.\", \"- [ ] Router QA: Review all artifacts for mathematical correctness, implementation accuracy, and consistency between the report, code, and derivation.\"], \"difficulty\": \"advanced\", \"tags\": [\"matrix-calculus\", \"spectral-analysis\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The submitted LaTeX proof is mathematically rigorous and correctly establishes a relationship between the Hessian's spectral norm and a formal generalization bound.\", \"- The Python script successfully runs, computing the top Hessian eigenvalue and producing a plot that demonstrates a positive correlation with the generalization gap.\", \"- The final report clearly articulates the methodology, results, and conclusion, citing at least 2 relevant arXiv papers with their IDs.\", \"- The implemented eigenvalue computation method is validated against a trusted baseline.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
{"index": 408, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the efficient influence function for the Average Treatment Effect (ATE) using an Augmented Inverse Probability Weighting (AIPW) estimator in a setting with a binary treatment, continuous outcome, and confounding variables. Then, using this derivation, provide the asymptotic variance of the estimator. Please also implement a Monte Carlo simulation in Python to validate that the empirical variance converges to the derived asymptotic variance under a known data-generating process involving a Bernoulli treatment assignment model.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:biometrika.oxfordjournals.org 'efficient influence function' 'augmented inverse probability weighting' semiparametric theory econometrics\\\", mode=web)\", \"/math(Derive the efficient influence function and asymptotic variance for the AIPW estimator under a Bernoulli treatment assignment, referencing causal inference and semiparametric econometrics theory.)\", \"/code(Implement a Monte Carlo simulation in Python to verify the derived AIPW asymptotic variance, performing a bootstrap analysis to assess confidence interval coverage and estimator stability.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find foundational papers on AIPW estimators and influence functions, which is crucial for a rigorous derivation. Next, /math is used for the core theoretical work: deriving the influence function and asymptotic variance. Finally, /code implements a simulation to empirically validate the mathematical derivation, a critical step in theoretical statistics.\", \"expected_artifacts\": [\"- A LaTeX document containing the full mathematical derivation of the efficient influence function and the resulting asymptotic variance formula for the AIPW estimator.\", \"- A Python script implementing the Monte Carlo simulation, including the data-generating process, the AIPW estimator, and the comparison of empirical vs. asymptotic variance.\", \"- A summary report (.md) explaining the results, including plots showing variance convergence and a list of cited arXiv papers.\", \"- A verification log detailing the check between the theoretical variance from the math derivation and the empirical variance from the code simulation.\"], \"thinking_outline\": [\"1. Formally define the statistical model within the potential outcomes framework, specifying the roles of treatment, outcome, and confounders, and define the Average Treatment Effect (ATE) as the target estimand.\", \"2. Consult seminal literature on semiparametric efficiency theory to ground the derivation of the efficient influence function for the specified AIPW estimator.\", \"3. Execute the step-by-step derivation of the influence function and then use its second moment to establish the formula for the asymptotic variance of the estimator.\", \"4. Verify the algebraic correctness of the derivation, paying close attention to the application of the law of iterated expectations and the assumptions of consistency for the nuisance models (outcome regression and propensity score).\", \"5. Design and implement a Monte Carlo simulation with a known data-generating process to empirically calculate the variance of the AIPW estimator across many samples.\", \"6. Validate that the empirical variance from the simulation converges to the theoretical asymptotic variance as the sample size increases and audit the coverage probability of the constructed confidence intervals.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The blue search agent provides foundational papers for the /math agent's derivation. The final formula from /math is the ground truth that the /code agent must implement and verify. The final handoff to the router is for QA. Verification: The empirical variance from code must match the theoretical formula from math within a specified tolerance. Fallback: If they diverge, the /math agent must re-audit the derivation for algebraic errors or missed assumptions.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize at least two key papers on AIPW estimators and influence functions.\", \"- [ ] /math: Produce a formal, step-by-step derivation of the efficient influence function.\", \"- [ ] /math: Use the influence function to derive the asymptotic variance formula.\", \"- [ ] /code: Implement the data-generating process and the AIPW estimator in Python.\", \"- [ ] /code: Run the Monte Carlo simulation to compute the empirical variance and bootstrap confidence intervals.\", \"- [ ] /math: Verify that the implemented estimator in the code correctly reflects the derived mathematical formula.\", \"- [ ] /code: Validate that the simulation's empirical variance converges to the theoretical value from the math derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"statistics\", \"econometrics\"], \"acceptance_criteria\": [\"- The provided derivation of the influence function must be mathematically sound and complete.\", \"- The Monte Carlo simulation must show that the empirical variance of the AIPW estimator is within 5% of the derived theoretical asymptotic variance for a large sample size (e.g., n > 10000).\", \"- The final report must cite at least two relevant academic papers with their arXiv IDs or DOIs.\", \"- The implemented Python code must be executable and reproduce the reported simulation results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["advanced"]}
