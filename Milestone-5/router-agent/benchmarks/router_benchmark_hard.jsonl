{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 14, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for an Augmented Lagrangian Method (ALM) applied to a neural network weight matrix regularization problem. The objective is to minimize a standard loss function subject to a hard constraint on the Frobenius norm of the weights and an inequality constraint on the largest eigenvalue (spectral radius) to promote stability. Then, implement this in PyTorch, showing the convergence behavior of the primal and dual variables, and verify that the Karush-Kuhn-Tucker (KKT) conditions are approximately satisfied at the solution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org augmented lagrangian methods spectral norm deep learning convergence\\\", mode=rag)\", \"/math(Derive the Karush-Kuhn-Tucker conditions and corresponding Augmented Lagrangian update rules for a neural network with hard constraints on Frobenius norm and maximum eigenvalue, analyzing theoretical convergence properties.)\", \"/code(Implement the derived ALM optimizer in PyTorch, applying it to a small MLP to track primal-dual convergence and KKT residuals. Ensure the implementation is compatible with CUDA for potential scaling., using Python)\", \"/math(Analyze the empirical convergence plots from the Python code to numerically audit the satisfaction of the KKT conditions, specifically complementary slackness and stationarity, against the theoretical derivation and its complexity.)\"], \"route_rationale\": \"The plan follows a standard research workflow: the blue /general-search agent first gathers foundational literature on this advanced optimization topic. /math then develops the core theoretical framework and derivations. /code translates this theory into a practical, verifiable implementation. A final /math call closes the loop by rigorously validating the empirical results from /code against the initial theory, ensuring correctness.\", \"expected_artifacts\": [\"- A LaTeX/PDF document containing the full mathematical derivation of the ALM formulation and update rules.\", \"- A Python script (`alm_optimizer.py`) with the PyTorch implementation of the constrained optimizer.\", \"- A set of PNG plots visualizing the convergence of the objective function, constraint violations, and dual variables.\", \"- A final markdown report (`report.md`) summarizing the methodology and presenting the numerical verification of the KKT conditions, including citations.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem, clearly defining the objective function (e.g., cross-entropy loss), the equality constraint (Frobenius norm), and the inequality constraint (spectral radius).\", \"2. Construct the Augmented Lagrangian function associated with the problem, incorporating both the Lagrange multipliers and the quadratic penalty terms.\", \"3. Derive the gradient-based update rules for the primal variables (network weights) and the iterative update rules for the dual variables (Lagrange multipliers).\", \"4. Implement the complete ALM algorithm in PyTorch, including functions to project onto the feasible set if necessary and to track all relevant metrics for analysis.\", \"5. Validate the implementation by running it on a synthetic problem and numerically checking that the final solution satisfies all KKT conditions (stationarity, primal/dual feasibility, complementary slackness) to a pre-defined tolerance.\", \"6. Audit the mathematical derivation against the numerical results to ensure there are no discrepancies between the theoretical model and the empirical behavior.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: The final /math agent verifies that the KKT residuals computed by the /code agent converge to near-zero, confirming the derivation's correctness; fallback: If KKT residuals do not converge, the task is routed back to /math to re-check the gradient derivations and update rules.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on applying Augmented Lagrangian Methods to problems with spectral or matrix norm constraints in machine learning.\", \"- [ ] /math: Formulate the optimization problem and derive the complete set of ALM update equations for both primal and dual variables.\", \"- [ ] /math: Verify the analytical gradients derived for the Lagrangian using symbolic differentiation.\", \"- [ ] /code: Implement the ALM optimizer in PyTorch and a training loop for a simple MLP on a toy dataset like MNIST.\", \"- [ ] /code: Instrument the code to compute and log the KKT residuals, objective value, and constraint violations at each iteration.\", \"- [ ] /math: Analyze the logged data and plots from the /code agent to confirm convergence and explicitly verify that the KKT conditions are met at the solution within a tolerance of 1e-4.\", \"- [ ] /router: Perform a final quality assurance check on all artifacts, ensuring the derivation, code, and verification report are consistent and correct.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"lagrangian-methods\", \"spectral-regularization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ALM update rules must be complete, self-contained, and mathematically sound.\", \"- The PyTorch implementation must correctly execute the derived updates, and the model must converge on a provided test case.\", \"- The final report must numerically demonstrate that the primal solution is feasible and that all KKT residuals are below a tolerance of 1e-4.\", \"- The report must include convergence plots for primal objective, dual variables, and norm/spectral constraints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 29, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of the 2D toric code's performance threshold under a bit-flip and phase-flip (depolarizing) noise model. Please provide a formal derivation of the logical error rate using the stabilizer formalism, and then develop a Monte Carlo simulation to numerically determine the error threshold. The simulation should use a minimum-weight perfect matching decoder for syndrome processing and demonstrate the fault-tolerant properties of the code by plotting the logical error rate against the physical error rate for increasing system sizes.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org toric code error threshold depolarizing noise 'fault tolerance' minimum weight perfect matching\\\", mode=web, context=\\\"Curate seminal arXiv papers on toric code fault-tolerance, focusing on analytical bounds for the logical error rate under depolarizing noise to inform our Bayesian complexity analysis.\\\")\", \"/math(context=\\\"Derive the logical error rate for the toric code's stabilizer formalism, analyzing the convergence properties and the smallest non-trivial logical operator eigenvalue to establish a theoretical threshold bound.\\\")\", \"/code(context=\\\"Implement a Monte Carlo simulation in Python to model the toric code with a perfect matching decoder, using bootstrap resampling to estimate logical error rates for analyzing quantum encryption scheme viability.\\\")\", \"/math(context=\\\"Analyze simulation output by plotting logical error vs. physical error for different lattice sizes. Use curve fitting and confidence interval estimation to determine the error threshold, verifying consistency with the initial complexity analysis.\\\")\"], \"route_rationale\": \"The plan starts with the blue general-search agent to ground the work in established literature. The first /math agent then formalizes the theoretical model based on these findings. The /code agent implements a simulation to generate numerical evidence. Finally, the second /math agent analyzes the simulation data, comparing empirical results with the initial theory for verification.\", \"expected_artifacts\": [\"- A PDF report containing the full mathematical derivation of the toric code's properties and the logical error rate bound.\", \"- A well-commented Python script implementing the Monte Carlo simulation of the toric code with a perfect matching decoder.\", \"- A plot (`threshold.png`) showing the logical error rate vs. physical error rate for at least three different code distances, clearly indicating the error threshold crossing.\", \"- A `results.json` file containing the raw data for the plots and the computed numerical threshold value.\", \"- A `CITATIONS.md` file listing at least two cited arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Research and formalize the stabilizer group representation of the 2D toric code and the mathematical model for depolarizing noise.\", \"2. Derive an analytical expression or a tight bound for the logical error rate as a function of the physical error probability `p`.\", \"3. Implement a Monte Carlo simulation including lattice generation, noise application, syndrome measurement, and a minimum-weight perfect matching decoder.\", \"4. Validate the simulation by running it for various system sizes (e.g., L=4, 6, 8) and confirming that the logical error rate decreases with L for `p` below the expected threshold.\", \"5. Check the decoder's implementation by feeding it known correctable error patterns and verifying it produces the correct recovery operation.\", \"6. Compare the numerically computed error threshold from the simulation data's crossing point with the theoretical bounds from the derivation and cited literature.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (analysis) -> router QA. The initial search results provide the basis for the mathematical derivation. The code must implement the exact model derived by /math. The final analysis from the second /math agent must verify the code's output against the theory. Verification: At each step, artifacts are checked for consistency (e.g., code parameters match derivation variables). Fallback: If simulation results diverge significantly from theoretical predictions, the task is routed back to /math to re-check the derivation and its assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on toric code error thresholds, such as those by Dennis, Kitaev, Landahl, and Preskill.\", \"- [ ] /math: Provide a step-by-step derivation of the logical error rate bounds based on the stabilizer formalism and the noise model.\", \"- [ ] /code: Implement the toric code simulation, including stabilizer check matrix generation, noise simulation, and syndrome extraction.\", \"- [ ] /code: Implement the minimum-weight perfect matching decoder and integrate it into the simulation loop.\", \"- [ ] /math: Verify that the stabilizer generators implemented in the code are correct and match the mathematical definition.\", \"- [ ] /code: Validate the decoder's logic on a small set of known single-qubit and two-qubit error patterns.\", \"- [ ] /math: Process the simulation data to generate the threshold plot and calculate the numerical threshold value with a confidence interval.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-error-correction\", \"fault-tolerance\", \"stabilizer-codes\"], \"acceptance_criteria\": [\"- A formal mathematical derivation establishing the relationship between physical and logical error rates is present and correct.\", \"- The Python simulation correctly implements the 2D toric code, depolarizing noise channel, and a minimum-weight perfect matching decoder.\", \"- The final report includes a plot showing a clear error threshold crossing for at least three system sizes, and the reported threshold value is consistent with established literature (approx. 10-11%).\", \"- The `CITATIONS.md` file contains at least two valid and relevant arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 69, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nPlease provide a detailed derivation of the reverse-time SDE for a score-based generative model using the variance-exploding (VE) SDE formulation. Start from the forward process and its associated Fokker-Planck equation to justify the drift term of the reverse SDE. Then, implement a simple Euler-Maruyama sampler in JAX and analyze the numerical stability conditions, particularly how the step size relates to the Lipschitz continuity of the score function to ensure convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time SDE Fokker-Planck\\\", mode=both)\", \"/math(Derive the reverse-time SDE drift term from the forward process's Fokker-Planck equation, clearly showing how the score function emerges from the diffusion process. Ensure the derivation is rigorous and references foundational Bayesian concepts.)\", \"/code(Implement a JAX-based Euler-Maruyama sampler for the derived reverse-time SDE. The implementation should include a simple score network placeholder and track sample evolution to allow for FID metric calculation and convergence diagnostics., using Python)\", \"/math(Analyze the numerical stability of the Euler-Maruyama discretization for the reverse SDE. Establish a formal relationship between the step size, the noise schedule, and the score network's Lipschitz constant to guarantee convergence of the Bayesian updates.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the work in authoritative literature. /math is then used for the core theoretical derivation. /code translates this theory into a practical implementation. A final /math call is crucial for verification, providing a theoretical stability analysis of the numerical method used in the code, ensuring the solution is both correct and robust.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full derivation of the reverse-time SDE from the Fokker-Planck equation.\", \"A self-contained Python script (`sampler.py`) implementing the SDE sampler using JAX.\", \"A `CITATIONS.md` file listing at least two foundational arXiv papers with their IDs.\", \"A summary report (`stability_analysis.md`) explaining the derivation of the stability condition and its implications for the sampler.\", \"A metrics file (`results.json`) reporting the numerical stability check outcome.\"], \"thinking_outline\": [\"1. Use search to find authoritative sources (e.g., Song et al., 2020) that formally connect score matching with SDEs, focusing on the reverse-time process derivation.\", \"2. Formally derive the reverse-time SDE by applying time reversal to the Fokker-Planck equation of the forward VE-SDE, explicitly showing the emergence of the score function, \u2207_x log p_t(x).\", \"3. Implement the Euler-Maruyama method to discretize and solve the derived reverse SDE, creating a generative sampler.\", \"4. Verify the mathematical derivation by cross-referencing it with the logic implemented in the sampler code to ensure they are perfectly aligned.\", \"5. Analyze the conditions for numerical stability of the discretization, linking the maximum allowable step size to the Lipschitz constant of the score model and the parameters of the noise schedule.\", \"6. Validate the stability analysis by running the sampler with step sizes that both satisfy and violate the derived condition, observing whether the process converges or diverges.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (stability analysis) -> router QA. The /math agent's derivation provides the exact equations for the /code agent. The /code agent's implementation is then passed to the second /math agent for stability verification. Verification: The final stability condition from /math must be consistent with empirical behavior observed by /code. Fallback: If the derivation is flawed, revert to /general-search for alternative formulations; if the code is unstable, the /math agent must refine the stability bounds.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on score-based generative modeling with SDEs (e.g., from Song, Ermon).\", \"- [ ] /math: Produce a step-by-step derivation of the reverse-time SDE from the forward process and its Fokker-Planck equation.\", \"- [ ] /code: Implement the Euler-Maruyama sampler in JAX for the derived SDE.\", \"- [ ] /math: Derive the numerical stability condition for the implemented sampler, relating step size to the score's Lipschitz constant.\", \"- [ ] /math: Verify that the equations derived in the first math step are correctly translated into the JAX code.\", \"- [ ] /code: Add an empirical test to the script that demonstrates sampler divergence when the stability condition is violated.\", \"- [ ] /router: Review all artifacts for correctness, consistency, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-differential-equations\", \"generative-modeling\"], \"acceptance_criteria\": [\"The reverse-time SDE derivation is mathematically sound and correctly references the Fokker-Planck equation.\", \"The JAX implementation of the sampler executes and produces non-NaN values when run with a stable step size.\", \"The stability analysis report correctly derives a bound on the step size as a function of the score network's Lipschitz constant.\", \"The `CITATIONS.md` file contains at least two valid and relevant arXiv paper identifiers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 92, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to understand the REINFORCE algorithm, specifically how the policy gradient theorem is derived and how to implement a simple version in Python to solve the CartPole-v1 environment. Show me the key mathematical steps and a coded example.\n", "completion": "{\"route_plan\": [\"/math(Derive the log-derivative trick and the full policy gradient theorem, showing how it enables gradient ascent on the expected reward objective function within a classic control theory context.)\", \"/code(Implement the REINFORCE algorithm using Python and PyTorch for the CartPole-v1 environment. The agent should learn a policy, and the code must include convergence checks and plot the learning curve.)\", \"/general-search(query=\\\"site:arxiv.org OR site:*.edu policy gradient REINFORCE algorithm sutton barto\\\", mode=rag)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation of the policy gradient theorem. Then, /code implements this theory in a practical Python example to validate understanding. Finally, the blue /general-search agent gathers authoritative citations to ground the work in established literature.\", \"expected_artifacts\": [\"A formal mathematical derivation of the policy gradient theorem.\", \"A Python script implementing the REINFORCE algorithm for CartPole-v1.\", \"A plot showing the agent's reward per episode over time, demonstrating learning.\", \"A summary of findings with citations to at least 2 relevant academic papers or textbooks.\", \"A verification report confirming the code's output matches expected learning behavior.\"], \"thinking_outline\": [\"1. Formulate the reinforcement learning objective function as maximizing expected total reward.\", \"2. Use the log-derivative trick to derive the policy gradient theorem, showing how to estimate the gradient without knowing the environment's dynamics.\", \"3. Implement the REINFORCE algorithm, including a policy network, sampling actions, and updating weights based on the derived gradient.\", \"4. Train the agent on the CartPole-v1 environment and validate that the average reward consistently increases over training episodes, indicating successful learning.\", \"5. Check the implementation for common pitfalls, such as incorrect gradient updates or unstable learning, and reference authoritative sources.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: code implementation must correctly apply the derived theorem and show a positive learning trend; fallback: if code fails to learn, revisit the math derivation for potential misunderstandings before debugging).\", \"todo_list\": [\"- [ ] /math: Derive the policy gradient theorem using the log-derivative trick.\", \"- [ ] /code: Implement the REINFORCE algorithm for CartPole-v1 in Python.\", \"- [ ] /code: Generate a plot of episodic rewards to visualize learning convergence.\", \"- [ ] /general-search: Find and cite at least two foundational papers on policy gradients.\", \"- [ ] Verification: Confirm that the final average reward exceeds the environment's solved threshold (e.g., >195 for CartPole-v1).\", \"- [ ] Submit all artifacts for final router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"neural-networks\"], \"acceptance_criteria\": [\"- The mathematical derivation of the policy gradient theorem is correct and self-contained.\", \"- The Python implementation successfully solves the CartPole-v1 environment, achieving an average reward of at least 195 over 100 consecutive episodes.\", \"- The final report includes a learning curve plot and citations for at least two authoritative sources.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 105, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating posterior sampling in Bayesian Neural Networks. Derive the continuous-time stochastic differential equation (SDE) for a particle governed by Langevin dynamics targeting the true posterior. Then, show how an Euler-Maruyama discretization of this SDE leads to the Stochastic Gradient Langevin Dynamics (SGLD) update rule. Implement SGLD in Python using JAX to sample from the posterior of a BNN on a synthetic dataset, and validate convergence by analyzing trace plots and autocorrelation functions. Finally, I need a comparative analysis with a pre-conditioned SGLD variant, referencing seminal literature on Riemannian manifold Langevin dynamics from arXiv.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Stochastic Gradient Langevin Dynamics SDE derivation Bayesian posterior\\\", mode=rag)\", \"/math(Derive the overdamped Langevin SDE targeting a Bayesian posterior p(\u03b8|D) and discretize it via Euler-Maruyama to obtain the SGLD update equations, carefully handling the learning rate and noise variance terms for convergence.)\", \"/code(Implement SGLD and a pre-conditioned SGLD in JAX for a Bayesian neural network on a synthetic regression task. Track key MCMC diagnostics like effective sample size and generate trace plots to assess convergence., using Python)\", \"/math(Analyze the autocorrelation plots and effective sample size metrics from the code's output to formally verify if the SGLD sampler has converged to a stationary distribution, comparing the empirical results with theoretical expectations., includes cybersecurity, gradient)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational theory from authoritative sources like arXiv. This informs the /math agent for a rigorous derivation. The derived equations are then handed to the /code agent for empirical implementation and validation. A final /math step is used to formally analyze the numerical output from the code, providing a closed-loop verification of the entire process.\", \"expected_artifacts\": [\"A LaTeX document detailing the SDE derivation and its discretization to SGLD.\", \"A self-contained JAX Python script implementing the BNN and SGLD samplers.\", \"A summary report in Markdown (`results.md`) containing convergence plots (trace plots, autocorrelation), performance metrics, and a comparative analysis.\", \"A bibliography file (`references.bib`) with citations for at least two relevant arXiv papers.\", \"A verification log (`verification.log`) comparing theoretical convergence properties against the empirical MCMC diagnostics.\"], \"thinking_outline\": [\"1. Formulate the log-posterior probability of a neural network's weights, which will serve as the potential energy function for the Langevin dynamics.\", \"2. Derive the continuous-time Stochastic Differential Equation (SDE) for Langevin dynamics whose stationary distribution is the target Bayesian posterior.\", \"3. Apply the Euler-Maruyama discretization method to this SDE to derive the iterative update rule for Stochastic Gradient Langevin Dynamics (SGLD).\", \"4. Implement the SGLD algorithm in JAX to sample from the posterior of a simple BNN on a regression dataset, generating MCMC diagnostics.\", \"5. Validate the implementation's correctness by analyzing the generated trace plots and autocorrelation functions to check for good mixing and convergence to a stationary distribution.\", \"6. Audit the empirical effective sample size (ESS) per unit of compute time to quantitatively compare the efficiency of standard SGLD versus a pre-conditioned variant.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math produces a formal derivation -> /code implements the derivation and generates numerical results -> /math analyzes the results for verification -> router QA (verification: consistency between derivation, code, and empirical diagnostics; fallback: if sampler fails to converge in /code, return to /math to re-check noise schedule or pre-conditioner derivation).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on SGLD (e.g., Welling & Teh, 2011) and Riemannian manifold Langevin methods on arXiv.\", \"- [ ] /math: Derive the Langevin SDE and its Euler-Maruyama discretization for the SGLD algorithm.\", \"- [ ] /code: Implement a simple BNN and the SGLD sampler in JAX, using a synthetic dataset.\", \"- [ ] /code: Add implementation for a pre-conditioned SGLD variant (e.g., using Fisher information).\", \"- [ ] /code: Generate trace plots, autocorrelation plots, and compute effective sample size (ESS) metrics.\", \"- [ ] /math: Verify that the implemented learning rate schedule and noise injection in the code align with the theoretical requirements for convergence.\", \"- [ ] /math: Analyze the output plots and metrics to confirm sampler convergence and compare the efficiency of the two SGLD variants.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"mcmc\", \"bayesian-neural-networks\", \"stochastic-gradient-methods\"], \"acceptance_criteria\": [\"The mathematical derivation correctly links the Langevin SDE to the discrete SGLD algorithm via Euler-Maruyama discretization.\", \"The Python code runs without errors and produces MCMC trace plots that demonstrate convergence (i.e., stationary, well-mixed chains without significant drift).\", \"The final report provides a quantitative comparison of standard vs. pre-conditioned SGLD based on the Effective Sample Size metric.\", \"The bibliography includes at least two relevant arXiv pre-prints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute ESS for a subset of key model parameters. Higher ESS indicates lower autocorrelation and more efficient exploration of the posterior. The goal is to maximize ESS per second of runtime.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Evaluate the average log-likelihood on a held-out test set by averaging predictions from multiple posterior samples. This measures the model's generalization performance.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 115, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the rate-distortion function for a quantized language model's output distribution under a bounded adversarial perturbation, framing this as a channel coding problem. Can we establish a formal connection between this function's distortion-rate slope and the model's (\u03b5, \u03b4)-differential privacy guarantee, particularly concerning membership inference attacks? Validate this theoretical link via a proof-of-concept simulation using Bayesian estimators.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org rate-distortion theory differential privacy adversarial information leakage membership inference\\\", mode=rag)\", \"/math(Derive the rate-distortion function R(D) for a discrete memoryless source representing LLM logits under a specified distortion measure, like Hamming distance or KL-divergence, while considering adversarial input perturbations. Formally connect the R(D) curve's properties to Bayesian privacy accounting.)\", \"/code(Implement a JAX simulation of a small-scale generative model, apply quantization, and simulate a membership inference attack to empirically measure information leakage. Track the attack's AUPRC metric as a function of the quantization rate., using Python)\", \"/math(Verify that the empirical AUPRC-vs-rate curve from the simulation aligns with the privacy bounds predicted by the theoretical rate-distortion analysis, checking for convergence and statistical significance.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on information-theoretic privacy. This context is then handed to /math to build a rigorous theoretical model. /code is used for empirical validation of the theory through simulation. A final /math step provides a crucial verification loop, ensuring the simulation results are consistent with the derived mathematical framework.\", \"expected_artifacts\": [\"A formal mathematical derivation connecting the rate-distortion function to (\u03b5, \u03b4)-differential privacy.\", \"A Python script (JAX) implementing the simulation of the model, quantization, and membership inference attack.\", \"A plot visualizing the empirical rate-distortion curve and its relationship to the privacy leakage metric (AUPRC).\", \"A final report in Markdown format summarizing the theory, methods, results, and citing at least two relevant arXiv papers.\", \"A JSON file containing the primary and secondary metrics from the simulation runs.\"], \"thinking_outline\": [\"1. Formalize the problem setup using concepts from information theory, defining the source (LLM outputs), channel (quantization and adversarial noise), and distortion measure.\", \"2. Use rate-distortion theory to derive the theoretical lower bound on the number of bits (rate) required to represent the source data within a certain level of expected distortion.\", \"3. Connect the derived rate-distortion function to differential privacy by interpreting the distortion as a measure of privacy loss, potentially using the slope of the R(D) curve as a proxy for the privacy budget \u03b5.\", \"4. Develop a simulation to empirically measure the trade-off by running a membership inference attack against models quantized at different bit rates and calculating the attack's success (AUPRC).\", \"5. Validate that the empirical results from the simulation (AUPRC vs. rate) qualitatively and quantitatively match the predictions from the theoretical derivation.\", \"6. Audit the mathematical proof for logical consistency, ensuring all assumptions are stated and justified, and check the derivation against known results from the literature.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (simulation) -> /math (verification) -> router QA. Verification: The /code agent must produce a plot that the final /math agent uses as input to verify against its theoretical curve. Fallback: If the empirical curve from /code significantly deviates from the theory derived by /math, the /code agent must re-run with finer-grained quantization levels and report sensitivity analysis.\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on information-theoretic privacy and rate-distortion theory in machine learning.\", \"- [ ] /math: Complete the formal derivation of the rate-distortion function and its connection to the (\u03b5, \u03b4)-DP budget.\", \"- [ ] /code: Implement the simulation environment, including the toy model, quantization function, and membership inference attacker.\", \"- [ ] /code: Generate plots for the empirical rate-distortion curve and the AUPRC of the membership inference attack vs. rate.\", \"- [ ] /math: Verify that the derivation from the first math step correctly predicts the shape and bounds of the empirical curve from the code step.\", \"- [ ] /code: Audit the simulation code for correctness, ensuring the attack implementation is standard and the metrics are calculated properly.\", \"- [ ] /router: Perform final QA review on the consolidated report, checking for consistency between theory, code, and results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"differential-privacy\", \"generative-ai-safety\"], \"acceptance_criteria\": [\"- The final report must contain a complete mathematical derivation that is logically sound and well-justified.\", \"- The provided Python code must be executable and reproduce the plots and metrics presented in the report.\", \"- The empirical results from the simulation must qualitatively support the theoretical claims made in the derivation.\", \"- The report must cite at least two relevant arXiv papers with their IDs, as per the citation policy.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 117, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the loss landscape of Transformer attention mechanisms. Can you devise a method using persistent homology from algebraic topology to characterize the geometric complexity of the parameter space near a local minimum? I need a formal derivation connecting the Betti numbers of a Vietoris-Rips filtration to generalization bounds, and a computational prototype to generate and analyze the persistence diagram for a single-head attention model's weight space to identify robust topological features.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'persistent homology' 'transformer loss landscape' 'generalization gap' Betti numbers\\\", mode=web)\", \"/math(Derive the theoretical link between the Betti numbers of the parameter space's Vietoris-Rips filtration and Bayesian generalization bounds, referencing the core concepts from the arxiv literature search.)\", \"/code(Implement in Python with JAX/Gudhi a pipeline to sample a trained attention head's weight space, construct the point cloud, and compute its persistence diagram. Use CUDA acceleration and track eigenvalue stability during any decomposition.)\", \"/math(Formally interpret the barcode output from the code, verifying that the persistence of H1 features aligns with the derived theoretical claims about flatter minima and providing a proof sketch for convergence., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey foundational arXiv papers. This informs the /math agent's formal derivation of the topology-generalization link. The /code agent then provides empirical validation by implementing the computation. A final /math step synthesizes the theoretical and empirical results for a cohesive analysis.\", \"expected_artifacts\": [\"- A formal mathematical derivation in LaTeX connecting Betti numbers to generalization.\", \"- A commented Python script for generating and analyzing persistence diagrams from model weights.\", \"- A PNG image of the persistence barcode diagram.\", \"- A final report in PDF format synthesizing the theory, methods, results, and citing at least two relevant papers.\", \"- A JSON file containing computed metrics like Betti numbers and feature persistence lifetimes.\"], \"thinking_outline\": [\"1. Formalize the problem by defining the Transformer's parameter space as a point cloud and the loss function as a height function on this space.\", \"2. Use insights from the literature search to hypothesize a relationship between the persistence of topological features (e.g., 1-cycles in the Vietoris-Rips filtration) and the flatness of the loss landscape, which is linked to generalization.\", \"3. Develop a computational strategy using JAX to sample parameters around a trained local minimum and use the Gudhi library to compute the persistent homology.\", \"4. Verify the implementation's correctness by testing it on a manifold with known homology, such as a torus, and checking that the computed Betti numbers match the theoretical values.\", \"5. Execute the experiment on the actual attention head's parameter space and generate the persistence barcode diagram.\", \"6. Validate the initial hypothesis by analyzing the barcode: check if long-lived features correspond to models with better generalization metrics (e.g., lower test loss) and document any discrepancies.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (interpretation) -> router QA. The initial math derivation provides a blueprint for the code. The code's output (barcode diagram) is then handed back to the math agent for final verification against the theory. Fallback: If the empirical results from /code contradict the derivation, the /math agent must revisit the initial assumptions and refine the model.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on TDA for deep learning loss landscapes.\", \"- [ ] /math: Produce the initial LaTeX derivation linking Betti numbers of the parameter space to generalization bounds.\", \"- [ ] /code: Write and test the Python script for sampling, filtration, and persistence computation.\", \"- [ ] /math: Verify that the data structures and algorithms in the Python script correctly map to the formal definitions in the derivation.\", \"- [ ] /code: Run the full experiment on a pre-trained attention head model and generate the final barcode plot and metrics JSON.\", \"- [ ] /math: Write the final interpretation, synthesizing the theoretical proof with the empirical barcode evidence.\", \"- [ ] /router: Perform a final QA review, ensuring consistency between the derivation, code, plot, and final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"transformer-optimization\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The mathematical proof correctly applies the Vietoris-Rips filtration and formally links Betti numbers to a recognized measure of generalization (e.g., PAC-Bayes bounds).\", \"- The Python code runs without error and produces a valid persistence barcode diagram for a given set of model parameters.\", \"- The final report's conclusion is explicitly supported by both the mathematical derivation and the computational results.\", \"- The report includes citations for at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 118, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for Latent Dirichlet Allocation and implement a mean-field variational inference solver. Then, compare its convergence rate against a structured VI approach for a Bayesian topic model on a synthetic dataset, ensuring perplexity improves by at least 15%.\n", "completion": "{\"route_plan\": [\"/math(Derive the full Evidence Lower Bound (ELBO) for a Bayesian Latent Dirichlet Allocation model, clearly showing the expectation terms and the KL-divergence between the approximate posterior and the prior. The derivation must use standard variational calculus.)\", \"/general-search(query=\\\"site:arxiv.org structured variational inference topic models convergence analysis\\\", mode=both)\", \"/code(Implement both mean-field and a selected structured variational inference algorithm for LDA in Python using PyTorch. Validate the implementation on a synthetic dataset, tracking ELBO convergence and final topic perplexity as key metrics. Ensure the final perplexity reduction exceeds 15%.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation by deriving the ELBO. Next, the blue /general-search agent is used to find a suitable, non-trivial structured VI method from recent literature to serve as a comparison point. Finally, /code implements both the derived mean-field method and the researched structured method to perform a direct empirical comparison and validate the theoretical benefits.\", \"expected_artifacts\": [\"- A PDF or LaTeX document containing the full mathematical derivation of the ELBO for LDA.\", \"- A standalone Python script implementing both mean-field and structured VI for LDA.\", \"- A summary report in Markdown format with plots visualizing ELBO convergence and a table comparing final perplexity scores.\", \"- A bibliography file citing at least two relevant arXiv papers.\", \"- A `results.json` file containing the final primary and secondary metric values.\"], \"thinking_outline\": [\"1. Formally define the Latent Dirichlet Allocation (LDA) model's generative process, its parameters, and the joint probability distribution.\", \"2. Apply variational calculus to derive the complete Evidence Lower Bound (ELBO) for the mean-field variational family, explicitly detailing the update equations for each parameter.\", \"3. Research and select a tractable structured variational inference method for topic models that improves upon the mean-field assumption, such as one using copulas or normalizing flows.\", \"4. Implement both the mean-field and the selected structured VI algorithms in Python, including a synthetic data generator to create a controlled test environment.\", \"5. Validate the correctness of the ELBO implementations by checking for monotonic non-decreasing behavior during optimization and verify that the structured approach achieves the target perplexity improvement over the baseline.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math informs the search queries for /general-search. The paper identified by /general-search provides the algorithm specification for the advanced model in /code. The final artifacts from /code are passed to the router for a final QA check. Verification: The router will execute the submitted code to confirm the metrics match the report. Fallback: If the structured VI method from the search is not tractable, the /code agent should default to implementing a simpler correlated topic model and document the deviation.\", \"todo_list\": [\"- [ ] /math: Derive the complete ELBO for the LDA model using variational calculus, showing coordinate ascent update equations.\", \"- [ ] /general-search: Identify and summarize a suitable structured VI method for topic models from recent arXiv literature.\", \"- [ ] /code: Implement the mean-field VI algorithm and a synthetic data generator based on the math derivation.\", \"- [ ] /code: Implement the structured VI algorithm identified by the search agent.\", \"- [ ] /code: Execute comparison experiments, validate that the ELBO is non-decreasing, and generate final plots and metrics JSON.\", \"- [ ] /router: Perform final QA review, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"bayesian-statistics\"], \"acceptance_criteria\": [\"- The provided ELBO derivation must be mathematically sound, complete, and match established literature.\", \"- The submitted Python code must be executable and reproduce the reported perplexity and ELBO metrics to within a 1% tolerance.\", \"- The structured VI method must achieve a perplexity score at least 15% lower than the mean-field baseline on the provided synthetic dataset.\", \"- The final report must contain convergence plots showing the ELBO per iteration for both methods, demonstrating monotonic increase.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 139, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on a GPU-accelerated finite element simulation where the bottleneck is solving large, sparse, ill-conditioned linear systems (Ax=b). I need a comprehensive analysis and implementation of the Preconditioned Conjugate Gradient (PCG) method using CUDA. Specifically, derive the PCG algorithm, then implement and benchmark it using two different preconditioners\u2014Jacobi and Incomplete Cholesky (IC(0))\u2014to analyze their impact on convergence rate and wall-clock time for a model Poisson problem.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org parallel incomplete cholesky factorization for CUDA GPU conjugate gradient\\\", mode=rag)\", \"/math(Derive the Preconditioned Conjugate Gradient algorithm from first principles. Analyze the effect of a preconditioner M on the matrix A's condition number and its theoretical impact on convergence, paying close attention to the eigenvalue distribution of M\u207b\u00b9A.)\", \"/code(Implement PCG in Python using CuPy for sparse matrices. Build modular functions for Jacobi and Incomplete Cholesky (IC(0)) preconditioners. Generate a sparse, symmetric positive-definite matrix from a 2D finite difference discretization to test convergence.)\", \"/math(Verify that the empirically observed convergence rates from the CUDA implementation for both preconditioners align with the theoretical bounds derived from the eigenvalue analysis. Plot the residual norm versus iteration count and check for superlinear convergence.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on parallel preconditioning, which is essential for advanced implementation. The first /math call establishes the core theoretical foundation. The /code call translates this theory into a practical, GPU-accelerated implementation for empirical testing. The final /math call closes the loop by rigorously verifying that the experimental results match the theoretical predictions, ensuring correctness and a deep understanding.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Preconditioned Conjugate Gradient method and its convergence properties.\", \"A well-commented Python script using CuPy to implement and benchmark the PCG solvers.\", \"A convergence plot (log-scale residual norm vs. iteration) comparing no preconditioner, Jacobi, and Incomplete Cholesky.\", \"A summary report (report.md) explaining the results, linking them back to theory, and including citations.\", \"A verification document (verification_log.txt) detailing the comparison between theoretical and empirical convergence rates.\"], \"thinking_outline\": [\"1. Formalize the standard Conjugate Gradient algorithm and derive the preconditioned variant, explaining how the preconditioner M transforms the system to improve the condition number.\", \"2. Research and describe the implementation details of Jacobi and sparse Incomplete Cholesky (IC(0)) factorization, especially concerning their suitability for parallel GPU architectures.\", \"3. Implement the solvers in Python with CuPy, ensuring efficient handling of sparse matrices in CSR format and minimizing host-device memory transfers.\", \"4. Create a synthetic, ill-conditioned, sparse positive-definite matrix A (e.g., from a 2D Poisson equation discretization) to serve as a realistic test case.\", \"5. Validate the implementation by running benchmarks and comparing the iteration counts and wall-clock times; verify that the final residual norm ||b - Ax_k||_2 is below the specified tolerance.\", \"6. Audit the numerical results against the theoretical convergence bounds related to \u03ba(M\u207b\u00b9A) to confirm the preconditioners are behaving as expected.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> Router QA (verification: router will cross-reference the convergence plot from /code with the theoretical analysis from /math to ensure the speedup is correctly justified; fallback: if IC(0) proves difficult to parallelize efficiently, the plan will pivot to researching and implementing a simpler but parallel-friendly preconditioner like Symmetric Successive-Over-Relaxation (SSOR)).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize two key papers on implementing Incomplete Cholesky on GPUs.\", \"- [ ] /math: Produce a clean LaTeX derivation of the PCG algorithm and its convergence theory.\", \"- [ ] /code: Implement the PCG solver framework using CuPy's sparse matrix routines (cupyx.scipy.sparse).\", \"- [ ] /code: Add modules for the Jacobi and IC(0) preconditioners and a matrix generator for the test problem.\", \"- [ ] /code: Run benchmarks, collect timing and iteration data, and generate the final convergence plot.\", \"- [ ] /math: Verify the code's output, ensuring the residual reduction per iteration is consistent with theory.\", \"- [ ] /math: Formally write up the verification log comparing empirical results to the expected behavior based on the condition number reduction.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"krylov-methods\"], \"acceptance_criteria\": [\"- The Python code must correctly solve Ax=b for a 1000x1000 sparse SPD matrix to a tolerance of 1e-8.\", \"- The final convergence plot must demonstrate that the Incomplete Cholesky preconditioner requires at least 50% fewer iterations than the Jacobi preconditioner.\", \"- The final report must cite at least two peer-reviewed articles or arXiv preprints with DOIs/IDs related to parallel preconditioners for Krylov subspace methods.\", \"- The mathematical derivation must correctly relate the convergence rate to the condition number of the preconditioned matrix.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"diagnostic\": \"final_residual_norm: The L2 norm of the final residual vector, `||b - Ax_k||_2`, to confirm convergence was achieved.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 144, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating dictionary learning for sparse representations using an ADMM formulation. Can you derive the update steps for both the dictionary and sparse coefficients for the objective function ||Y - DX||_F^2 + lambda * ||X||_1, subject to ||d_j||_2 = 1 for all dictionary columns d_j? Furthermore, provide a formal proof of convergence for this alternating optimization scheme, discussing the conditions under which it converges to a critical point despite the non-convexity of the joint problem. Finally, implement this algorithm and validate its performance on a synthetic compressed sensing task, analyzing the mutual coherence of the learned dictionary.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org admm dictionary learning non-convex convergence proof\\\", mode=both)\", \"/math(Derive the ADMM update rules for the dictionary and sparse codes from the augmented Lagrangian. Formulate the convergence proof strategy, referencing foundational papers on non-convex ADMM and its convergence rate.)\", \"/code(Implement the derived ADMM algorithm in Python/JAX. Create a synthetic dataset with a ground-truth dictionary and sparse signals. Track reconstruction MSE and dictionary mutual coherence over iterations for robust convergence analysis.)\", \"/math(Complete the formal proof of convergence, verifying that the implementation's decreasing objective function aligns with theoretical guarantees. Analyze the mutual coherence of the final learned dictionary and its implications for signal recovery., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on non-convex ADMM for dictionary learning. This informs the /math agent's formal derivation and proof strategy. The /code agent then implements this theory for empirical validation. A final /math step is used to formally verify the numerical results against the derived theory, ensuring correctness and providing a deeper analysis.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the ADMM update steps and the formal proof of convergence.\", \"A Python script (`.py`) implementing the dictionary learning algorithm and the experimental validation.\", \"A summary report (`.md`) including plots of the reconstruction error and objective function value over iterations.\", \"A BibTeX file (`.bib`) with citations for at least two relevant academic papers.\", \"A JSON file with final performance metrics (MSE, coherence, runtime).\"], \"thinking_outline\": [\"1. Formulate the dictionary learning problem as a constrained, bi-linear, non-convex optimization problem and construct its augmented Lagrangian for the ADMM framework.\", \"2. Derive the iterative update rules for the sparse codes (X), the dictionary (D), and the dual variables by solving the corresponding subproblems, one of which involves soft-thresholding and the other a projection onto the unit sphere.\", \"3. Develop a proof of convergence by showing that the augmented Lagrangian is a Lyapunov function for the algorithm, i.e., it is non-increasing at each iteration and bounded below, thus guaranteeing convergence to a critical point.\", \"4. Implement the algorithm and a synthetic data generation pipeline to test sparse recovery performance.\", \"5. Validate the implementation by checking if the empirical objective function decreases monotonically, consistent with the convergence proof.\", \"6. Audit the final learned dictionary's mutual coherence and compare the empirical reconstruction error against theoretical bounds found in the literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent for the derivation. The math agent's derivation is the specification for the code agent. The code agent's numerical results are then passed back to the math agent for theoretical verification. Verification: The final proof must align with the empirical convergence shown in the code's output plots. Fallback: If convergence fails empirically, the math agent must revisit the derivation and ADMM penalty parameter settings.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on ADMM for dictionary learning (e.g., Boyd et al.) and convergence proofs for non-convex variants.\", \"- [ ] /math: Derive the update equations for X, D, and the dual variables from the augmented Lagrangian.\", \"- [ ] /code: Implement the ADMM algorithm, including the soft-thresholding and dictionary column projection steps.\", \"- [ ] /math: Write the formal proof showing the objective function is non-increasing and converges.\", \"- [ ] /code: Run experiments on synthetic data, generating plots for reconstruction MSE vs. iteration.\", \"- [ ] /math: Verify that the empirical convergence from the code's plots is consistent with the derived theoretical properties.\", \"- [ ] /code: Calculate the mutual coherence of the learned dictionary.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"admm\"], \"acceptance_criteria\": [\"- The provided mathematical proof of convergence must be complete and logically sound.\", \"- The Python implementation must correctly execute the derived ADMM updates and reproduce the reported metrics.\", \"- The final reconstruction Mean Squared Error (MSE) on the test set must be below a threshold of 1e-4.\", \"- The summary report must contain a plot clearly showing the monotonic decrease of the objective function value.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 167, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on an IOI-style problem involving a tree where each edge is weighted by an element from a finite, possibly non-abelian group G. The objective is to calculate the number of paths between two specified vertices u, v whose path product\u2014the composition of edge weights along the path\u2014equals the group's identity element. I need a full solution: the formal dynamic programming recurrence relations, a proof of correctness grounded in algebraic graph theory, a time/space complexity analysis in terms of |V| and |G|, and a robust C++ or Python implementation.\n", "completion": "{\"route_plan\": [\"/math(Derive the DP state transitions and recurrence relations for counting paths on a tree with edges labeled by elements of a finite group. Analyze the time complexity and memory usage, considering the group order |G| and its impact on the state space. Relate this to control theory perspectives on state propagation.)\", \"/general-search(query=\\\"site:arxiv.org counting homomorphisms graphs finite groups OR dynamic programming tree decomposition algebraic applications in econometrics\\\", mode=rag)\", \"/code(Implement the verified DP algorithm in Python, representing group elements and operations. Profile the code for various tree structures and group orders, logging performance metrics suitable for a federated learning context where subtrees are processed independently.)\"], \"route_rationale\": \"/math is first to establish the rigorous mathematical foundation, deriving the DP recurrences and proving correctness. The 'blue' /general-search agent is then used to survey existing literature on similar algebraic graph problems, ensuring our approach is sound and well-cited. Finally, /code translates the verified mathematical model into a high-performance, tested implementation.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical formulation, DP state definitions, recurrence relations, and proof of correctness.\", \"An optimized Python implementation of the tree DP algorithm.\", \"A complexity analysis report with derivations for time and space complexity, specifically O(|V| * |G|^2).\", \"A verification document showing test case results for both abelian (Z_n) and non-abelian (S_3) groups.\", \"A summary of findings from the literature search, including at least 2 cited arXiv papers.\"], \"thinking_outline\": [\"1. Define the problem formally using algebraic graph theory concepts, including the path product operation over a finite group G.\", \"2. Design the dynamic programming states, likely involving `DP_down[u][g]` (paths within u's subtree starting at u) and `DP_up[u][g]` (paths starting at the root and ending at u).\", \"3. Derive the precise recurrence relations for both the bottom-up (leaves to root) and top-down (root to leaves) DP passes.\", \"4. Verify the correctness of the recurrences via proof by induction on the structure of the tree and validate the base cases at the leaf nodes.\", \"5. Analyze the algorithm's time and space complexity, expressing it as a function of the number of vertices |V| and the group order |G|.\", \"6. Check the implementation against the mathematical derivation by testing with known small groups (e.g., Z_n, S_3) and simple tree structures where the answer can be computed manually.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (literature validation) -> /code (implementation) -> router QA (verification: check if code output on test cases matches manually computed results from the math derivation; fallback: if mismatch, escalate to /math to re-audit the recurrence relations for off-by-one or base case errors).\", \"todo_list\": [\"- [ ] /math: Formulate the DP state and recurrence relations for path counting with group-labeled edges.\", \"- [ ] /math: Derive the time and space complexity, O(|V| * |G|^2) or better, and prove correctness of the formulation.\", \"- [ ] /general-search: Find and summarize at least two authoritative papers on algorithms for group-labeled graphs or counting homomorphisms.\", \"- [ ] /code: Implement the DP algorithm in Python, including data structures for the group and tree.\", \"- [ ] /code: Create a test suite with at least one abelian and one non-abelian group to validate the implementation.\", \"- [ ] /math: Verify that the implemented logic in the /code agent's submission correctly reflects the derived recurrence relations.\", \"- [ ] /router: Perform final QA review, confirming all acceptance criteria are met and artifacts are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"algebraic-graph-theory\", \"dynamic-programming-on-trees\", \"computational-group-theory\"], \"acceptance_criteria\": [\"- The final report must contain a complete mathematical proof of the DP recurrence relations' correctness.\", \"- The implementation must pass all test cases for both an abelian group (e.g., Z_5) and a non-abelian group (e.g., S_3).\", \"- The derived time complexity must be at most O(|V| * |G|^2) and this must be consistent with the implemented algorithm's performance profile.\", \"- The literature review must cite at least two relevant papers from arXiv or a peer-reviewed journal.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 183, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the mitigation of barren plateaus in Variational Quantum Eigensolvers. Please derive the update rule for the Quantum Natural Gradient (QNG) optimizer based on the Fubini-Study metric tensor for a two-qubit hardware-efficient ansatz. Then, develop a Python simulation using PennyLane to find the ground state energy of the LiH molecule, comparing the convergence of QNG against the Adam optimizer under a realistic depolarizing noise model. The analysis should quantify the eigenvalue convergence rate and final accuracy.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Quantum Natural Gradient Fubini-Study metric VQE barren plateaus\\\", mode=both)\", \"/math(Derive the analytical form of the Fubini-Study metric tensor and the resulting Quantum Natural Gradient update rule for a two-qubit hardware-efficient ansatz, focusing on the Hamiltonian eigenvalue problem.)\", \"/code(Implement a PennyLane simulation for the LiH molecular Hamiltonian, comparing the convergence of the derived QNG optimizer against Adam under a depolarizing noise model, tracking energy expectation values., using Python)\", \"/math(Analyze simulation data to verify that QNG's accelerated convergence and improved final eigenvalue accuracy align with theoretical predictions from information geometry, providing statistical significance for the results.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on Quantum Natural Gradient (QNG). This theoretical foundation is then passed to /math for the core analytical derivation. The derived equations are implemented and tested empirically using /code for simulation. A final /math step is used to rigorously analyze the simulation output, linking the experimental results back to the initial theory and verifying the hypothesis.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the Quantum Natural Gradient update rule.\", \"A documented Python script (`vqe_simulation.py`) implementing the VQE for LiH with both QNG and Adam optimizers.\", \"A comparative plot (`convergence.png`) showing the ground state energy vs. optimization iterations for both methods.\", \"A final report in Markdown (`report.md`) summarizing the theory, methodology, results, and including all required citations.\", \"A JSON file (`results.json`) containing the final computed ground state energy and convergence metrics for both optimizers.\"], \"thinking_outline\": [\"1. Research the theoretical underpinnings of the Quantum Natural Gradient, its relationship to the Fubini-Study metric, and its role in mitigating barren plateaus in variational quantum algorithms.\", \"2. Formally derive the components of the Fubini-Study metric tensor for a general parameterized quantum circuit, then specialize it for the specified two-qubit hardware-efficient ansatz.\", \"3. Implement the VQE simulation for the LiH molecule, including constructing the Hamiltonian, defining the ansatz, and integrating the custom QNG optimizer alongside a standard library optimizer like Adam.\", \"4. Execute the simulation under a realistic noise model and collect data on the energy expectation value at each optimization step for both optimizers.\", \"5. Verify that the implemented QNG update rule in the code correctly reflects the mathematical derivation by comparing numerical gradients with analytical ones for a small test case.\", \"6. Validate the final computed ground state energy against established benchmark values for LiH to confirm the simulation's physical accuracy.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers (arXiv links) to the math agent. The math agent provides the final QNG update equations to the code agent. The code agent produces numerical data (logs, plots) for the final math agent to analyze. The router performs a final quality assurance check. Verification: The final ground state energy from the simulation must be within chemical accuracy of known values. Fallback: If verification fails, the code agent will be tasked to refine the ansatz or adjust the noise model parameters and re-run.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Quantum Natural Gradient from arXiv.\", \"- [ ] /math: Produce a self-contained derivation of the QNG update rule for the specified ansatz.\", \"- [ ] /code: Implement the VQE simulation for the LiH molecule using PennyLane, including both optimizers and a noise model.\", \"- [ ] /math: Verify that the analytical derivation of the metric tensor aligns with a numerical approximation computed by the code agent for a test parameter set.\", \"- [ ] /code: Execute the simulations and generate the convergence plot and results.json file.\", \"- [ ] /code: Add unit tests to validate the Hamiltonian construction and ansatz parameter counting.\", \"- [ ] /math: Write the final analysis comparing the two optimizers, referencing the generated plots and data.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"variational-algorithms\", \"numerical-optimization\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for the QNG update rule must be complete and mathematically sound.\", \"- The Python simulation must converge to a ground state energy for LiH that is within 1% of the known Full CI value.\", \"- The final report must clearly demonstrate, with plots and metrics, that the QNG optimizer achieves convergence in at least 25% fewer iterations than the Adam optimizer under the specified noise conditions.\", \"- The code must be well-documented and include a `requirements.txt` file.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 206, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to understand and implement the Jacobi method for solving a diagonally dominant linear system. Can you first derive the matrix form of the iteration and discuss its convergence criteria, then create a simple CUDA implementation in Python to compare its performance against a CPU version?\n", "completion": "{\"route_plan\": [\"/math(Derive the matrix splitting form (A = D + L + U) for the Jacobi method and prove the convergence condition for a strictly diagonally dominant system, including the role of the iteration matrix's eigenvalue.)\", \"/general-search(query=\\\"site:developer.nvidia.com site:arxiv.org Jacobi method CUDA tutorial performance\\\", mode=rag)\", \"/code(Implement the Jacobi method in Python using both NumPy for a CPU baseline and CuPy for a simple CUDA-accelerated version. Profile the execution time for both on a sample diagonally dominant matrix.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation of the Jacobi method and its convergence. Next, the blue /general-search agent will gather practical context and best practices for GPU implementation. Finally, /code will be used to create and benchmark both the baseline CPU and accelerated GPU implementations based on the theory and research.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the Jacobi method and its convergence proof.\", \"A Python script `jacobi_solver.py` containing both CPU (NumPy) and GPU (CuPy) implementations.\", \"A performance report in `performance_report.md` comparing the wall-clock time and speedup of the two implementations.\", \"A verification log showing the residual norm ||Ax-b|| converging below a threshold for both implementations.\", \"A list of 2+ citations from authoritative sources on GPU-accelerated linear solvers.\"], \"thinking_outline\": [\"1. Deconstruct the linear system Ax=b into the Jacobi iteration form x^(k+1) = D^-1(b - (L+U)x^k).\", \"2. Formally prove that the method converges if the matrix A is strictly diagonally dominant by analyzing the spectral radius of the iteration matrix T = -D^-1(L+U).\", \"3. Implement a CPU version using NumPy to serve as a correct baseline and for performance comparison.\", \"4. Implement a GPU version using a library like CuPy, which mirrors the NumPy API, to accelerate the vector-matrix operations.\", \"5. Verify the correctness of both implementations by checking that the final solution x satisfies ||Ax-b|| < \u03b5 for a small tolerance \u03b5 and validate that the GPU version shows a speedup.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: Compare the final solution vectors from CPU and GPU implementations for numerical stability and check that the residual norm is below the tolerance; fallback: If implementations diverge, re-audit the math derivation for errors in the iteration matrix.)\", \"todo_list\": [\"- [ ] Use /math to derive the Jacobi iteration and its convergence proof.\", \"- [ ] Use /general-search to find best practices and tutorials for CUDA implementation of iterative solvers.\", \"- [ ] Use /code to implement and profile both CPU and GPU versions of the Jacobi method.\", \"- [ ] Verify that the residual norm ||Ax-b|| for the computed solution is below the specified tolerance for both implementations.\", \"- [ ] Submit all artifacts for router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"iterative-methods\", \"gpu-acceleration\", \"linear-algebra\"], \"acceptance_criteria\": [\"The mathematical derivation correctly shows the Jacobi iteration formula and proves convergence for diagonally dominant matrices.\", \"The Python code implements both a NumPy and a CuPy version of the solver, and it runs without errors.\", \"The performance report shows a clear wall-clock time comparison, and the GPU version is faster for a sufficiently large matrix.\", \"The final solution vector from both implementations is numerically close and reduces the residual norm ||Ax-b|| to less than 1e-6.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Computation: `cpu_wall_time / gpu_wall_time` for a fixed matrix size and iteration count.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Computation: `||Ax - b||_2` where x is the final solution vector. Must be < 1e-6.\"}}", "categories": ["math_first", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 228, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the components of the Fubini-Study metric tensor for a two-qubit hardware-efficient ansatz, specifically the RY-CNOT ladder. Then, implement a numerical calculation of this quantum geometric tensor to analyze the optimization landscape geometry and its relation to barren plateaus in a Variational Quantum Eigensolver (VQE) for the molecular Hydrogen (H2) Hamiltonian. I need to see how the tensor's eigenvalue distribution correlates with gradient magnitudes and overall convergence behavior.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org quantum geometric tensor VQE 'barren plateaus' Fubini-Study metric\\\", mode=both)\", \"/math(Derive the analytical form of the Fubini-Study metric tensor for the specified two-qubit RY-CNOT ansatz, focusing on the real part of the quantum geometric tensor and its relation to the ground state energy eigenvalue.)\", \"/code(Implement the VQE for the H2 Hamiltonian using a hardware-efficient ansatz in Python; numerically compute the quantum geometric tensor across the parameter space, ensuring the convergence criteria are met and tracking the energy.)\", \"/math(Analyze the numerical results by correlating the eigenvalues of the computed metric tensor with the VQE energy landscape's local curvature and gradient magnitudes, proving the connection to optimizer convergence rates.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on the quantum geometric tensor. This informs the formal derivation by /math. The /code agent then implements this theory numerically for a concrete physical system. Finally, /math returns to interpret the numerical results, connecting them back to the initial theoretical framework and verifying the hypothesis.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Fubini-Study metric for the specified ansatz, delivered as a LaTeX document.\", \"- A documented Python script implementing the VQE and the quantum geometric tensor calculation.\", \"- A set of plots including the VQE energy landscape, the components of the metric tensor, and its eigenvalue distribution.\", \"- A final analysis report in Markdown summarizing the findings and including all required citations.\", \"- A JSON file containing the final computed ground state energy and the condition number of the metric tensor at the solution.\"], \"thinking_outline\": [\"1. Research the mathematical formalism of the Quantum Geometric Tensor (QGT), its relation to the Fubini-Study metric, and its role in VQE optimization, particularly concerning barren plateaus and quantum natural gradient methods.\", \"2. Formulate the explicit analytical derivation of the QGT components for the specified two-qubit RY-CNOT parameterized quantum circuit.\", \"3. Implement a numerical simulation using a quantum computing framework (e.g., PennyLane) to compute the VQE energy landscape and the QGT at each point in the parameter grid for the H2 molecular Hamiltonian.\", \"4. Validate the numerical implementation by comparing the computed metric tensor at select parameter points with the analytically derived values from step 2.\", \"5. Analyze the relationship between regions of large/small metric tensor eigenvalues (indicating high/low parameter sensitivity) and features of the VQE energy landscape, such as gradient magnitudes and the presence of barren plateaus.\", \"6. Verify that the conclusions drawn from the analysis are consistent with the established literature on VQE optimization landscapes identified in the initial research phase.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides foundational papers (arXiv links) to the /math agent. The /math agent produces a formal derivation that serves as a specification for the /code agent. The /code agent generates numerical data (plots, JSON metrics) which the final /math agent analyzes. Verification: The code output must be consistent with the analytical derivation for a simplified test case. Fallback: If numerical instability occurs in the code, flag it for the /math agent to investigate the parameterization's properties and suggest a re-parameterization or regularization.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on the quantum geometric tensor in VQE.\", \"- [ ] /math: Produce the complete analytical derivation of the Fubini-Study metric for the given ansatz.\", \"- [ ] /code: Implement the VQE for H2 and the numerical calculation of the metric tensor.\", \"- [ ] /math: Verify that the analytical derivation is self-consistent and correctly applies the principles of quantum mechanics.\", \"- [ ] /code: Validate the numerical code by comparing its output for a 1-qubit trivial case against a manually calculated result.\", \"- [ ] /code: Generate all required plots and the final metrics JSON file.\", \"- [ ] /math: Write the final analysis connecting the numerical results to the theory of VQE optimization landscapes.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-machine-learning\", \"variational-quantum-eigensolver\", \"differential-geometry\"], \"acceptance_criteria\": [\"- The formal derivation of the Fubini-Study metric tensor is mathematically sound and complete for the specified circuit.\", \"- The Python implementation correctly computes the ground state energy of H2 at a standard bond length (0.74 \u00c5) to within 1% of the known FCI value.\", \"- The final report clearly articulates the relationship between the metric tensor's eigenvalues and the VQE optimization landscape, supported by generated plots and at least two citations to arXiv papers.\", \"- The secondary metric (condition number) is correctly calculated and reported.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 262, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating neural-symbolic methods for program synthesis. Can you devise a framework that synthesizes a certified program for verifying red-black tree invariants using higher-order logic? The synthesis process should be guided by a neural network trained on a corpus of Coq or Isabelle proofs, and the final output must be a verifiable proof term. The core task is to bridge the gap between a learned policy and a symbolic verifier, possibly using Monte Carlo Tree Search (MCTS) guided by a transformer-based value network. The generated program must satisfy specified Hoare logic pre/post conditions.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural program synthesis formal verification MCTS transformer\\\", mode=web)\", \"/math(Formalize red-black tree invariants using higher-order logic, defining Hoare triples for insertion and specifying Bayesian priors on proof search complexity.)\", \"/code(Implement a Python prototype of the MCTS-guided synthesizer using PyTorch, focusing on a policy network with attention and supporting functional currying for proof term generation.)\", \"/math(Verify the logical soundness of a synthesized code fragment by checking its proof term against the established invariants, analyzing the convergence properties of the underlying control theory model.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on neural theorem proving, grounding the approach in established research. The first /math call is essential for creating a rigorous, formal specification of the problem. /code is then used to implement the complex synthesis algorithm. A final /math call is crucial for verifying that the synthesized artifact meets the formal requirements established initially.\", \"expected_artifacts\": [\"- A literature review summary with at least two recent arXiv citations.\", \"- A formal specification of red-black tree invariants and Hoare logic conditions in LaTeX.\", \"- A Python prototype of the neural-guided MCTS synthesizer.\", \"- A verification report containing a proof sketch that validates a synthesized program segment against the formal specification.\"], \"thinking_outline\": [\"1. Research current neural-symbolic theorem provers and program synthesizers, focusing on transformer-based models and MCTS guidance.\", \"2. Formally define the red-black tree properties and the target program's pre/post-conditions using a formal system like the Calculus of Constructions.\", \"3. Design the architecture of the neural-guided search, specifying the state representation (partial proof trees), action space (proof tactics), and the neural value/policy network structure.\", \"4. Implement the MCTS-based synthesizer prototype in Python with PyTorch.\", \"5. Validate a generated program sketch against the formal specification, checking for logical soundness and type consistency within the formal system.\", \"6. Audit the implementation for potential search space explosion and propose heuristics for pruning the MCTS search tree based on policy network confidence.\"], \"handoff_plan\": \"/general-search -> /math (formalism) -> /code -> /math (verification) -> router QA (verification: router will cross-reference the formal spec from the first /math step with the verification report from the second /math step; fallback: if verification fails, re-run /code with an expanded MCTS search budget).\", \"todo_list\": [\"- [ ] /general-search: Find SOTA papers on neural program synthesis for formal verification, focusing on MCTS and transformer models.\", \"- [ ] /math: Formalize red-black tree invariants and associated Hoare triples in a higher-order logic notation.\", \"- [ ] /code: Implement the MCTS synthesizer with a PyTorch-based policy network stub for generating proof steps.\", \"- [ ] /math: Draft a proof sketch verifying a synthesized program segment against the formal invariants.\", \"- [ ] /code: Add unit tests to the MCTS implementation to check search tree expansion and backpropagation logic.\", \"- [ ] /math: Verify the logical correctness and completeness of the proof sketch produced in the previous math step.\", \"- [ ] router: QA all generated artifacts for consistency, correctness, and adherence to the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"program-synthesis\", \"theorem-proving\", \"neural-symbolic-methods\"], \"acceptance_criteria\": [\"- The synthesized program snippet must be syntactically correct and successfully type-check against the defined higher-order logic.\", \"- The mathematical verification report must demonstrate that the snippet satisfies at least one key red-black tree invariant (e.g., the black-height property).\", \"- The literature review must cite at least 2 relevant arXiv papers published within the last 3 years, with IDs provided.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 263, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nFor the non-convex objective function f(x, y) = (x^2 - 1)^2 + y^2, analytically derive all critical points and classify them using the Hessian eigenvalue test. Subsequently, implement a gradient-based optimization in Python to numerically locate the strict saddle point and validate its spectral properties.\n", "completion": "{\"route_plan\": [\"/math(Analytically solve for all critical points where the gradient is zero. Compute the Hessian matrix for the function and evaluate its eigenvalues at each critical point to classify them as local minima, maxima, or saddle points, with a focus on identifying any strict saddles.)\", \"/general-search(query=\\\"site:arxiv.org site:*.edu non-convex optimization strict saddle property convergence first-order methods\\\", mode=rag)\", \"/code(Implement gradient descent in Python with NumPy to find the saddle point numerically. Start from a perturbed initialization. Validate the final position against the analytical solution and compute the numerical Hessian's eigenvalue spectrum to confirm it matches the theoretical signature of a strict saddle.)\"], \"route_rationale\": \"The plan follows a theory-validate-report structure. /math is used first to establish the analytical ground truth for critical points and their classifications. The blue /general-search agent then gathers theoretical context on the importance of strict saddles for optimization convergence. Finally, /code implements a numerical method to find the saddle point and validates its results against the analytical findings from /math, providing a complete, verified solution.\", \"expected_artifacts\": [\"A formal mathematical derivation of all critical points and their classification via the Hessian test.\", \"A Python script implementing gradient descent to locate the saddle point, including code for numerical Hessian computation.\", \"A summary report citing at least two academic sources on the role of saddle points in non-convex optimization.\", \"A verification log comparing the analytically derived saddle point location and Hessian eigenvalues with the numerical results.\"], \"thinking_outline\": [\"1. Analytically compute the gradient of the function f(x, y) and find all points (x, y) where it equals the zero vector.\", \"2. Compute the Hessian matrix of second partial derivatives for f(x, y).\", \"3. For each critical point found in step 1, evaluate the Hessian matrix and compute its eigenvalues to classify the point as a local minimum (positive definite), local maximum (negative definite), or saddle point (indefinite).\", \"4. Implement a numerical optimization algorithm (e.g., gradient descent) in Python to converge to one of the saddle points.\", \"5. Validate the numerical solution by comparing its final coordinates to the analytical solution and checking if the numerically computed Hessian at that point has the expected eigenvalue signature (one positive, one negative).\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent produces the analytical ground truth (coordinates and Hessian eigenvalues). The /general-search agent provides theoretical context. The /code agent uses the analytical solution as a target for verification of its numerical optimizer. router QA performs final validation. Verification: The L2 norm difference between analytical and numerical saddle point coordinates must be < 1e-4. Fallback: If verification fails, the /code agent must re-run with a different learning rate or more iterations before escalating to the router.\", \"todo_list\": [\"- [ ] /math: Derive critical points and classify them using the Hessian eigenvalue test.\", \"- [ ] /general-search: Find and summarize two papers on the importance of the strict saddle property for first-order method convergence.\", \"- [ ] /code: Implement gradient descent to find the saddle point at (0,0).\", \"- [ ] /code: Numerically compute the Hessian at the converged point and verify its eigenvalues match theory.\", \"- [ ] /code: Generate a plot showing the optimization trajectory on a contour plot of the function.\", \"- [ ] /router: Review all artifacts for correctness, consistency, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-matrix\"], \"acceptance_criteria\": [\"- The analytical derivation must correctly identify all three critical points: two local minima and one saddle point.\", \"- The Python script must converge to the saddle point (0,0) with a final L2 error < 1e-4 compared to the analytical location.\", \"- The numerically computed Hessian at the converged point must have one strictly positive and one strictly negative eigenvalue, matching the analytical result.\", \"- The final report must cite at least two relevant academic papers with valid arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 269, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of the Preconditioned Conjugate Gradient (PCG) method for solving sparse, symmetric positive-definite (SPD) linear systems arising from finite difference discretizations. Derive the algorithm, analyze its convergence in relation to the eigenvalue distribution of the preconditioned matrix M\u207b\u00b9A, and implement a performant version in CUDA. Specifically, demonstrate how an incomplete Cholesky factorization preconditioner clusters eigenvalues and accelerates convergence compared to the standard CG method, providing both theoretical proof and empirical validation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org preconditioned conjugate gradient convergence analysis eigenvalue clustering incomplete cholesky\\\", mode=web)\", \"/math(Derive the PCG algorithm from first principles. Prove the convergence rate improvement by analyzing the condition number and eigenvalue spectrum of the preconditioned system M\u207b\u00b9A, referencing the Chebyshev polynomials.)\", \"/code(Implement the PCG algorithm using Python with CUDA (CuPy) for a sparse SPD matrix from a 2D Poisson equation. Implement an Incomplete Cholesky preconditioner. Track the L2 norm of the residual vector at each iteration.)\", \"/math(Validate the empirical convergence rate from the CUDA implementation against the theoretical bounds derived earlier. Analyze the computed eigenvalue distribution of the preconditioned matrix to verify the clustering effect.)\"], \"route_rationale\": \"The blue general-search agent first gathers foundational literature on PCG convergence theory. The first /math agent then formalizes this theory with a rigorous derivation and proof. The /code agent translates the mathematical algorithm into a practical, high-performance GPU implementation. The final /math agent provides critical validation by comparing the empirical results from the code back to the initial theoretical model, ensuring correctness.\", \"expected_artifacts\": [\"* A formal mathematical document detailing the derivation of the PCG algorithm and its convergence proof.\", \"* A Python script containing a well-documented, CUDA-accelerated implementation of PCG with an Incomplete Cholesky preconditioner.\", \"* A PNG plot comparing the convergence (residual norm vs. iteration) of standard CG and PCG.\", \"* A summary report (.md) synthesizing the theoretical analysis and empirical results, including citations for at least two papers.\", \"* A JSON file containing the final metrics, including convergence rate and kernel timings.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of Krylov subspace methods, specifically the Conjugate Gradient and Preconditioned Conjugate Gradient algorithms, focusing on convergence bounds related to the condition number and eigenvalue distribution.\", \"2. Formally derive the PCG algorithm and the theoretical rate of convergence, showing its dependency on the spectral properties of the preconditioned matrix.\", \"3. Implement the PCG algorithm using CUDA for GPU acceleration, including a sparse matrix generator for a test case (e.g., 2D Poisson problem) and an Incomplete Cholesky preconditioner.\", \"4. Execute the implementation, logging the residual norm at each iteration, and generate convergence plots for both standard CG and PCG.\", \"5. Verify that the observed convergence rate from the plots matches the theoretical bounds derived in step 2. Discrepancies should be investigated as potential implementation errors or numerical precision issues.\", \"6. Audit the eigenvalue spectrum of the original and preconditioned matrices to visually validate the clustering effect of the preconditioner.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (validation) -> router QA. The derivation from the first /math agent must serve as the blueprint for the /code agent. The empirical data (residual norms, timings) from the /code agent must be the direct input for the second /math agent's validation task. Verification: The final /math agent confirms that the code's convergence behavior aligns with the initial proof. Fallback: If validation fails, the task is routed back to the /code agent to debug the implementation or to the first /math agent to reconsider the preconditioner's theoretical properties.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize two seminal papers on PCG convergence and incomplete factorization preconditioners.\", \"- [ ] /math: Produce a LaTeX document with the full derivation of the PCG algorithm and the proof of its convergence rate.\", \"- [ ] /code: Write a Python script using CuPy to implement PCG and generate the convergence data.\", \"- [ ] /math: Compute the condition numbers and eigenvalue distributions for the matrices A and M\u207b\u00b9A.\", \"- [ ] /code: Generate a plot comparing the convergence history of CG vs. PCG.\", \"- [ ] /math: Verify that the implementation's convergence rate is consistent with the derived theoretical bound `2 * (1 - 2/(sqrt(kappa) + 1))`, where kappa is the effective condition number.\", \"- [ ] /math: Write a verification summary confirming the alignment between theory and empirical results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"krylov-subspace-methods\", \"gpu-computing\"], \"acceptance_criteria\": [\"* The mathematical derivation of PCG must be correct and self-contained.\", \"* The CUDA implementation must successfully converge to the correct solution for a 1000x1000 sparse SPD system.\", \"* The generated plot must clearly show a significant acceleration in convergence for PCG compared to standard CG.\", \"* The final report must cite at least two authoritative sources with DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 274, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the posterior predictive distribution for a Bayesian Gaussian Process model with a rational quadratic kernel, then implement it to analyze convergence on noisy sinusoidal data.\n", "completion": "{\"route_plan\": [\"/math(Derive the full posterior predictive distribution, including mean and covariance, for a Bayesian Gaussian Process model, specifying the matrix inversion and Cholesky decomposition steps.)\", \"/general-search(query=\\\"site:jmlr.org site:arxiv.org gaussian process kernel selection convergence criteria\\\", mode=rag)\", \"/code(Implement the Gaussian Process in Python/NumPy, using the derived formulas. Validate correctness against scikit-learn's implementation and plot the negative log marginal likelihood to audit model convergence.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation by deriving the core equations. Then, the 'blue' /general-search agent is tasked with gathering authoritative literature on kernel selection, a crucial practical consideration. Finally, /code implements and empirically validates the derived theory, ensuring the solution is both theoretically sound and practically correct.\", \"expected_artifacts\": [\"- A markdown file containing the complete mathematical derivation of the GP posterior predictive distribution.\", \"- A Python script (`gp_implementation.py`) with a from-scratch implementation of the Gaussian Process regression model.\", \"- A PNG image (`gp_plot.png`) visualizing the model's predictions, including mean and 95% confidence interval, against the synthetic data.\", \"- A JSON file (`metrics.json`) reporting the Mean Squared Error and Mean Log Predictive Density on a hold-out test set.\", \"- A list of citations for at least two academic papers on Gaussian Processes.\"], \"thinking_outline\": [\"1. Formally derive the equations for the mean and covariance of the posterior predictive distribution for a Gaussian Process, given training data and a specified kernel function.\", \"2. Research common kernel functions (e.g., RBF, Rational Quadratic, Mat\u00e9rn) and their suitability for different types of data, consulting authoritative sources.\", \"3. Implement the Gaussian Process regression algorithm from scratch in Python using NumPy, based directly on the derived mathematical formulas.\", \"4. Generate a synthetic sinusoidal dataset with added Gaussian noise to serve as the training and testing data.\", \"5. Validate the custom implementation by comparing its posterior mean and covariance outputs against the results from a trusted library like scikit-learn or GPy on the same dataset. The difference should be within a small numerical tolerance.\", \"6. Plot the model's predictions to visually inspect the fit and analyze the negative log marginal likelihood over training iterations to check for hyperparameter convergence.\"], \"handoff_plan\": \"/math provides the analytical equations for the GP posterior -> /general-search uses these terms to find relevant papers on kernel properties -> /code receives both the derivation and literature to build a robust implementation -> router QA performs final review. Verification: The code's outputs are compared against a standard library. Fallback: If verification fails, the derivation from /math is re-audited for errors before re-running the /code implementation.\", \"todo_list\": [\"- [ ] /math: Derive the posterior predictive mean and covariance for a Bayesian Gaussian Process with a rational quadratic kernel.\", \"- [ ] /general-search: Find canonical references for Gaussian Process regression, focusing on kernel selection criteria for periodic functions.\", \"- [ ] /code: Implement the Gaussian Process regression model in Python/NumPy based on the mathematical derivation.\", \"- [ ] /code: Generate a synthetic dataset from a known sinusoidal function and visualize the model's predictions, including the confidence interval.\", \"- [ ] /code: Validate the custom implementation's outputs (posterior mean and variance) against scikit-learn's GaussianProcessRegressor.\", \"- [ ] /router: QA final report, ensuring the derivation, code, plots, and validation metrics are consistent and correct.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"bayesian-inference\", \"stochastic-processes\", \"gaussian-process\"], \"acceptance_criteria\": [\"- The mathematical derivation of the posterior predictive distribution must be complete, correct, and clearly articulated.\", \"- The Python implementation's predicted mean must match the output of scikit-learn's `GaussianProcessRegressor` with an MSE below 1e-6 on a held-out test set.\", \"- The final report includes a plot that correctly visualizes the GP's predictive mean and the 95% confidence interval overlayed on the training and test data points.\", \"- The solution cites at least two relevant, peer-reviewed articles or seminal arXiv preprints on Gaussian Processes.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the MSE between the predicted mean of the custom GP and the true function values on a hold-out test set.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate the average log probability density of the test points under the model's posterior predictive distribution to assess model calibration and uncertainty quantification.\"}}", "categories": ["math_first", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 279, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the computational complexity of determining the Grundy value (nim-value) for impartial games defined by dynamically evolving graph structures, specifically 'subtraction games with varying subtrahend sets'. Derive a formal proof demonstrating that the generalized problem is PSPACE-complete. Subsequently, develop a Python implementation of an efficient algorithm for a constrained version where the subtrahend set is bounded, and experimentally verify its correctness and performance against theoretical complexity bounds, analyzing its convergence properties.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:cs.cmu.edu \\\\\\\"combinatorial game theory\\\\\\\" PSPACE-completeness \\\\\\\"subtraction games\\\\\\\" \\\\\\\"Sprague-Grundy\\\\\\\"\\\", mode=web)\", \"/math(Formalize the generalized subtraction game and construct a polynomial-time reduction from a known PSPACE-complete problem like QSAT to prove its complexity, establishing the theoretical adversarial bounds.)\", \"/code(Implement a Python dynamic programming algorithm to compute Grundy values for the bounded subtrahend set variant, tracking runtime complexity and ensuring convergence for acyclic game states.)\", \"/math(Validate the Python implementation's output against the derived recurrence relations for a set of test cases and statistically analyze the performance data to confirm the asymptotic complexity.)\"], \"route_rationale\": \"/general-search (the blue agent) is used first to survey existing literature on PSPACE-completeness in games and the Sprague-Grundy theorem, providing a foundation. /math then formalizes the specific problem and constructs the complex proof. /code translates the theory for the constrained case into a practical, verifiable implementation. A final /math step is crucial for cross-validating the code's empirical results against the theoretical derivations, ensuring correctness and rigor.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the formal PSPACE-completeness proof.\", \"A documented Python script implementing the Grundy value computation algorithm.\", \"A report summarizing the experimental performance analysis with plots of runtime vs. input size.\", \"A list of at least 2 cited papers from arXiv or peer-reviewed journals.\", \"Verification logs showing the algorithm's output matches brute-force on small test cases.\"], \"thinking_outline\": [\"1. Research existing PSPACE-completeness proofs for impartial games to identify common reduction techniques (e.g., from QSAT or Generalized Geography).\", \"2. Formally define the generalized subtraction game and construct a polynomial-time reduction from a known PSPACE-complete problem to prove hardness.\", \"3. Derive the recurrence relation for the Grundy values (nim-sum) for the constrained version of the game, forming the basis for an algorithm.\", \"4. Implement the algorithm in Python using memoization to handle the state space efficiently and verify its correctness on small, manually-solvable instances.\", \"5. Design and run experiments to measure the algorithm's runtime performance as a function of the game's parameters.\", \"6. Validate that the empirical runtime complexity matches the theoretical asymptotic analysis and document any discrepancies.\"], \"handoff_plan\": \"/general-search -> /math (Literature review informs the formal proof construction) -> /code (Mathematical recurrence is translated into a Python algorithm) -> /math (Experimental data from code is used to verify theoretical complexity). Verification: /code output is checked against manual calculations from /math for small N. Fallback: If the proof in /math is flawed, return to /general-search for alternative reduction strategies. If the code fails verification, debug implementation against the /math derivation. Final review by router QA.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on PSPACE-completeness of impartial games and efficient computation of Grundy numbers.\", \"- [ ] /math: Draft the PSPACE-completeness proof via reduction from Quantified Boolean Formulas (QBF).\", \"- [ ] /math: Derive the specific recurrence relation for Grundy numbers in the constrained subtraction game.\", \"- [ ] /code: Implement the memoized recursion algorithm in Python for computing the Grundy values.\", \"- [ ] /code: Develop a test suite with small, verifiable game instances and a framework for performance benchmarking.\", \"- [ ] /math: Verify that the algorithm's output from /code matches the derived recurrence for all test cases.\", \"- [ ] /math: Analyze the performance benchmark data and compare empirical results against the theoretical time complexity.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"sprague-grundy-theorem\"], \"acceptance_criteria\": [\"- The PSPACE-completeness proof must be logically sound and use a valid reduction from a known PSPACE-complete problem.\", \"- The Python code must correctly compute Grundy values for all provided test cases and include clear documentation.\", \"- The performance analysis report must show a clear relationship between input size and runtime, consistent with the theoretical complexity.\", \"- The solution must cite at least two relevant academic papers with arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 303, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of the Variational Quantum Eigensolver (VQE) for the LiH molecular Hamiltonian. Specifically, derive the analytical gradient for a Unitary Coupled Cluster Singles and Doubles (UCCSD) ansatz using the parameter-shift rule, implement this in Python to find the ground state energy eigenvalue, and benchmark its convergence against established theoretical values. Finally, research and report on recent arXiv preprints discussing advanced Bayesian optimization techniques for noisy VQE.\n", "completion": "{\"route_plan\": [\"/math(Derive the analytical gradient of the VQE cost function, defined by the expectation value of the LiH molecular Hamiltonian H with a UCCSD ansatz, using the parameter-shift rule to find the partial derivative with respect to each ansatz parameter \u03b8_k. The derivation must explicitly handle the Pauli structure of the Hamiltonian and its eigenvalue properties.)\", \"/code(Implement a VQE simulation for the LiH molecule in Python using PennyLane or Qiskit. Construct the UCCSD ansatz, implement the analytically derived parameter-shift gradient, and use a gradient-based optimizer to find the ground state. Track the convergence of the energy and the number of circuit evaluations, targeting high-fidelity chemistry results.)\", \"/general-search(query=\\\"site:arxiv.org variational quantum eigensolver 'Bayesian optimization' UCCSD noise mitigation after:2020\\\", mode=web)\"], \"route_rationale\": \"The route starts with /math to establish the core theoretical foundation for the VQE gradient, which is essential for efficient optimization. This derivation is then handed to /code for a concrete Python implementation and validation against a known chemistry problem. Finally, the blue /general-search agent is tasked with finding state-of-the-art literature to contextualize the implemented baseline and suggest improvements, fulfilling the full-stack reasoning requirement.\", \"expected_artifacts\": [\"- A LaTeX document detailing the step-by-step derivation of the parameter-shift rule for the UCCSD ansatz.\", \"- A Python script implementing the VQE simulation for LiH, including the custom gradient function and optimizer loop.\", \"- A results summary in a JSON file containing the final ground state energy, convergence history, and number of function evaluations.\", \"- A brief report summarizing findings from at least two recent arXiv papers on advanced VQE optimization.\", \"- A verification log comparing the analytical gradient implementation to a numerical finite-difference check for a set of random parameters.\"], \"thinking_outline\": [\"1. Formulate the VQE problem for the LiH molecular Hamiltonian, defining the cost function L(\u03b8) as the expectation value \u27e8\u03c8(\u03b8)|H|\u03c8(\u03b8)\u27e9 and identifying the structure of the UCCSD ansatz U(\u03b8).\", \"2. Derive the analytical gradient \u2202L/\u2202\u03b8_k for a general ansatz parameter \u03b8_k using the parameter-shift rule, expressing it as a linear combination of two expectation values with shifted parameters.\", \"3. Implement the VQE algorithm in Python, including the construction of the LiH Hamiltonian, the UCCSD circuit, and a classical optimization loop driven by the derived analytical gradient.\", \"4. Validate the correctness of the implemented gradient by comparing its output for several random parameter vectors against a numerical gradient computed via the finite-difference method. The relative error should be below a small tolerance (e.g., 1e-6).\", \"5. Execute the VQE optimization to find the ground state energy eigenvalue and analyze its convergence. Verify the final energy against the known Full Configuration Interaction (FCI) value for LiH to assess accuracy.\", \"6. Survey recent literature to identify and summarize advanced techniques, such as Bayesian optimization or adaptive optimizers, that improve VQE performance in noisy environments.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math is a direct input for the gradient function in the /code implementation. A core verification step is for /code to numerically validate the math derivation's correctness. The performance results from /code (e.g., convergence speed) provide context for the /general-search query on advanced optimizers. The final synthesis of theory, implementation, and literature is reviewed by the router. Fallback: If the analytical gradient fails verification, /code will revert to a finite-difference gradient and flag the discrepancy for review.\", \"todo_list\": [\"- [ ] /math: Derive the gradient of the VQE cost function for the UCCSD ansatz using the parameter-shift rule.\", \"- [ ] /code: Set up the LiH molecular Hamiltonian and the UCCSD quantum circuit.\", \"- [ ] /code: Implement the VQE optimization loop using the gradient derived by the /math agent.\", \"- [ ] /code: Add a verification function to compare the analytical gradient against a finite-difference approximation.\", \"- [ ] /code: Run the simulation, save the convergence plot, and record the final ground state energy.\", \"- [ ] /general-search: Find and summarize two recent arXiv papers on Bayesian optimization for VQE.\", \"- [ ] /router: Perform final QA review, checking all acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-machine-learning\", \"variational-quantum-eigensolver\", \"quantum-chemistry\"], \"acceptance_criteria\": [\"- The derived analytical gradient must be mathematically correct and explicitly use the parameter-shift rule formulation.\", \"- The final ground state energy computed by the Python script for the LiH molecule must be within 0.01 Hartrees of the known FCI value (~-7.88 Hartrees).\", \"- The numerical validation must show a relative error of less than 1e-5 between the analytical and finite-difference gradients.\", \"- The final report must cite at least two relevant arXiv papers (published after 2020) with their IDs and summarize their contributions to VQE optimization.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 327, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project on fluid dynamics simulation and need to understand how to solve large systems of linear equations using GPU acceleration. Can you explain the foundational concepts of Jacobi iteration and demonstrate how its convergence properties are analyzed, then show a basic CUDA implementation for a sparse matrix?\n", "completion": "{\"route_plan\": [\"/math(Derive the update rule for the Jacobi iteration method for solving Ax=b and explain the condition for convergence based on the spectral radius of the iteration matrix, considering its connection to eigenvalue analysis.)\", \"/general-search(query=\\\"site:developer.nvidia.com site:arxiv.org Jacobi iteration CUDA implementation for sparse matrices convergence\\\", mode=rag)\", \"/code(Implement the Jacobi iteration in Python using the CuPy library to demonstrate GPU acceleration for solving a sparse linear system. The code must include checks for diagonal dominance to assess convergence likelihood and track the residual norm.)\"], \"route_rationale\": \"/math is used first to establish the theoretical foundation of Jacobi iteration and its convergence. The blue /general-search agent then finds practical implementation patterns and benchmarks for CUDA. Finally, /code synthesizes this knowledge into a working GPU-accelerated example.\", \"expected_artifacts\": [\"A formal derivation of the Jacobi iteration method and its convergence proof based on the spectral radius.\", \"A summary of findings from authoritative sources on implementing iterative solvers on GPUs, with at least two citations.\", \"A Python script using CuPy that implements Jacobi iteration for a sample sparse matrix.\", \"A plot showing the convergence of the residual norm over iterations.\", \"A verification report confirming the solution `x` satisfies `Ax=b` within a specified tolerance.\"], \"thinking_outline\": [\"1. Formulate the Jacobi iteration by decomposing the matrix A into its diagonal, lower, and upper triangular parts.\", \"2. Derive the iteration matrix T_J and prove that the method converges if and only if the spectral radius rho(T_J) is less than 1.\", \"3. Research best practices for representing sparse matrices (e.g., CSR, COO) for efficient GPU computation using CUDA or CuPy.\", \"4. Implement the Jacobi update rule using GPU-accelerated vector operations.\", \"5. Verify the implementation's correctness by checking the solution against a known ground truth and validating that the residual norm `||Ax - b||` decreases monotonically to a small tolerance.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: The final code's solution vector is checked against the mathematical derivation's convergence criteria and residual norm tolerance. fallback: If convergence is not achieved, /general-search is re-invoked to find preconditioning techniques for the given matrix type.)\", \"todo_list\": [\"- [ ] /math: Derive the Jacobi iteration update rule and its spectral radius convergence condition.\", \"- [ ] /general-search: Find authoritative guides and papers on CUDA/CuPy implementations for iterative solvers.\", \"- [ ] /code: Implement the Jacobi solver in Python with CuPy, including residual tracking.\", \"- [ ] /verification: Check if the implemented solution converges for a diagonally dominant matrix and satisfies the residual tolerance.\", \"- [ ] /router QA: Submit all artifacts for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"cuda\"], \"acceptance_criteria\": [\"The mathematical derivation of the Jacobi convergence condition (spectral radius < 1) is correct and clearly explained.\", \"The CUDA/CuPy code successfully solves a pre-defined sparse linear system and produces a convergence plot.\", \"The final solution `x` satisfies `||Ax - b||_2 < 1e-6`.\", \"The report includes at least two citations to relevant arXiv papers or NVIDIA developer documentation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 351, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the coordinate ascent variational inference (CAVI) update equations for a Bayesian logistic regression model with Gaussian priors on the weights. Then, implement this in Python, validate the non-decreasing property of the Evidence Lower Bound (ELBO), and compare the resulting approximate posterior distributions against a gold-standard MCMC-based posterior approximation.\n", "completion": "{\"route_plan\": [\"/math(Derive the CAVI update equations for the variational parameters in a Bayesian logistic regression model, ensuring the Evidence Lower Bound (ELBO) formulation is explicit to track convergence.)\", \"/general-search(query=\\\"site:arxiv.org site:jmlr.org convergence properties of stochastic variational inference for non-conjugate models\\\", mode=web)\", \"/code(Implement the derived Bayesian CAVI algorithm in Python using JAX, including a function to monitor ELBO convergence. Also, code a Metropolis-Hastings sampler to generate a reference posterior for validation.)\"], \"route_rationale\": \"The task requires a theoretical derivation (/math), a literature search for context on modern methods and convergence theory (/general-search), and a validated implementation (/code). The blue general-search agent will find state-of-the-art context on SVI convergence which informs the analysis. The math agent provides the core equations for the code agent, which then implements and validates the entire system.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the CAVI update equations and the ELBO.\", \"A Python script (`.py`) with the CAVI implementation, the MCMC sampler, and visualization code.\", \"A comparative plot showing the learned variational posterior distributions against the MCMC posterior samples.\", \"A JSON file reporting the final ELBO, KL-divergence metric, and computation times.\", \"A list of at least two cited arXiv papers on variational inference convergence.\"], \"thinking_outline\": [\"1. Define the probabilistic graphical model for Bayesian logistic regression, specifying the likelihood and prior distributions (e.g., Gaussian priors on weights).\", \"2. Derive the analytical forms for the optimal variational distributions q*(\u03b2_j) using the mean-field assumption and the principles of coordinate ascent variational inference.\", \"3. Formulate the Evidence Lower Bound (ELBO) for this model and show how it is used as the objective function for the CAVI algorithm.\", \"4. Implement the derived CAVI update loop and validate that the ELBO is monotonically non-decreasing during optimization on a synthetic dataset.\", \"5. Implement a baseline MCMC sampler (e.g., Metropolis-Hastings) to generate samples from the true posterior distribution.\", \"6. Verify the accuracy of the variational approximation by comparing the moments (mean and variance) of the q* distributions to the empirical moments from the MCMC samples and report the KL-divergence.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the core algorithm. The /general-search agent provides theoretical context on convergence guarantees, which the /code agent should reference in its analysis. The /code agent implements and validates the math. Verification: The router QA will check that the ELBO trace from the code is non-decreasing as proven in the derivation and that the posterior comparison is statistically sound. Fallback: If ELBO diverges or the posterior approximation is poor, the /math agent will be re-invoked to check the derivation for errors.\", \"todo_list\": [\"- [ ] /math: Derive the CAVI update equations for a logistic regression model with Gaussian priors.\", \"- [ ] /math: Derive the full expression for the ELBO to be used as a convergence diagnostic.\", \"- [ ] /general-search: Find and summarize two key papers on the convergence properties of VI, especially for non-conjugate models.\", \"- [ ] /code: Implement the CAVI algorithm based on the mathematical derivation.\", \"- [ ] /code: Implement an MCMC sampler to serve as a ground truth for posterior comparison.\", \"- [ ] /math: Verify that the implemented ELBO calculation in the code correctly matches the derived mathematical formula.\", \"- [ ] /code: Validate the VI posterior against the MCMC posterior using KL-divergence and moment matching.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-methods\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"The mathematical derivation of the CAVI updates must be complete and self-contained.\", \"The implemented code must demonstrate a monotonically non-decreasing ELBO trace on a synthetic dataset.\", \"The final report must quantitatively compare the variational posterior to the MCMC posterior using the specified KL-divergence metric.\", \"At least two relevant academic papers (with arXiv IDs) on variational inference must be cited.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the Kullback-Leibler divergence from the true posterior (approximated by MCMC) to the variational posterior q. Lower is better.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Report the final value of the Evidence Lower Bound after the algorithm has converged. This should be a single scalar value.\"}}", "categories": ["math_first", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 373, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the local geometry of the Transformer's self-attention parameter manifold using information geometry. Specifically, derive the Fisher Information Matrix (FIM) as the Riemannian metric for a single attention head. How does the Hessian eigenvalue spectrum relate to this metric, and what are the implications for the convergence rate of second-order optimization methods like natural gradient descent compared to standard SGD?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org information geometry transformer 'Fisher Information Matrix' natural gradient descent\\\", mode=web)\", \"/math(Derive the analytical form of the Fisher Information Matrix for the query-key-value parameterization of a single self-attention head, treating the output distribution as a categorical distribution over the vocabulary. Analyze its block structure and potential for eigenvalue clustering to predict convergence behavior.)\", \"/code(Implement a numerical computation of the FIM in JAX for a simplified attention mechanism. Use automatic differentiation to compute the Hessian of the log-likelihood and validate the FIM's positive semi-definiteness via eigenvalue decomposition on CUDA-enabled hardware., using Python)\", \"/math(Compare the analytically derived FIM structure with the numerical results. Formally prove the relationship between the FIM's largest eigenvalue and the local curvature, and explain how this impacts the theoretical convergence guarantee of preconditioned gradient methods.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to survey foundational literature on information geometry in deep learning. This informs the /math agent's formal derivation of the FIM. A /code agent is then used for numerical verification of the mathematical theory, providing an empirical check on the derivation. Finally, the /math agent synthesizes the theoretical and empirical results to draw conclusions about optimization convergence.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the Fisher Information Matrix for the attention mechanism.\", \"A Python script (JAX) that numerically computes the FIM and its eigenvalue spectrum.\", \"A plot comparing the analytical predictions of eigenvalue distribution with the numerically computed spectrum.\", \"A summary report (.md) explaining the implications for optimizer design, with citations to at least two relevant arXiv papers.\", \"A verification log detailing the Frobenius norm error between the analytical and numerical FIM.\"], \"thinking_outline\": [\"1. Review the principles of information geometry and how the Fisher Information Matrix (FIM) defines a Riemannian metric on the parameter space of a statistical model.\", \"2. Formulate the self-attention mechanism as a probabilistic model and derive the analytical expression for its FIM with respect to the model parameters (W_Q, W_K, W_V).\", \"3. Implement a numerical routine to compute the FIM for a test case and validate its properties, such as positive semi-definiteness, by checking for non-negative eigenvalues.\", \"4. Analyze the spectral properties (eigenvalues and eigenvectors) of the derived FIM to understand the parameter space curvature.\", \"5. Verify that the numerical eigenvalue spectrum computed by the code aligns with the theoretical predictions from the analytical derivation within a defined tolerance.\", \"6. Synthesize the findings to explain how using the inverse FIM as a preconditioner in natural gradient descent can accelerate training convergence compared to first-order methods.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: Numerical FIM from /code must match symbolic FIM from /math within tolerance; fallback: if mismatch, /math agent re-derives specific components and /code agent adds targeted unit tests).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on natural gradient and information geometry in neural networks, focusing on Transformers.\", \"- [ ] /math: Derive the complete Fisher Information Matrix for the softmax-based attention mechanism.\", \"- [ ] /code: Implement the FIM computation in JAX for a single-head attention layer, including necessary Hessian calculations.\", \"- [ ] /code: Compute the numerical eigenvalue spectrum of the FIM for several sample inputs to check for stability.\", \"- [ ] /math: Formally analyze the derived FIM to predict its spectral properties and structure.\", \"- [ ] /math: Verify that the analytical spectral properties match the numerical results from the /code agent by comparing the top-k eigenvalues.\", \"- [ ] /code: Verify the positive semi-definiteness of the computed FIM by asserting that all eigenvalues are non-negative.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-geometry\", \"transformer-optimization\", \"riemannian-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Fisher Information Matrix is complete, rigorous, and self-contained.\", \"- The JAX implementation correctly computes the FIM, and its numerical eigenvalues match theoretical predictions with a relative error below 5%.\", \"- The final report clearly explains the connection between the FIM spectrum, loss landscape curvature, and natural gradient descent, citing at least 2 relevant arXiv papers with IDs.\", \"- The primary metric (Frobenius Norm Error) is below a threshold of 1e-4.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 379, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and rigorously compare structured mean-field variational inference against natural-gradient Stochastic Variational Inference (SVI) for a Bayesian Latent Dirichlet Allocation model. The goal is to analyze the convergence properties of the Evidence Lower Bound (ELBO) and the computational complexity trade-offs, specifically focusing on how the natural gradient accelerates convergence over standard coordinate ascent updates.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org structured variational inference natural gradient bayesian topic models convergence\\\", mode=rag)\", \"/math(Derive the complete ELBO and the variational update equations for both structured mean-field and natural gradient SVI for a Latent Dirichlet Allocation model, noting the role of the Fisher information matrix.)\", \"/code(Implement both SVI algorithms in Python using PyTorch, tracking ELBO convergence on a synthetic corpus. The implementation must include clear separation between the bayesian model definition and the inference logic.)\", \"/math(Analyze the generated ELBO convergence plots to verify theoretical properties, such as the non-decreasing nature for batch updates, and provide a formal comparison of the empirical convergence rates., includes numpy)\"], \"route_rationale\": \"The blue general-search agent is used first to gather state-of-the-art context on SVI. This informs the /math agent's formal derivation of the necessary update equations. The /code agent then implements these derivations for empirical testing. A final /math call is required to formally analyze and verify the empirical results from the code, ensuring they align with the theoretical expectations.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the ELBO and the variational updates for both inference methods.\", \"A well-documented Python script (`svi_comparison.py`) implementing the LDA model and both inference algorithms.\", \"A JSON file (`metrics.json`) containing the ELBO values per iteration for each method, facilitating reproducible analysis.\", \"A summary report in Markdown (`report.md`) that interprets the results, includes convergence plots, and cites at least two relevant academic papers.\", \"Verification logs (`verification.log`) detailing the check between mathematical derivations and code output.\"], \"thinking_outline\": [\"1. Formulate the Latent Dirichlet Allocation (LDA) model as a probabilistic graphical model and define the structured mean-field variational family.\", \"2. Derive the Evidence Lower Bound (ELBO) for this model and derive the coordinate ascent update equations for all local and global latent variables.\", \"3. Re-derive the updates for the global parameters using the stochastic natural gradient, explicitly showing its connection to the Fisher information metric.\", \"4. Implement both algorithms and validate the implementation by checking the ELBO on a small, known dataset to ensure it is non-decreasing for the batch algorithm.\", \"5. Execute a comparative experiment on a larger synthetic dataset, plotting ELBO versus wall-clock time and iteration count to evaluate convergence speed and final bound tightness.\", \"6. Audit the final results to confirm that the natural gradient method demonstrates faster convergence as predicted by theory and summarize the complexity trade-offs.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The derivation from /math serves as the formal specification for the /code agent. The code's output (metrics.json, plots) is then handed back to the /math agent for final verification and analysis. Fallback: If the ELBO diverges or behaves unexpectedly in the code's output, the task is routed back to the /math agent to re-verify the gradient derivations and update equations.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on SVI and natural gradients.\", \"- [ ] /math: Derive the ELBO and update equations for both structured mean-field VI and natural gradient SVI.\", \"- [ ] /code: Implement the LDA model, synthetic data generator, and both inference algorithms.\", \"- [ ] /code: Instrument the implementation to log ELBO values, iteration count, and execution time.\", \"- [ ] /math: Verify that the generated ELBO plots from the code align with theoretical expectations (e.g., monotonic increase for batch updates).\", \"- [ ] /code: Add a unit test to check the shape and basic statistics of the computed gradients against a known small example.\", \"- [ ] /math: Generate the final comparison plots and write the analytical summary.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"bayesian-statistics\"], \"acceptance_criteria\": [\"- The provided mathematical derivations for both VI methods must be complete and correct.\", \"- The Python code must execute successfully, and the generated ELBO plot for the batch mean-field algorithm must be monotonically non-decreasing.\", \"- The final report must contain plots comparing both methods and correctly cite at least two relevant papers with arXiv IDs or DOIs.\", \"- The reported metrics in `metrics.json` must be reproducible given the provided code and seed.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 387, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a Physics-Informed Neural Network (PINN) to solve the 2D incompressible Navier-Stokes equations for a lid-driven cavity problem. Please provide a full derivation of the composite loss function, including the PDE residual and boundary condition terms. Then, implement this in JAX, ensuring the use of automatic differentiation for the differential operators. The model's convergence must be validated against a known benchmark.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org PINN Navier-Stokes loss function convergence analysis\\\", mode=web)\", \"/math(Derive the full composite loss function for the Navier-Stokes PINN, incorporating Euler-Lagrange principles for the fluid dynamics PDE residual and ensuring the boundary condition penalties guarantee convergence.)\", \"/code(Implement the PINN in Python with JAX, using automatic differentiation for the Navier-Stokes operators. The implementation must be deployed on a CUDA-enabled runtime and track convergence metrics throughout training.)\", \"/math(Verify the numerical implementation by checking the computed PDE residual against a known analytical solution, like Kovasznay flow, analyzing the convergence rate and ensuring it aligns with theoretical expectations for Chebyshev collocation points.)\"], \"route_rationale\": \"The blue /general-search agent first gathers state-of-the-art formulations from recent literature. This informs the /math agent's formal derivation of the complex, physics-based loss function. The /code agent then translates this mathematical specification into a JAX implementation. A final /math step is critical for verifying that the code's numerical behavior correctly matches the derived physical principles.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the Navier-Stokes residual loss function.\", \"A Python script implementing the PINN solver using JAX and Optax.\", \"A results summary in JSON format, containing final loss values, L2 relative error, and convergence plots for velocity/pressure fields.\", \"A summary of findings from the literature search, including citations for at least two relevant arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Research current best practices for formulating PINN loss functions for the Navier-Stokes equations, focusing on stability and methods for handling boundary conditions.\", \"2. Formally derive the composite loss function, clearly separating the PDE residual terms (momentum and continuity equations) from the Dirichlet boundary condition terms.\", \"3. Implement the PINN architecture and the custom, physics-informed loss function in JAX, leveraging its automatic differentiation capabilities to compute the necessary partial derivatives for the PDE residual.\", \"4. Create a verification suite by testing the implementation on a simplified problem with a known analytical solution (e.g., Kovasznay flow) and validate that the PDE residual component of the loss approaches zero.\", \"5. Train the model on the lid-driven cavity benchmark and audit the convergence of the individual loss components and the physical plausibility of the resulting velocity and pressure fields.\", \"6. Check the final predicted velocity field against established high-fidelity CFD simulation results to quantify the L2 relative error.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. Search results provide the theoretical foundation for the mathematical derivation. The derivation acts as a formal specification for the JAX implementation. The code's numerical output is passed back to the math agent for rigorous verification. Fallback: If verification fails (e.g., residual does not converge), the task is re-routed to /code with specific error analysis from /math.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on PINNs for Navier-Stokes equations.\", \"- [ ] /math: Derive the complete loss function, including all PDE residual and boundary terms.\", \"- [ ] /code: Implement the PINN model, custom loss function, and training loop in JAX.\", \"- [ ] /code: Set up the lid-driven cavity benchmark, including geometry, boundary conditions, and collocation point sampling.\", \"- [ ] /math: Formulate a verification test case using a known analytical solution to the Navier-Stokes equations.\", \"- [ ] /code: Execute the verification test and log the PDE residual error over training.\", \"- [ ] /math: Verify that the code implementation correctly minimizes the PDE residual according to the verification test logs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-neural-networks\", \"numerical-pde\", \"scientific-computing\"], \"acceptance_criteria\": [\"- The mathematical derivation of the loss function is complete, correct, and references the vector calculus identities used.\", \"- The JAX implementation successfully trains, and the total loss decreases monotonically to a value below 1e-4.\", \"- The verification step demonstrates that for a known analytical solution, the computed PDE residual is minimized to a value below 1e-5.\", \"- The final report cites at least two relevant arXiv papers with full IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 393, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on a physics-informed neural network (PINN) to solve the 2D incompressible Navier-Stokes equations for lid-driven cavity flow. I need a rigorous derivation of the composite loss function, treating the pressure and velocity fields as outputs. How can we formulate the differential operators using tensor calculus and ensure the network respects the divergence-free constraint? Furthermore, what are the theoretical convergence guarantees for such a formulation in the context of Sobolev spaces, and how can we verify the implementation's correctness against the analytical formulation?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org physics-informed neural network convergence theory Navier-Stokes Sobolev\\\", mode=rag)\", \"/math(Derive the strong form of the Navier-Stokes PDE residual using tensor calculus and formulate the composite loss, including boundary and incompressibility terms. Reference Euler-Lagrange principles for the variational form.)\", \"/code(Implement the PINN architecture and the derived loss function in JAX. Use automatic differentiation for differential operators and target CUDA for acceleration, ensuring the implementation reports convergence metrics., using Python)\", \"/math(Verify the auto-differentiated gradients from the JAX code against the analytical gradients derived for a simplified potential flow case. Analyze the convergence plot to check for stability and signs of mode collapse.)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature on PINN convergence. Then, the math agent provides a rigorous theoretical derivation. This is followed by the code agent implementing the derived model. A final math agent step verifies the implementation's correctness against the theory, ensuring a robust and validated solution.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Navier-Stokes PINN loss function in LaTeX.\", \"A runnable JAX/Optax Python script implementing the PINN solver.\", \"A report containing convergence plots (loss vs. epoch) and a quiver plot of the final velocity field.\", \"A verification document comparing analytical vs. automatic differentiation gradients for a test case.\", \"A bibliography with citations for at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the incompressible Navier-Stokes equations in their strong form, identifying all terms (transient, convective, pressure, diffusion) that will constitute the PDE residual.\", \"2. Derive the composite loss function as a weighted sum of mean squared errors: the PDE residual over collocation points, the boundary condition mismatch, and the divergence-free constraint violation.\", \"3. Implement the neural network in JAX, using its gradient functionalities (`jax.grad`, `jacfwd`, etc.) to represent the differential operators (e.g., gradient, Laplacian, divergence) in the loss function.\", \"4. Verify the correctness of the coded differential operators by testing them on known analytical functions and comparing the output with hand-calculated derivatives.\", \"5. Train the PINN model on the lid-driven cavity problem, monitoring individual loss components to diagnose training dynamics.\", \"6. Validate the final trained model by evaluating the Mean Squared PDE Residual and Max Divergence Error on a fine grid of unseen points within the domain.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent for the derivation. The math agent's derivation PDF is the spec for the code agent. The code agent provides its script and gradient outputs to the second math agent for verification. Verification failure (gradient mismatch > 1e-6) triggers a fallback to the code agent to debug the operator implementation. The final artifacts are passed to the router for a quality assurance review.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key arXiv papers on PINN convergence for fluid dynamics.\", \"- [ ] /math: Produce a PDF document with the full derivation of the Navier-Stokes loss function.\", \"- [ ] /code: Write a JAX script to implement and train the PINN, saving the model and metrics.\", \"- [ ] /math: Verify that the JAX implementation of the divergence and Laplacian operators matches analytical results for a known vector field.\", \"- [ ] /code: Generate convergence plots and a final quiver plot of the velocity field.\", \"- [ ] /math: Write a short report analyzing the convergence results and the verification outcomes.\", \"- [ ] router: Review all artifacts for correctness, consistency, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"tensor-calculus\"], \"acceptance_criteria\": [\"The mathematical derivation in the report must be complete and rigorously follow tensor notation.\", \"The JAX implementation must successfully train, with the total loss converging below 1e-4.\", \"The final report must include plots showing the velocity field and a histogram of the divergence error, with the mean error below 1e-3.\", \"The bibliography must cite at least two arXiv preprints with their corresponding IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 405, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for a Bayesian neural network with a Bernoulli likelihood and Gaussian priors, then implement mean-field variational inference to approximate the posterior. Validate the algorithm's convergence guarantees on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) for a Bayesian neural network with a Bernoulli likelihood and standard Gaussian priors, ensuring all steps in the Kullback-Leibler divergence expansion are explicitly shown.)\", \"/general-search(query=\\\"site:arxiv.org mean-field variational inference convergence guarantees Bayesian neural networks\\\", mode=both)\", \"/code(Implement the derived mean-field variational inference algorithm in Python using PyTorch, optimizing the ELBO. Validate the implementation by checking for monotonic convergence of the ELBO on a synthetic dataset and report the final log-likelihood.)\"], \"route_rationale\": \"/math is first to establish the theoretical foundation by deriving the objective function (ELBO). The 'blue' /general-search agent then gathers supporting literature on the algorithm's theoretical convergence properties. Finally, /code implements the derived model and empirically validates the theory.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the ELBO.\", \"A Python script implementing the mean-field VI algorithm for the specified Bayesian neural network.\", \"A summary report citing at least two academic papers on VI convergence, with arXiv IDs.\", \"A plot showing the ELBO convergence over training iterations.\", \"A JSON file containing the final ELBO value and model parameters.\"], \"thinking_outline\": [\"1. Define the probabilistic graphical model for the Bayesian neural network, specifying the likelihood and prior distributions.\", \"2. Formulate the variational inference problem by defining the variational family (mean-field Gaussian) and the objective (ELBO).\", \"3. Derive the closed-form update equations for the variational parameters by taking gradients of the ELBO.\", \"4. Implement the algorithm, paying close attention to numerical stability and correct gradient estimation for the stochastic optimization.\", \"5. Validate the implementation by running it on a known synthetic dataset and checking if the ELBO monotonically increases, which is a key convergence diagnostic.\"], \"handoff_plan\": \"/math produces the ELBO derivation -> /general-search uses keywords from the derivation to find convergence proofs -> /code implements the math and verifies its convergence against the properties found by search. router QA (verification: code's ELBO plot must be monotonic; fallback: if not, flag the /math derivation for potential errors).\", \"todo_list\": [\"- [ ] /math: Derive the complete ELBO for a Bayesian neural net with Bernoulli likelihood.\", \"- [ ] /general-search: Find two seminal papers on the convergence properties of mean-field VI.\", \"- [ ] /code: Implement the mean-field VI algorithm in PyTorch.\", \"- [ ] /code: Generate a synthetic dataset for a binary classification task.\", \"- [ ] /code: Validate the implementation by plotting the ELBO vs. training iterations to confirm monotonic increase.\", \"- [ ] /router: Perform final QA by checking if the implemented ELBO matches the derivation and if convergence is demonstrated.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"probabilistic-models\"], \"acceptance_criteria\": [\"The mathematical derivation of the ELBO must be complete and correct.\", \"The Python code must run without errors and produce a plot showing a monotonically increasing ELBO.\", \"The final report must cite at least two relevant arXiv papers with valid IDs.\", \"The final reported ELBO value must be within a reasonable range for the synthetic data.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 434, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for a score-based generative model, starting from a forward variance-preserving SDE. Please connect this derivation to the Fokker-Planck equation and justify how the score function, \u2207_x log p_t(x), emerges as the key component for the drift term. Then, implement this reverse SDE sampler in Python.\n", "completion": "{\"route_plan\": [\"/math(Derive the reverse-time SDE from the forward process dxt = f(xt, t)dt + g(t)dwt by linking the Fokker-Planck partial differential equation to its adjoint, showing how the score function informs the drift term and analyzing its convergence properties.)\", \"/code(Implement a PyTorch-based Euler-Maruyama solver for the derived reverse-time diffusion SDE. Validate the sampler on a 2D Gaussian mixture dataset, tracking the Fr\u00e9chet Inception Distance (FID) and ensuring numerical stability., using Python)\", \"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time stochastic differential equation bayesian\\\", mode=both)\"], \"route_rationale\": \"The /math agent is required for the rigorous theoretical derivation of the reverse-time SDE from first principles involving stochastic calculus. The /code agent is needed to translate this theory into a practical Python implementation and validate its generative capabilities via FID metrics. The \\\"blue\\\" /general-search agent is used last to find authoritative arXiv preprints to cross-reference and cite, ensuring the derivation and implementation align with the established scientific literature.\", \"expected_artifacts\": [\"- A LaTeX document containing the full mathematical derivation of the reverse-time SDE from the Fokker-Planck equation.\", \"- A Python script (`sampler.py`) implementing the score-based sampler for a toy 2D dataset.\", \"- A plot (`samples.png`) visualizing the generated data distribution vs. the true distribution.\", \"- A verification report (`verification.md`) confirming the derivation matches literature and the code's FID score.\", \"- A `references.bib` file with at least two citations to foundational arXiv papers.\"], \"thinking_outline\": [\"1. Define the forward variance-preserving SDE and derive its corresponding Fokker-Planck equation which governs the evolution of the probability density p_t(x).\", \"2. Leverage the time-reversal theorem for diffusions (e.g., Anderson's theorem) to postulate the form of the reverse-time SDE.\", \"3. Explicitly derive the reverse drift term, demonstrating that it is composed of the forward drift and a term proportional to the gradient of the log-probability density (the score function).\", \"4. Validate the mathematical derivation by comparing the derived reverse-time SDE against foundational papers on score-based modeling (e.g., Song et al.).\", \"5. Implement a numerical solver (e.g., Euler-Maruyama) for the reverse-time SDE in Python to sample from the data distribution.\", \"6. Verify the implementation by generating samples from a simple distribution (e.g., a Gaussian mixture) and assessing their quality using the FID score, checking for divergence or instability.\"], \"handoff_plan\": \"/math produces the formal SDE derivation -> /general-search finds reference papers for validation -> /math verifies its derivation against literature -> /code implements the verified SDE and computes metrics -> router QA (verification: code output matches theoretical distribution properties, FID is below threshold; fallback: if derivation is incorrect, re-run /math with context from /general-search; if implementation fails, debug /code with focus on numerical stability).\", \"todo_list\": [\"- [ ] /math: Derive the reverse-time SDE drift term using the Fokker-Planck equation and the definition of the score function.\", \"- [ ] /general-search: Find at least two foundational arXiv papers on score-based generative models and SDEs (e.g., by Yang Song, Stefano Ermon).\", \"- [ ] /math: Verify that the derived SDE form precisely matches the formulation in the retrieved literature.\", \"- [ ] /code: Implement the Euler-Maruyama integrator for the reverse-time SDE in PyTorch, assuming a pre-trained score network.\", \"- [ ] /code: Generate 1000 samples from a 2D Gaussian mixture model and compute the FID score against 1000 true samples.\", \"- [ ] /math: Audit the Python implementation's discretization scheme for potential sources of numerical instability or divergence not predicted by the continuous-time theory.\", \"- [ ] router: Review all artifacts for correctness, consistency, and fulfillment of acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-modeling\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly relates the forward and reverse SDE drift terms via the score function `\u2207_x log p_t(x)`.\", \"- The Python implementation successfully generates samples without numerical errors (NaNs or infinities).\", \"- The generated samples for the 2D Gaussian mixture achieve an FID score below 50.\", \"- The final report includes citations for at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 444, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nPlease provide a full theoretical derivation of the reverse-time stochastic differential equation (SDE) used in score-based generative models. Start from the forward process SDE and its associated Fokker-Planck equation. Show explicitly how the reverse drift term is defined by the score function, \u2207_x log p_t(x). Then, implement a simple 1D toy example in Python to numerically validate that a neural network trained with denoising score matching can accurately approximate this score function, thus enabling sampling via the reverse SDE.\n", "completion": "{\"route_plan\": [\"/math(Derive the reverse-time stochastic differential equation (SDE) for a diffusion process, starting from the forward Fokker-Planck equation and showing how the drift is determined by the score function, `\u2207_x log p_t(x)`. Ensure the derivation is rigorous and self-contained.)\", \"/code(Implement a 1D score-based generative model in Python, using the derived SDE. Validate the model by comparing the learned score to the true score of a perturbed Gaussian mixture. Analyze the model's convergence properties from a Bayesian perspective.)\", \"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time SDE Fokker-Planck\\\", mode=web. Gather seminal and recent papers discussing the theoretical links between score matching, diffusion processes, and their eigenvalue spectrum implications.)\"], \"route_rationale\": \"The route starts with /math to establish the core theoretical foundation by deriving the reverse-time SDE. This derivation is then passed to /code for empirical validation and implementation, grounding the theory in a practical example. Finally, the blue /general-search agent is tasked with gathering authoritative literature to provide context, justify the theoretical claims, and cite foundational work.\", \"expected_artifacts\": [\"- A full mathematical derivation of the reverse-time SDE in LaTeX/PDF format.\", \"- A Python script implementing the 1D score-based model and sampler.\", \"- A plot comparing the learned score function against the true score function at different time steps.\", \"- A summary report including the derivation, code analysis, validation plot, and a bibliography with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Define the forward diffusion process SDE and derive its corresponding Fokker-Planck partial differential equation, which governs the evolution of the probability density p_t(x).\", \"2. Invoke the time-reversal theorem for diffusion processes (e.g., Anderson's theorem) to derive the form of the reverse-time SDE, identifying the reverse drift term's dependency on the score, \u2207_x log p_t(x).\", \"3. Formally demonstrate that the objective of denoising score matching provides a method for learning this score function using a neural network.\", \"4. Implement a 1D diffusion model in Python, simulating the forward process on a simple data distribution (e.g., a Gaussian mixture) and training a score network.\", \"5. Validate the implementation by generating a plot that overlays the learned score function with the analytically computed true score function for several values of time t.\", \"6. Audit the generated samples from the reverse SDE to check if their distribution converges to the original data distribution as the number of discretization steps increases.\"], \"handoff_plan\": \"/math (derivation) -> /code (implementation & validation). /general-search (literature) -> /math & /code (context & citations). The SDE derived by /math is the direct specification for the sampler implemented by /code. The papers from /general-search are used by both to ensure correctness and provide context. All artifacts are passed to a final router QA review. Verification: /code's output plot must visually confirm the learned score matches the true score. Fallback: If verification fails, /math will review the SDE discretization scheme used by /code for potential errors.\", \"todo_list\": [\"- [ ] /math: Derive the reverse-time SDE from the Fokker-Planck equation.\", \"- [ ] /math: Prove the connection between the reverse drift and the score function.\", \"- [ ] /code: Implement the forward process, score network, and training loop.\", \"- [ ] /code: Implement the reverse-time Euler-Maruyama or other appropriate SDE solver for sampling.\", \"- [ ] /general-search: Find and summarize at least two foundational papers on score-based generative modeling.\", \"- [ ] /math: Verify that the discretization scheme implemented in the /code agent's solution is a valid approximation of the continuous-time SDE derived.\", \"- [ ] /code: Generate plots comparing the learned score to the true score to validate the model's correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"score-matching\"], \"acceptance_criteria\": [\"- The mathematical derivation must be complete, logically sound, and follow directly from first principles.\", \"- The Python code must be executable and produce a validation plot clearly showing the learned score closely matching the true score for t > 0.\", \"- The final report must cite at least two relevant arXiv papers with correct IDs and integrate their findings.\", \"- The KL divergence between the generated and target distributions must be below a pre-specified threshold (e.g., 0.05).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 446, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive the Natural Policy Gradient (NPG) update rule from first principles, emphasizing its connection to the Fisher Information Matrix (FIM) as a Riemannian metric on the parameter space. Then, implement this using a Kronecker-factored approximation (K-FAC) for a deep neural network policy in a continuous control environment and empirically validate its superior convergence properties against the vanilla REINFORCE algorithm.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Natural Policy Gradient Fisher Information Matrix control theory Kakade\\\", mode=web)\", \"/math(Derive the trust-region optimization objective for policy improvement and show how the Natural Policy Gradient arises from using the Fisher Information Matrix as the Hessian, ensuring the derivation covers KL-divergence constraints and convergence properties.)\", \"/code(Implement both vanilla policy gradient and Natural Policy Gradient with K-FAC in Python/PyTorch for a classic control theory benchmark like Pendulum-v1, tracking average reward and policy KL-divergence for convergence analysis.)\", \"/math(Analyze the empirical convergence data from the code artifact, formally connecting the observed performance lift of NPG to the theoretical derivation and the preconditioning effect of the FIM inverse on the optimization landscape.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the work in seminal literature. This is followed by /math for the core theoretical derivation. /code is then used to translate the theory into a testable implementation for empirical validation. A final /math step closes the loop by analyzing the experimental results against the initial theory, ensuring a rigorous math-to-code-to-math verification cycle.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Natural Policy Gradient in LaTeX/PDF format.\", \"- A clean, commented Python script implementing both vanilla PG and NPG with K-FAC.\", \"- A results report containing convergence plots (reward vs. episodes) comparing the two algorithms.\", \"- Verification output confirming the K-FAC implementation's numerical stability.\", \"- A bibliography with citations to at least 3 relevant academic papers.\"], \"thinking_outline\": [\"1. Research the foundational papers on Natural Policy Gradients, particularly the work by Amari and Kakade, to understand its information geometry origins.\", \"2. Formally derive the NPG update rule as a solution to a trust-region problem constrained by KL-divergence, explicitly defining the Fisher Information Matrix.\", \"3. Detail the Kronecker-factored (K-FAC) approximation for the FIM, which makes computing its inverse tractable for deep neural networks.\", \"4. Implement the algorithms and run experiments on a suitable continuous control benchmark, collecting data on reward, sample efficiency, and update stability.\", \"5. Validate that the K-FAC implementation is numerically stable and correctly approximates the FIM by checking the condition number of the factors.\", \"6. Verify that the empirical results from the code align with the theoretical expectation that NPG provides faster and more stable convergence than vanilla PG.\"], \"handoff_plan\": \"/general-search (literature) -> /math (derivation) -> /code (implementation). The code agent must use the exact formula from the /math derivation. A verification step checks for discrepancies between the math and code. If the code's convergence is poor, it triggers a fallback to /math to re-audit the derivation for potential errors before proceeding to the final analysis.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on Natural Policy Gradient (Amari, Kakade) and K-FAC (Martens & Grosse) (verify results)\", \"- [ ] /math: Produce a self-contained LaTeX derivation of the NPG update from a KL-constrained trust-region optimization perspective.\", \"- [ ] /code: Implement the baseline vanilla policy gradient (REINFORCE) agent for the `Pendulum-v1` environment.\", \"- [ ] /code: Implement the NPG agent using a K-FAC module to approximate the FIM inverse for the policy network.\", \"- [ ] /code: Run experiments for both agents, logging episodic rewards and policy KL divergence per update.\", \"- [ ] /math: Verify that the K-FAC implementation logic in the code directly corresponds to the mathematical formulation of the approximation.\", \"- [ ] /math: Analyze the generated plots and metrics to write a formal conclusion comparing empirical performance to theoretical guarantees.\", \"- [ ] Router QA: Review all artifacts for consistency, correctness, and fulfillment of acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"policy-gradient\", \"control-theory\", \"information-geometry\"], \"acceptance_criteria\": [\"- The LaTeX derivation must be mathematically sound, clearly linking the FIM to the KL-divergence constraint.\", \"- The final Python script must be executable and successfully train both a vanilla PG and an NPG agent on the specified environment.\", \"- The final report must include a plot demonstrating that the NPG agent achieves a higher average reward faster (fewer episodes) than the vanilla PG agent.\", \"- The project must cite at least 3 foundational papers on the topic with proper arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 455, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the evidence lower bound (ELBO) for a Bayesian Canonical Polyadic (CP) tensor decomposition model assuming Gaussian priors on the factor matrices. Then, implement a stochastic variational inference algorithm in Python using PyTorch to learn disentangled latent representations from a synthetic tensor, and validate the model's convergence and reconstruction fidelity against a standard alternating least squares (ALS) baseline.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian CANDECOMP/PARAFAC tensor decomposition variational inference\\\", mode=web)\", \"/math(Derive the evidence lower bound (ELBO) for the Bayesian CP model with Gaussian priors, ensuring the derivation supports efficient CUDA implementation and addresses convergence criteria.)\", \"/code(Implement the stochastic variational inference algorithm in PyTorch, alongside a classic ALS baseline for performance comparison, tracking reconstruction error and ensuring CUDA compatibility for future federated learning simulations., using Python)\", \"/math(Verify the analytical gradient of the ELBO with respect to one factor matrix against a numerical estimate from the Python code, checking the convergence properties of the Euler-Lagrange update.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on Bayesian tensor models, as this is a research-level topic. The /math agent then takes these findings to produce a formal derivation. This derivation is the blueprint for the /code agent to implement the SVI algorithm and a baseline. Finally, /math is re-engaged for a critical verification step, cross-validating its analytical gradients against numerical estimates from the code, ensuring correctness before full-scale experimentation.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the ELBO.\", \"A Python script implementing both Bayesian CP (SVI) and ALS algorithms, with synthetic data generation.\", \"A Jupyter notebook comparing the convergence plots (ELBO, reconstruction error) and factor recovery metrics for both methods.\", \"A summary report citing at least two relevant arXiv papers on the topic.\", \"A log file showing the output of the gradient verification check.\"], \"thinking_outline\": [\"1. Research foundational literature on Bayesian Canonical Polyadic (CP) decomposition models and stochastic variational inference (SVI) for tensors.\", \"2. Formulate the full probabilistic model, specifying priors and variational distributions, then derive the Evidence Lower Bound (ELBO) analytically.\", \"3. Implement the SVI algorithm in PyTorch, including the data generation process for a synthetic tensor with known ground-truth factors, and an Alternating Least Squares (ALS) baseline for comparison.\", \"4. Validate the correctness of the ELBO gradient derivation by comparing the analytical gradient with a numerical approximation (e.g., finite differences) computed by the code.\", \"5. Audit the model's performance by training it on the synthetic data, checking for ELBO convergence, and evaluating the final reconstruction error and Factor Match Score against the ALS baseline.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math (verification). The 'blue' search agent provides foundational papers to the math agent for the derivation. The math agent hands the complete derivation to the code agent for implementation. The code and math agents then collaborate on verification. Verification: numerical gradient from code must match analytical gradient from math within a tight tolerance. Fallback: if gradients mismatch, the math agent must revise the derivation and the code agent must adjust the implementation accordingly.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Bayesian CP/PARAFAC decomposition using variational inference.\", \"- [ ] /math: Derive the full ELBO for the specified probabilistic model.\", \"- [ ] /code: Implement the SVI training loop, model components, and synthetic data generator.\", \"- [ ] /code: Implement a standard ALS algorithm as a performance baseline.\", \"- [ ] /math: Formulate the analytical gradient of the ELBO with respect to a single factor matrix's variational parameters.\", \"- [ ] /code: Implement a numerical gradient checker and run the verification against the analytical gradient.\", \"- [ ] /code: Execute comparison experiments and generate all required plots and metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"bayesian-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO must be complete, self-contained, and mathematically sound.\", \"- The Python implementation must run without errors, show monotonic improvement of the ELBO during training, and achieve a final relative reconstruction error below 0.05 on the synthetic dataset.\", \"- The analytical gradient must match the numerical gradient check with an absolute difference of less than 1e-4.\", \"- The final report must include all specified artifacts and cite at least two relevant papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 478, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the identification formula for the average treatment effect P(Y(a=1)) - P(Y(a=0)) in a non-parametric structural causal model with unobserved confounding, given a valid instrumental variable Z. Formalize the proof using Pearl's do-calculus and verify the derivation's consistency with the back-door and front-door adjustment criteria where applicable. Then, implement a simulation in Python to validate the derived estimator's unbiasedness under generated data from a known SCM, assessing its convergence properties.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org non-parametric instrumental variable do-calculus identification causal inference\\\", mode=rag)\", \"/math(Derive the identification formula for the causal ATE using do-calculus rules on the SCM graph, proving the conditions under which the instrumental variable allows bypassing unobserved confounding. Reference seminal works from arxiv on Bayesian causal models.)\", \"/code(Implement a Python simulation of the SCM. Generate data, apply the derived causal estimator for the ATE, and use bootstrap methods to estimate confidence intervals. Track estimator convergence as sample size increases.)\", \"/math(Analyze the simulation output from the Python script. Formally verify the estimator's unbiasedness and consistency, and plot the convergence of the bootstrap confidence intervals against the true ATE.)\"], \"route_rationale\": \"The 'blue' /general-search agent first gathers foundational literature from authoritative sources like arXiv. /math then takes this context to perform the core theoretical derivation and proof. /code is used for empirical validation of the mathematical theory through simulation, which is a standard practice in statistics. A final /math call is needed to formally analyze the simulation results, connecting the empirical evidence back to the theory.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF containing the formal derivation and proof of the ATE identification formula.\", \"- A Python script (`simulation.py`) that implements the data generation process and the derived estimator.\", \"- A final report summarizing the simulation results, including plots showing estimator convergence and confidence interval coverage.\", \"- A bibliography with citations to at least two relevant arXiv papers.\", \"- A verification log documenting the alignment check between the mathematical formula and its code implementation.\"], \"thinking_outline\": [\"1. Define the non-parametric Structural Causal Model (SCM) graph, explicitly showing the treatment, outcome, instrumental variable, and the unobserved confounder.\", \"2. Apply Pearl's do-calculus rules (primarily rules 2 and 3) to the graph to manipulate the expression P(y|do(x)) until it is expressed solely in terms of observable probability distributions.\", \"3. Verify that the derived identification formula is consistent with the three core axioms of instrumental variables: relevance, exclusion restriction, and independence.\", \"4. Construct a Python simulation that generates data according to the SCM, then implement the derived formula as a statistical estimator.\", \"5. Execute the simulation across a range of sample sizes, calculating the ATE estimate and its bootstrap confidence interval at each size. Validate that the data generation process is correct.\", \"6. Analyze the simulation output to check for the estimator's unbiasedness (mean estimate vs. true ATE) and consistency (variance decreasing with sample size).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /math agent verifies the relevance of sources from /general-search before starting the proof. The /code agent implements the formula derived by /math, flagging any ambiguities. The final /math agent analyzes the code's output. Router QA performs a final consistency check across all artifacts. Fallback: If the derivation in /math is not possible, the agent should return to /general-search for alternative identification strategies.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on instrumental variable identification using do-calculus from arxiv.org.\", \"- [ ] /math: Formally derive the ATE identification formula using the provided SCM structure and IV assumptions.\", \"- [ ] /code: Implement the SCM data generating process and the derived causal estimator in Python using numpy/scipy.\", \"- [ ] /code: Run simulations to estimate the ATE and its bootstrap confidence interval across varying sample sizes.\", \"- [ ] /math: Verify that the implemented estimator in the Python code correctly reflects the mathematical derivation from the previous step.\", \"- [ ] /math: Analyze simulation results, plot convergence of the MSE, and formally report on the estimator's unbiasedness and consistency.\", \"- [ ] /router: Perform final QA review, checking for consistency between the proof, code implementation, and final analysis report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"structural-causal-models\"], \"acceptance_criteria\": [\"- The formal proof for ATE identification using do-calculus is mathematically sound, complete, and references the specific rules applied.\", \"- The Python simulation correctly implements the specified SCM and the derived estimator, and the code is well-commented.\", \"- The final report shows empirical evidence via plots and tables of the estimator's unbiasedness and convergence.\", \"- At least two relevant arXiv pre-prints are cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 487, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the computational complexity of determining the winner in a partisan variant of the game of Geography played on a directed graph. I need a formal proof of its PSPACE-completeness via a reduction from QSAT, a Python implementation of a solver for the tractable case of acyclic graphs using Sprague-Grundy values, and a brief survey of modern approximation algorithms for such adversarial pathfinding problems.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org combinatorial game theory PSPACE-completeness geography QSAT reduction\\\", mode=rag)\", \"/math(Derive the formal polynomial-time reduction from Quantified Boolean Formulas (QSAT) to partisan Geography, proving PSPACE-hardness and analyzing the game's complexity class., includes cybersecurity)\", \"/code(Implement a Python solver using dynamic programming to compute Sprague-Grundy values for partisan Geography restricted to directed acyclic graphs, ensuring adversarial correctness and convergence.)\", \"/math(Formally verify the correctness and polynomial-time complexity of the implemented acyclic graph solver, contrasting its performance with the general PSPACE-hard case., includes cybersecurity)\"], \"route_rationale\": \"/general-search (the blue agent) is used first to gather authoritative literature from arXiv on PSPACE-completeness proofs in combinatorial games. /math then uses this foundation to construct the formal reduction proof. /code implements the theory for a tractable subproblem, providing an empirical anchor. A final /math step formally analyzes and verifies the implemented algorithm's complexity and correctness, closing the theory-practice loop.\", \"expected_artifacts\": [\"* A formal proof document detailing the reduction from QSAT to partisan Geography, establishing PSPACE-completeness.\", \"* A Python script implementing a solver for the acyclic version of the game, including unit tests with known outcomes.\", \"* A summary report comparing the theoretical complexity of the general game with the implemented solver's efficiency.\", \"* A list of at least two relevant academic citations for approximation algorithms in this domain.\"], \"thinking_outline\": [\"1. Research the foundational theory of impartial and partisan games, the Sprague-Grundy theorem, and the complexity class PSPACE, focusing on reductions from QSAT.\", \"2. Construct a formal, step-by-step proof reducing an arbitrary QSAT instance to a partisan Geography game instance in polynomial time, thereby proving PSPACE-hardness.\", \"3. Implement an algorithm to compute Grundy numbers (nim-values) for the tractable case of directed acyclic graphs, demonstrating how the problem simplifies without cycles.\", \"4. Validate the Python implementation's output against manually computed nim-values for several small, non-trivial DAGs to ensure correctness.\", \"5. Formally analyze the time and space complexity of the implemented solver, proving it runs in polynomial time, and contrast this with the general case.\", \"6. Audit the entire reasoning chain, ensuring the formal proof is sound and the code correctly reflects the simplified theory for the acyclic case.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides papers to the math agent for the proof. The proof's logic informs the code agent's implementation for a simpler case. The code is then handed back to the math agent for formal verification of its complexity. Fallback: If the code fails verification against theoretical complexity, it is returned to the code agent with a specific counterexample graph. The final, verified package is reviewed by the router QA.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on PSPACE-completeness of positional games, especially reductions from QSAT to Geography.\", \"- [ ] /math: Construct the formal proof of PSPACE-hardness for partisan Geography.\", \"- [ ] /code: Implement the recursive solver with memoization for computing Sprague-Grundy values on DAGs.\", \"- [ ] /code: Write unit tests for the solver using at least three distinct acyclic graphs with known winning/losing positions.\", \"- [ ] /math: Verify the Python solver's time complexity is polynomial with respect to the number of vertices and edges.\", \"- [ ] /math: Write the final analysis comparing the tractable (DAG) and intractable (general graph) versions of the problem.\", \"- [ ] /router: Perform final QA check on all artifacts for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"computational-complexity\", \"pspace\"], \"acceptance_criteria\": [\"* The PSPACE-completeness proof must be logically sound, with a clear polynomial-time mapping from a QSAT instance to a game graph.\", \"* The Python code must correctly solve for the winner on any given directed acyclic graph and pass all provided unit tests.\", \"* The final report must correctly state the Big-O complexity of the implemented solver and cite at least two relevant arXiv papers.\", \"* The primary metric (Proof Soundness Score) must be at least 4/5, as evaluated by a domain expert.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 518, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the convergence properties of Stochastic Gradient Langevin Dynamics (SGLD) for sampling from the posterior of a Bayesian neural network. Can you derive the discrete-time update rule from the underlying continuous-time Langevin diffusion process, provide its theoretical non-asymptotic convergence guarantees under non-convexity, and then implement it in JAX to compare its uncertainty quantification against SGD with momentum on the CIFAR-10 dataset using Expected Calibration Error as the primary metric?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Stochastic Gradient Langevin Dynamics bayesian deep learning non-convex convergence\\\", mode=both)\", \"/math(Derive the SGLD update rule by discretizing the Langevin diffusion SDE. Formally prove the convergence rate bounds for the Bayesian posterior approximation, referencing key theorems from the arxiv sources.)\", \"/code(Implement the SGLD sampler in JAX for a ResNet on CIFAR-10. Track Expected Calibration Error and test accuracy metrics, ensuring numerical stability via gradient clipping and a proper learning rate schedule., using Python)\", \"/math(Analyze the empirical convergence data from the JAX implementation. Verify that the observed error decay aligns with the theoretical bounds derived earlier and audit the calibration plots for statistical consistency.)\"], \"route_rationale\": \"The plan starts with literature review via the blue /general-search agent to ground the task in current research. /math is then used for the formal theoretical derivation. /code implements the derived algorithm for empirical validation. A final /math step closes the loop by comparing theory to practice, which is crucial for advanced analysis.\", \"expected_artifacts\": [\"- A LaTeX formatted PDF containing the full mathematical derivation of the SGLD update rule and its convergence proof.\", \"- A Python script (`sglm_bnn_cifar10.py`) implementing the SGLD sampler and the Bayesian ResNet model using JAX.\", \"- A JSON file (`results.json`) reporting the final test accuracy, Expected Calibration Error (ECE), and negative log-likelihood (NLL).\", \"- A set of PNG plots visualizing the reliability diagram for uncertainty calibration and the convergence trace of the metrics.\", \"- A bibliography file (`references.bib`) with at least two cited arXiv preprints.\"], \"thinking_outline\": [\"1. Use search to find foundational papers on SGLD, focusing on its application to Bayesian deep learning and theoretical analyses of its convergence in non-convex landscapes.\", \"2. Formally derive the SGLD algorithm as a discretization of the Langevin stochastic differential equation (SDE) and outline the proof of its convergence rate, paying close attention to assumptions about the posterior distribution and gradient noise.\", \"3. Implement the SGLD sampler and a suitable Bayesian neural network architecture in code, including robust metric tracking for accuracy and uncertainty calibration.\", \"4. Validate the implementation by checking if the sampler explores the parameter space and if the gradient computations are numerically stable.\", \"5. Execute the experiment to gather empirical data on convergence speed and calibration performance.\", \"6. Verify that the empirical results are consistent with the theoretical guarantees from the derivation, analyzing any discrepancies and documenting the final comparison.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (analysis) -> router QA (verification: code output matches theoretical predictions; fallback: if divergence, re-run /code with adjusted hyperparameters or revisit /math assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find at least two key papers on SGLD theory and its application in deep learning.\", \"- [ ] /math: Produce a formal derivation of the SGLD update rule and its convergence proof based on the search results.\", \"- [ ] /code: Implement the SGLD sampler in JAX for a Bayesian ResNet on CIFAR-10.\", \"- [ ] /code: Instrument the code to log test accuracy and Expected Calibration Error (ECE) at each epoch.\", \"- [ ] /math: Verify that the discrete-time update rule implemented in the code correctly corresponds to the derived mathematical formula.\", \"- [ ] /math: Analyze the logged metrics from the code execution and compare the empirical convergence with the theoretical bounds.\", \"- [ ] /router: Perform final QA check, ensuring all artifacts are present and the final report synthesizes the theoretical and empirical findings.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-processes\", \"bayesian-inference\", \"mcmc\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly discretizes the Langevin SDE and states the assumptions for convergence.\", \"- The JAX implementation runs without errors and produces a final ECE metric below 0.05 on the CIFAR-10 test set.\", \"- The final report includes a reliability diagram plot and explicitly compares the empirical convergence rate to the derived theoretical bound.\", \"- At least two relevant arXiv papers are cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 537, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous comparison of privacy-preserving aggregation in federated learning. Specifically, analyze the theoretical convergence guarantees and communication complexity of systems using secure multi-party computation (SMPC) versus those employing differential privacy (DP) with noise addition. The analysis must account for non-IID data distributions across clients and consider potential Byzantine resilience. Provide both the formal mathematical derivations and an empirical simulation to validate the findings.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning convergence non-IID \\\\\\\"secure aggregation\\\\\\\" OR \\\\\\\"differential privacy\\\\\\\" byzantine\\\", mode=both)\", \"/math(Derive and compare the federated learning convergence guarantees for DP-SGD and an SMPC-based scheme, focusing on the impact of encryption noise and communication rounds on the final error bound.)\", \"/code(Implement a federated learning simulation in Python comparing DP-FedAvg and a secure aggregation protocol, measuring communication complexity and accuracy convergence on a non-IID CIFAR-10 split.)\", \"/math(Verify the simulation's empirical convergence against the derived theoretical bounds, calculating the confidence interval of the error gap between theory and practice under different privacy budgets.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational academic papers on privacy-preserving federated learning. This literature informs the /math agent's formal derivation of convergence bounds. The /code agent then implements a simulation to provide empirical evidence for the theoretical claims. A final /math step verifies the simulation's results against the theory, ensuring a robust and well-grounded analysis.\", \"expected_artifacts\": [\"- A formal mathematical derivation comparing convergence bounds for DP and SMPC-based federated aggregation.\", \"- A Python simulation script (e.g., using PyTorch/PySyft) that models the two privacy schemes.\", \"- A final report in Markdown format containing a literature review, the theoretical analysis, and plots of empirical results (e.g., accuracy vs. rounds, communication cost).\", \"- A verification document confirming the alignment between theoretical bounds and empirical measurements.\"], \"thinking_outline\": [\"1. Survey foundational literature from authoritative sources (arXiv, NeurIPS, ICML) on the convergence analysis of federated learning with differential privacy and secure multi-party computation, particularly under non-IID data settings.\", \"2. Formulate the optimization problems for both privacy-preserving schemes and formally derive their respective convergence bounds, highlighting how privacy parameters (noise variance, number of parties) affect the error terms.\", \"3. Implement a federated learning simulation environment that models training on a partitioned, non-IID dataset (e.g., CIFAR-10) and includes modules for both DP-based and SMPC-based model aggregation.\", \"4. Empirically measure and plot key metrics: model test accuracy vs. communication round, and total communication overhead vs. target accuracy for each privacy scheme.\", \"5. Verify that the empirical convergence rates observed in the simulation are consistent with the derived theoretical bounds, analyzing and explaining any significant deviations.\", \"6. Audit the simulation code for correctness in implementing the privacy mechanisms and communication cost models, then synthesize all findings into a comprehensive report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides seminal papers to the /math agent for derivation. The /math agent provides formal bounds and assumptions to the /code agent for implementation. The /code agent provides empirical results back to the /math agent for the final verification step. (Verification: Empirical curves must align with theoretical bounds within a reasonable error margin. Fallback: If divergence is large, the /code agent must debug the simulation's privacy or data-handling logic, or the /math agent must revisit simplifying assumptions in the derivation).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on convergence of federated learning with DP and SMPC under non-IID data distributions.\", \"- [ ] /math: Derive and formalize the convergence bounds for both privacy-preserving aggregation schemes based on the literature.\", \"- [ ] /code: Implement a Python simulation comparing DP-FedAvg and an SMPC-based aggregation protocol on a non-IID partitioned dataset.\", \"- [ ] /code: Instrument the simulation to log primary and secondary metrics (accuracy, loss, communication bytes per round).\", \"- [ ] /math: Verify that the empirical results from the code align with the theoretical bounds derived earlier by plotting both on the same axes.\", \"- [ ] /math: Quantify the gap between theory and practice and provide a statistical analysis.\", \"- [ ] /router: Perform final QA check, ensuring all acceptance criteria are met and artifacts are consistent and cross-referenced.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"differential-privacy\", \"secure-computation\"], \"acceptance_criteria\": [\"- The final report must contain a formal mathematical comparison of convergence bounds for DP and SMPC in federated learning.\", \"- The submitted Python code must be executable and reproduce the plots showing the trade-off between privacy, accuracy, and communication overhead.\", \"- The analysis must cite at least 3 relevant academic papers, including their arXiv IDs or DOIs.\", \"- The verification artifact must explicitly show the comparison between the theoretical bounds and the empirical simulation results, with a discussion of any discrepancies.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"For a fixed number of communication rounds T (e.g., T=100), report the final test accuracy achieved by the global model. This measures the utility of the model under a fixed communication budget.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Sum of the total bytes transmitted and received by the central server across all T rounds. This measures the communication efficiency of the privacy-preserving protocol.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 538, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the evidence lower bound (ELBO) for a sparse Variational Gaussian Process (VGP) model using inducing points, assuming a Mat\u00e9rn 5/2 covariance kernel. Then, implement a stochastic variational inference (SVI) algorithm in Python to optimize the VGP hyperparameters and inducing point locations for a Bayesian optimization task. Validate the model's convergence and calibration on a benchmark high-dimensional function.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org sparse variational gaussian process stochastic variational inference mat\u00e9rn kernel\\\", mode=rag)\", \"/math(Derive the Evidence Lower Bound (ELBO) for a sparse Bayesian Gaussian Process with Mat\u00e9rn 5/2 kernel, detailing the KL-divergence and expected log-likelihood terms for stochastic convergence.)\", \"/code(Implement the derived SVI algorithm in Python using JAX, optimizing inducing point locations and kernel hyperparameters. Track the ELBO and acquisition function values for a Bayesian optimization task.)\", \"/math(Analyze the convergence properties of the implemented stochastic optimization, verifying that the gradient updates for the variational parameters and inducing points correctly follow the derived Bayesian formulation.)\"], \"route_rationale\": \"The blue /general-search agent will first gather state-of-the-art literature on sparse VGPs and SVI. The /math agent will then formalize the problem by deriving the necessary equations. The /code agent will translate the math into a working implementation. A final /math step is crucial for theoretical verification of the code's convergence properties against the derived model.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the ELBO for the sparse VGP model.\", \"A Python script implementing the SVI-based Bayesian optimization loop, including the VGP model.\", \"A summary report comparing the optimizer's performance against a standard baseline, with plots showing ELBO convergence and posterior uncertainty.\", \"A bibliography citing at least 2 relevant research papers from arXiv with their IDs.\"], \"thinking_outline\": [\"1. Establish the theoretical framework for sparse Variational Gaussian Processes, defining the model with inducing points and the Mat\u00e9rn 5/2 kernel.\", \"2. Derive the Evidence Lower Bound (ELBO) as the objective function, explicitly formulating the expected log-likelihood and the KL-divergence between the variational and prior distributions.\", \"3. Develop the stochastic gradient ascent updates for the variational parameters and inducing point locations based on the derived ELBO.\", \"4. Implement the SVI algorithm and validate the implementation by checking the gradient computations on a simple, low-dimensional test case.\", \"5. Run the full Bayesian optimization experiment on a benchmark function (e.g., Hartmann6), and verify that the ELBO converges and the model provides sensible uncertainty estimates.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: Final mathematical analysis of convergence must align with empirical results from the code's execution; fallback: If divergence occurs, the /code agent must revisit the gradient implementation based on feedback from the /math agent).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on sparse VGPs (e.g., Titsias, 2009) and SVI for GPs (e.g., Hensman et al., 2013).\", \"- [ ] /math: Derive the ELBO for the sparse VGP model with a Mat\u00e9rn 5/2 kernel.\", \"- [ ] /math: Formulate the stochastic gradient updates for all model and variational parameters.\", \"- [ ] /code: Implement the VGP model and the SVI training loop in Python using JAX.\", \"- [ ] /code: Set up the Bayesian optimization experiment using the trained VGP as the surrogate model on the Hartmann6 function.\", \"- [ ] /math: Verify that the implemented gradient updates in the code match the derived mathematical expressions.\", \"- [ ] /code: Validate the model's convergence on the benchmark function by plotting the ELBO over iterations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-optimization\", \"stochastic-processes\", \"variational-inference\"], \"acceptance_criteria\": [\"The submitted LaTeX derivation of the ELBO must be mathematically sound and complete.\", \"The Python code must execute without errors and demonstrate monotonic convergence of the ELBO on the specified benchmark function.\", \"The final report must show that the implemented Bayesian optimizer finds a near-optimal value for the Hartmann6 function, achieving a final value within 5% of the known global minimum.\", \"The primary metric (Cumulative Regret) must be below a pre-specified threshold after 100 optimization steps.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 565, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the clipped surrogate objective function for Proximal Policy Optimization (PPO) starting from the Trust Region Policy Optimization (TRPO) formulation. Then, implement a PPO agent in PyTorch to solve the continuous control LunarLander-v2 environment, ensuring the implementation correctly handles generalized advantage estimation (GAE) and that the KL-divergence between old and new policies remains bounded. I need a formal proof of the lower bound property of the clipped objective and a performance analysis demonstrating convergence and sample efficiency.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Proximal Policy Optimization TRPO Generalized Advantage Estimation causal\\\", mode=rag)\", \"/math(Derive the PPO clipped surrogate objective from the TRPO conservative policy iteration, proving it forms a lower bound on improvement with respect to the KL-divergence and control theory constraints., includes cybersecurity)\", \"/code(Implement the PPO actor-critic agent in PyTorch with GAE for the LunarLander-v2 continuous control task, tracking convergence metrics and using a Bayesian interpretation for network uncertainty., using Python)\", \"/math(Audit the implemented loss function against the formal derivation, verifying the advantage normalization and clipping logic aligns with the proof's constraints on policy convergence., includes cybersecurity)\"], \"route_rationale\": \"The blue general-search agent is first used to gather the foundational TRPO and PPO papers from arXiv. This theoretical grounding is passed to /math for the formal derivation of the PPO objective. The resulting derivation serves as a precise specification for the /code agent to implement the algorithm in PyTorch. A final /math call is crucial for verification, ensuring the code's loss function is a faithful representation of the derived theory, which is a common source of bugs in policy gradient implementations.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the PPO clipped objective from the TRPO formulation.\", \"A standalone Python script implementing the PPO agent with Generalized Advantage Estimation (GAE).\", \"A Markdown report summarizing the training performance, including a learning curve plot (average reward vs. episodes).\", \"A verification log confirming the alignment between the mathematical derivation and the code's loss function.\", \"Citations for at least two relevant academic papers, including arXiv IDs.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation by reviewing the Trust Region Policy Optimization (TRPO) objective function and its KL-divergence constraint on policy updates.\", \"2. Formally derive the PPO clipped surrogate objective as a first-order approximation that penalizes large policy changes, proving it forms a lower bound on policy improvement.\", \"3. Implement the actor-critic architecture in PyTorch, incorporating Generalized Advantage Estimation (GAE) for variance reduction in the advantage calculation.\", \"4. Validate that the implemented loss function in the code directly corresponds to the derived clipped surrogate objective, paying close attention to the clipping mechanism and signs.\", \"5. Train the agent and analyze its convergence behavior, plotting the episodic rewards to ensure it reaches the environment's solved threshold.\", \"6. Check that hyperparameters (e.g., clipping epsilon, GAE lambda) are within standard ranges and document their impact on training stability.\"], \"handoff_plan\": \"/general-search (arXiv papers) -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The derivation from /math serves as a formal spec for /code. The implementation and derivation are then jointly reviewed by the second /math call for verification. Fallback: If verification fails, /code must refactor the loss function based on feedback from /math and resubmit.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize the key papers on TRPO (Schulman et al.), PPO (Schulman et al.), and GAE.\", \"- [ ] /math: Derive the PPO clipped surrogate objective from the TRPO formulation, explicitly showing the lower bound property.\", \"- [ ] /code: Implement the PPO agent in PyTorch with GAE for the LunarLanderContinuous-v2 environment.\", \"- [ ] /code: Instrument the implementation to log average episodic reward and an estimate of the KL-divergence per epoch.\", \"- [ ] /math: Verify that the PyTorch loss function in the code is a correct implementation of the derived clipped surrogate objective.\", \"- [ ] /code: Execute the training run and generate a learning curve plot for the final report.\", \"- [ ] /router: Conduct final QA review of all artifacts, ensuring consistency between derivation, code, and results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"actor-critic\"], \"acceptance_criteria\": [\"The mathematical derivation correctly shows how the PPO clipped objective relates to the TRPO objective with a KL-divergence constraint.\", \"The Python implementation successfully trains an agent that solves the LunarLanderContinuous-v2 environment (average score > 200 over 100 consecutive episodes).\", \"The final report includes a learning curve plot and properly cites at least two relevant arXiv papers with their IDs.\", \"The verification check between the derivation and code implementation is documented and confirms their alignment.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 566, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to conduct a comparative analysis of causal inference estimators for the Average Treatment Effect (ATE) under the unconfoundedness assumption. Specifically, derive the Augmented Inverse Propensity Weighting (AIPW) estimator, contrast its doubly robust properties with the standard IPW estimator, and implement a simulation study. Furthermore, incorporate a sensitivity analysis using the E-value to quantify the required strength of an unobserved confounder to explain away the observed causal effect. The analysis must be grounded in modern econometrics literature.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:.edu doubly robust AIPW estimator derivation causal inference econometrics E-value sensitivity analysis\\\", mode=web)\", \"/math(Derive the functional form of the Augmented Inverse Propensity Weighting (AIPW) estimator and prove its double-robustness property. Formulate the calculation for the E-value based on the observed causal effect and confidence interval.)\", \"/code(Implement a Python simulation to compare IPW and AIPW estimators. Generate data with a known ATE and simulate unobserved confounding. Use bootstrap resampling to compute variance and confidence interval metrics for each estimator.)\", \"/math(Verify that the simulation's empirical bias and variance for the AIPW estimator align with its theoretical properties, particularly checking for unbiasedness when either the propensity or outcome model is misspecified, which is a core causal econometrics check.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational papers on AIPW and E-values. The first /math agent then formalizes these concepts through derivation. /code agent operationalizes the theory in a simulation environment. The final /math agent provides a crucial verification step, ensuring the code's output matches the established mathematical theory before the final report is generated.\", \"expected_artifacts\": [\"A formal mathematical derivation of the AIPW estimator and the E-value calculation, formatted in LaTeX.\", \"A Python script (`simulation.py`) containing the data generating process, estimator implementations, and bootstrap analysis.\", \"A JSON file (`results.json`) reporting the bias, variance, and MSE for each estimator under different simulation conditions.\", \"A summary report (`report.md`) interpreting the results, explaining the concept of double robustness, and discussing the E-value's implications.\", \"A bibliography file (`references.bib`) with at least two cited articles from authoritative sources.\"], \"thinking_outline\": [\"1. Use /general-search to find seminal papers on doubly robust estimation (e.g., Robins, Rotnitzky) and modern sensitivity analysis techniques like the E-value (e.g., VanderWeele, Ding).\", \"2. Formally derive the AIPW estimator from first principles, showing how it combines a regression-based outcome model and a propensity score model.\", \"3. Prove the double-robustness property: the estimator is consistent if at least one of the two component models is correctly specified.\", \"4. Implement a simulation in Python where the true ATE is known and where the outcome and propensity models can be deliberately misspecified to test the double-robustness property.\", \"5. Validate the implemented estimators by checking that the AIPW estimator's bias approaches zero under misspecification of one model, unlike the standard IPW or regression estimators.\", \"6. Calculate and interpret the E-value for the simulated ATE, explaining how it quantifies resilience to unobserved confounding.\"], \"handoff_plan\": \"/general-search -> /math (derivations based on literature) -> /code (implementation of derivations) -> /math (verification of code against theory) -> router QA (verification: check for consistency between derivations, code, and results; fallback: request re-derivation from /math if verification fails).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 key papers on AIPW estimators and E-value sensitivity analysis.\", \"- [ ] /math: Provide a step-by-step derivation of the AIPW estimator and its variance.\", \"- [ ] /math: Formulate the equation for the E-value given an effect estimate and its confidence interval.\", \"- [ ] /code: Develop a Python simulation framework with a clear data generating process for a causal model.\", \"- [ ] /code: Implement IPW and AIPW estimators, including a bootstrap procedure for confidence intervals.\", \"- [ ] /math: Verify that the empirical results from the simulation for the doubly robust property match the theoretical derivation.\", \"- [ ] /code: Verify that the implemented bootstrap confidence intervals have the correct nominal coverage.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"econometrics\"], \"acceptance_criteria\": [\"The provided mathematical derivation for AIPW's double-robustness property must be complete and correct.\", \"The Python code must successfully run and generate results that empirically demonstrate the double-robustness property of AIPW (i.e., low bias when one model is misspecified).\", \"The final report must correctly calculate and interpret the E-value for the simulated treatment effect.\", \"The submission must cite at least two relevant, authoritative sources with DOIs or arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 577, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a detailed walkthrough of Differentiable Architecture Search (DARTS). Please provide a full mathematical derivation of the bilevel optimization gradient for the architecture parameters, including the Hessian-vector product approximation. Then, implement this in Python using the Gumbel-Softmax relaxation to validate the convergence on a proxy task and verify that the analytical gradient matches a numerical estimate.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org differentiable architecture search DARTS bilevel optimization gradient approximation\\\", mode=web)\", \"/math(Derive the gradient for the outer bilevel optimization variable (architecture parameters) using the implicit function theorem, focusing on the Hessian-vector product approximation for computational tractability and analyzing its convergence criteria.)\", \"/code(Implement the DARTS bilevel optimization loop in Python/JAX, using CUDA for acceleration. The implementation must include Gumbel-Softmax for categorical relaxation and track AUPRC metrics for the AutoML search process.)\", \"/math(Verify the numerical stability of the implemented Hessian approximation by analyzing its eigenvalue distribution and formally check the alignment between the derived analytical gradient and the code's finite-difference numerical gradient.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on DARTS. This informs the /math agent's formal derivation of the complex bilevel gradient. The /code agent then implements this derivation for empirical testing. A final /math step provides theoretical verification of the implementation's correctness and stability, ensuring a rigorous math-code loop.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the complete mathematical derivation of the DARTS gradient.\", \"A Python script (`main.py`) implementing the differentiable architecture search algorithm.\", \"A JSON file (`results.json`) reporting the final validation accuracy and the gradient approximation error.\", \"A verification report in Markdown (`verification.md`) detailing the comparison between the analytical and numerical gradients.\", \"A bibliography citing at least two seminal papers on the topic with their arXiv IDs.\"], \"thinking_outline\": [\"1. Research the foundational Differentiable Architecture Search (DARTS) paper to understand the continuous relaxation of the search space and the bilevel optimization formulation.\", \"2. Formulate the bilevel optimization problem mathematically and derive the gradient for the architecture parameters (alpha) by applying the chain rule and the implicit function theorem.\", \"3. Detail the finite difference approximation for the inverse Hessian-vector product, which makes the gradient computation tractable, and discuss its impact on convergence.\", \"4. Implement the complete DARTS training pipeline in Python, including the inner loop (weight optimization) and outer loop (architecture optimization) on a dataset like CIFAR-10.\", \"5. Verify the correctness of the implemented analytical gradient by writing a separate function that computes the numerical gradient via finite differences and asserts their closeness.\", \"6. Validate the entire process by analyzing the convergence of the architecture parameters and the performance of the discovered cell on the validation set.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal derivation) -> /code (implementation & empirical test). The code's numerical gradient outputs are then passed back to /math for a final verification step. Handoff includes a check of the analytical vs. numerical gradient L2 norm. Fallback: If the norm is high (>1e-3), the /math agent must revisit the derivation for potential errors, and the /code agent must check for implementation bugs.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Differentiable Architecture Search, such as the original DARTS paper.\", \"- [ ] /math: Produce the full derivation for the architecture gradient, clearly showing the Hessian approximation.\", \"- [ ] /code: Implement the DARTS bilevel training loop for a small search space on CIFAR-10.\", \"- [ ] /code: Implement a numerical gradient checker using finite differences.\", \"- [ ] /math: Verify that the analytical derivation is consistent with the numerical gradient computed by the /code agent, explaining any discrepancies.\", \"- [ ] /code: Run the full search, log metrics to `results.json`, and save the final discovered architecture.\", \"- [ ] /router: Perform final QA review of all artifacts, ensuring they meet the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"automl\", \"neural-architecture-search\", \"bilevel-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation in the PDF must be complete, rigorous, and match the implemented logic.\", \"- The Python code must execute without errors and converge to a stable architecture.\", \"- The L2 norm of the difference between the analytical and numerical gradients reported in `verification.md` must be less than 1e-4.\", \"- The final report must cite at least two relevant arXiv papers with IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"primary_guidance\": \"Final accuracy of the discovered architecture on the hold-out validation set after being fully trained.\", \"secondary_guidance\": \"L2 norm of the difference between the analytical gradient and a finite-difference numerical approximation, calculated at several random steps during the search.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 609, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Matern covariance kernel from its stochastic differential equation (SDE) representation, specifically for the case where the process is a solution to a linear time-invariant SDE driven by Gaussian white noise. Then, implement a Bayesian Gaussian Process regression model in JAX using this derived Matern kernel, and validate its posterior predictive convergence properties on a synthetic dataset with known ground truth.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:stat.ML.cmu.edu \\\\\\\"Gaussian Process\\\\\\\" \\\\\\\"stochastic differential equation\\\\\\\" Matern kernel Whittle-Sobolev representation\\\", mode=rag)\", \"/math(Derive the spectral density and covariance function for the Matern class from the stochastic differential equation solution, ensuring the Bayesian derivation handles boundary conditions and hyperparameter priors correctly, referencing findings from arxiv.)\", \"/code(Implement a Bayesian Gaussian Process regression model in JAX, using the derived Matern kernel. Include Cholesky decomposition for numerical stability and track the posterior predictive convergence via Negative Log Predictive Density., using Python)\", \"/math(Formally verify the implemented Matern kernel by checking the generated covariance matrix against the canonical analytical form for a discrete 1D grid, auditing for numerical precision issues and hyperparameter sensitivity.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on the SDE-GP connection. This informs the /math agent's formal derivation. The /code agent then translates this mathematical theory into a JAX implementation. A final /math step is crucial for verification, ensuring the code's numerical output matches the analytical derivation, which is a common failure point in complex statistical models.\", \"expected_artifacts\": [\"- A LaTeX document containing the full mathematical derivation of the Matern kernel from the SDE, including the spectral density.\", \"- A Python script with the JAX implementation of the Gaussian Process regression model.\", \"- A verification report showing the numerical comparison between the implemented kernel's output and the analytical form.\", \"- A plot of the posterior predictive distribution on the synthetic dataset, including confidence intervals and performance metrics.\", \"- A bibliography citing at least two foundational papers on the topic.\"], \"thinking_outline\": [\"1. Research the theoretical link between solutions to linear time-invariant stochastic differential equations driven by white noise and Gaussian Processes, focusing on the Whittle-Sobolev representation.\", \"2. Formally derive the Matern covariance function by first finding the spectral density of the SDE's solution using the Fourier transform, and then inverse-transforming to get the covariance.\", \"3. Implement the Matern kernel and a full GP regression model in JAX, paying attention to numerical stability (e.g., using Cholesky decomposition) and hyperparameter handling.\", \"4. Verify the analytical derivation against the implementation by generating a covariance matrix from the code for a simple 1D grid and comparing it element-wise against the known analytical formula.\", \"5. Validate the complete model's performance on a synthetic dataset by computing the Negative Log Predictive Density (NLPD) and Mean Squared Error (MSE) on a hold-out set.\", \"6. Consolidate the derivation, code, and validation results into a comprehensive report.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation) -> /math (verification). The /math agent uses papers from /general-search to produce a LaTeX derivation. /code implements this derivation. A second /math call verifies that the code's output matches the theory for a test case. Verification failure: The /math agent flags the discrepancy and sends it back to /code with specific counterexamples. Fallback: If derivation is intractable, loop back to /general-search with more specific queries about approximation methods.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers (e.g., Rasmussen & Williams, or work by Hartikainen & S\u00e4rkk\u00e4) on the SDE formulation of GP kernels.\", \"- [ ] /math: Produce a step-by-step derivation of the Matern covariance function from the SDE formulation.\", \"- [ ] /code: Implement the Matern kernel and a GP regression model in JAX.\", \"- [ ] /code: Implement metric calculation for NLPD and MSE for model validation.\", \"- [ ] /math: Create a verification script that compares the output of the coded Matern kernel with its analytical form.\", \"- [ ] /math: verify that the mathematical derivation in the final report is consistent with the code's logic.\", \"- [ ] /code: verify that the JAX implementation is numerically stable and includes comments referencing the derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-processes\", \"bayesian-inference\", \"gaussian-processes\"], \"acceptance_criteria\": [\"- The provided mathematical derivation of the Matern kernel must be correct and self-contained.\", \"- The JAX implementation must run without errors and produce a posterior predictive distribution for a given synthetic dataset.\", \"- The verification report must show a numerical difference of less than 1e-6 between the implemented kernel and the analytical formula for a test case.\", \"- The final report must achieve an NLPD score below a pre-specified threshold on the test set.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 629, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a Gaussian Process Latent Variable Model (GP-LVM) using a non-stationary kernel, for instance, one whose lengthscale is input-dependent. Please derive the full evidence lower bound (ELBO) for a Bayesian treatment of this model using stochastic variational inference. Then, implement this in Python using GPyTorch, demonstrating its ability to capture heteroscedastic noise on a synthetic dataset. The final analysis must include verification of the analytical gradients against the automatic differentiation implementation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian gp-lvm non-stationary kernel stochastic variational inference convergence\\\", mode=rag)\", \"/math(Derive the evidence lower bound (ELBO) for a Bayesian GP-LVM with an input-dependent lengthscale kernel, detailing the reparameterization trick for stochastic gradients and analyzing convergence criteria.)\", \"/code(Implement the derived Bayesian GP-LVM using GPyTorch, focusing on the stochastic optimization loop. Apply it to a synthetic finance dataset, tracking ELBO convergence and posterior predictive uncertainty., using Python)\", \"/math(Validate the GPyTorch implementation by performing a numerical gradient check, comparing the analytical gradients of the ELBO w.r.t. variational parameters against the framework's automatic differentiation output.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art approaches for non-stationary GP-LVMs. This literature informs the /math agent's theoretical derivation of the ELBO. The derived mathematics then provide a precise specification for the /code agent to implement the model. A final /math step is crucial for verifying that the complex code correctly implements the derived theory, closing the loop between theory and practice.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the ELBO and its gradients.\", \"A Python script (`.py`) containing the GPyTorch implementation of the Bayesian GP-LVM.\", \"A summary report (`.md`) analyzing model convergence and performance on the synthetic dataset, including plots of the learned latent space and predictive uncertainty.\", \"A verification script and its log output (`.txt`) confirming the successful comparison between analytical and numerical gradients.\", \"A bibliography file (`.bib`) with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the complete probabilistic graphical model for a Bayesian GP-LVM with a non-stationary kernel, defining priors and variational distributions.\", \"2. Derive the Evidence Lower Bound (ELBO) using Jensen's inequality, clearly separating the expected log-likelihood and the KL divergence terms for all latent variables and parameters.\", \"3. Implement the model and the stochastic variational inference (SVI) optimization loop in GPyTorch, ensuring proper mini-batching and gradient estimation using the reparameterization trick.\", \"4. Validate the implementation's correctness by writing a gradient checker function that compares the analytical gradients from the derivation with the numerical gradients computed by the framework.\", \"5. Execute the model on a synthetic dataset exhibiting heteroscedasticity, analyze the ELBO convergence plot, and visualize the learned latent variable manifold and predictive uncertainty.\", \"6. Audit the final report to ensure the theoretical derivations align perfectly with the code's logic and the empirical results are interpreted correctly.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results inform the mathematical derivation. The completed derivation is the blueprint for the coding task. The code artifact is then handed back to the math tool for gradient verification. The full package is reviewed by the router. Fallback: If gradient verification fails, the task reverts to the /code agent to debug the implementation against the /math agent's derivation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 recent arXiv papers on non-stationary kernels for GPs or SVI for GP-LVMs.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the ELBO for the specified model.\", \"- [ ] /code: Implement the GP-LVM model and the SVI training loop in a Python script.\", \"- [ ] /code: Generate a synthetic dataset and produce visualizations of the model's output.\", \"- [ ] /math: Derive the analytical gradients for key variational parameters.\", \"- [ ] /math: Implement and run a numerical gradient check to verify the code against the analytical derivation.\", \"- [ ] /code: Write a final markdown report summarizing the methodology and results, including all plots and citations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"gaussian-process\", \"variational-inference\", \"bayesian-machine-learning\"], \"acceptance_criteria\": [\"- The provided LaTeX derivation of the ELBO must be mathematically sound and complete.\", \"- The Python implementation must execute without errors, and the training process must show the ELBO converging on the provided synthetic data.\", \"- The gradient verification script must run successfully and show a difference between analytical and numerical gradients below a tolerance of 1e-4.\", \"- The final report must correctly interpret the results and include all specified visualizations and citations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 641, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and analyze an online dictionary learning algorithm for sparse signal representation, specifically focusing on its convergence properties. Derive the update rules from the Lagrangian formulation for an algorithm like Online Dictionary Learning (ODL) with l1-norm regularization. Then, implement it, test it on a synthetic dataset of sparse signals (e.g., sparse sinusoids), and compare its reconstruction error and dictionary mutual coherence against the Method of Optimal Directions (MOD). Critically, I need to see a verification of the empirical convergence rate against theoretical bounds discussed in recent literature.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org online dictionary learning convergence analysis K-SVD ODL Mairal\\\", mode=web)\", \"/math(Derive the block-coordinate descent update rules for Online Dictionary Learning (ODL), analyzing the Lagrangian with l1 sparsity. Focus on the conditions required for convergence and the expected decay rate based on the Hessian eigenvalue properties.)\", \"/code(Implement the ODL algorithm and a MOD baseline in Python. Generate synthetic sparse signals using a random Fourier basis. Track reconstruction SNR and dictionary mutual coherence over epochs, ensuring CUDA is not required for this complexity.)\", \"/math(Verify the empirical convergence curve from the ODL implementation against the theoretical bounds sourced from the literature. Analyze any discrepancies and check if the final dictionary's mutual coherence aligns with theoretical guarantees for sparse recovery., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find foundational papers on ODL convergence. /math then translates this theory into formal derivations. /code implements the derived algorithm for empirical validation. A final /math step is crucial for verification, closing the loop by comparing empirical results against the initial theory.\", \"expected_artifacts\": [\"- A formal PDF report containing the mathematical derivation of the ODL update rules and its convergence analysis.\", \"- A well-documented Python script implementing ODL, MOD, and the simulation environment.\", \"- A literature review summary citing at least two key papers with their arXiv IDs.\", \"- PNG plots showing Reconstruction SNR vs. Iteration and Dictionary Coherence vs. Iteration for both ODL and MOD.\", \"- A verification log file comparing empirical error decay rates to theoretical predictions.\"], \"thinking_outline\": [\"1. Research state-of-the-art online dictionary learning algorithms, focusing on ODL and K-SVD, and identify key papers that provide theoretical convergence guarantees.\", \"2. Formulate the dictionary learning optimization problem, derive the Lagrangian, and solve for the dictionary and sparse code update rules using block-coordinate descent or a similar iterative method.\", \"3. Implement the chosen algorithm and a suitable baseline (e.g., MOD) in Python. Design a synthetic data generation process that produces signals with a known sparse basis.\", \"4. Execute the simulation, collecting metrics such as mean squared error (or SNR) and the dictionary's mutual coherence at each iteration.\", \"5. Verify that the empirical convergence rate observed in the simulation aligns with the theoretical bounds established in the literature. Plot both on the same graph for comparison.\", \"6. Validate the quality of the learned dictionary by ensuring its atoms are normalized and its mutual coherence is sufficiently low to guarantee sparse recovery.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides foundational papers (e.g., arXiv PDFs) to the /math agent. The /math agent derives the update rules and theoretical bounds, which become the specification for the /code agent. The /code agent produces empirical data (plots, logs) which are then passed to the final /math agent for verification against the initial theory. Fallback: If the verification in the final step fails, the task is routed back to the /code agent to debug the implementation's convergence behavior.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize >=2 key papers on Online Dictionary Learning (ODL) convergence, extracting theoretical rate bounds.\", \"- [ ] /math: Derive the complete update equations for ODL based on its Lagrangian formulation.\", \"- [ ] /code: Implement the ODL algorithm, the MOD baseline, and the synthetic data generator.\", \"- [ ] /code: Run simulations and generate plots for Reconstruction SNR and Dictionary Mutual Coherence vs. iteration.\", \"- [ ] /math: Verify that the empirical error decay from the simulation matches the theoretical rate sourced from literature.\", \"- [ ] /code: Validate the learned dictionary's properties, specifically checking for normalized atoms and low mutual coherence.\", \"- [ ] router: Perform final QA review of the report, code, and verification log for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The submitted Python code must be executable and successfully reproduce the reported plots and metrics.\", \"- The final report must include a clear mathematical derivation of the algorithm's update rules from its objective function.\", \"- The analysis must explicitly compare the empirical convergence rate with a theoretical bound from a cited paper.\", \"- The dictionary coherence metric must be correctly calculated and reported.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute in dB as 10 * log10(variance(original_signals) / MSE(original_signals, reconstructed_signals)). Higher is better.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Compute as max_{i!=j} |d_i^T d_j| for all normalized dictionary columns d_i, d_j. Lower is better.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 644, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm exploring Physics-Informed Neural Networks for solving the 2D incompressible Navier-Stokes equations. How can I derive the strong and residual forms of the PDE for the loss function, and then implement a PINN in PyTorch that uses a Fourier feature mapping layer to improve convergence for high Reynolds number flows, where high-frequency solution components dominate? Please also validate the model's accuracy against a known analytical solution like the Kovasznay flow, tracking both MSE loss and the PDE residual norm.\n", "completion": "{\"route_plan\": [\"/math(Derive the strong and residual forms of the 2D incompressible Navier-Stokes equations for PINN loss construction. Focus on the momentum and continuity equations, analyzing the Fourier basis for capturing high-frequency solution components and its relation to kinetic energy spectra.)\", \"/code(Implement a PyTorch PINN incorporating a Fourier feature mapping layer to solve the Kovasznay flow problem. Use custom CUDA kernels for the differential operators to optimize performance and track convergence against the analytical solution., using Python)\", \"/general-search(query=\\\"site:arxiv.org physics informed neural networks fourier features \\\\\\\"navier stokes\\\\\\\" convergence spectral bias\\\", mode=rag)\"], \"route_rationale\": \"/math is first to establish the theoretical PDE formulation for the PINN loss. /code then implements this theory in PyTorch, providing an empirical validation of the Fourier feature approach. Finally, the blue /general-search agent gathers peer-reviewed literature to contextualize the method and justify its use over alternatives.\", \"expected_artifacts\": [\"A LaTeX document providing the full mathematical derivation of the Navier-Stokes PDE residual and the theory of Fourier feature mappings for mitigating spectral bias.\", \"A commented Python script (PyTorch) implementing the Fourier-feature PINN and the Kovasznay flow validation case.\", \"A summary report citing at least two relevant arXiv papers on PINNs, including performance plots (loss curves, PDE residual norm, error heatmaps).\", \"A JSON file containing final metrics: final MSE vs. analytical solution and final L2 norm of the PDE residual.\"], \"thinking_outline\": [\"1. Formulate the 2D incompressible Navier-Stokes equations (momentum and continuity) into their residual form, suitable for a deep learning loss function.\", \"2. Derive the mathematical justification for using Fourier feature mappings to mitigate the spectral bias of standard MLPs, enabling them to learn high-frequency functions.\", \"3. Implement the PINN architecture in PyTorch, ensuring automatic differentiation correctly computes all necessary partial derivatives (e.g., u_t, u_x, u_xx) for the PDE residual.\", \"4. Validate the implementation by comparing the PINN's solution for the Kovasznay flow problem against the known analytical solution. Check that the error decreases with training.\", \"5. Systematically evaluate the impact of the Fourier feature mapping's hyperparameters (scale and number of frequencies) on the final solution accuracy and convergence speed.\", \"6. Verify that the cited literature supports the observed empirical results and the choice of architecture for this class of fluid dynamics problems.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: The PDE residual derived by /math must exactly match the implementation in the /code loss function; fallback: If mismatch, /math re-derives and /code refactors the loss function. /code results must be contextualized by papers found by /general-search.)\", \"todo_list\": [\"- [ ] /math: Derive the residual form of the 2D incompressible Navier-Stokes equations.\", \"- [ ] /math: Formulate the theory for Fourier feature mappings mitigating spectral bias in MLPs.\", \"- [ ] /code: Implement the PINN with Fourier features in PyTorch, using automatic differentiation for PDE terms.\", \"- [ ] /code: Set up the Kovasznay flow validation case and implement metric tracking (MSE, residual norm).\", \"- [ ] /general-search: Find \u22652 recent arXiv papers on Fourier features for PINNs in fluid dynamics.\", \"- [ ] /math: Verify that the symbolic PDE residual from the derivation matches the computational graph implemented in the /code loss function.\", \"- [ ] /code: Validate that the final solution error is within an acceptable tolerance (<1e-4) of the analytical Kovasznay solution.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"fourier-analysis\"], \"acceptance_criteria\": [\"The mathematical derivation of the Navier-Stokes residual must be complete and correct.\", \"The Python code must successfully train the PINN and reproduce the Kovasznay flow solution with a final Mean Squared Error below 1e-4.\", \"The final report must correctly cite at least two relevant arXiv papers and include plots showing the convergence of both the training loss and the PDE residual norm over epochs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 645, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a Bayesian Neural Stochastic Differential Equation (SDE) model within a continuous-time variational autoencoder framework. The goal is to model high-frequency financial time-series data. Specifically, derive the evidence lower bound (ELBO) using a continuous-time prior based on the Ornstein-Uhlenbeck process, and then implement a numerically stable solver using the Euler-Maruyama method in JAX. The final deliverable must demonstrate ELBO convergence and robust uncertainty quantification on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian neural stochastic differential equation continuous time VAE finance\\\", mode=both)\", \"/math(Derive the continuous-time evidence lower bound (ELBO) for a Bayesian VAE with an Ornstein-Uhlenbeck SDE prior, ensuring the derivation handles the KL divergence term's path integral and respects Ito calculus conventions for financial models.)\", \"/code(Implement the Bayesian Neural SDE in JAX, using an Euler-Maruyama solver for the diffusion process. The implementation must track ELBO convergence and include routines for posterior predictive sampling to quantify uncertainty., using Python)\", \"/math(Audit the numerical stability of the implemented Euler-Maruyama solver and verify that the discretized loss function is a valid Monte Carlo approximation of the continuous-time ELBO, checking for convergence issues.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature. The first /math agent is tasked with the core theoretical derivation. The /code agent then translates this theory into a high-performance JAX implementation. A final /math agent is used for verification, cross-checking the numerical implementation against the formal theory to ensure correctness and stability.\", \"expected_artifacts\": [\"- A PDF document containing the complete mathematical derivation of the continuous-time ELBO.\", \"- A well-commented Python script (`main.py`) implementing the Bayesian Neural SDE in JAX.\", \"- A folder of plots (`./plots/`) showing ELBO convergence and posterior predictive distributions for uncertainty quantification.\", \"- A `metrics.json` file reporting the final test ELBO and posterior predictive variance.\", \"- A `report.md` summarizing the approach, results, and citing at least two relevant papers.\"], \"thinking_outline\": [\"1. Conduct a literature survey to understand the latest advancements in Neural SDEs and continuous-time Bayesian models for generative tasks.\", \"2. Formally derive the continuous-time Evidence Lower Bound (ELBO) for a Variational Autoencoder where the prior is an SDE, specifically the Ornstein-Uhlenbeck process.\", \"3. Implement the Neural SDE architecture, the VAE framework, and the Euler-Maruyama solver for the SDE latent dynamics using JAX for performance.\", \"4. Train the model on a suitable synthetic financial time-series dataset, monitoring the ELBO for convergence.\", \"5. Validate the model's ability to perform uncertainty quantification by analyzing the posterior predictive distribution on a held-out test set.\", \"6. Verify the numerical stability of the solver and check that the implemented loss function correctly approximates the derived continuous-time ELBO.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (verification) -> router QA (verification: Cross-check math derivation against code logic and numerical results; fallback: If instability is found, request /code to implement a higher-order SDE solver like the Milstein method).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent arXiv papers on Neural SDEs for generative modeling.\", \"- [ ] /math: Produce a complete LaTeX derivation of the continuous-time ELBO with an Ornstein-Uhlenbeck prior.\", \"- [ ] /code: Implement the Neural SDE model, VAE framework, and Euler-Maruyama solver in JAX.\", \"- [ ] /code: Train the model on a synthetic dataset and generate plots for ELBO convergence and posterior predictive distributions.\", \"- [ ] /math: Verify that the discretized implementation in code correctly approximates the continuous-time mathematical derivation, noting any potential sources of discretization error.\", \"- [ ] /math: Audit the numerical stability of the solver and provide theoretical conditions for convergence.\", \"- [ ] /router: Perform final QA review, ensuring all artifacts are consistent and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-differential-equations\", \"bayesian-deep-learning\", \"variational-inference\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO must be complete, self-contained, and mathematically sound, referencing principles of Ito calculus.\", \"- The JAX implementation must execute without errors, demonstrate monotonic convergence of the ELBO on a synthetic dataset, and be well-documented.\", \"- The final report must include plots clearly visualizing the model's uncertainty estimates (e.g., predictive mean and +/- 2 standard deviations).\", \"- The project must cite at least two relevant arXiv papers with IDs, as specified in the citation policy.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 654, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the strong and weak forms for the 2D incompressible Navier-Stokes equations, then formulate the physics-informed neural network (PINN) loss function. Implement this using JAX to solve a lid-driven cavity flow problem, and analyze the convergence behavior of the residual loss with respect to the network's Fourier feature embedding.\n", "completion": "{\"route_plan\": [\"/math(Derive the strong and weak (variational) forms of the 2D incompressible Navier-Stokes equations. Formulate the composite loss function for a physics-informed neural network (PINN), incorporating boundary conditions and the PDE residual, referencing the underlying Bayesian inference framework.)\", \"/general-search(query=\\\"site:arxiv.org 'physics informed neural networks' 'Navier-Stokes' convergence 'Fourier feature networks' spectral bias\\\", mode=rag)\", \"/code(Implement a PINN in JAX to solve the lid-driven cavity flow benchmark. Use Fourier feature mapping for the input coordinates and track the convergence of the PDE residual loss and L2 error against a known finite element solution to validate the approach., using Python)\"], \"route_rationale\": \"/math is first to establish the theoretical foundation of the PDE and the PINN loss function. The blue /general-search agent is then used to gather state-of-the-art techniques and convergence analyses for this specific problem class. Finally, /code implements the derived theory, using insights from the literature search for a robust solution.\", \"expected_artifacts\": [\"A LaTeX document with the full mathematical derivation of the Navier-Stokes strong/weak forms and the PINN loss function.\", \"A well-documented Python script (JAX) implementing the PINN solver for the lid-driven cavity problem.\", \"A summary report (.md) analyzing the convergence results, including plots of the velocity field and residual loss, with citations to at least 2 relevant arXiv papers.\", \"A JSON file containing the final L2 error metric and the loss history.\"], \"thinking_outline\": [\"1. Formulate the 2D incompressible Navier-Stokes equations and derive their residual form suitable for a PINN loss function, clearly defining the differential operators.\", \"2. Research recent advancements in PINN architectures, particularly the use of Fourier feature mappings to mitigate spectral bias and improve convergence for high-frequency solution components.\", \"3. Implement the PINN architecture in JAX, including the network, the Fourier feature mapping, and the automatic differentiation pipeline to compute the PDE residual.\", \"4. Train the model on the lid-driven cavity problem. Systematically log the loss components and compute the L2 relative error against a reference solution at the end of training.\", \"5. Validate the implementation by checking if the learned velocity field correctly satisfies the incompressibility constraint (divergence-free) pointwise across the domain.\", \"6. Audit the final solution's physical plausibility by visualizing streamlines and comparing them to established results from the literature gathered during the search phase.\"], \"handoff_plan\": \"/math provides the core equations. /general-search finds implementation best practices and theoretical papers to contextualize the math derivation. /code receives both the equations and the research to build the JAX solver. -> router QA (verification: L2 error of the final solution is checked against known benchmarks from literature; fallback: if error is high, re-evaluate the loss weighting or network architecture in /code and potentially revisit the loss formulation in /math).\", \"todo_list\": [\"- [ ] /math: Derive the residual form of the 2D Navier-Stokes equations for the PINN loss.\", \"- [ ] /math: Formulate the complete loss function, including boundary, initial, and physics residual terms.\", \"- [ ] /general-search: Find SOTA papers on PINN for fluid dynamics, focusing on convergence and Fourier features.\", \"- [ ] /code: Implement the JAX model with Fourier feature mapping and automatic differentiation for the PDE residual.\", \"- [ ] /code: Train the model on the lid-driven cavity benchmark and log loss metrics.\", \"- [ ] /math: Verify that the symbolic PDE residual derived matches the one implemented via automatic differentiation in the code.\", \"- [ ] /code: Validate the final velocity field solution by visualizing the streamlines and checking the incompressibility constraint.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"fourier-analysis\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Navier-Stokes PINN loss is complete and correct.\", \"- The JAX implementation runs without errors and produces a qualitatively correct velocity field for the lid-driven cavity problem.\", \"- The final report shows convergence of the training loss and achieves an L2 relative error below 5% compared to a reference solution.\", \"- The report includes citations to at least 2 relevant arXiv papers on PINNs or Fourier feature networks.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 664, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the spectral properties of the loss landscape's Hessian matrix in deep neural networks. Please derive the Hessian for a feed-forward network with ReLU activations, then use random matrix theory to predict its eigenvalue distribution. Finally, write a JAX implementation to empirically compute the Hessian's spectrum for a VGG-like model on CIFAR-10 and validate how the spectral norm and top eigenvalue correlate with generalization gaps.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian eigenspectrum deep learning random matrix theory generalization gap\\\", mode=rag, includes cybersecurity)\", \"/math(Derive the analytical form of the Hessian matrix for a multi-layer perceptron, analyzing its block structure and relating its eigenvalue spectrum to network layer properties and convergence behavior.)\", \"/code(Implement a JAX program to train a VGG-style network on CIFAR-10, compute the Hessian of the loss function using efficient vector products, and calculate its top eigenvalue and spectral density for Bayesian analysis., using Python)\", \"/math(Verify the empirical spectral density from the code against the theoretical predictions. Analyze any discrepancies and refine the model, considering the effect of finite width and depth on the Hessian eigenvalue distribution.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on Hessian spectral analysis. This informs the /math agent's theoretical derivation. The /code agent then implements a numerical experiment to generate empirical data. A final /math call is used to verify the theory against the experiment, creating a rigorous validation loop.\", \"expected_artifacts\": [\"- A LaTeX document containing the full mathematical derivation of the Hessian and its spectral analysis.\", \"- A Python script (`hessian_analysis.py`) implementing the numerical experiment in JAX.\", \"- A JSON file (`metrics.json`) reporting the top eigenvalue, spectral norm, and generalization gap.\", \"- A summary report (`report.md`) comparing theoretical predictions with empirical results, including plots of the eigenvalue distribution.\", \"- A list of all cited arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Conduct a literature survey to understand the connection between the Hessian's eigenspectrum, training dynamics, and generalization in deep learning.\", \"2. Formally derive the Hessian of the cross-entropy loss for a feed-forward neural network, expressing it in terms of network Jacobians and activation function derivatives.\", \"3. Implement a numerical routine in JAX to compute the Hessian's eigenvalues for a trained model without materializing the full matrix, likely using the Lanczos algorithm.\", \"4. Validate the implemented Hessian computation by checking for symmetry and comparing its action on random vectors with finite difference approximations.\", \"5. Compare the empirical eigenvalue distribution against theoretical predictions from relevant random matrix theory ensembles (e.g., Marchenko-Pastur).\", \"6. Audit the final report to ensure the theoretical claims are rigorously supported by the empirical data and that all metrics are correctly reported.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results provide foundational papers for the math agent's derivation. The math agent produces the equations for the code agent to implement. The code agent generates empirical data (eigenvalue spectra) which the final math agent call uses for verification against the theory. Fallback: If the empirical spectrum significantly deviates from theory, the code agent must re-verify its implementation of the Hessian-vector product.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on Hessian spectral analysis and random matrix theory in NNs.\", \"- [ ] /math: Produce a complete LaTeX derivation for the Hessian of a feed-forward network.\", \"- [ ] /code: Write and test the JAX implementation for training a network and computing its Hessian's eigenspectrum.\", \"- [ ] /math: Verify the analytical derivation aligns with the structure of the implemented computation.\", \"- [ ] /code: Validate the numerical stability and correctness of the eigenvalue computation algorithm.\", \"- [ ] /math: Perform the comparative analysis between the theoretical and empirical results.\", \"- [ ] /router: Perform final QA check, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"matrix-calculus\", \"spectral-analysis\", \"neural-network-dynamics\"], \"acceptance_criteria\": [\"- The LaTeX derivation of the Hessian must be mathematically sound and complete.\", \"- The JAX code must be executable and reproduce the eigenvalue spectrum plot and metrics reported.\", \"- The final report's conclusion on the theory-experiment match must be justified by the KS-test result in the metrics file.\", \"- At least 2 relevant arXiv papers must be cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 673, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating a class of impartial games defined on succinctly represented directed acyclic graphs. Can you analyze its computational complexity, specifically proving whether the problem of determining if the first player has a winning strategy is PSPACE-complete? Furthermore, for tractable subclasses, derive and implement an efficient algorithm for computing the Sprague-Grundy (nim-sum) values, and analyze its convergence properties under misere play conventions.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:.edu combinatorial game PSPACE-completeness reduction Sprague-Grundy theory\\\", mode=web, context=\\\"Find foundational papers on the computational complexity of impartial games and efficient algorithms for nim-value calculation. Focus on sources from arxiv or academic domains.\\\")\", \"/math(context=\\\"Formalize the game rules and construct a polynomial-time reduction from Quantified Boolean Formulas (QBF) to the game's decision problem. This involves mapping boolean variables to game states in an adversarial context to prove complexity.\\\")\", \"/code(context=\\\"Implement a Python algorithm using memoization to compute the Grundy numbers for a specified subclass of the game. The implementation must include validation tests and performance profiling to check convergence behavior.\\\")\", \"/math(context=\\\"Verify the correctness of the implemented algorithm by proving its convergence to the true Grundy numbers. Formally analyze its time and space complexity, comparing it against the theoretical bounds for such adversarial problems.\\\")\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational academic literature on game complexity. This research informs the formal proof construction in /math. Then, /code implements the derived theory for a practical subclass. A final /math step provides a crucial verification loop, ensuring the implementation rigorously aligns with the formal mathematical analysis.\", \"expected_artifacts\": [\"- A formal proof in LaTeX detailing the reduction from a known PSPACE-complete problem to the game.\", \"- A well-documented Python script implementing the Grundy number calculation algorithm.\", \"- A verification report comparing the algorithm's empirical performance against its theoretical complexity.\", \"- A bibliography with citations to at least two relevant academic papers with arXiv IDs or DOIs.\"], \"thinking_outline\": [\"1. Research established PSPACE-completeness proofs for similar combinatorial games (e.g., Generalized Geography, Kayles) to identify common reduction patterns.\", \"2. Formulate a rigorous mathematical reduction from Quantified Boolean Formulas (QBF) to the problem of determining the winner in the specified impartial game.\", \"3. Derive the recursive formulation for the game's Grundy numbers (nim-values) based on the mex (minimum excluded value) rule.\", \"4. Implement the Grundy number calculation algorithm, paying close attention to memoization to handle overlapping subproblems efficiently.\", \"5. Verify the implementation's correctness by testing it against small, manually solvable game instances and check for logical consistency with the mathematical derivation.\", \"6. Audit the final complexity proof and algorithmic analysis to ensure they are mutually consistent and correctly synthesized in the final report.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: The final proof from /math must validate the complexity class of the algorithm implemented in /code, which should be grounded by literature from /general-search; fallback: If verification fails, return to /math to re-examine the recurrence relation or to /code to debug the implementation logic.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on PSPACE-completeness in games and efficient Sprague-Grundy value computation.\", \"- [ ] /math: Formulate the game's decision problem and construct a polynomial-time reduction from Quantified Boolean Formulas (QBF).\", \"- [ ] /math: Derive the recurrence relation for the game's Grundy numbers (nim-values).\", \"- [ ] /code: Implement the Grundy number algorithm in Python using memoization and profile its performance.\", \"- [ ] /code: Add validation test cases for the Grundy number computation on small, manually solvable game instances.\", \"- [ ] /math: Verify the implemented algorithm's correctness against the derived mathematical recurrence and analyze its complexity.\", \"- [ ] router: QA review of the formal proof, implementation, and verification report for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-games\", \"computational-complexity\", \"pspace\"], \"acceptance_criteria\": [\"- The PSPACE-completeness proof must contain a valid, polynomial-time reduction from a known PSPACE-complete problem like QBF.\", \"- The Python implementation must correctly compute the Grundy numbers for all provided test cases within the specified performance envelope.\", \"- The final report must cite at least two authoritative papers and present a cohesive analysis where the theoretical complexity and empirical results align.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 746, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a neural-symbolic system to synthesize a program that formally proves the correctness of a unification algorithm for simply typed lambda calculus with let-polymorphism. The system must generate a machine-checkable proof in Coq or Isabelle, leveraging recent advancements in transformer-based guidance for proof search. I require the full formal derivation, the synthesized Haskell/OCaml code for the prover, and an analysis of the proof search convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural theorem proving transformer program synthesis higher-order logic\\\", mode=both)\", \"/math(Formalize the unification algorithm and its correctness theorem in the Calculus of Constructions, establishing convergence criteria and referencing key lemmas from the arxiv search.)\", \"/code(Implement a Monte Carlo Tree Search in Python with a transformer-based policy network to guide the synthesis of a Coq proof script for the formalized theorem.)\", \"/math(Verify the generated Coq proof object against the initial formal specification, checking for logical soundness and completeness of the proof steps., includes graphcut, jax)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find state-of-the-art methods, as this is a rapidly evolving field. /math then formalizes the problem based on this research. /code implements the core synthesis engine. A final /math step provides crucial verification of the generated artifact, ensuring correctness.\", \"expected_artifacts\": [\"- A formal specification of the unification theorem in Coq syntax (`unification_proof.v`).\", \"- The Python source code for the neural-guided synthesis agent.\", \"- A compiled Coq proof object (`.vo` file) confirming the theorem's validity.\", \"- A summary report (report.md) detailing the methodology, literature review with citations, and analysis of results.\"], \"thinking_outline\": [\"1. Research current state-of-the-art neural theorem provers and program synthesizers for higher-order logic, focusing on transformer-based architectures and their application in proof assistants like Coq.\", \"2. Formally specify the unification algorithm and its correctness property (e.g., soundness and completeness) within the Calculus of Constructions, the logical foundation of Coq.\", \"3. Design and implement the neural-guided search algorithm to explore the space of possible proof terms, synthesizing a program that constitutes a formal proof.\", \"4. Validate the generated proof script using the Coq proof assistant to ensure it is free of logical errors and fully proves the theorem.\", \"5. Audit the synthesized program's logic against the formal derivation to ensure the implementation correctly reflects the mathematical reasoning.\", \"6. Document the system's architecture, the formalization process, and the verification results, citing at least two recent, relevant publications.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: The final /math step uses the Coq kernel to check the proof object generated by /code against the formal specification from the first /math step; fallback: If verification fails, /code will be re-invoked with an augmented prompt to refine the search heuristics or model architecture based on the specific failure mode reported by Coq.)\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 key arxiv papers on transformer-guided proof synthesis.\", \"- [ ] /math: Write the formal Coq specification for the unification algorithm and its correctness theorem.\", \"- [ ] /code: Implement the transformer-guided MCTS agent for synthesizing the proof script.\", \"- [ ] /math: Verify the mathematical soundness of the lemmas used in the formal specification.\", \"- [ ] /code: Execute the agent and generate the final `unification_proof.v` file.\", \"- [ ] /math: Run `coqc` on the generated proof file and confirm it compiles without errors, producing a valid proof object.\", \"- [ ] /router: Perform final QA, ensuring all artifacts are present and the report aligns with the verified proof.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic\", \"theorem-proving\", \"program-synthesis\"], \"acceptance_criteria\": [\"- The generated Coq script (`.v` file) must compile successfully with the specified Coq version, proving the target theorem.\", \"- The final report must cite at least two relevant arXiv papers with IDs, published within the last three years.\", \"- The primary metric `proof_validity` must be 1.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 759, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the continuous-time It\u00f4 diffusion process corresponding to Stochastic Gradient Langevin Dynamics (SGLD) for sampling from the posterior of a Bayesian Neural Network. Then, implement this sampler in Python to estimate the posterior predictive distribution on a regression task, and empirically validate the algorithm's convergence by analyzing trace plots and comparing predictive uncertainty against a standard variational inference baseline.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Stochastic Gradient Langevin Dynamics It\u00f4 diffusion convergence Bayesian neural network\\\", mode=rag)\", \"/math(Derive the update rule for Stochastic Gradient Langevin Dynamics from the discretized Euler-Maruyama approximation of the corresponding Langevin It\u00f4 diffusion for a Bayesian posterior, ensuring the noise term correctly scales with the step size.)\", \"/code(Implement the SGLD sampler in Python with JAX for a Bayesian neural network regression model. Track the evolution of the posterior using trace plots and compute predictive uncertainty, comparing against a variational inference baseline to evaluate convergence.)\", \"/math(Analyze the convergence of the SGLD sampler's MCMC chain using trace plots and the Gelman-Rubin diagnostic from the Python output. Formally relate the empirical findings on predictive uncertainty to the theoretical ergodic properties of the Langevin diffusion.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on SGLD theory. This informs the /math agent's formal derivation of the algorithm. The derivation is then handed to the /code agent for implementation and empirical validation. Finally, the numerical results from /code are passed back to the /math agent for a concluding analysis that connects the empirical evidence with the initial theory, providing a full verification loop.\", \"expected_artifacts\": [\"A formal mathematical derivation of the SGLD update rule from the continuous-time Langevin diffusion in a PDF.\", \"A documented Python script implementing the SGLD sampler and a variational inference baseline.\", \"A JSON file containing computed metrics: Negative Log-Likelihood (NLL) and Root Mean Squared Error (RMSE) for both models.\", \"A summary report (.md) including MCMC trace plots and a comparative plot of predictive uncertainty.\", \"A bibliography with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Research the theoretical underpinnings of SGLD, focusing on its connection to the Langevin It\u00f4 diffusion and its discretization via the Euler-Maruyama method.\", \"2. Formally derive the discrete-time update equation for SGLD, paying close attention to the scaling of the gradient and the injected Gaussian noise with respect to the learning rate.\", \"3. Implement the SGLD algorithm in Python to sample from the posterior of a simple Bayesian neural network on a regression task. Also implement a baseline, such as Mean-Field Variational Inference.\", \"4. Run the simulation to collect samples from the posterior distribution and generate predictions on a test set.\", \"5. Validate the implementation by visually inspecting trace plots for convergence and comparing the empirical predictive uncertainty and error metrics (NLL, RMSE) against the VI baseline.\", \"6. Audit the final derivation against the implementation logic to ensure the noise term scaling and gradient updates are correctly translated from theory to code.\"], \"handoff_plan\": \"/general-search (papers) -> /math (derivation) -> /code (implementation & metrics) -> /math (analysis) -> router QA (verification: derivation matches implementation, metrics support conclusion; fallback: /math re-derives based on corrected literature if code behavior diverges from theory).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on SGLD, its continuous-time limit, and application to Bayesian NNs.\", \"- [ ] /math: Derive the SGLD update rule from the Euler-Maruyama discretization of the Langevin diffusion.\", \"- [ ] /code: Implement the SGLD sampler and a Mean-Field VI baseline for a BNN in JAX.\", \"- [ ] /code: Generate trace plots for key parameters and compute predictive NLL and RMSE on a hold-out set.\", \"- [ ] /math: Verify the SGLD implementation in the code correctly reflects the derived noise scaling and gradient terms.\", \"- [ ] /math: Analyze the convergence diagnostics (trace plots) and compare SGLD/VI performance based on the code's output.\", \"- [ ] /router: Review final report, ensuring derivation, code, and analysis are consistent and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-calculus\", \"bayesian-inference\", \"mcmc\"], \"acceptance_criteria\": [\"- The SGLD derivation correctly applies the Euler-Maruyama method to the continuous-time Langevin diffusion.\", \"- The Python code successfully runs and produces trace plots that indicate the MCMC chain is mixing and converging.\", \"- The final report shows that the SGLD sampler achieves a test NLL comparable to or better than the VI baseline.\", \"- At least two relevant arXiv papers are cited correctly.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the average NLL on a held-out test set. Lower is better, indicating superior generalization and uncertainty calibration.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Compute the RMSE on the test set predictions. Measures point estimate accuracy.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 799, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the theoretical underpinnings of the softmax function in transformer attention as an information bottleneck. Can you derive a formal upper bound on the mutual information between the input and output representations of an attention layer and then empirically verify this bound using a self-supervised contrastive learning setup in JAX? Explore how this bound influences representational collapse under adversarial training conditions.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org self-supervised learning 'information bottleneck' attention mechanism bayesian\\\", mode=both)\", \"/math(Derive the mutual information bound of the softmax attention operator as an information bottleneck, analyzing the Bayesian convergence properties and its impact on representational complexity.)\", \"/code(Implement a Siamese network in JAX using adversarial training to empirically measure the mutual information, tracking AUPRC for a downstream task and ensuring numerical stability., using Python)\", \"/math(Verify that the JAX implementation of the mutual information estimator is consistent with the derived formula, analyzing the Hessian eigenvalue spectrum for signs of optimization instability.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on information bottlenecks in attention. This informs the /math agent's formal derivation of the mutual information bound. The /code agent then implements a model to test this theory empirically. A final /math step is crucial for verifying that the code's logic correctly reflects the complex mathematical derivation, ensuring correctness before final analysis.\", \"expected_artifacts\": [\"A formal mathematical derivation of the mutual information upper bound for the attention mechanism, delivered as a LaTeX-formatted PDF.\", \"A clean, well-documented JAX implementation of the self-supervised learning model and the mutual information estimator.\", \"A summary report in Markdown connecting the theoretical derivation with empirical results, including plots.\", \"A plot visualizing the empirical mutual information against the theoretical bound over training epochs.\", \"A bibliography file with at least two citations to relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Conduct a literature search to ground the theoretical investigation in existing work on information bottlenecks, self-supervised learning, and attention mechanisms.\", \"2. Formulate and derive a rigorous mathematical upper bound on the mutual information I(X; Z) where X are the input tokens and Z is the context vector produced by the attention layer.\", \"3. Implement a self-supervised learning framework (e.g., SimCLR) in JAX to train a model with an attention layer, including a function to empirically estimate the mutual information.\", \"4. Verify that the code for the mutual information estimator is a correct and numerically stable implementation of the formula derived in step 2.\", \"5. Execute the training experiments, record the empirical mutual information at each epoch, and compare it against the theoretical bound.\", \"6. Validate the final results by checking for convergence and ensuring the observed trends are consistent with the information bottleneck theory; document any discrepancies.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results from /general-search directly inform the derivation by /math. The resulting mathematical formula from /math serves as the blueprint for the implementation by /code. The final /math agent acts as a verifier, checking the code's output against the derivation. Verification failure: if the implementation does not match the derivation, the task is handed back to /code with specific correction requests from /math. Fallback: if derivation itself is flawed, revert to /math for revision based on empirical oddities.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on the Information Bottleneck principle and its application to attention.\", \"- [ ] /math: Derive the mutual information bound for the softmax attention mechanism, clearly stating all assumptions.\", \"- [ ] /code: Implement the JAX model for self-supervised learning and the MI estimation function.\", \"- [ ] /math: Analytically verify that the MI estimation function in the code correctly implements the derived formula.\", \"- [ ] /code: Instrument the model to log the primary and secondary metrics during training.\", \"- [ ] /code: Add runtime validation checks for tensor shapes and potential NaN values in the attention logits.\", \"- [ ] /router: Perform final QA review of the derivation, code, and summary report for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"self-supervised-learning\", \"attention-mechanisms\", \"information-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the mutual information bound is complete, rigorous, and logically sound.\", \"- The JAX code runs to completion, is well-commented, and the reported 'Mutual Information Gap' metric is non-negative.\", \"- The final report includes a plot that clearly shows the empirical MI staying below the derived theoretical bound throughout training.\", \"- The project correctly cites at least two relevant papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the derived theoretical upper bound (a scalar) and subtract the empirically measured mutual information from the model at the end of training. The value should be non-negative; a smaller value indicates a tighter bound.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"After self-supervised training, freeze the encoder and train a linear classifier on a holdout set for a simple classification task. Report the Area Under the Precision-Recall Curve (AUPRC).\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 805, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a provably safe reinforcement learning controller for a continuous-action robotic manipulator. The approach must integrate Control Barrier Functions (CBFs) to enforce state constraints, ensuring forward invariance of a safe set. Please derive the Lagrangian formulation for the policy optimization problem with these CBF constraints and an additional Lyapunov stability condition, then implement and validate the resulting primal-dual algorithm.\n", "completion": "{\"route_plan\": [\"/math(Formulate the constrained policy optimization objective using a Lagrangian relaxation. Derive the primal-dual update rules based on KKT conditions for a system with Control Barrier Function safety constraints and Lyapunov-based control theory guarantees on convergence.)\", \"/general-search(query=\\\"site:arxiv.org safe reinforcement learning 'control barrier function' 'lagrangian relaxation' adversarial\\\", mode=both)\", \"/code(Implement the derived CBF-RL algorithm in Python with CUDA support for a simulated robotics task. The implementation must include federated logging of constraint violations and episodic reward to track performance.)\"], \"route_rationale\": \"The route starts with /math to establish the formal theoretical foundation of the constrained optimization problem. This is followed by the blue /general-search agent to survey the latest literature on CBF-based RL, ensuring the approach is current. Finally, /code is used to empirically validate the derived theory in a simulated environment.\", \"expected_artifacts\": [\"A PDF report containing the full mathematical derivation of the Lagrangian, the KKT conditions, and the final primal-dual update rules.\", \"A well-documented Python script implementing the CBF-constrained RL agent and the simulation environment.\", \"A literature review summary with citations and links to at least two relevant arXiv papers.\", \"A results document with plots showing the convergence of episodic reward and the rate of constraint violations over training.\", \"A verification log confirming that the numerical gradients in the code match the analytical gradients from the derivation.\"], \"thinking_outline\": [\"1. Formulate the robotics control problem as a Constrained Markov Decision Process (CMDP), defining the state space, action space, reward function, and safety constraints.\", \"2. Derive the Lagrangian for the policy optimization objective, incorporating Control Barrier Function (CBF) constraints to ensure forward invariance of the safe set.\", \"3. Apply Karush-Kuhn-Tucker (KKT) conditions to derive the primal-dual update rules for the policy network parameters and the Lagrangian multipliers associated with the safety constraints.\", \"4. Validate the analytical derivation by checking for logical consistency and ensuring the gradient expressions are well-defined.\", \"5. Implement the algorithm and verify that the coded gradients (computed via automatic differentiation) match the analytical gradients derived in the math step for a simplified test case.\", \"6. Evaluate the trained agent's performance based on its ability to maximize cumulative reward while minimizing safety constraint violations across numerous test episodes.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (literature context) -> /code (implementation) -> router QA (verification: code gradients match math derivation; fallback: if mismatch, return to /math to re-check calculus and assumptions)\", \"todo_list\": [\"- [ ] /math: Derive the primal-dual update rules for policy parameters and Lagrangian multipliers from the CBF-constrained objective.\", \"- [ ] /general-search: Find and summarize at least two state-of-the-art papers from arXiv on Lagrangian methods for safe RL with CBFs.\", \"- [ ] /code: Implement the derived algorithm in PyTorch, including the environment, agent, and training loop.\", \"- [ ] /math: Formally verify the conditions under which the derived updates guarantee convergence or non-increasing constraint violation.\", \"- [ ] /code: Create a unit test to perform a numerical gradient check, comparing the auto-differentiated gradients with the analytically derived ones.\", \"- [ ] /code: Run the final experiment, generate performance plots, and collect metrics.\", \"- [ ] /router: QA final artifacts, ensuring all acceptance criteria are met and derivations are consistent with the implementation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"control-theory\", \"constrained-optimization\"], \"acceptance_criteria\": [\"- The final report must contain a complete and correct mathematical derivation of the primal-dual update rules.\", \"- The implemented agent must achieve a constraint violation rate below 1% on the specified test environment after training.\", \"- The provided code must be runnable and include a unit test that successfully verifies the analytical gradient against a numerical approximation.\", \"- The literature review must cite at least two relevant arXiv pre-prints published within the last three years.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 863, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of Model-Agnostic Meta-Learning (MAML) from a Bayesian perspective. Please derive the probabilistic interpretation of MAML as a hierarchical Bayesian model, prove its convergence guarantees under non-convex assumptions by referencing recent literature on optimization theory, and then implement it in JAX to demonstrate its few-shot adaptation capabilities on a sinusoidal regression benchmark.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian MAML convergence non-convex optimization theory\\\", mode=web)\", \"/math(Derive the hierarchical Bayesian model for MAML, establishing the posterior over task-specific parameters and proving its convergence rate under Polyak-Lojasiewicz conditions identified in the literature search.)\", \"/code(Implement the Bayesian MAML algorithm in JAX with CUDA acceleration, setting up a sinusoidal few-shot regression benchmark and tracking Mean Squared Error (MSE) as the primary Bayesian performance metric., using Python)\", \"/math(Verify the analytical gradient derivation for the evidence lower bound (ELBO) against the auto-differentiated gradients produced by the JAX implementation to ensure theoretical and practical alignment.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on Bayesian MAML convergence, which is essential for the advanced mathematical proof. The findings are passed to /math for formal derivation and proof construction. /code is then used to implement the derived algorithm and validate it empirically. A final call to /math provides a crucial verification step, comparing the analytical and implemented gradients, ensuring correctness before final handoff.\", \"expected_artifacts\": [\"A formal mathematical derivation and convergence proof in a LaTeX PDF.\", \"A self-contained Python script implementing Bayesian MAML in JAX.\", \"A JSON file containing the final post-adaptation Mean Squared Error metrics and convergence data.\", \"A verification report confirming the consistency between the analytical and auto-differentiated gradients.\", \"A bibliography file with citations to at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Conduct a literature review to identify recent theoretical analyses of Bayesian MAML, focusing on convergence proofs in non-convex settings.\", \"2. Formulate MAML within a hierarchical Bayesian framework, deriving the evidence lower bound (ELBO) and the corresponding meta-objective for optimization.\", \"3. Construct a formal proof of convergence for the meta-training process, clearly stating all assumptions (e.g., Lipschitz continuity, smoothness) based on the literature.\", \"4. Implement the Bayesian MAML algorithm and the sinusoidal regression benchmark in JAX, ensuring correct task sampling and meta-update logic.\", \"5. Validate the implementation by performing a numerical gradient check, comparing the analytical gradients from the derivation with the results from JAX's automatic differentiation.\", \"6. Check that the empirical results on the benchmark demonstrate successful few-shot adaptation and align with the theoretical convergence properties.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: Cross-reference the proof's assumptions in /math with the literature from /general-search and ensure the JAX implementation in /code matches the final derived equations. fallback: If gradients mismatch, flag for /math to re-derive and /code to debug implementation.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on Bayesian MAML convergence.\", \"- [ ] /math: Derive the ELBO for the hierarchical Bayesian model of MAML.\", \"- [ ] /math: Formulate the convergence proof based on the literature review findings.\", \"- [ ] /code: Implement the Bayesian MAML algorithm and sinusoidal regression task in JAX.\", \"- [ ] /code: Instrument the code to log post-adaptation MSE and meta-loss per epoch.\", \"- [ ] /math: Verify that the analytical gradients derived for the meta-objective match the output of `jax.grad` on the implemented loss function.\", \"- [ ] /code: Audit the benchmark implementation to ensure it follows standard practices for few-shot regression evaluation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"bayesian-inference\", \"optimization-theory\"], \"acceptance_criteria\": [\"The provided convergence proof must be mathematically rigorous and reference at least two peer-reviewed papers or arXiv preprints.\", \"The JAX implementation must run successfully and demonstrate a decrease in post-adaptation MSE on held-out tasks over the course of meta-training.\", \"The final report must include a plot showing the meta-loss converging over training epochs.\", \"The gradient verification step must pass, showing negligible difference between analytical and auto-differentiated results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 866, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on a physics-informed neural network (PINN) for 2D incompressible fluid flow. I need to formulate the loss function for the steady-state Navier-Stokes equations, ensuring the divergence-free velocity field constraint is strongly enforced. Can you derive the full PDE residual loss, implement it in JAX for a lid-driven cavity problem, and analyze the convergence properties, especially concerning the spectral bias of the MLP architecture?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org physics-informed navier-stokes divergence-free euler-lagrange\\\", mode=rag)\", \"/math(Derive the composite loss for a PINN solving incompressible Navier-Stokes, ensuring the Euler-Lagrange functional includes boundary terms and a penalty for the divergence-free continuity equation, analyzing potential for spectral convergence.)\", \"/code(Implement the derived loss in JAX, using automatic differentiation for the PDE residuals. Train on a lid-driven cavity problem with CUDA acceleration, tracking the mean squared error and performing a bootstrap analysis on the final velocity field., using Python)\", \"/math(Verify the JAX implementation's automatic gradients against the symbolic derivatives from the Euler-Lagrange derivation. Audit the convergence behavior and check for potential numerical diffusion or instability.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on PINNs for fluid dynamics. This informs the theoretical work by /math, which will derive the specific loss function. /code then implements this theory in a practical JAX model for empirical validation. A final /math call is crucial for verifying that the code's automatic differentiation correctly implements the derived symbolic gradients, closing the theory-practice loop.\", \"expected_artifacts\": [\"A LaTeX document detailing the derivation of the Navier-Stokes residual loss function.\", \"A fully functional and documented JAX Python script for the PINN model and training loop.\", \"A summary report (PDF) containing convergence plots, analysis of the results, and at least two citations to relevant arXiv papers.\", \"A verification log confirming the consistency between the symbolic derivation and the code's gradient computations.\"], \"thinking_outline\": [\"1. Research state-of-the-art PINN formulations for the incompressible Navier-Stokes equations, focusing on methods for enforcing the divergence-free constraint.\", \"2. Formulate the strong form of the PDE system and derive the composite loss function, which includes terms for the momentum equation residuals, the continuity equation (incompressibility), and the boundary conditions.\", \"3. Implement the PINN architecture and the custom loss function in JAX, leveraging automatic differentiation (`jax.grad`) to compute all necessary partial derivatives for the PDE residuals.\", \"4. Train the model on the lid-driven cavity flow problem, monitoring the convergence of individual loss components (momentum, continuity, boundary).\", \"5. Validate that the learned velocity field is approximately divergence-free across the entire computational domain by evaluating the continuity equation residual on a fine grid.\", \"6. Verify the correctness of the implementation by comparing the automatic differentiation outputs for specific points against manually computed symbolic derivatives from the mathematical derivation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. Search results provide foundational papers for the /math agent's derivation. The resulting equations from /math are the direct specification for the /code agent's implementation. The trained model and its gradients from /code are then passed back to /math for a final verification step. Verification failure at the last step triggers a loop back to /code with specific error reports. The full chain is audited by the router before finalizing.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on PINNs for solving Navier-Stokes equations, noting different approaches to the incompressibility constraint.\", \"- [ ] /math: Provide a full derivation of the composite loss function, defining all terms, variables, and parameters.\", \"- [ ] /code: Implement the PINN in JAX, set up the lid-driven cavity problem, and write the training loop to minimize the loss from /math.\", \"- [ ] /code: Generate plots for loss convergence and a final quiver plot of the learned velocity field.\", \"- [ ] /math: Symbolically compute the gradients for a sample input and verify they match the output of the JAX implementation's `jax.grad` functions.\", \"- [ ] /code: Audit the trained model to quantify the maximum divergence error on a 100x100 grid of interior points.\", \"- [ ] router: Review all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-neural-networks\", \"partial-differential-equations\", \"scientific-computing\"], \"acceptance_criteria\": [\"- The derived loss function in the LaTeX document must correctly represent the steady-state, incompressible Navier-Stokes equations.\", \"- The JAX implementation must converge, with the total loss decreasing by at least two orders of magnitude during training.\", \"- The final report must cite at least 2 relevant arXiv papers with IDs, and the learned velocity field must qualitatively resemble the expected lid-driven cavity flow pattern.\", \"- The verification step must show less than 1e-6 difference between the symbolic and automatic differentiation gradients.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"computation_guidance\": \"Primary: Compute the L2 norm of the full PDE residual vector, averaged over all collocation points. Secondary: Evaluate |du/dx + dv/dy| on a fine grid within the domain and report the maximum absolute value.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_computation", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 871, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a formal derivation of the computational and parameter complexity reduction achieved by applying Tucker decomposition to a standard 2D convolutional layer in a CNN. Please demonstrate how the 4th-order convolution kernel tensor can be factorized into a core tensor and three factor matrices, and prove the resulting reduction in FLOPs. Subsequently, implement this in PyTorch and verify the theoretical speedup and parameter savings on a VGG-style architecture, analyzing the eigenvalue decay of the unfolded kernel tensor to justify the low-rank approximation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org tucker decomposition convolutional neural network compression complexity analysis\\\", mode=both)\", \"/math(Derive the asymptotic complexity and parameter count for a convolutional layer parameterized by a Tucker decomposition. Formulate the convolution as a 4-mode tensor contraction and analyze the eigenvalue spectrum of the core tensor for stability.)\", \"/code(Implement a Tucker-decomposed convolutional layer in PyTorch with CUDA support, comparing its inference latency and parameter count against a standard Conv2d layer. Measure convergence behavior on a synthetic dataset to validate theoretical complexity gains., using Python)\", \"/math(Validate the empirical FLOPs and parameter counts from the CUDA implementation against the derived asymptotic complexity formulas. Check for discrepancies and explain any deviations by analyzing hardware-specific kernel performance.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on tensor decomposition in CNNs. This informs the /math agent's formal derivation of complexity. Next, the /code agent provides an empirical implementation and benchmark. A final /math call is used for cross-validation, comparing the theoretical model with the empirical results to ensure correctness and account for practical overhead.\", \"expected_artifacts\": [\"- A formal mathematical proof in LaTeX detailing the parameter and FLOPs reduction from Tucker decomposition of a convolutional kernel.\", \"- A Python script (`tucker_conv.py`) implementing the decomposed convolutional layer and a benchmarking routine.\", \"- A summary report (`report.md`) containing a plot comparing theoretical vs. empirical complexity reduction and citing at least two relevant papers.\", \"- A JSON file (`metrics.json`) with the measured compression ratio, FLOPs reduction, and latency speedup.\", \"- A verification log (`verification.log`) documenting the check between the mathematical derivation and the code's output.\"], \"thinking_outline\": [\"1. Research the state-of-the-art on using Tucker and CP decompositions for CNN model compression via authoritative sources like arXiv.\", \"2. Formulate the standard 2D convolution operation as a 4th-order tensor contraction, defining the input, output, and kernel tensors.\", \"3. Formally derive the exact number of parameters and multiplication operations for both the standard and the Tucker-decomposed convolutional layers, expressing the reduction as a function of the Tucker ranks.\", \"4. Implement the decomposed layer in PyTorch, ensuring the implementation correctly reflects the mathematical formulation of factor matrices and the core tensor.\", \"5. Validate that the empirical parameter counts and FLOPs measured from the implementation match the formulas derived in step 3 within a 5% tolerance.\", \"6. Audit the final report to ensure the theoretical proof, empirical results, and cited literature form a coherent and rigorous analysis.\"], \"handoff_plan\": \"/general-search -> /math (theory derivation) -> /code (empirical implementation) -> /math (verification) -> router QA (final review). The second /math agent must verify its results against the /code agent's output; fallback involves the /code agent adding more detailed profiling or the /math agent refining its complexity model to account for implementation overhead.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 seminal papers on Tucker decomposition for CNNs, extracting key formulas.\", \"- [ ] /math: Produce a step-by-step derivation of parameter and FLOPs counts for standard vs. Tucker-decomposed conv layers.\", \"- [ ] /code: Implement the Tucker-decomposed layer in PyTorch and a script to benchmark it against `torch.nn.Conv2d`.\", \"- [ ] /code: Generate plots comparing parameters, FLOPs, and latency for a range of ranks.\", \"- [ ] /math: verify that the parameter count and FLOPs produced by the /code agent's implementation align with the derived mathematical formulas.\", \"- [ ] /math: Write a brief analysis on the source of any minor discrepancies between theory and practice (e.g., framework overhead).\", \"- [ ] router: Perform final QA, checking all artifacts for correctness, clarity, and consistency.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"model-compression\"], \"acceptance_criteria\": [\"- The mathematical proof for complexity reduction must be correct and rigorously derived.\", \"- The Python implementation must run without errors and produce measurable parameter and latency differences compared to a baseline.\", \"- The final report must show that the empirical FLOPs reduction is within 10% of the theoretically predicted value.\", \"- The solution correctly cites at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 890, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a network pruning algorithm for a ResNet block by enforcing a hard constraint on the spectral norm of the convolutional weight tensors. Please derive the augmented Lagrangian and the corresponding update steps using the Alternating Direction Method of Multipliers (ADMM). Then, formulate the Karush-Kuhn-Tucker (KKT) conditions for this constrained optimization problem and verify that the algorithm's fixed point satisfies these conditions.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Alternating Direction Method of Multipliers ADMM deep learning spectral norm constraint\\\", mode=web)\", \"/math(Derive the augmented Lagrangian for the spectral norm constrained weight optimization problem. Decompose it for ADMM and formulate the primal-dual update rules, analyzing the theoretical convergence criteria and its connection to Euler-Lagrange equations.)\", \"/code(Implement the derived ADMM update rules in JAX to train a ResNet block on a proxy dataset. The implementation must include a projection step onto the spectral norm ball using SVD and track the primal residual, dual residual, and largest weight matrix eigenvalue during training., using Python)\", \"/math(Analyze the numerical output from the JAX code. Formally verify that the converged weight matrices and dual variables satisfy the derived KKT conditions within a small tolerance, cross-referencing the primal-dual gap.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative literature on ADMM for deep learning. This grounds the theoretical work in /math, which will formally derive the optimization steps. /code then translates the mathematical derivation into a practical JAX implementation for a concrete deep learning model. A final /math step is crucial for verification, ensuring the implemented algorithm's output aligns with the established KKT optimality conditions.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full derivation of the augmented Lagrangian, ADMM update steps, and the KKT conditions.\", \"A Python script (`admm_pruning.py`) implementing the ADMM algorithm for a ResNet block using JAX.\", \"A JSON file (`metrics.json`) logging the primal residual, dual residual, and constraint violation over optimization steps.\", \"A verification report (`verification.md`) summarizing the check of the final solution against the KKT conditions.\"], \"thinking_outline\": [\"1. Formulate the deep learning weight pruning problem as a constrained optimization problem with an objective function (e.g., cross-entropy loss) and an inequality constraint on the spectral norm of the weight tensors.\", \"2. Construct the augmented Lagrangian for this problem and derive the iterative update rules for the primal variables (weights) and dual variables using the ADMM framework.\", \"3. Implement the ADMM algorithm, paying close attention to the projection step which involves singular value decomposition (SVD) to enforce the spectral norm constraint.\", \"4. Validate the implementation by monitoring convergence diagnostics such as primal and dual residuals to ensure the algorithm is stable and progressing towards a solution.\", \"5. Formulate the KKT conditions for the original constrained problem: primal feasibility, dual feasibility, complementary slackness, and stationarity of the Lagrangian.\", \"6. Verify that the converged solution obtained from the code satisfies the KKT conditions up to a predefined numerical tolerance, confirming its optimality.\"], \"handoff_plan\": \"/general-search -> /math (provides foundational papers for derivation) -> /code (implements the derived equations) -> /math (receives numerical results for formal verification). If verification fails, /math will provide a report to /code detailing the KKT condition violations, triggering a debugging loop. Fallback for failed convergence is to adjust the ADMM penalty parameter rho and re-run.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on ADMM for constrained deep learning.\", \"- [ ] /math: Produce the complete mathematical derivation for the ADMM updates and the specific KKT conditions.\", \"- [ ] /code: Write a JAX script that implements the ADMM training loop for a ResNet convolutional layer.\", \"- [ ] /code: Log the primal/dual residuals and constraint violation per iteration to a structured file.\", \"- [ ] /math: Ingest the final weights and dual variables from the code's output.\", \"- [ ] /math: Verify that the numerical solution satisfies the KKT stationarity and complementary slackness conditions.\", \"- [ ] /code: Add comments to the implementation cross-referencing the equations in the math derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"lagrangian-multipliers\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ADMM update rules and KKT conditions must be complete and correct.\", \"- The JAX implementation must converge, with both primal and dual residuals decreasing below a tolerance of 1e-4.\", \"- The final verification report must demonstrate that the converged solution satisfies all KKT conditions with a numerical error of less than 1e-5.\", \"- The project must cite at least two relevant academic papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 905, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the theoretical limits of watermarking a large language model against adversarial paraphrasing attacks by framing it as a channel coding problem. Derive the rate-distortion function for a simplified text generation model under a semantic similarity constraint (e.g., bounded BERTScore delta) and an adversarial channel model. Implement a proof-of-concept using arithmetic coding to embed a watermark and evaluate its robustness in terms of bit error rate versus the KL divergence of the adversarial perturbation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org information theory llm watermarking adversarial channel coding\\\", mode=rag)\", \"/math(Derive the rate-distortion function for a Bernoulli text source under an adversarial channel model, defining the distortion metric using Kullback-Leibler divergence constraints.)\", \"/code(Implement the watermarking scheme using arithmetic coding in PyTorch; simulate an adversarial attack and measure the resulting bit-error-rate (BER) and confidence interval., using Python)\", \"/math(Validate the empirical BER curve against the theoretical channel capacity derived earlier, checking for convergence and noting any discrepancy due to finite blocklength effects., includes numpy)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art literature on information-theoretic AI safety. Next, /math is used to establish a rigorous theoretical foundation. /code then provides empirical evidence by implementing and testing the derived concepts. A final /math call is crucial for validating the experimental results against the theory, ensuring a closed-loop analysis.\", \"expected_artifacts\": [\"- A summary of relevant literature with citations (\u22652 arXiv IDs).\", \"- A formal mathematical derivation of the rate-distortion function in LaTeX.\", \"- A Python script implementing the watermarking algorithm and evaluation.\", \"- A plot of Bit Error Rate vs. Adversarial Perturbation Strength (KL Divergence).\", \"- A final report synthesizing theoretical and empirical findings.\"], \"thinking_outline\": [\"1. Conduct a literature review to ground the approach in existing research on information-theoretic methods for generative AI safety and watermarking.\", \"2. Formalize the watermarking problem using concepts from rate-distortion theory and channel coding, precisely defining the source, channel, and distortion metric.\", \"3. Implement the core components: a watermarking encoder/decoder (e.g., arithmetic coding) and a simulated adversarial channel that perturbs text while respecting semantic constraints.\", \"4. Execute a series of experiments to generate empirical data on watermark robustness under varying levels of adversarial attack strength.\", \"5. Verify that the empirical results, specifically the trade-off between watermark rate and robustness, align with the theoretical bounds derived in step 2.\", \"6. Audit the implementation to confirm the correctness of the KL divergence calculation and the statistical significance of the bit error rate measurements.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The literature review from /general-search informs the theoretical model in /math. The formal derivation from /math provides the specification for the /code agent. The empirical results from /code are then passed to the second /math agent for theoretical validation. (Verification: Each step's output must conform to the io_schema; Fallback: If the code's empirical results diverge significantly from theory, flag for a review of the mathematical derivation or the experimental setup).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on information-theoretic watermarking for LLMs.\", \"- [ ] /math: Produce a complete LaTeX derivation of the rate-distortion function for the specified problem.\", \"- [ ] /code: Implement the arithmetic coding watermarking scheme and the adversarial attack simulation in Python.\", \"- [ ] /code: Generate a plot showing the Bit Error Rate (BER) as a function of the adversarial perturbation's KL divergence.\", \"- [ ] /math: Write a formal analysis comparing the empirical BER curve from the code with the derived theoretical channel capacity limit.\", \"- [ ] /code: Verify that the KL divergence implementation is numerically stable and correct.\", \"- [ ] /math: Validate that the assumptions made in the derivation are consistent with the conditions in the code's simulation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"adversarial-robustness\"], \"acceptance_criteria\": [\"- The mathematical derivation must be self-contained and mathematically sound.\", \"- The Python code must be executable and reproduce the plot artifact.\", \"- The final report must clearly connect the theoretical bounds to the empirical results, explaining any discrepancies.\", \"- The submission must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate as the number of incorrectly decoded watermark bits divided by the total number of embedded bits after the model output is passed through the adversarial channel.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the percentage increase in perplexity on a held-out dataset for the watermarked model compared to the baseline model to quantify the impact on text quality.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 916, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for a variance-exploding (VE) diffusion model, starting from its associated Fokker-Planck equation. Subsequently, implement a predictor-corrector sampler in JAX that leverages this SDE, and validate its convergence by analyzing the KL divergence between the generated and a known target distribution. Finally, research and provide a comparative analysis of this approach against variance-preserving (VP) SDEs, citing recent arXiv literature on score function approximation errors.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative modeling SDE Fokker-Planck connection variance-exploding diffusion\\\", mode=web)\", \"/math(Derive the reverse-time SDE for a variance-exploding (VE) diffusion process from its corresponding Fokker-Planck partial differential equation, ensuring the derivation for the drift term is explicit.)\", \"/code(Implement a JAX-based predictor-corrector sampler for the derived VE SDE, including tracking KL divergence metrics and ensuring numerical stability for the Langevin dynamics steps., using Python)\", \"/math(Formulate the theoretical analysis comparing the score function approximation error propagation between VE and VP diffusion models, referencing the sourced arXiv literature and analyzing convergence behavior.)\"], \"route_rationale\": \"The 'blue' /general-search agent first gathers foundational papers on VE/VP SDEs. The /math agent then handles the rigorous derivation from the Fokker-Planck equation. The /code agent implements the derived sampler for empirical validation. A final /math step provides a theoretical comparison and analysis, synthesizing the research and derivation.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the reverse-time SDE.\", \"A Python script (`sde_sampler.py`) with the JAX implementation of the predictor-corrector sampler.\", \"A summary report comparing VE and VP SDEs, citing at least two relevant arXiv preprints with their IDs.\", \"A JSON file (`convergence_metrics.json`) logging the KL divergence over sampling steps.\", \"A verification log confirming the consistency between the mathematical derivation and the code implementation.\"], \"thinking_outline\": [\"1. Establish the forward diffusion process as a stochastic differential equation (SDE) and its corresponding Fokker-Planck equation which governs the evolution of the data distribution's density.\", \"2. Leverage Anderson's theorem on reverse-time SDEs to derive the reverse process, expressing the drift term as a function of the score, \u2207x log p_t(x).\", \"3. Implement a numerical solver, such as a predictor-corrector scheme (e.g., annealed Langevin dynamics), to discretize and sample from the derived reverse-time SDE.\", \"4. Validate the implementation by checking for numerical stability and ensuring the sampler's output distribution converges to a simple target distribution (e.g., a Gaussian mixture) for which the true score is known.\", \"5. Research the properties of variance-preserving (VP) SDEs and contrast their score matching error propagation characteristics with the variance-exploding (VE) case based on the sourced literature.\", \"6. Audit the final report to confirm the mathematical derivations are consistent with the code implementation and the theoretical comparison is well-supported by the cited literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: code logic must match SDE derivation, and theoretical analysis must use sourced papers; fallback: if derivation is flawed, return to /math with annotated errors from /code agent).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers (e.g., Song et al.) on score-based generative modeling with SDEs, focusing on VE and VP formulations from arXiv.\", \"- [ ] /math: Derive the reverse-time SDE for a VE process, clearly showing the connection to the Fokker-Planck equation and the score function.\", \"- [ ] /code: Implement the predictor-corrector sampler in JAX based on the derived reverse SDE.\", \"- [ ] /code: Add instrumentation to track KL divergence to a known target distribution during sampling.\", \"- [ ] /math: Verify that the implemented update rule in the JAX code correctly discretizes the derived continuous-time SDE.\", \"- [ ] /math: Write a formal comparison of error propagation in VE vs. VP models, using insights from the searched literature.\", \"- [ ] /router: Perform final QA review, checking consistency between the derivation, code, and final analysis report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-modeling\"], \"acceptance_criteria\": [\"- The derivation of the reverse-time SDE from the Fokker-Planck equation is mathematically sound and fully detailed.\", \"- The JAX implementation of the predictor-corrector sampler successfully generates samples and shows convergence via decreasing KL divergence.\", \"- The comparative analysis correctly identifies the tradeoffs between VE and VP SDEs, supported by citations to at least two relevant arXiv preprints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 938, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the formal proof structure for an inductive synthesis task, specifically for synthesizing a functional program that reverses a list. Then, develop a neural-symbolic framework in Python using a differentiable prover to guide a program synthesizer, ensuring the synthesized program's correctness via Curry-Howard isomorphism. Analyze its computational complexity and convergence rate against a baseline using Bayesian optimization.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:semanticscholar.org differentiable theorem prover inductive program synthesis higher-order logic Curry-Howard\\\", mode=both, includes cybersecurity)\", \"/math(Formalize the inductive proof for list reversal in a higher-order logic. Derive a continuous relaxation of this proof structure for gradient-based optimization, defining a differentiable loss function based on proof-term similarity and Bayesian convergence criteria.)\", \"/code(Implement a neural program synthesizer in PyTorch that uses the derived loss to guide its search. The synthesizer must generate a functional program for list reversal, validating correctness against the formal specification and analyzing the computational complexity and energy consumption of the search., using Python)\", \"/math(Validate the synthesized program's proof trace against the initial formal derivation. Analyze the empirical convergence rate and formally verify that the final program adheres to the Curry-Howard isomorphism, ensuring the proof and program are structurally equivalent.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on differentiable provers. Next, /math is used to establish the formal theoretical foundation and derive the core loss function. /code then implements this theory in a practical Python prototype. A final /math step provides rigorous verification by cross-validating the theoretical model against the empirical results from the code, ensuring correctness.\", \"expected_artifacts\": [\"- A literature review summarizing at least two key papers on neural-symbolic synthesis.\", \"- A formal mathematical derivation of the inductive proof and the differentiable loss function.\", \"- A documented Python implementation of the neural program synthesizer and differentiable prover.\", \"- A final report containing the synthesized program, its proof trace, and an analysis of its convergence and complexity.\", \"- A verification log confirming the alignment between the mathematical derivation and the code's behavior.\"], \"thinking_outline\": [\"1. Research existing literature on differentiable theorem provers and inductive program synthesis to establish a theoretical baseline.\", \"2. Formalize the list reversal problem within a higher-order logic (e.g., Calculus of Constructions) and derive the mathematical structure of an inductive proof.\", \"3. Design a differentiable loss function that relaxes the discrete proof-checking process, enabling gradient-based guidance for the program synthesizer.\", \"4. Implement the synthesizer and the neural guidance mechanism in Python, ensuring the search space and actions are well-defined.\", \"5. Execute the synthesis process, collect performance metrics, and validate the correctness of the synthesized program against a symbolic interpreter.\", \"6. Audit the final program's proof trace against the initial formal derivation to verify that the neural-symbolic loop produced a logically sound artifact consistent with the Curry-Howard isomorphism.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal derivation) -> /code (implementation) -> /math (verification & analysis) -> router QA (verification: all artifacts are cross-consistent; fallback: re-run /code with corrected logic from /math if verification fails).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 seminal arXiv papers on differentiable provers for inductive synthesis.\", \"- [ ] /math: Produce a LaTeX document with the formal derivation of the inductive proof and the differentiable loss function.\", \"- [ ] /code: Implement the neural program synthesizer in PyTorch, including the search algorithm and neural guidance module.\", \"- [ ] /code: Run synthesis experiments and log the primary and secondary metrics to a JSON file.\", \"- [ ] /math: Verify that the implemented loss function in the Python code correctly matches the mathematical derivation.\", \"- [ ] /math: Formally check the synthesized program's proof trace for correctness and structural equivalence (Curry-Howard).\", \"- [ ] /router: Perform final QA check, ensuring all acceptance criteria are met and artifacts are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic-methods\", \"program-synthesis\", \"theorem-proving\"], \"acceptance_criteria\": [\"- The system successfully synthesizes a provably correct list-reversal program.\", \"- A formal mathematical derivation for the differentiable loss function is provided and referenced in the code.\", \"- The final report includes an empirical analysis of convergence steps and cites at least two relevant arXiv papers.\", \"- The synthesized program's structure is shown to be isomorphic to its formal proof.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 959, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a comparative analysis of dictionary learning algorithms for sparse signal recovery. Specifically, implement both K-SVD and the Method of Optimal Directions (MOD) to reconstruct synthetic signals from compressed measurements under a Bernoulli-Gaussian noise model. Your analysis must include a formal derivation of the update rules, a discussion on their convergence properties, and an empirical evaluation of reconstruction accuracy (PSNR) versus dictionary atom count and signal sparsity level. The implementation must be robust and well-documented.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD MOD dictionary learning convergence proof compressed sensing\\\", mode=both)\", \"/math(Derive the iterative update rules for both K-SVD and MOD, analyzing the Lagrangian for the sparse coding and dictionary update stages. Investigate convergence guarantees and complexity based on the search results.)\", \"/code(Implement K-SVD and MOD in Python using NumPy. Construct a simulation to generate sparse signals, apply a sensing matrix, add Bernoulli-Gaussian noise, and evaluate reconstruction performance. Track PSNR and dictionary recovery error metrics for different Bayesian hyperparameter settings.)\", \"/math(Verify the correctness of the implemented numerical optimization steps in the Python code against the formal mathematical derivations. Specifically, check the gradient calculations for the dictionary update and the sparse coding solver's optimality conditions.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on K-SVD and MOD. This theoretical grounding is then handed to /math to produce formal derivations. /code implements these derivations in a verifiable simulation environment. A final /math call is crucial for auditing the code's numerical methods against the formal theory, ensuring correctness before final analysis.\", \"expected_artifacts\": [\"A technical report in Markdown format containing the full mathematical derivations for K-SVD and MOD, convergence analysis, and a summary of empirical results.\", \"A Python script (`dictionary_learning_comparison.py`) with implementations of both algorithms and the simulation framework.\", \"A set of plots (`results.png`) visualizing PSNR vs. signal sparsity and dictionary size for both algorithms.\", \"A JSON file (`metrics.json`) reporting the final PSNR, SSIM, and convergence rates.\", \"A bibliography file (`references.bib`) with at least two cited papers on dictionary learning.\"], \"thinking_outline\": [\"1. Research the foundational papers on K-SVD (Aharon, Elad, Bruckstein) and MOD to understand their algorithmic structure and theoretical underpinnings.\", \"2. Formally derive the two main steps for each algorithm: sparse coding (e.g., via Orthogonal Matching Pursuit) and the dictionary update step, paying close attention to the optimization problem being solved.\", \"3. Implement the complete algorithms and a testing harness for generating synthetic sparse data and compressed measurements.\", \"4. Validate the implementation by checking the objective function's monotonic decrease per iteration and comparing reconstruction results on a toy problem with a known ground-truth dictionary.\", \"5. Systematically run experiments comparing K-SVD and MOD across varying signal sparsity levels and measurement rates, collecting PSNR and dictionary recovery metrics.\", \"6. Verify that the mathematical expressions in the final report are identical to the logic implemented in the Python code's update rules, ensuring theoretical and practical alignment.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent for derivation. The math agent's derivations serve as a blueprint for the code agent. The code implementation is then passed back to the math agent for a formal verification of the numerical methods. All artifacts are reviewed by the router QA. Fallback: If verification fails, the code agent is tasked with debugging the implementation against the math agent's derivations.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on K-SVD and MOD, focusing on convergence proofs and complexity analysis.\", \"- [ ] /math: Derive the update rules for the sparse coding and dictionary update stages of both algorithms.\", \"- [ ] /code: Implement the K-SVD and MOD algorithms, along with a data simulation pipeline for sparse signals.\", \"- [ ] /code: Run simulations to compare the algorithms based on PSNR and dictionary recovery error metrics.\", \"- [ ] /math: Verify that the implemented gradient descent or SVD update steps in the code align with the formal derivations.\", \"- [ ] /code: Generate plots and a final metrics summary JSON from the simulation results.\", \"- [ ] /router: Perform final QA on all artifacts, checking for consistency between the report, code, and results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The Python code must correctly implement both K-SVD and MOD algorithms, runnable via a single command.\", \"- The final report must contain detailed mathematical derivations for both algorithms' update steps.\", \"- The empirical results must demonstrate a clear performance comparison, with plots showing PSNR as a function of problem parameters.\", \"- The code must pass a linter and include comments explaining the core logic of the dictionary updates.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 970, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the InfoNCE loss as a lower bound on mutual information for contrastive learning and implement a Siamese network using a simple attention mechanism to verify its convergence properties on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/math(Derive the InfoNCE loss as a lower bound on mutual information using Jensen's inequality, clearly stating all assumptions about the sample distribution and the encoder function's complexity.)\", \"/general-search(query=\\\"site:arxiv.org contrastive learning InfoNCE mutual information lower bound bayesian proof\\\", mode=both)\", \"/code(Implement a PyTorch Siamese network with a simple self-attention mechanism, using InfoNCE loss. Validate its convergence on a synthetic dataset by plotting loss and reporting final representational similarity., using Python)\"], \"route_rationale\": \"/math is chosen to formally derive the theoretical underpinnings of the InfoNCE loss. The blue /general-search agent will then be used to find authoritative academic papers on arXiv to corroborate the derivation and provide context. Finally, /code is used to implement the model and empirically validate the theoretical convergence properties.\", \"expected_artifacts\": [\"A formal mathematical derivation of the InfoNCE loss as a lower bound on mutual information, delivered as a LaTeX document.\", \"A Python script implementing the Siamese network, attention mechanism, and InfoNCE loss.\", \"A summary report including loss curves, visualization of learned embeddings, and a discussion of the results.\", \"A bibliography with at least two cited arXiv papers supporting the theoretical derivation.\"], \"thinking_outline\": [\"1. Formulate the problem of contrastive representation learning in the language of probability theory, defining positive and negative pairs.\", \"2. Apply the definition of mutual information between the original sample and its augmented view, and use Jensen's inequality to establish the InfoNCE objective as a lower bound.\", \"3. Research existing proofs and formulations of InfoNCE (e.g., CPC, MoCo, SimCLR) to cross-reference the derivation and identify key assumptions.\", \"4. Design and implement a simple Siamese network with a self-attention layer to process input sequences and generate embeddings.\", \"5. Validate the implementation by running it on a synthetic dataset where positive pairs have known similarity, ensuring the InfoNCE loss consistently decreases over training epochs and converges to a low value.\"], \"handoff_plan\": \"/math will produce a formal proof. -> /general-search will use keywords from the proof to find corroborating literature. -> /code will implement the derived loss function and a model, using the math and papers for reference. -> router QA (verification: The implemented loss in /code must match the final equation from /math, and the loss curve must show convergence; fallback: If the derivation is flawed, re-run /math with context from /general-search results).\", \"todo_list\": [\"- [ ] /math: Derive the InfoNCE loss function as a lower bound on mutual information.\", \"- [ ] /general-search: Find at least two key papers on arXiv that detail the InfoNCE loss derivation (e.g., Oord et al. 2018).\", \"- [ ] /code: Implement a Siamese network with a dot-product attention module in PyTorch.\", \"- [ ] /code: Implement the InfoNCE loss function based on the mathematical derivation.\", \"- [ ] /code: Create a synthetic dataset and a training loop to validate the model's convergence.\", \"- [ ] /router: Review all artifacts for consistency between the theoretical derivation, cited literature, and code implementation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"self-supervised-learning\", \"attention-mechanisms\", \"information-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies Jensen's inequality and arrives at the standard InfoNCE loss formulation.\", \"- The Python implementation successfully runs, and the training loss curve shows a clear downward trend, indicating convergence.\", \"- The final report cites at least two relevant arXiv papers with correct IDs that support the theoretical claims.\", \"- The learned representations for positive pairs are demonstrably more similar (e.g., higher cosine similarity) than for negative pairs post-training.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 986, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous comparison of Mean-Field Variational Inference (MFVI) and Structured Variational Inference, specifically using Variational Message Passing (VMP), for a Bayesian logistic regression model. Derive the evidence lower bound (ELBO) update rules for both, implement them, and empirically analyze their convergence rates and the quality of the posterior approximation. The analysis should investigate the impact of the posterior covariance structure on convergence speed and final ELBO.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational message passing bayesian logistic regression convergence analysis\\\", mode=rag)\", \"/math(Derive the complete ELBO and coordinate ascent updates for both MFVI and VMP applied to a Bayesian logistic regression model, paying close attention to the Kullback-Leibler divergence term under different factorization assumptions.)\", \"/code(Implement both the MFVI and VMP algorithms in Python using JAX, tracking the ELBO at each iteration and ensuring numerical stability. The code must handle synthetic data generation and compute posterior predictive distributions for model evaluation.)\", \"/math(Analyze the empirical convergence plots from the JAX implementation, relating the observed convergence speed to the Hessian of the ELBO and the eigenvalue structure of the approximate posterior covariance matrix for each method.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on VMP convergence. This informs the /math agent's formal derivation of the update equations. The /code agent then implements these derivations for empirical testing. Finally, the /math agent returns to synthesize the theoretical and empirical results, providing a deep analysis of the algorithms' behavior.\", \"expected_artifacts\": [\"- A PDF document containing the full mathematical derivations of the MFVI and VMP updates for Bayesian logistic regression.\", \"- A Python script (`.py`) implementing both algorithms, including data simulation and visualization code.\", \"- A set of plots (`.png`) comparing the ELBO convergence curves and posterior approximations from both methods.\", \"- A final summary report (`.md`) synthesizing the findings, including a verification log and at least two citations from arXiv.\", \"- A JSON file with final metrics (ELBO, PLL, iterations to convergence).\"], \"thinking_outline\": [\"1. Formulate the Bayesian logistic regression model, specifying priors on the weights.\", \"2. Derive the Evidence Lower Bound (ELBO) for this model.\", \"3. Derive the specific coordinate ascent update equations under the mean-field assumption (fully-factorized posterior).\", \"4. Derive the update equations for Variational Message Passing, retaining some posterior dependency structure.\", \"5. Implement both algorithms and validate the code's output by checking for monotonic ELBO increase and comparing results on a trivial, low-dimensional problem.\", \"6. Run experiments, analyze the convergence plots and posterior quality, and verify that the empirical results align with the theoretical differences between the methods.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (analysis) -> router QA. The code agent must produce ELBO convergence plots that the math agent will analyze. Verification: The final math analysis must explicitly reference the plots generated by the code. Fallback: If the code implementation shows unstable ELBO (non-monotonic), the code agent must flag this and revert to the math agent to check the derivation for potential errors before proceeding.\", \"todo_list\": [\"- [ ] /general-search: Find 2-3 key papers on VMP and its convergence properties for generalized linear models.\", \"- [ ] /math: Produce a clean, step-by-step LaTeX derivation of the MFVI and VMP update equations.\", \"- [ ] /code: Implement the derived updates in a JAX script, ensuring the ELBO is tracked per iteration.\", \"- [ ] /code: Generate synthetic data and create plots for ELBO convergence and posterior visualization.\", \"- [ ] /math: Verify that the implemented updates in the JAX script correspond exactly to the derived equations.\", \"- [ ] /math: Write a formal analysis of the empirical results, connecting them to the theoretical properties of each VI method.\", \"- [ ] /code: Perform a final code audit for clarity, correctness, and reproducibility.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"bayesian-deep-learning\"], \"acceptance_criteria\": [\"- The mathematical derivation for both MFVI and VMP must be complete and self-contained.\", \"- The Python code must be executable and reproduce the plots and metrics presented in the final report.\", \"- The final report must clearly articulate why one method converges faster or achieves a better ELBO, referencing both the theory and the empirical plots.\", \"- The final ELBO from the VMP implementation must be greater than or equal to the final ELBO from the MFVI implementation, as expected from theory.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 993, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Karush-Kuhn-Tucker (KKT) conditions for a sparsity-inducing L1 regularization problem on a deep neural network's weight matrix, subject to an inequality constraint on the spectral norm to control the Lipschitz constant. Then, implement a primal-dual interior-point method in JAX to find the optimal weights and verify the convergence properties against theoretical bounds.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org primal-dual interior point spectral norm deep learning convergence\\\", mode=both)\", \"/math(Derive the Euler-Lagrange equations for the Lagrangian of the L1-regularized problem, establishing the KKT conditions for optimality under the spectral norm and Bayesian prior constraints.)\", \"/code(Implement the primal-dual solver in JAX, using CUDA for acceleration. Track the duality gap and KKT residuals to monitor convergence and ensure the solution is robust., using Python)\", \"/math(Verify the final solution's KKT residuals are below tolerance. Analyze the empirical convergence rate against theoretical bounds from control theory literature on optimization.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather state-of-the-art literature. This informs the /math agent for the core theoretical derivation. The /code agent then implements the derived algorithm. A final /math step is crucial for verifying the implementation's correctness against the initial theory.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the optimization problem, Lagrangian, and KKT conditions.\", \"A Python script implementing the primal-dual interior-point method using JAX.\", \"A summary report including plots of the duality gap and KKT residuals over iterations, comparing empirical convergence to theoretical rates.\", \"A verification log confirming the final weight matrix satisfies the KKT conditions to a tolerance of 1e-6.\", \"A bibliography citing at least two relevant arXiv pre-prints with their IDs.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem mathematically, defining the objective function (e.g., cross-entropy loss plus L1 norm) and the inequality constraint (spectral norm of weights <= C).\", \"2. Construct the Lagrangian for the problem and formally derive the Karush-Kuhn-Tucker (KKT) conditions: stationarity, primal feasibility, dual feasibility, and complementary slackness.\", \"3. Design a primal-dual interior-point algorithm by applying Newton's method to the perturbed KKT system, specifying the search direction calculation.\", \"4. Implement the solver in JAX, paying close attention to the numerical stability of solving the linear system for the Newton step at each iteration.\", \"5. Validate that the converged solution satisfies the derived KKT conditions, particularly complementary slackness and dual feasibility, within a small numerical tolerance.\", \"6. Audit the empirical convergence rate by plotting the duality gap against iteration count on a log-log scale and comparing the slope to theoretical predictions for interior-point methods.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides foundational papers to /math. /math produces a formal derivation which serves as a specification for the /code agent. The /code agent's output (a trained model's weights and convergence logs) is then handed back to /math for formal verification. Fallback: If verification fails, the task is routed back to /code with specific residual analysis to debug the implementation.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on primal-dual methods for constrained deep learning optimization.\", \"- [ ] /math: Derive the complete KKT conditions for the L1-regularized, spectral-norm-constrained problem.\", \"- [ ] /code: Implement the primal-dual interior-point method in JAX, including the Newton step computation.\", \"- [ ] /code: Add logging to track primal/dual residuals, the duality gap, and the spectral norm at each iteration.\", \"- [ ] /math: Verify that the final solution from the JAX code satisfies the derived KKT conditions.\", \"- [ ] /math: Create plots comparing the empirical convergence rate to theoretical complexity bounds.\", \"- [ ] /router: Perform final QA review of all artifacts, checking for consistency between derivation, code, and verification report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"deep-learning-theory\", \"spectral-norm-regularization\"], \"acceptance_criteria\": [\"- The mathematical derivation in the LaTeX document correctly applies the KKT conditions to the specified problem.\", \"- The JAX implementation converges to a solution where the L2 norm of the KKT residuals is below a tolerance of 1e-6.\", \"- The final report includes a plot showing the duality gap decreasing over iterations and correctly cites at least two relevant arXiv papers with IDs.\", \"- The spectral norm of the final weight matrix respects the inequality constraint.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Final difference between the primal and dual objective function values. Should approach zero for optimal solutions; target < 1e-6.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"L2 norm of the residual vector for the full KKT system. Measures how well the optimality conditions are met; target < 1e-6.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1021, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous derivation of the evidence lower bound (ELBO) for a variational autoencoder where the latent space dynamics are governed by a Stochastic Differential Equation (SDE). Subsequently, implement a Neural SDE solver using the Euler-Maruyama method to train this Bayesian deep learning model and analyze the posterior collapse phenomenon by tracking the Kullback-Leibler divergence term's convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian deep learning stochastic differential equation ELBO derivation\\\", mode=both)\", \"/math(Derive the continuous-time evidence lower bound (ELBO) for a Bayesian VAE with SDE-based latent dynamics, referencing the Girsanov theorem for the change of measure between the prior and variational posterior.)\", \"/code(Implement a Neural SDE VAE in PyTorch and torchsde using the Euler-Maruyama method. Optimize the ELBO and track KL divergence for convergence analysis on a synthetic spiral dataset., using Python)\", \"/math(Formally analyze the Kullback-Leibler divergence term from the training logs to diagnose posterior collapse in the Bayesian model, connecting the empirical results to the theoretical ELBO derivation and SDE properties.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on Neural SDEs and variational inference. This is handed to /math for the core theoretical derivation of the ELBO. The derived formula then informs the /code agent's implementation of the model and training loop. Finally, the empirical results from /code are passed back to /math for a rigorous analysis connecting theory to practice.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the SDE-based ELBO.\", \"A commented Python script implementing the Neural SDE Variational Autoencoder.\", \"A summary report (PDF) analyzing the posterior collapse phenomenon, including plots of the KL divergence and reconstruction loss over training epochs.\", \"A bibliography file (`.bib`) with citations for at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research the formulation of continuous-time Bayesian models using SDEs and the associated variational inference objectives (ELBO), focusing on works that leverage Girsanov's theorem.\", \"2. Formally derive the ELBO for an SDE-based latent variable model, explicitly defining the prior SDE, the variational posterior SDE, and the likelihood function.\", \"3. Implement the derived model in code, including a stable SDE solver (Euler-Maruyama) and the full ELBO loss function.\", \"4. Validate the implementation by checking for ELBO convergence on a synthetic dataset and verifying that the KL divergence term behaves as expected during training.\", \"5. Analyze the trained model's logs to diagnose and explain the posterior collapse phenomenon, connecting empirical observations back to the theoretical derivation.\", \"6. Audit the final report to ensure the mathematical derivation, code implementation, and empirical analysis are consistent and rigorously presented.\"], \"handoff_plan\": \"/general-search -> /math (verification: check if found literature is sufficient for derivation) -> /code (verification: ensure implemented loss matches derived ELBO) -> /math (verification: ensure analysis uses correct log data) -> router QA (verification: all artifacts are consistent; fallback: request targeted rework from agent if discrepancy is found).\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on Neural SDEs and variational inference for continuous-time models.\", \"- [ ] /math: Derive the evidence lower bound (ELBO) based on the researched literature, detailing the application of Girsanov's theorem.\", \"- [ ] /code: Implement the VAE model with an SDE latent space using PyTorch and the torchsde library.\", \"- [ ] /code: Implement the training loop, loss function, and logging for KL divergence and reconstruction error.\", \"- [ ] /math: Verify that the implemented loss function in the code correctly matches the derived ELBO expression symbol-for-symbol.\", \"- [ ] /math: Analyze the logged metrics to produce a formal, written explanation of the observed posterior collapse.\", \"- [ ] /code: Generate plots visualizing the latent space and the evolution of the KL divergence over training epochs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-differential-equations\", \"variational-inference\"], \"acceptance_criteria\": [\"The ELBO derivation is mathematically sound, self-contained, and correctly applies principles from stochastic calculus.\", \"The Python implementation successfully trains on a synthetic dataset, demonstrating a decreasing loss trend and generating coherent latent space visualizations.\", \"The final analysis report provides a clear, evidence-based explanation for posterior collapse, supported by both the derivation and plotted metrics from the code's execution.\", \"The solution cites at least two relevant arXiv papers with their corresponding IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1059, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the asymptotic variance of the Augmented Inverse Probability Weighting (AIPW) estimator for the Average Treatment Effect (ATE) under a non-parametric structural causal model. I need the full influence function derivation, a discussion of its double robustness property, and a simulation study in Python to verify the theoretical variance against a bootstrap estimate for a challenging causal scenario with high confounder dimensionality.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:stat.berkeley.edu semi-parametric efficiency influence function causal inference AIPW econometrics\\\", mode=both)\", \"/math(Derive the influence function and asymptotic variance for the AIPW estimator in a non-parametric causal model, referencing foundational econometrics literature on semi-parametric theory.)\", \"/code(Implement a Python simulation using a non-parametric bootstrap method to empirically estimate the AIPW variance and compare it to the theoretical result, ensuring numerical stability for the causal model.)\", \"/math(Verify that the Python implementation of the asymptotic variance formula exactly matches the derived result from the previous math step, checking all terms and coefficients for the causal estimators.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather seminal papers on semi-parametric theory for AIPW. This informs the /math agent's formal derivation of the influence function and variance. The /code agent then implements a simulation to empirically validate the theory. A final /math step is crucial for cross-validating the code's mathematical implementation against the formal proof, ensuring correctness before final reporting.\", \"expected_artifacts\": [\"A LaTeX document providing the full derivation of the AIPW estimator's influence function and its asymptotic variance.\", \"A commented Python script (.py) that implements the data generating process, the AIPW estimator, and the bootstrap validation procedure.\", \"A JSON file containing the simulation results, including the theoretical variance, the mean of bootstrap variances, and the estimator bias.\", \"A summary report in Markdown that interprets the simulation results and confirms the alignment between theoretical and empirical findings, including plots.\", \"A bibliography file with at least two cited arXiv preprints on semi-parametric causal inference.\"], \"thinking_outline\": [\"1. Research the semi-parametric theory behind the AIPW estimator, focusing on influence functions and the efficient influence function for the Average Treatment Effect (ATE) parameter in econometrics.\", \"2. Formally derive the influence function for the AIPW estimator and use it to derive the closed-form expression for its asymptotic variance.\", \"3. Implement a simulation study in Python. This involves creating a data-generating process from a known structural causal model, implementing the AIPW estimator (with nuisance models for propensity score and outcome), and a non-parametric bootstrap procedure to estimate its sampling variance.\", \"4. Validate the simulation results by comparing the mean of the bootstrap variance estimates to the theoretical asymptotic variance derived in step 2. Check for convergence as the sample size increases.\", \"5. Audit the Python code to ensure the mathematical formulas for the estimator and its theoretical variance are implemented correctly, paying close attention to potential numerical stability issues.\", \"6. Synthesize all findings into a cohesive report, including the mathematical derivation, simulation code, results, plots, and required citations.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (simulation) -> /math (verification) -> router QA (verification: Compare theoretical variance from /math with empirical from /code; fallback: If discrepancy > 5%, return to /code for implementation audit).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 key papers on AIPW, influence functions, and semi-parametric efficiency.\", \"- [ ] /math: Produce a full LaTeX derivation of the AIPW influence function and asymptotic variance.\", \"- [ ] /code: Implement the simulation environment, including the SCM, AIPW estimator, and bootstrap procedure.\", \"- [ ] /code: Run the simulation and generate a JSON artifact with key metrics (theoretical variance, mean bootstrap variance, bias).\", \"- [ ] /math: Formally verify that the variance formula implemented in the Python script is identical to the derived mathematical expression.\", \"- [ ] /code: Generate plots visualizing the distribution of bootstrap estimates and the comparison of variances.\", \"- [ ] /router: Perform final QA review, checking acceptance criteria and ensuring all artifacts are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"semi-parametric-theory\", \"econometrics\"], \"acceptance_criteria\": [\"- The mathematical derivation of the influence function and asymptotic variance must be complete and mathematically sound.\", \"- The Python simulation must run without errors and produce the specified JSON and plot artifacts.\", \"- The primary metric, `VarianceDiscrepancy`, must be less than 5% for a large sample size (e.g., n=10000).\", \"- The final report must correctly cite at least two relevant academic papers with their arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1063, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and evaluate a belief propagation-based decoder for the toric code subjected to a depolarizing noise model. Your task is to formalize the message-passing update rules based on Bayesian inference principles, implement the decoder in Python, and simulate its performance to plot the logical error rate versus the physical error rate to identify the error threshold. Finally, analyze the algorithm's computational complexity and convergence properties.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org belief propagation decoder toric code bayesian inference depolarizing noise\\\", mode=web)\", \"/math(Derive the factor graph representation for the toric code's stabilizer generators. Formalize the Bayesian message-passing update rules for the belief propagation decoder and analyze the theoretical convergence criteria.)\", \"/code(Implement the toric code simulator and the belief propagation decoder in Python using JAX for performance. Track the logical error rate against physical error rate and verify the convergence behavior observed in simulation.)\", \"/math(Analyze the computational complexity of the implemented decoder per round and its scaling with lattice size. Compare this complexity to established results for minimum-weight perfect matching decoders, referencing the eigenvalue properties.)\"], \"route_rationale\": \"The plan starts with literature review via the blue /general-search agent to ground the approach in current research. /math is used first to establish a solid theoretical and mathematical foundation for the decoder's logic. /code then translates this formal model into a verifiable simulation. A final /math call provides a rigorous analysis of the implemented algorithm's performance characteristics, ensuring a complete theoretical and practical evaluation.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF report detailing the mathematical derivation of the belief propagation update rules and the complexity analysis.\", \"- A well-documented Python script (`decoder_simulation.py`) containing the toric code simulator and the BP decoder.\", \"- A plot (`ler_vs_p.png`) visualizing the logical error rate as a function of the physical error rate, clearly showing the error threshold.\", \"- A `results.json` file containing the raw data for the plot and the calculated complexity metrics.\", \"- A bibliography with at least two cited arXiv papers on the topic.\"], \"thinking_outline\": [\"1. Research existing literature on belief propagation (BP) decoders for quantum stabilizer codes, specifically the toric code, to understand the standard factor graph construction and message-passing schedules.\", \"2. Formulate the decoding problem within a Bayesian inference framework. Derive the specific message update equations for the variable and check nodes in the toric code's factor graph under a depolarizing noise model.\", \"3. Implement the simulator and decoder. The implementation must include functions for initializing a toric code lattice, applying random Pauli errors based on the physical error rate, running the BP decoder until convergence or a maximum number of iterations, and checking for logical errors.\", \"4. Validate the implementation by checking for convergence on small, known problem instances. Verify that the logical error rate behaves as expected (decreases with code size for p below the threshold).\", \"5. Systematically sweep the physical error rate `p` for different code distances `d` to gather statistics for the logical error rate `p_L`. Plot `p_L` vs. `p` to visually identify the error correction threshold.\", \"6. Audit the complexity analysis by comparing the derived scaling laws with empirical runtime measurements from the simulation and with theoretical results from the cited literature.\"], \"handoff_plan\": \"/general-search -> /math (verification: mathematical derivations must align with principles from cited papers; fallback: refine search queries for alternative formulations) -> /code (verification: simulation output must match theoretical predictions for trivial cases; fallback: debug implementation against the math derivation) -> /math (verification: complexity analysis must match observed scaling in the code; fallback: re-evaluate the complexity derivation) -> router QA.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on BP decoders for the toric code.\", \"- [ ] /math: Produce a formal derivation of the message-passing rules on the code's factor graph.\", \"- [ ] /code: Implement the toric code simulation environment and the BP decoder algorithm.\", \"- [ ] /code: Generate simulation data by sweeping physical error rates for multiple code distances.\", \"- [ ] /math: Verify that the implemented decoder's convergence matches the derived theoretical conditions.\", \"- [ ] /math: Conduct a formal big-O complexity analysis of the algorithm.\", \"- [ ] /code: Verify that empirical runtimes from the simulation scale according to the complexity analysis.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"fault-tolerance\"], \"acceptance_criteria\": [\"- The submitted Python code must be executable and reproduce the submitted plot of logical vs. physical error rate.\", \"- The final report must contain a complete mathematical derivation of the belief propagation update equations, consistent with the implemented code.\", \"- The plot must clearly demonstrate an error threshold where the lines for different code distances cross, consistent with known results for the toric code (around 10-11%).\", \"- The complexity analysis must correctly state the scaling of the decoder's runtime with respect to the number of qubits.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1065, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the loss landscape of a minimax optimization problem, specifically for a toy Wasserstein GAN (WGAN). I need to derive the conditions for saddle points and implement a Python script using JAX to find these points. The goal is to analyze the Hessian eigenvalue spectrum at these critical points to distinguish them from local minima. Furthermore, I want to compare the convergence behavior of Gradient Descent/Ascent (GDA) against a second-order method designed for escaping saddles, like a Newton-based approach, measuring the generator's FID score.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Wasserstein GAN saddle point Hessian eigenvalue analysis convergence\\\", mode=rag)\", \"/math(Derive the Euler-Lagrange equations for the WGAN minimax objective to find critical points, then formulate the Hessian and detail the eigenvalue criteria for classifying saddles, referencing adversarial training convergence theory.)\", \"/code(Implement a WGAN in JAX with a Newton-CG solver for escaping saddle points. Track FID score and adversarial loss convergence, ensuring CUDA compatibility and deterministic execution for reproducibility., using Python)\", \"/math(Verify the numerical Hessian eigenvalue spectrum computed by the JAX code at converged points against the analytical saddle point conditions, confirming the theoretical complexity and stability properties.)\"], \"route_rationale\": \"/general-search (blue agent) is used first to gather state-of-the-art literature on saddle point optimization in GANs. /math then uses this context to build a formal theoretical foundation. /code implements the derived theory in a high-performance framework (JAX). A final /math call is crucial for verification, ensuring the empirical results from the code align with the mathematical theory.\", \"expected_artifacts\": [\"A PDF document with the full mathematical derivation of the WGAN objective's Hessian and eigenvalue conditions for saddle points.\", \"A Python script (`wgan_saddle_analysis.py`) implementing the WGAN, optimizers, and Hessian analysis using JAX.\", \"A results summary (`results.json`) containing final FID scores, loss curves, and eigenvalue spectra for points found by each optimizer.\", \"A report (`report.md`) summarizing the findings, including plots comparing optimizer convergence and citing at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Review literature on non-convex optimization in GANs to identify appropriate second-order methods and analysis techniques.\", \"2. Formulate the mathematical problem: define the WGAN loss, derive the gradient and Hessian for the joint parameter space.\", \"3. Implement the WGAN and two optimizers (GDA, Newton-CG) in JAX, including a function to compute the Hessian eigenvalue spectrum.\", \"4. Execute training runs, log metrics (FID, loss), and save model checkpoints at critical points identified by small gradient norms.\", \"5. Validate the implementation by comparing the numerically computed Hessian eigenvalues at these critical points against the derived theoretical conditions for saddle points.\", \"6. Synthesize results into a report comparing the optimizers' effectiveness at escaping saddles and achieving better generator performance.\"], \"handoff_plan\": \"/general-search -> /math (theory derivation) -> /code (implementation & experimentation) -> /math (verification) -> router QA (verification: code outputs match theoretical predictions; fallback: if mismatch, return to /code to debug Hessian implementation).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on saddle point analysis in GANs and second-order optimizers.\", \"- [ ] /math: Derive the first and second-order conditions for the WGAN minimax problem.\", \"- [ ] /code: Implement the WGAN, GDA optimizer, and a Newton-CG optimizer in JAX.\", \"- [ ] /code: Add functionality to compute the Hessian matrix and its eigenvalues at arbitrary parameter configurations.\", \"- [ ] /code: Run experiments comparing GDA and Newton-CG, logging loss, FID, and gradient norms.\", \"- [ ] /math: Verify that the eigenvalues computed by the code at critical points correctly identify them as saddle points or local minima based on the prior derivation.\", \"- [ ] /router: Perform final QA review, ensuring the report, code, and derivations are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"saddle-point-optimization\", \"adversarial-training\", \"hessian-analysis\"], \"acceptance_criteria\": [\"- The submitted JAX code must be runnable and reproduce the reported FID scores and loss curves within a 5% tolerance.\", \"- The mathematical derivation must correctly define the Hessian for the WGAN and the eigenvalue conditions for a saddle point.\", \"- The final report must show at least one instance where the Newton-CG optimizer escaped a region (identified via Hessian analysis) where GDA stalled.\", \"- The report must cite at least two relevant academic papers from arXiv or peer-reviewed conferences.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1074, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a variational inference framework for a continuous-time latent variable model defined by a Stochastic Differential Equation (SDE). Can you help me derive the Evidence Lower Bound (ELBO) by applying Girsanov's theorem for the change of measure between the prior and the approximate posterior stochastic processes, and then implement a JAX-based solution to train this Bayesian neural SDE on time-series data?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference neural stochastic differential equations girsanov theorem bayesian\\\", mode=rag)\", \"/math(Derive the continuous-time Evidence Lower Bound for the Bayesian neural SDE, applying Girsanov's theorem to manage the change of measure and analyzing the KL divergence for convergence properties.)\", \"/code(Implement the derived SDE model and variational inference scheme in JAX, utilizing an Euler-Maruyama solver and tracking the ELBO metric throughout the bayesian optimization process., using Python)\", \"/math(Formally verify the analytical gradient of the KL divergence term from the derivation by comparing it with a finite difference approximation to audit correctness., includes cybersecurity)\"], \"route_rationale\": \"The blue general-search agent first grounds the task in authoritative literature from arXiv. The /math agent is then used to perform the complex theoretical derivation of the ELBO, which is central to the problem. Next, the /code agent translates this mathematical framework into a runnable JAX implementation. A final /math call is crucial for verifying the correctness of the derived gradients, a common failure point in such complex models.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the SDE ELBO.\", \"A Python script (`sde_vi.py`) implementing the Bayesian neural SDE and the training loop in JAX.\", \"A JSON file (`metrics.json`) reporting the final ELBO and posterior predictive log-likelihood.\", \"A PNG plot (`elbo_convergence.png`) showing the ELBO value per training epoch.\", \"A bibliography file with at least two cited arXiv preprints.\"], \"thinking_outline\": [\"1. Formulate the generative model as a Neural SDE and define the variational family for the approximate posterior process.\", \"2. Apply Girsanov's theorem to compute the Radon-Nikodym derivative needed to express the KL divergence between the prior and posterior path measures, thereby deriving the ELBO.\", \"3. Implement the SDE solver, the variational posterior network, and the derived ELBO loss function in JAX.\", \"4. Validate the implementation's stability and convergence by training the model on a known synthetic dataset, such as samples from an Ornstein-Uhlenbeck process.\", \"5. Verify the correctness of the mathematical derivation by cross-referencing against the sourced literature and checking for term consistency.\", \"6. Audit the code's gradient computation by comparing the analytical gradient of a key term (like the KL divergence) with a numerical approximation to ensure there are no implementation bugs.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation, which the code agent implements. The code agent hands the implementation back to the math agent to verify the gradient logic. (verification: successful gradient check; fallback: if gradients mismatch, the math agent revisits the derivation and the code agent checks the implementation of the reparameterization trick).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on variational inference for neural SDEs, focusing on the use of Girsanov's theorem.\", \"- [ ] /math: Derive the full ELBO, clearly showing the application of the change of measure and the resulting KL divergence and expected log-likelihood terms.\", \"- [ ] /code: Implement the SDE solver (e.g., Euler-Maruyama) and the neural networks for the drift and diffusion terms.\", \"- [ ] /code: Implement the training loop, including the derived ELBO as the loss function and an Adam optimizer.\", \"- [ ] /math: Verify that the derived KL divergence between the prior and posterior SDEs is consistent with formulations in the sourced literature.\", \"- [ ] /code: Validate the model's performance by confirming that the ELBO steadily increases during training on a synthetic dataset.\", \"- [ ] /router: Perform final QA, ensuring all acceptance criteria are met and artifacts are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-differential-equations\", \"variational-inference\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies Girsanov's theorem to derive the ELBO for the specified SDE.\", \"- The JAX implementation runs without errors and demonstrates a monotonically increasing ELBO trend over at least 1000 training iterations on a synthetic dataset.\", \"- The final report cites at least two relevant arXiv papers with their IDs.\", \"- The analytical gradient verification step in the final report shows a relative error of less than 1e-4 compared to the numerical approximation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Track the ELBO on the training set per epoch. The value should be non-decreasing on average, indicating successful optimization.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"After training, compute the log-likelihood of a held-out test set under the posterior predictive distribution generated by the model.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1076, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nImplement a persistent homology pipeline using Vietoris-Rips filtrations to analyze a high-dimensional point cloud sampled from a simulated Lorenz attractor. I need to identify the Betti numbers (\u03b2\u2080, \u03b2\u2081) to characterize its manifold structure and verify that the persistence diagram correctly captures the underlying toroidal topology. Compare the computational complexity against witness complexes, citing relevant literature on convergence guarantees from arXiv.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org computational complexity Vietoris-Rips vs witness complex convergence guarantees for manifold learning in bioinformatics\\\", mode=rag)\", \"/math(Formalize the algebraic topology definitions for the Vietoris-Rips filtration and persistence diagram, ensuring the logic supports the required Betti number convergence analysis., includes numpy)\", \"/code(Implement a Python pipeline using Ripser to compute the persistence diagram for a Lorenz system point cloud, profile the computational complexity, and track Betti number convergence.)\", \"/math(Verify the computed Betti numbers from the code's persistence diagram against the theoretical topology of the Lorenz attractor. Formulate the big-O complexity based on the arXiv findings.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on the complexity and convergence of TDA methods. This research informs the /math agent's formal derivation of the underlying topological concepts. The /code agent then implements the pipeline and performs empirical analysis. A final /math step verifies the empirical results against theory and formalizes the complexity analysis, ensuring a rigorous and well-supported solution.\", \"expected_artifacts\": [\"A technical report (.pdf) detailing the mathematical framework, implementation, and complexity analysis.\", \"Well-commented Python source code (.py) for the Lorenz simulation and persistent homology computation.\", \"A persistence diagram plot (.png) visualizing the topological features of the dataset.\", \"A verification log (.md) confirming that the computed Betti numbers match theoretical expectations.\", \"A citations file (.json) listing at least two relevant arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Research the theoretical underpinnings of persistent homology, focusing on Vietoris-Rips and witness complexes, their computational complexity, and stability theorems via authoritative sources.\", \"2. Formulate the precise mathematical definitions for the simplicial complex, filtration process, and the algorithm to compute Betti numbers from the persistence diagram.\", \"3. Implement the Lorenz system simulation to generate a point cloud, then apply the Vietoris-Rips filtration and compute its persistent homology using a standard library like Ripser or Gudhi.\", \"4. Visualize the results, specifically the persistence diagram, and interpret the features corresponding to the Betti numbers \u03b2\u2080 and \u03b2\u2081.\", \"5. Verify that the identified Betti numbers are consistent with the known topology of the Lorenz attractor (one connected component, one primary loop).\", \"6. Validate the implemented complexity by comparing empirical runtime profiling against the theoretical big-O bounds discovered during the research phase.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides arXiv papers to the /math agent for formalization. The math spec is handed to /code for implementation. The code's output (diagram, metrics) is sent back to /math for verification. Verification failure (e.g., incorrect Betti numbers) triggers a fallback to the /code agent to adjust filtration parameters. The complete set of artifacts is then passed to the router for final QA.\", \"todo_list\": [\"- [ ] /general-search: Find \u22652 recent arXiv papers on the computational complexity and convergence of Vietoris-Rips vs. witness complexes.\", \"- [ ] /math: Define the Vietoris-Rips filtration, Betti numbers, and the persistence diagram based on the search results.\", \"- [ ] /code: Implement the Lorenz attractor simulation and the persistent homology computation using a library like Ripser.\", \"- [ ] /code: Generate and save the persistence diagram plot, clearly labeling the axes.\", \"- [ ] /math: Verify the computed Betti numbers (\u03b2\u2080, \u03b2\u2081) from the code's output match the known topology of the Lorenz attractor (\u03b2\u2080=1, \u03b2\u2081=1).\", \"- [ ] /math: Formalize the big-O complexity analysis based on the literature and compare it with empirical runtime data from the code agent.\", \"- [ ] /router: Perform final QA review of all artifacts, ensuring consistency between the report, code, and verification log.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"persistent-homology\"], \"acceptance_criteria\": [\"The implementation correctly identifies Betti numbers \u03b2\u2080=1 and \u03b2\u2081=1 for the Lorenz system point cloud.\", \"The final report includes a formal complexity analysis comparing Vietoris-Rips and witness complexes, supported by at least two arXiv citations.\", \"The persistence diagram visualization clearly shows one significant feature in dimension 1 (a loop) and one in dimension 0 (a connected component) with high persistence.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1085, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for a Latent Dirichlet Allocation (LDA) model and formulate the natural gradient update for its global parameters in Stochastic Variational Inference (SVI). Then, implement this in Python using JAX, and empirically analyze the convergence behavior of the KL divergence against a standard stochastic gradient descent approach on a synthetic corpus. I'm particularly interested in the theoretical justification for the natural gradient's superior convergence properties in this Bayesian context.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'stochastic variational inference' 'natural gradient' 'latent dirichlet allocation' convergence\\\", mode=web)\", \"/math(Derive the complete ELBO for a Bayesian LDA model and then derive the natural gradient update equations for the global variational parameters, ensuring all steps follow from the Fisher information metric.)\", \"/code(Implement SVI for LDA in Python with JAX, comparing natural gradient and standard SGD updates. Track the ELBO convergence per iteration and profile computational complexity.)\", \"/math(Formally analyze the empirical convergence plots from the JAX implementation, linking the observed speedup of the natural gradient to its geometric interpretation as a preconditioned gradient respecting the Riemannian manifold of the Bayesian parameter space.)\"], \"route_rationale\": \"/general-search is used first by the blue agent to find authoritative papers on SVI for LDA, which will ground the mathematical derivation. /math then uses these sources to derive the core equations. /code implements the derived algorithms for empirical validation. Finally, /math returns to synthesize the theoretical and empirical results, providing a complete analysis.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the ELBO and the natural gradient update for LDA.\", \"A Python script (JAX) implementing SVI for LDA with both natural gradient and standard SGD optimizers.\", \"A report (PDF) containing plots comparing the convergence of the ELBO for both optimizers, a complexity analysis, and a discussion linking theory to empirical results.\", \"A list of cited academic papers (at least 2 arXiv IDs) used for the derivation.\"], \"thinking_outline\": [\"1. First, establish the theoretical foundation by reviewing seminal works on Stochastic Variational Inference (SVI) and the role of the natural gradient in Bayesian models.\", \"2. Formally derive the Evidence Lower Bound (ELBO) for the Latent Dirichlet Allocation (LDA) model and the specific update rule for the global parameters using the natural gradient, paying close attention to the Fisher information metric.\", \"3. Implement the derived update rules in code, alongside a baseline using standard stochastic gradient descent, to allow for empirical comparison on a controlled synthetic dataset.\", \"4. Verify the correctness of the mathematical derivation by cross-referencing against established literature and checking for dimensional consistency in the gradient expressions.\", \"5. Execute the comparative experiment, plotting the ELBO against iterations for both methods, and validate that the observed faster convergence of the natural gradient aligns with theoretical expectations.\", \"6. Conclude by synthesizing the theoretical derivation and empirical results into a cohesive report that explains *why* the natural gradient outperforms SGD in this context.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (analysis) -> router QA (verification: Cross-check that the code's gradient implementation matches the math derivation term-for-term; fallback: If mismatch, return to /math to re-verify the derivation and gradient computation).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers (e.g., Hoffman et al., 2013) on SVI and natural gradients for LDA.\", \"- [ ] /math: Derive the ELBO and the natural gradient update equations for LDA's global parameters.\", \"- [ ] /code: Implement the SVI algorithm for LDA using JAX, including both natural gradient and standard SGD optimizers.\", \"- [ ] /code: Generate a synthetic corpus and run experiments to collect ELBO convergence data.\", \"- [ ] /math: Verify that the implemented gradient update in the code correctly reflects the derived mathematical formula.\", \"- [ ] /math: Analyze the empirical results and provide a formal explanation for the observed convergence differences.\", \"- [ ] /router: Review final report, code, and derivations for coherence and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-inference\", \"graphical-models\"], \"acceptance_criteria\": [\"- The mathematical derivation of the natural gradient update must be complete, correct, and reference the Fisher information metric.\", \"- The Python implementation must correctly run for both optimizers and produce convergence plots showing the ELBO strictly increasing.\", \"- The final report must clearly demonstrate the superior convergence of the natural gradient and provide a theoretical justification consistent with the provided derivations and citations.\", \"- The work must cite at least two relevant academic papers, including the original SVI paper (Hoffman et al., 2013).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1092, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Natural Policy Gradient (NPG) update rule from first principles, treating the policy parameter space as a Riemannian manifold where the metric tensor is the Fisher Information Matrix (FIM). Prove that this update is equivalent to a trust-region optimization constrained by KL-divergence, and analyze its convergence properties regarding invariance to policy parameterization compared to the vanilla policy gradient. Finally, implement this in JAX and validate the convergence rate empirically.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Natural Policy Gradient Fisher Information Matrix Riemannian manifold convergence\\\", mode=rag)\", \"/math(Derive the Natural Policy Gradient update by defining the policy space as a Riemannian manifold with the Fisher Information Matrix as the metric tensor. Prove its equivalence to a KL-divergence-constrained trust region optimization, analyzing its preconditioning effect and convergence behavior.)\", \"/code(Implement both Vanilla and Natural Policy Gradient algorithms in JAX for a classic control theory environment. Track policy loss convergence and reward curves, ensuring the FIM inverse is computed efficiently and stably using the conjugate gradient method., using Python)\", \"/math(Verify the correctness of the JAX implementation by checking the FIM computation against a symbolic derivation for a simple policy. Audit the numerical stability of the conjugate gradient method used for the Fisher-vector product, ensuring convergence criteria are met.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature from arXiv on Natural Policy Gradients. This theoretical grounding is passed to the /math agent for the core derivations and proofs. The resulting mathematical specification is then handed to the /code agent for a high-performance JAX implementation and empirical analysis. A final, crucial /math step verifies that the code's FIM logic correctly reflects the derived theory, preventing subtle implementation bugs that could invalidate the results.\", \"expected_artifacts\": [\"A LaTeX document containing the full derivation of the Natural Policy Gradient and the proof of its KL-divergence trust-region equivalence.\", \"A Python script in JAX implementing both Vanilla and Natural Policy Gradients, with functions for running experiments and plotting results.\", \"A results summary file (`results.json`) containing the convergence metrics for both algorithms.\", \"A verification report detailing the symbolic check of the FIM implementation and the numerical stability audit.\", \"A bibliography citing at least 2 relevant arXiv papers on natural gradients.\"], \"thinking_outline\": [\"1. Establish the theoretical framework by defining the policy parameter space as a Riemannian manifold and the Fisher Information Matrix (FIM) as its metric tensor.\", \"2. Derive the Natural Policy Gradient update rule by finding the direction of steepest ascent on this manifold, which involves preconditioning the vanilla gradient with the inverse of the FIM.\", \"3. Prove that this update is equivalent to solving a trust-region optimization problem where the objective is to maximize the advantage function subject to a small, constant KL-divergence between the old and new policies.\", \"4. Implement both algorithms, paying close attention to the numerical stability of solving the linear system Fx=g using the conjugate gradient method to find the natural gradient without explicitly inverting the FIM.\", \"5. Validate the FIM implementation by comparing its output on a simple Gaussian policy with a manually computed symbolic result to ensure correctness.\", \"6. Analyze the empirical results to verify the theoretical claims about NPG's superior convergence rate and invariance to parameterization compared to the vanilla gradient.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers for the math agent's derivation. The math agent's derivation serves as the formal specification for the code agent's implementation. The final implementation is passed back to the math agent for formal verification. Verification: The FIM implementation in code must match a symbolic derivation for a test case. Fallback: If verification fails, the task is routed back to the /code agent to debug the FIM-vector product implementation, with a note to add more granular unit tests.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Natural Policy Gradients, focusing on the FIM and convergence analysis (e.g., by Kakade, Amari).\", \"- [ ] /math: Formulate and derive the Natural Policy Gradient update rule and prove its equivalence to a KL-divergence constrained trust-region method.\", \"- [ ] /code: Implement Vanilla and Natural Policy Gradient algorithms in JAX for a standard control theory environment (e.g., CartPole-v1).\", \"- [ ] /code: Instrument the implementation to log policy loss convergence and average reward per epoch for comparative analysis.\", \"- [ ] /math: Symbolically derive the FIM for a simple Gaussian policy and create a numerical test case to serve as a ground truth.\", \"- [ ] /math: Verify the JAX implementation of the Fisher-vector product against the symbolic test case and audit the conjugate gradient solver's stability.\", \"- [ ] /router: Review final artifacts, ensuring the empirical convergence results align with the theoretical proofs and all acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"information-geometry\"], \"acceptance_criteria\": [\"The mathematical derivation of NPG is correct and the proof of equivalence to a KL-constrained trust region is rigorous and complete.\", \"The JAX implementation correctly computes the Fisher-vector product using the conjugate gradient method, avoiding explicit FIM formation and inversion.\", \"The final report includes plots demonstrating that NPG exhibits significantly faster convergence (in epochs) than the vanilla policy gradient on the chosen task.\", \"The code passes the symbolic FIM verification test with a tolerance of at most 1e-6.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Computed as the number of training epochs required for the policy's average reward to reach a predefined threshold (e.g., 95% of the theoretical maximum for the environment).\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measured by the total number of environment steps (state-action-reward tuples) processed to achieve the convergence threshold.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1108, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Structured Variational Autoencoder (SVAE) for a conjugate-exponential family graphical model, as described in recent Bayesian deep learning literature. Can you help me derive the structured mean-field VI update equations, implement the model in PyTorch, and analyze the amortization gap by comparing the model's ELBO convergence against a non-amortized (per-datapoint) inference baseline? The analysis should also consider the impact of the recognition network's architecture on the final gap.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org structured variational autoencoder conjugate exponential family amortization gap\\\", mode=both)\", \"/math(Derive the structured mean-field variational inference update equations for a Bayesian model with conjugate-exponential priors, ensuring the ELBO convergence properties are clearly stated.)\", \"/code(Implement the SVAE in PyTorch using the derived updates, including a recognition network and a non-amortized inference baseline. Track ELBO and amortization gap metrics., using Python)\", \"/math(Analyze the Hessian eigenvalue spectrum of the recognition network's loss landscape to connect its curvature to the observed amortization gap and model convergence.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational papers on SVAEs and the amortization gap. This research informs the /math agent's formal derivation of the VI updates. The /code agent then implements this derivation in PyTorch. A final /math step is needed for a deeper theoretical analysis connecting the empirical results to the underlying optimization geometry.\", \"expected_artifacts\": [\"A LaTeX document with the full mathematical derivation of the structured VI updates.\", \"A Python script implementing the SVAE and the non-amortized baseline, with clear comments.\", \"A results report (PDF) containing plots of ELBO convergence and amortization gap, plus the analysis of the Hessian spectrum.\", \"A JSON file containing the final ELBO and amortization gap metrics.\", \"A bibliography file citing at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of Structured Variational Autoencoders (SVAEs) and the concept of the amortization gap in variational inference.\", \"2. Formally derive the mean-field update equations for the structured variational distribution within a conjugate-exponential family model.\", \"3. Implement the SVAE and a non-amortized VI baseline in PyTorch, ensuring the data loading and model architectures are consistent for a fair comparison.\", \"4. Validate the correctness of the ELBO implementation by checking for non-negativity of the KL divergence term and ensuring the loss decreases during training.\", \"5. Train both models and generate plots comparing their ELBO convergence curves and illustrating the amortization gap over training epochs.\", \"6. Verify the final analysis by cross-referencing the empirical amortization gap with the theoretical properties derived and the Hessian analysis.\"], \"handoff_plan\": \"/general-search -> /math (verification: derivations must match key equations in sourced papers) -> /code (verification: implementation must match mathematical formalism; fallback: revise math derivation if implementation reveals inconsistencies) -> /math (analysis) -> router QA (verification: all artifacts must be consistent and address the user query; fallback: re-run /code with corrected parameters if analysis is inconclusive).\", \"todo_list\": [\"- [ ] /general-search: Find 2-3 seminal papers on SVAEs and the amortization gap in VI.\", \"- [ ] /math: Derive the structured mean-field VI updates based on the literature.\", \"- [ ] /code: Implement the SVAE model, recognition network, and non-amortized baseline in PyTorch.\", \"- [ ] /code: Instrument the implementation to log ELBO and amortization gap per epoch.\", \"- [ ] /math: Verify the mathematical derivation aligns with the PyTorch implementation of the loss function.\", \"- [ ] /math: Perform the Hessian eigenvalue analysis on the trained recognition network.\", \"- [ ] /code: Generate plots and a final metrics JSON from the training logs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"bayesian-deep-learning\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for the SVAE updates must be complete and correct.\", \"- The Python code must run without errors and reproduce the reported ELBO and amortization gap metrics.\", \"- The final report must clearly visualize the amortization gap and provide a cogent analysis connecting it to the recognition network's properties, supported by at least two citations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1117, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the strong form of the residual loss for a Physics-Informed Neural Network (PINN) designed to solve the 1D viscous Burgers' equation. Implement this PINN in PyTorch, ensuring automatic differentiation correctly computes the PDE residual. Critically evaluate its convergence and accuracy against a finite difference scheme, particularly regarding shock formation and dissipation dynamics. Source key theoretical papers on PINN convergence from arXiv.\n", "completion": "{\"route_plan\": [\"/math(Derive the PINN loss function for the 1D viscous Burgers' equation, combining MSE on boundary/initial conditions with the PDE residual on collocation points. Detail the Euler-Lagrange perspective on this energy minimization problem and its expected convergence properties.)\", \"/general-search(query=\\\"site:arxiv.org physics-informed neural network convergence guarantees burgers equation Bayesian\\\", mode=both)\", \"/code(Implement the PINN in PyTorch with CUDA support, including a custom loss function based on the math derivation. Validate against a Fourier spectral method baseline, tracking L2 error and residual loss for convergence analysis., using Python)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation and loss function. The blue /general-search agent then gathers supporting literature and theoretical context from arXiv. Finally, /code implements the derived theory and validates it empirically against a high-fidelity baseline, providing a full-stack, verified solution.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the PINN loss for the Burgers' equation.\", \"- A documented Python script implementing the PINN solver and a finite difference baseline.\", \"- A set of plots visualizing the PINN solution, the baseline solution, and the point-wise error surface.\", \"- A JSON file containing the final L2 relative error and mean residual loss metrics.\", \"- A summary report citing and contextualizing at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the 1D viscous Burgers' equation, defining the problem with specific initial (e.g., sine wave) and Dirichlet boundary conditions.\", \"2. Derive the PINN loss function, L = L_data + L_pde, explicitly writing out the terms for the partial derivatives (u_t, u_x, u_xx) that form the PDE residual.\", \"3. Implement the PINN architecture in PyTorch, using `torch.autograd.grad` to compute the derivatives required for the residual loss from the network's output.\", \"4. Validate the automatic differentiation implementation by comparing the computed gradients for a known analytical function against their true derivatives to ensure correctness.\", \"5. Implement a baseline numerical solver (e.g., finite difference method) to generate a reference solution for accuracy comparison.\", \"6. Train the PINN, evaluate its performance against the baseline using L2 relative error, and verify that the final residual loss is minimized, confirming the physics constraint is being enforced.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the exact formula for the loss function. /general-search provides theoretical context and justification for the approach. /code implements this formula. The router QA performs the final verification: the loss function in the Python code must exactly match the symbolic form from the math derivation. Fallback: If the PINN fails to converge, /general-search will be re-queried for alternative architectures or hyperparameter tuning strategies for solving stiff PDEs.\", \"todo_list\": [\"- [ ] /math: Derive the complete loss function for the 1D Burgers' equation PINN, specifying data and PDE residual terms.\", \"- [ ] /general-search: Find \u22652 seminal arXiv papers on PINN convergence analysis for non-linear parabolic PDEs.\", \"- [ ] /code: Implement the PINN architecture and the custom loss function in PyTorch.\", \"- [ ] /code: Implement a finite difference solver for the Burgers' equation to serve as a baseline for comparison.\", \"- [ ] /math: Verify that the symbolic gradients in the derivation align with the output of PyTorch's autograd on a test case.\", \"- [ ] /code: Train the model, generate solution plots, and compute L2 relative error metrics against the baseline.\", \"- [ ] /router: Conduct final QA, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-neural-networks\", \"partial-differential-equations\", \"scientific-computing\"], \"acceptance_criteria\": [\"- The mathematical derivation of the loss function must be complete, correct, and self-contained.\", \"- The PyTorch implementation must run without errors and produce a solution that qualitatively matches the finite difference baseline, especially capturing the shock wave.\", \"- The final report must include visualizations and a quantitative L2 relative error below 5% compared to the baseline.\", \"- At least two relevant arXiv papers must be cited with their IDs and a brief summary of their contribution.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1124, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm mentoring a new student. Can you explain how to apply the method of Lagrange multipliers to find the optimal weights for a simple linear regression model under an L2-norm constraint? We need a clear, step-by-step derivation and a simple code visualization.\n", "completion": "{\"route_plan\": [\"/math(Formulate the optimization problem for a simple regression model with an L2 weight norm constraint, and then derive the first-order optimality conditions using the Euler-Lagrange equation for the corresponding Lagrangian function.)\", \"/general-search(query=\\\"site:distill.pub OR site:arxiv.org tutorial Lagrange multipliers L2 regularization visualization\\\", mode=web)\", \"/code(Implement the derived analytical solution in Python using NumPy and Matplotlib to visualize the loss landscape, the constraint boundary, and the path to the optimal solution, verifying the convergence to the theoretical result.)\"], \"route_rationale\": \"The route starts with /math to establish the core theoretical foundation and derive the necessary equations. Then, the blue /general-search agent is used to gather contextual, high-quality visualizations and explanations to aid understanding. Finally, /code provides a practical implementation of the theory, allowing for empirical verification and visualization of the mathematical concepts.\", \"expected_artifacts\": [\"A formal mathematical derivation in a Markdown file using LaTeX for the optimal weights using the method of Lagrange multipliers.\", \"A Python script that implements the analytical solution and generates a contour plot of the loss function with the L2 constraint boundary.\", \"A PNG image file visualizing the constrained optimum on the loss landscape, showing the point of tangency.\", \"A summary report citing at least two authoritative sources on L2 regularization and constrained optimization.\"], \"thinking_outline\": [\"1. Define the objective function (mean squared error for linear regression) and the L2-norm equality constraint for the model's weights.\", \"2. Construct the Lagrangian function by combining the objective function and the constraint with a Lagrange multiplier, \u03bb.\", \"3. Compute the gradient of the Lagrangian with respect to the weights and \u03bb, and set these partial derivatives to zero to find the first-order optimality conditions.\", \"4. Solve the resulting system of equations to express the optimal weights as a function of the data and the Lagrange multiplier.\", \"5. Validate the derived analytical solution by implementing it in code and verifying that the gradient of the loss at the solution is parallel to the gradient of the constraint function.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The formal derivation from /math provides the equations for /code to implement. The /general-search agent provides canonical examples to guide the visualization style in /code. The final code and derivation are passed to the router for QA. Verification: The numerical results from /code must match the analytical solution from /math. Fallback: If verification fails, revisit the /math derivation to check for errors in the gradient calculation.\", \"todo_list\": [\"- [ ] Use /math to derive the analytical solution for L2-constrained linear regression weights.\", \"- [ ] Use /general-search to find canonical explanations and visualizations of L2 regularization as a constrained optimization problem.\", \"- [ ] Use /code to implement the solution, generate a contour plot visualization, and verify the result.\", \"- [ ] Verify that the implemented solution satisfies the first-order optimality conditions derived in the math step.\", \"- [ ] Submit all artifacts (derivation, code, plot, citations) for final router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"lagrange-multipliers\", \"constrained-optimization\", \"regularization\"], \"acceptance_criteria\": [\"The mathematical derivation correctly applies the method of Lagrange multipliers and solves for the optimal weights.\", \"The Python code produces a visualization that accurately shows the unconstrained minimum, the constraint circle, and the constrained minimum at the point of tangency.\", \"The final report includes citations for at least two authoritative sources (e.g., textbook chapters, peer-reviewed papers).\", \"The computed optimal weights in the code satisfy the L2 constraint equation to a numerical tolerance of 1e-6.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1146, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Stochastic Variational Inference update equations for a Bayesian Latent Dirichlet Allocation model. Implement this in Python, demonstrating monotonic Evidence Lower Bound (ELBO) convergence on a synthetic corpus and compare its performance against a batch variational Bayes baseline.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) for Bayesian Latent Dirichlet Allocation and then derive the natural gradient update rules for both local and global variational parameters under the Stochastic Variational Inference (SVI) framework, ensuring the convergence criteria are explicitly stated.)\", \"/general-search(query=\\\"site:arxiv.org Stochastic Variational Inference LDA 'learning rate schedule' 'natural gradient'\\\", mode=web)\", \"/code(Implement the derived SVI updates for LDA in Python using numpy/scipy. Generate a synthetic corpus and demonstrate noisy but upward-trending ELBO convergence over mini-batches. Include a baseline batch variational Bayes implementation for comparison, tracking wall-clock time and final ELBO.)\"], \"route_rationale\": \"The /math tool is required for the core theoretical derivation of the variational updates. The blue /general-search agent will then be used to find authoritative research papers on implementation details like setting the learning rate schedule for SVI, which is crucial for stable convergence. Finally, the /code tool will translate the mathematical theory and implementation best practices into a working simulation to validate the entire process.\", \"expected_artifacts\": [\"A PDF document with the full mathematical derivation of the ELBO and SVI updates for LDA.\", \"A Python script implementing SVI and batch Variational Bayes for LDA, including synthetic data generation.\", \"A PNG plot comparing the ELBO convergence and wall-clock time for SVI vs. batch VB.\", \"A summary report in Markdown citing at least two relevant academic papers with their arXiv IDs.\", \"A JSON file containing the final ELBO values and total execution times for both algorithms.\"], \"thinking_outline\": [\"1. Formulate the Latent Dirichlet Allocation model as a probabilistic graphical model and write down the complete data joint distribution.\", \"2. Define the mean-field variational family for the posterior and derive the Evidence Lower Bound (ELBO) objective function.\", \"3. Derive the stochastic natural gradient update for the global parameters (topic-word distributions) using a single data point.\", \"4. Formulate the full SVI algorithm, including the update for local parameters (document-topic distributions) and the learning rate schedule.\", \"5. Implement both SVI and a standard batch VB algorithm to numerically validate that the ELBO converges correctly for both, and compare their performance characteristics.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivations from /math provide the core equations. /general-search uses keywords from the derivation to find best practices (e.g., learning rate schedules) to inform the implementation. /code implements the equations from /math refined by insights from /general-search. The final artifacts from /code (plots, metrics) are passed to the router for final verification against the acceptance criteria. Fallback: If ELBO convergence fails in the /code step, the task is routed back to /math to double-check the gradient derivations.\", \"todo_list\": [\"- [ ] /math: Derive the complete ELBO objective for the LDA model under a mean-field assumption.\", \"- [ ] /math: Derive the SVI natural gradient updates for the global variational parameters (lambda).\", \"- [ ] /general-search: Find canonical papers on SVI for LDA, focusing on setting the learning rate schedule (rho_t).\", \"- [ ] /code: Implement the SVI algorithm for LDA based on the mathematical derivations.\", \"- [ ] /code: Implement a batch Variational Bayes baseline for performance comparison.\", \"- [ ] /code: Generate synthetic data and run experiments to validate ELBO convergence for both methods.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"bayesian-statistics\"], \"acceptance_criteria\": [\"- The mathematical derivation of the SVI update for the global parameters is correct and matches canonical literature.\", \"- The Python implementation successfully runs, and the generated plot shows the ELBO is monotonically non-decreasing for the batch VB implementation.\", \"- The SVI implementation shows a clear upward trend in its noisy ELBO estimate, achieving a final value within 5% of the batch VB algorithm's final ELBO.\", \"- The final report includes correct citations for at least two relevant arXiv papers with valid IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1165, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm building a project on model pruning and need a foundational understanding. Can you explain how the method of Lagrange multipliers is used to impose an L1 sparsity constraint on a simple regression model's weights, and then code a basic Python example to show its convergence?\n", "completion": "{\"route_plan\": [\"/math(Derive the Lagrangian for a simple linear regression model with an L1 norm constraint on the weights. Clearly define the objective function, the constraint, and the resulting Euler-Lagrange equations.)\", \"/code(Implement a gradient descent optimizer in Python with NumPy to solve the constrained optimization problem. Visualize the convergence of the weights towards a sparse solution and validate the final L1 norm.)\", \"/general-search(query=\\\"site:arxiv.org Lagrange multipliers neural network pruning sparsity\\\", mode=rag)\"], \"route_rationale\": \"The /math tool will first derive the necessary equations for the optimization problem. Then, the /code tool will translate this mathematical theory into a practical Python implementation. Finally, the blue /general-search agent will find authoritative sources that apply these concepts in modern deep learning research, providing context and further reading.\", \"expected_artifacts\": [\"A PDF document with the complete mathematical derivation of the Lagrangian and its gradients.\", \"A Python script (`optimizer.py`) demonstrating the constrained optimization process with plots showing weight convergence.\", \"A verification report (`verification.log`) confirming the final weight vector satisfies the L1 constraint.\", \"A `references.md` file citing at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the primary objective function (mean squared error) and the equality constraint (L1 norm of weights equals a constant C).\", \"2. Construct the Lagrangian by combining the objective function and the constraint with a Lagrange multiplier, \u03bb.\", \"3. Compute the partial derivatives of the Lagrangian with respect to the model weights and \u03bb, and set them to zero to find the optimality conditions.\", \"4. Translate these update rules into a subgradient descent algorithm in Python, as the L1 norm is not differentiable at zero.\", \"5. Verify that the trained weights from the code satisfy the L1 norm constraint to within a small tolerance (e.g., 1e-5).\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: code output is checked against the math derivation's optimality conditions; fallback: if code fails to converge or violates constraints, return to /math to check gradient calculations).\", \"todo_list\": [\"- [ ] /math: Derive the Lagrangian and subgradient update rules for L1-constrained linear regression.\", \"- [ ] /code: Implement the optimization using Python and NumPy, plotting the results.\", \"- [ ] /general-search: Find two arXiv papers applying Lagrange multipliers for model pruning.\", \"- [ ] /verification: Check if the final L1 norm from the code matches the constraint value within a tolerance of 1e-4.\", \"- [ ] /router-qa: Await final review and approval.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"constrained-optimization\", \"lagrange-multipliers\", \"sparsity\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies the method of Lagrange multipliers and uses the subgradient for the L1 norm.\", \"- The Python code successfully converges and produces a weight vector that satisfies the L1 norm constraint as measured by the `constraint_satisfaction_error` metric.\", \"- The final report includes at least two citations to relevant arXiv preprints with their IDs.\", \"- The generated plot clearly shows the L1 norm of the weight vector converging to the constraint value over iterations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1185, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to analyze the topological structure of a Variational Autoencoder's latent manifold. Specifically, can you develop a pipeline using persistent homology via a Vietoris-Rips filtration to compute the Betti numbers of the latent space for a VAE trained on a synthetic dataset sampled from a torus? The goal is to quantify the topological dissimilarity between the learned manifold and the ground-truth torus using the Wasserstein distance between their persistence diagrams.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org persistent homology for variational autoencoder latent space analysis Betti numbers\\\", mode=web)\", \"/math(Formalize the Vietoris-Rips filtration process on a point cloud sampled from a manifold, define the persistence diagram, and specify the calculation for Betti numbers B_0, B_1, B_2. Define the p-Wasserstein distance for comparing two persistence diagrams from a Bayesian perspective.)\", \"/code(Implement a PyTorch VAE trained on a 3D torus point cloud. Then, use the giotto-tda library to sample from the latent space, construct the Vietoris-Rips complex, compute the persistence diagram, and extract the Betti numbers. Ensure the implementation tracks convergence., using Python)\", \"/math(Verify the correctness of the Betti number computation from the implemented code by applying it to a clean, analytically-defined torus point cloud. The expected Betti numbers are B_0=1, B_1=2, B_2=1. Analyze potential sources of numerical instability or eigenvalue estimation errors.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art theoretical context. This informs the /math agent to create a precise mathematical specification. The /code agent then implements this specification. A final /math agent call is crucial for verification, ensuring the code's output on a known case is correct before applying it to the experimental VAE output.\", \"expected_artifacts\": [\"A summary of 2+ cited papers on TDA for generative models.\", \"A formal mathematical derivation of the Vietoris-Rips filtration and Wasserstein distance for persistence diagrams.\", \"A documented Python script implementing the VAE and the TDA analysis pipeline.\", \"A results report containing generated persistence diagrams for both the ground-truth and latent manifolds, their Betti numbers, and the computed Wasserstein distance.\", \"A verification log confirming the TDA code correctly identifies the Betti numbers of a synthetic torus.\"], \"thinking_outline\": [\"1. Research established methodologies for applying persistent homology to analyze the latent spaces of generative models, particularly VAEs, focusing on the use of Vietoris-Rips filtrations.\", \"2. Formulate the mathematical pipeline: from a point cloud (sampled from the VAE's latent space) to a sequence of simplicial complexes (the filtration), and then to a persistence diagram from which Betti numbers are extracted.\", \"3. Implement a VAE in PyTorch and train it on a synthetic dataset of points sampled from a 3D torus. Implement the TDA pipeline using a library like `giotto-tda` or `ripser` to process the latent point cloud.\", \"4. Verify the TDA portion of the code by feeding it a 'perfect' point cloud sampled directly from the ground-truth torus manifold and confirm that the computed Betti numbers (B_0, B_1, B_2) match the theoretical values (1, 2, 1).\", \"5. Execute the full pipeline on the trained VAE's latent space, compute the persistence diagram and Betti numbers, and calculate the Wasserstein distance to the ground-truth diagram.\", \"6. Audit the results, analyzing how the VAE's hyperparameters (e.g., latent dimension, KL divergence weight) might influence the topological structure of the learned manifold as measured by the Betti numbers and Wasserstein distance.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal definitions) -> /code (implementation) -> /math (verification of code on ideal case) -> router QA (final review). Verification of the Betti numbers from the code against the analytical solution occurs at the /code -> /math handoff. Fallback: If verification fails, the /code agent must debug the TDA implementation, potentially revisiting the /math agent's formal definitions for clarity.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent arXiv papers on applying TDA to VAEs or other generative models.\", \"- [ ] /math: Provide the formal definitions for the Vietoris-Rips complex, persistence diagram, Betti numbers, and the Wasserstein distance between diagrams.\", \"- [ ] /code: Implement and train a VAE on a synthetic 3D torus dataset.\", \"- [ ] /code: Write a Python script to sample from the VAE latent space and use `giotto-tda` to compute its persistence diagram.\", \"- [ ] /math: verify the TDA code's output by confirming it produces the correct Betti numbers (1, 2, 1) for a clean torus point cloud.\", \"- [ ] /code: Calculate the Wasserstein distance between the persistence diagram of the latent space and that of the ground-truth torus.\", \"- [ ] /router: Review all artifacts for correctness, ensuring the final report's conclusions are supported by the verified code and metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"persistent-homology\"], \"acceptance_criteria\": [\"The TDA implementation must correctly report Betti numbers of (B_0=1, B_1=2, B_2=1) when run on a synthetic point cloud sampled directly from a torus.\", \"The final report must include visualizations of the persistence diagrams for both the ground-truth manifold and the VAE's latent manifold.\", \"The project must cite at least two relevant arXiv papers with their IDs.\", \"The computed Wasserstein distance must be reported and interpreted as a measure of topological similarity.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1194, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and rigorously compare two dictionary learning algorithms for sparse signal representation: K-SVD and the Method of Optimal Directions (MOD). The comparison should focus on convergence speed and dictionary recovery accuracy on synthetic data, where signals are sparse linear combinations of a known ground-truth dictionary. Please provide the mathematical derivations for the update rules of both algorithms, a Python implementation, and an analysis of their computational complexity and empirical performance in terms of reconstruction error.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD vs Method of Optimal Directions dictionary learning convergence complexity\\\", mode=web)\", \"/math(Derive the optimization steps for K-SVD and MOD, focusing on the sparse coding stage using Orthogonal Matching Pursuit and analyzing the dictionary update Lagrangian for convergence criteria based on Fourier properties.)\", \"/code(Implement both K-SVD and MOD algorithms in Python, generating synthetic sparse signals and comparing their reconstruction error and dictionary atom recovery rates. Add runtime validation for dictionary coherence and check algorithm convergence.)\", \"/math(Statistically analyze the empirical convergence rates from the code output, verifying if they align with theoretical complexity bounds from the arXiv literature and performing a Bayesian significance test on the reconstruction error difference.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational academic papers on K-SVD and MOD. This literature informs the /math agent's formal derivation of the algorithms. The /code agent then implements these verified derivations for empirical testing. A final /math agent provides a rigorous statistical analysis of the coded results, linking empirical findings back to the theory.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the K-SVD and MOD update rules.\", \"- A Python script implementing both algorithms and a synthetic data testbed.\", \"- A set of plots comparing the reconstruction Mean Squared Error (MSE) vs. iteration for both algorithms.\", \"- A summary report (.md) detailing the findings, including algorithm complexity analysis and citations to relevant literature.\", \"- A JSON file containing the final computed metrics for both algorithms.\"], \"thinking_outline\": [\"1. Research the K-SVD and Method of Optimal Directions (MOD) algorithms, focusing on their distinct objective functions, sparse coding stages (e.g., OMP), and dictionary update procedures.\", \"2. Formulate the mathematical optimization problem for each algorithm and derive the step-by-step update rules for both the sparse coefficients and the dictionary atoms.\", \"3. Implement the derived algorithms in Python, including a function to generate synthetic sparse data from a ground-truth dictionary.\", \"4. Execute a comparative experiment measuring reconstruction error, dictionary recovery rate, and wall-clock time per iteration for both methods.\", \"5. Verify that the implemented dictionary update steps correctly minimize the objective function by checking that the Frobenius norm of the residual error is non-increasing at each iteration.\", \"6. Validate the final results by comparing the empirical convergence rates against the theoretical complexities discussed in the researched literature.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math produces formal derivations -> /code implements the derivations and generates empirical results -> /math analyzes the empirical results against theory -> router QA (verification: consistency between derivations, code, and final analysis; fallback: if code performance contradicts theory, re-audit math derivations and implementation logic).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two seminal papers on K-SVD and MOD from arXiv.\", \"- [ ] /math: Derive the complete update rules for both algorithms, clearly stating the objective function and constraints.\", \"- [ ] /code: Implement the K-SVD and MOD algorithms based on the mathematical derivations.\", \"- [ ] /code: Create a synthetic data generator for sparse signals and a test harness to measure reconstruction MSE and dictionary recovery.\", \"- [ ] /math: Verify that the Python implementation's update steps correspond directly to the derived mathematical formulas.\", \"- [ ] /math: Analyze the empirical results from the code, plotting convergence curves and comparing them to theoretical expectations.\", \"- [ ] /router: QA review of all artifacts, checking for consistency between theory, implementation, and final analysis.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- Mathematical derivations for K-SVD and MOD update rules are complete and self-contained.\", \"- Python code runs without error and generates plots showing a decrease in reconstruction MSE over iterations for both algorithms on synthetic data.\", \"- The final report includes plots comparing MSE vs. iteration and dictionary recovery rate for both algorithms, and cites at least 2 relevant arXiv papers with IDs.\", \"- The reported computational complexity analysis is consistent with the cited literature.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as the mean of the squared Frobenius norm of the residual: mean(||X - D*Alpha||_F^2), where X are the original signals, D is the learned dictionary, and Alpha are the computed sparse codes.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate the percentage of learned dictionary atoms that have a cosine similarity greater than 0.99 with an atom from the true, ground-truth dictionary used for data generation.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1207, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement an Online Dictionary Learning (ODL) algorithm for sparse representation of biomedical signals. Please derive the update rules based on stochastic gradient descent for the dictionary atoms and the Lasso regression subproblem for the sparse codes. The implementation should demonstrate convergence on a synthetic dataset with a known ground-truth dictionary and measure the reconstruction error and dictionary recovery rate. Compare the empirical convergence rate with theoretical bounds from relevant arXiv literature.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org online dictionary learning lasso stochastic gradient convergence mairal\\\", mode=both)\", \"/math(Derive the stochastic update rules for both the dictionary matrix and sparse coefficients from the ODL objective function, detailing the Lasso subproblem and its relation to the overall convergence criteria.)\", \"/code(Implement the Online Dictionary Learning algorithm in Python with NumPy/SciPy, including a Lasso solver like LARS. Generate synthetic data based on a ground-truth dictionary and evaluate reconstruction MSE and dictionary recovery over epochs, ensuring proper convergence checks.)\", \"/math(Analyze the empirical convergence plot from the code execution against the theoretical guarantees found in the initial research. Verify if the asymptotic behavior matches the expected O(1/t) rate for stochastic approximation and check for eigenvalue stability in the dictionary update.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find foundational papers on ODL. /math then translates the theory into formal derivations for the update rules. /code implements this mathematical specification and runs experiments. A final /math call provides rigorous verification by comparing the empirical results from the code against the theoretical bounds established by the research, ensuring the implementation is correct.\", \"expected_artifacts\": [\"A PDF document with the complete mathematical derivation of the ODL update rules.\", \"A Python script (`odl_implementation.py`) containing the algorithm, synthetic data generator, and experiment execution.\", \"A summary report (`report.md`) including plots for reconstruction MSE vs. iteration and a visualization comparing the learned dictionary to the ground-truth dictionary.\", \"A list of at least two cited arXiv papers with their IDs.\", \"A verification document (`verification_log.txt`) detailing the comparison between empirical and theoretical convergence rates.\"], \"thinking_outline\": [\"1. Research the foundational literature on Online Dictionary Learning (ODL), focusing on the work by Mairal et al. to establish the objective function and theoretical convergence guarantees.\", \"2. Formulate the ODL optimization problem and formally derive the stochastic gradient descent update rule for the dictionary matrix and the Lasso formulation for the sparse coding step.\", \"3. Implement the complete algorithm in Python, including a function to generate synthetic data from a known sparse dictionary and a robust solver for the Lasso subproblem.\", \"4. Execute the implementation on the synthetic data, tracking key metrics such as the value of the objective function, signal reconstruction Mean Squared Error (MSE), and dictionary recovery rate over iterations.\", \"5. Validate the implementation by checking that the objective function decreases over time, indicating convergence towards a local minimum.\", \"6. Audit the final learned dictionary by comparing its atoms to the ground-truth dictionary using a similarity metric like cosine similarity, and verify that the empirical convergence rate aligns with the theoretical bounds from the literature.\"], \"handoff_plan\": \"/general-search (papers) -> /math (derivations) -> /code (implementation, empirical results) -> /math (verification of results against theory) -> router QA (final review). Verification: The final /math step validates the /code output against the /general-search findings. Fallback: If empirical convergence fails in /code, escalate to /math to re-check the derivation for potential errors or missed constraints.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Online Dictionary Learning (e.g., Mairal et al., 2009).\", \"- [ ] /math: Derive the update equations for the dictionary and sparse codes based on the identified papers.\", \"- [ ] /code: Implement the ODL algorithm, synthetic data generation, and metric tracking (MSE, dictionary recovery).\", \"- [ ] /code: Execute the experiment and generate plots visualizing convergence and dictionary learning performance.\", \"- [ ] /math: Verify that the implemented algorithm's update rules exactly match the mathematical derivation.\", \"- [ ] /math: Analyze the plots from /code to confirm the empirical results align with the theoretical convergence guarantees.\", \"- [ ] /router: Perform final QA check on all artifacts for coherence, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The implementation must achieve a final signal reconstruction Mean Squared Error (MSE) below 1e-4 on the synthetic dataset.\", \"- The learned dictionary must recover the ground-truth dictionary with an average atom-wise cosine similarity greater than 0.98.\", \"- The final report must cite at least two relevant arXiv papers with their full IDs and link the theoretical discussion to the empirical results.\", \"- The mathematical derivation must be self-contained, correct, and clearly reflected in the code's logic.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1212, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a variational Physics-Informed Neural Network (PINN) to solve the nonlinear Schr\u00f6dinger equation. Please derive the corresponding weak form from the Euler-Lagrange functional, implement the model in JAX, and validate its convergence against a Fourier spectral method baseline. The loss function must explicitly encode the variational principle, not just the strong-form PDE residual.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational physics-informed neural network nonlinear schr\u00f6dinger equation weak form\\\", mode=both)\", \"/math(Derive the weak form of the nonlinear Schr\u00f6dinger equation from its associated Euler-Lagrange functional. Detail the integration by parts and specify the Sobolev space for the trial and test functions, ensuring the final formulation minimizes PDE residual convergence issues.)\", \"/code(Implement a PINN in JAX to solve the derived weak form. The loss must use Gaussian quadrature to approximate the integrals. Compare the solution against a Fourier spectral method baseline on a test domain, leveraging CUDA for acceleration., using Python)\", \"/math(Verify the numerical convergence of the implemented PINN by analyzing the L2 norm of the PDE residual as a function of training epochs and network capacity. Compare these empirical results against theoretical error bounds from the literature., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on variational PINNs, which informs the theoretical approach. /math is then used to perform the core mathematical derivation of the weak form. /code implements this mathematical specification in a high-performance framework (JAX). A final /math call is crucial for verifying that the numerical implementation's convergence properties align with theoretical expectations, closing the loop between theory and practice.\", \"expected_artifacts\": [\"A formal mathematical proof detailing the derivation of the weak form of the nonlinear Schr\u00f6dinger equation.\", \"A Python script containing the JAX implementation of the variational PINN and the Fourier spectral method baseline.\", \"A report in Markdown format summarizing the findings, including convergence plots (L2 residual vs. epoch) and comparison figures (PINN solution vs. baseline).\", \"A JSON file containing the final metrics, including L2 residual norm and MAE against the baseline.\", \"A bibliography file citing at least two relevant arXiv preprints on variational PINNs.\"], \"thinking_outline\": [\"1. Use literature search to identify the correct Lagrangian functional for the nonlinear Schr\u00f6dinger equation and common test function choices for its weak form.\", \"2. Apply the calculus of variations to derive the weak (integral) form of the PDE, clearly documenting each step of the integration by parts.\", \"3. Implement both the variational PINN using the derived weak form as a loss and a standard Fourier spectral solver to serve as a high-fidelity ground truth.\", \"4. Train the PINN and systematically evaluate its performance against the baseline, calculating the L2 residual and MAE metrics.\", \"5. Validate that the mathematical derivation of the weak form is correctly translated into the JAX implementation of the loss function, checking the quadrature rules.\", \"6. Verify that the observed convergence rates are consistent with theoretical expectations for PINNs, citing the sources found in the initial search.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results provide the theoretical foundation for /math's derivation. The resulting weak form from /math provides the exact specification for the loss function implemented by /code. The numerical results and trained model from /code are then passed to the final /math agent for convergence analysis and verification. Verification: The final math agent checks if the code's convergence plots match theoretical predictions. Fallback: If convergence is poor, the task reverts to the /code agent to debug the loss implementation or network architecture.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on variational PINNs for solving nonlinear wave equations.\", \"- [ ] /math: Produce a LaTeX-formatted derivation of the weak form of the nonlinear Schr\u00f6dinger equation.\", \"- [ ] /code: Implement the PINN and the Fourier spectral method baseline in JAX.\", \"- [ ] /code: Generate convergence plots and comparison visualizations of the two solutions.\", \"- [ ] /math: Analyze the output from the code agent and produce a convergence verification report.\", \"- [ ] /math: Verify that the quadrature method used in the code accurately approximates the integral derived in the initial math step.\", \"- [ ] /router: Perform a final QA check, ensuring the mathematical derivation, code implementation, and final report are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"partial-differential-equations\", \"variational-methods\"], \"acceptance_criteria\": [\"- The derived weak form must be mathematically correct and properly use integration by parts to reduce the order of derivatives.\", \"- The PINN implementation must achieve an L2 residual norm below 1e-4 on the test domain.\", \"- The final report must include at least one plot comparing the PINN solution to the Fourier spectral method baseline, showing a Mean Absolute Error below 5%.\", \"- The analysis must cite at least two relevant arXiv papers with IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1223, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the conditions under which the causal effect P(y|do(x)) is identifiable from the observational distribution P(V) in a given Directed Acyclic Graph (DAG), G. Specifically, focus on Pearl's do-calculus rules and provide a formal proof for Rule 2 (action/observation exchange). Then, implement an algorithm to check for identifiability and compute the causal effect for a non-trivial semi-Markovian model, validating against a known ground truth causal query.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Pearl do-calculus proof Rule 2 causal identifiability semi-Markovian\\\", mode=rag)\", \"/math(Using the causal framework from the retrieved arXiv papers, formally derive the proof for Rule 2 of do-calculus, detailing all assumptions about the underlying Bayesian network structure and d-separation criteria.)\", \"/code(Implement a Python function using networkx that takes a DAG and a causal query, applies the rules of do-calculus to find a valid estimand, and handles backdoor adjustment for covariate control.)\", \"/math(For a provided 4-node semi-Markovian graph, manually apply the derived do-calculus rules to compute P(y|do(x)) and verify that the estimand matches the Python code's output, checking for convergence on the final expression.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather authoritative literature on do-calculus. This is handed to the /math agent for the formal proof derivation. The /code agent then implements the theory. A final /math agent provides a crucial verification step by comparing a manual derivation against the code's output on a concrete example.\", \"expected_artifacts\": [\"A formal proof of Rule 2 of do-calculus, written in LaTeX.\", \"A Python script (`identifiability_checker.py`) implementing the causal effect computation.\", \"A verification report in Markdown (`report.md`) comparing the manual calculation with the script's output for the test case.\", \"A list of at least two cited arXiv papers with their IDs.\", \"A JSON file (`metrics.json`) containing the final evaluation metrics.\"], \"thinking_outline\": [\"1. Formalize the problem using structural causal model (SCM) notation, defining interventions, observational distributions, and the target interventional distribution P(y|do(x)).\", \"2. Review the three rules of do-calculus and the graphical criterion of d-separation from the literature search, establishing the theoretical foundation for the proof.\", \"3. Construct the formal proof for Rule 2, leveraging graph-theoretic properties of the manipulated graph G_X and the conditions under which an observation is equivalent to an action.\", \"4. Validate the proof's logic against established results in the cited papers, checking for implicit assumptions and edge cases like M-graphs.\", \"5. Implement an algorithm that operationalizes the do-calculus rules to transform a causal query into a statistical estimand.\", \"6. Verify the algorithm's correctness on a non-trivial test case by manually applying the do-calculus and ensuring the resulting estimand matches the program's output.\"], \"handoff_plan\": \"/general-search -> /math (proof) -> /code -> /math (verification) -> router QA. The initial proof from /math must be logically sound before handoff to /code. The final verification step from the second /math call must match the /code output; fallback involves sending the implementation back to /code with the specific discrepancy for debugging.\", \"todo_list\": [\"- [ ] /general-search: Retrieve seminal papers by Pearl and others on do-calculus and causal identifiability.\", \"- [ ] /math: Formulate the complete, step-by-step proof for Rule 2 based on the literature.\", \"- [ ] /code: Implement the core algorithm in Python, including graph manipulation for d-separation checks.\", \"- [ ] /code: Add unit tests for helper functions, especially the d-separation check, to ensure correctness.\", \"- [ ] /math: Manually derive the causal estimand for the specified 4-node test case graph.\", \"- [ ] /code: Execute the implemented algorithm on the test case and generate an output estimand.\", \"- [ ] /math: Verify that the code's output expression is algebraically equivalent to the manually derived estimand.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"do-calculus\", \"structural-causal-models\"], \"acceptance_criteria\": [\"The provided proof for Rule 2 is complete, mathematically rigorous, and references appropriate graph-theoretic concepts.\", \"The Python implementation correctly computes the causal estimand for the provided semi-Markovian test case, matching the manual derivation.\", \"The final report includes citations for at least two relevant arXiv pre-prints with their IDs.\", \"All code is documented and includes unit tests for key graphical operations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1240, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the computational complexity of determining the nim-value for an impartial game played on a high-dimensional simplicial complex, where moves are restricted to face transitions. First, construct a formal proof of PSPACE-completeness for this game. Second, investigate if tensor network methods, specifically Projected Entangled Pair States (PEPS), can yield a fixed-parameter tractable algorithm parameterized by the complex's treewidth, and analyze its convergence properties using control theory principles.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org combinatorial game theory PSPACE-completeness simplicial complex treewidth tensor network game theory\\\", mode=web)\", \"/math(Formalize a polynomial-time reduction from Quantified Boolean Formulas (QBF) to the simplicial game to establish PSPACE-hardness, analyzing the problem's complexity class., includes cybersecurity)\", \"/code(Implement a Python prototype using 'tensornetwork' and 'numpy' for the PEPS-based nim-value algorithm. Validate against small, brute-forceable instances and track convergence eigenvalue metrics.)\", \"/math(Derive the formal proof of convergence for the iterative tensor algorithm, framing it as a discrete-time linear system and applying Lyapunov stability criteria from control theory.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey existing literature on the intersection of game theory, topology, and complexity. This informs the /math agent's formal PSPACE-completeness proof. The /code agent then implements the proposed advanced algorithm. Finally, the /math agent returns to rigorously prove the algorithm's convergence properties, a critical validation step.\", \"expected_artifacts\": [\"- A formal, typeset proof (PDF/LaTeX) of PSPACE-completeness for the specified game.\", \"- A documented Python script implementing the tensor network algorithm for nim-value computation.\", \"- A final report summarizing the theoretical findings, algorithmic performance, convergence analysis, and at least two citations to arXiv preprints.\", \"- A verification log detailing the alignment check between the mathematical derivation and the code's empirical results.\"], \"thinking_outline\": [\"1. Formalize the rules of the impartial game on a simplicial complex and precisely define the winning/losing positions based on nim-values (Grundy numbers).\", \"2. Construct a polynomial-time reduction from a known PSPACE-complete problem, such as Quantified Boolean Formula (QBF), to the problem of determining the winner in the simplicial complex game.\", \"3. Design the tensor network representation of the game's transition structure, mapping game states to tensor indices and moves to tensor contractions, ensuring the algorithm's complexity is polynomial in the number of states and exponential only in the treewidth.\", \"4. Check the implemented algorithm's output on small, well-understood graph structures (like paths and cycles) against manually computed or brute-forced nim-values.\", \"5. Formulate and prove the convergence criteria for the tensor network algorithm by analyzing the spectral radius of the update operator, ensuring it is less than one for guaranteed convergence to a unique fixed point.\", \"6. Validate that the assumptions made in the convergence proof (e.g., properties of the game graph) hold for the problem class being considered.\"], \"handoff_plan\": \"/general-search -> /math (proof) -> /code (implementation) -> /math (convergence) -> router QA (verification: code output matches theoretical nim-values for test cases; fallback: if divergence, /code agent must debug implementation and log spectral radius of iteration matrix).\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on PSPACE-completeness in games and tensor network applications.\", \"- [ ] /math: Draft the PSPACE-completeness proof via reduction from QBF.\", \"- [ ] /code: Implement the PEPS-based algorithm for computing nim-values.\", \"- [ ] /code: Create a suite of unit tests for the algorithm using simple, known game structures.\", \"- [ ] /math: Formulate the convergence proof using spectral analysis and Lyapunov theory.\", \"- [ ] /math: Verify that the implemented algorithm's update step correctly reflects the derived mathematical operator.\", \"- [ ] /code: Verify that empirical convergence rates on test cases align with theoretical predictions.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-games\", \"computational-complexity\", \"tensor-networks\"], \"acceptance_criteria\": [\"- The PSPACE-completeness proof must be logically sound and contain a valid, polynomial-time reduction from a known PSPACE-complete problem.\", \"- The Python code must execute, pass all defined unit tests, and correctly compute nim-values for graphs with treewidth <= 4.\", \"- The convergence proof must be rigorous and correctly identify the conditions under which the algorithm is guaranteed to find the correct nim-values.\", \"- The final report must be well-structured, clearly presenting all proofs, results, and at least two relevant arXiv citations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1263, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to design and formally analyze a federated learning protocol that uses additive homomorphic encryption for secure model aggregation. How can I adapt the Paillier cryptosystem to handle gradient updates with ReLU activations, analyze the resulting privacy-utility trade-off using differential privacy bounds, and prove the convergence of the federated averaging algorithm under these cryptographic constraints?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning homomorphic encryption paillier cryptosystem relu approximation convergence\\\", mode=web)\", \"/math(Derive the secure federated aggregation protocol using the Paillier cryptosystem, formally prove its homomorphic properties for gradient addition, and establish the theoretical convergence bounds for a convex loss function.)\", \"/code(Implement a Python simulation of the federated learning protocol with Paillier encryption, using a library like phe, and benchmark the communication overhead and model accuracy on a standard dataset like MNIST, tracking AUPRC.)\", \"/math(Formally analyze the differential privacy budget consumed by the noisy aggregation process and verify the empirical convergence rate from the simulation against the derived theoretical bounds.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on federated learning with homomorphic encryption. This research informs the /math agent's formal derivation of the protocol and its convergence proofs. Next, the /code agent implements the protocol to provide an empirical validation of the theory. Finally, the /math agent returns to analyze the privacy budget and verify that the empirical results from the simulation align with the theoretical bounds, closing the loop between theory and practice.\", \"expected_artifacts\": [\"A PDF document containing the formal mathematical derivation of the secure aggregation protocol and its convergence proof.\", \"A Python script (`main.py`) implementing the federated learning simulation with the Paillier cryptosystem.\", \"A final report (`report.md`) summarizing the methodology, results, and the privacy-utility trade-off analysis, complete with generated plots of accuracy and loss.\", \"A `citations.bib` file listing at least three relevant research papers from arXiv.\", \"A verification log (`verification.txt`) detailing the comparison between theoretical convergence bounds and empirical results.\"], \"thinking_outline\": [\"1. Research existing protocols for federated learning with homomorphic encryption, focusing on the Paillier cryptosystem and methods for handling non-linear activation functions.\", \"2. Formulate the Paillier-based secure aggregation mechanism and derive the formal proof of its correctness, privacy properties, and theoretical convergence rate under standard assumptions.\", \"3. Implement the complete protocol in Python, including client-side training, gradient encryption, server-side secure aggregation, and model update steps.\", \"4. Validate the cryptographic implementation by checking that the decrypted aggregate gradient from a small number of clients exactly matches the sum of their plaintext gradients in a non-private test case.\", \"5. Execute the simulation on a benchmark dataset (e.g., MNIST) to empirically measure model performance (accuracy), convergence speed, and communication overhead.\", \"6. Audit the final artifacts to ensure the mathematical proofs, simulation results, and privacy analysis are consistent and correctly reported.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA (verification: confirm empirical results in /code output match theoretical bounds from /math; fallback: if mismatch, return to /math to re-check assumptions in convergence proof).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Paillier cryptosystem in federated learning and common approximations for non-linear functions.\", \"- [ ] /math: Derive the secure aggregation protocol and prove its convergence properties under convex loss assumptions.\", \"- [ ] /code: Implement the federated protocol using Python and the `python-phe` library for Paillier operations.\", \"- [ ] /code: Instrument the simulation to collect and log per-round accuracy, loss, and aggregation timing metrics.\", \"- [ ] /math: Derive the differential privacy (\u03b5, \u03b4) bounds for the protocol, assuming noise is added before encryption.\", \"- [ ] /math: Verify that the empirical convergence rate from the /code agent's metrics output aligns with the derived theoretical bounds.\", \"- [ ] /code: Validate the correctness of the cryptographic implementation by comparing encrypted vs. plaintext aggregation on a small-scale test case with known inputs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"federated-learning\", \"homomorphic-encryption\", \"differential-privacy\"], \"acceptance_criteria\": [\"- The mathematical proof of convergence for the secure aggregation protocol is formally sound and complete.\", \"- The Python simulation runs without errors and achieves a test accuracy on MNIST of at least 85% after 20 federated rounds.\", \"- The final report correctly quantifies the privacy-utility trade-off, presenting a valid differential privacy budget (epsilon) and corresponding model accuracy, supported by plots.\", \"- All claims in the report are backed by either the mathematical derivations or the empirical results from the simulation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate the standard classification accuracy on the hold-out test set after the final round of federated training. This measures the utility of the final global model.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Compute the cumulative (\u03b5, \u03b4)-differential privacy loss over all rounds using a standard moments accountant or similar technique, based on the noise level applied to gradients before aggregation.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1287, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the policy gradient theorem for a stochastic policy in an infinite-horizon MDP, then implement the REINFORCE algorithm with a baseline to stabilize training on the CartPole-v1 environment. Analyze its convergence properties from a control theory perspective.\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient theorem for stochastic policies, explicitly showing how the score function trick avoids differentiating the state distribution and proving unbiasedness. Analyze the theorem's implications for convergence guarantees.)\", \"/general-search(query=\\\"site:arxiv.org policy gradient variance reduction baseline 'generalized advantage estimation'\\\", mode=web. Synthesize findings from at least two papers to inform a robust bayesian baseline implementation.)\", \"/code(Implement REINFORCE with a neural network value baseline in PyTorch for CartPole-v1, a classic control theory problem. Target an average return >400 over 100 episodes and log gradient variance., using Python)\"], \"route_rationale\": \"The route starts with /math to establish the core theoretical foundation of policy gradients. Next, the blue /general-search agent is tasked with researching modern best practices for variance reduction, which is a critical practical challenge not covered by the basic theorem. Finally, /code implements the algorithm, informed by both the mathematical derivation and the literature search, to provide an empirical validation of the theory.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the policy gradient theorem, including the score function estimator.\", \"- A Python script implementing the REINFORCE algorithm with a learned value function baseline.\", \"- A performance plot showing the agent's average episodic return versus training episodes, demonstrating convergence.\", \"- A summary report citing and briefly explaining at least two academic papers on policy gradient variance reduction techniques.\", \"- A JSON file containing final evaluation metrics, including average return and a sample of gradient variance.\"], \"thinking_outline\": [\"1. Formally derive the policy gradient theorem, starting from the objective function J(\u03b8) and using the log-derivative trick to arrive at the expected value form of the gradient.\", \"2. Research common variance reduction techniques for policy gradients, focusing on the subtraction of a state-dependent baseline and its effect on the gradient estimate's bias and variance.\", \"3. Implement the REINFORCE algorithm in Python, including a separate neural network to approximate the value function V(s) which will serve as the baseline.\", \"4. Train the agent on the CartPole-v1 environment, systematically logging the average return and the empirical variance of the policy gradient estimates.\", \"5. Validate that the implemented baseline demonstrably reduces the gradient variance compared to a naive implementation and verify that the agent's performance converges to the specified success criterion.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (literature review for baselines) -> /code (implementation and empirical validation). The final code artifacts and plots are passed to the router for final QA. Verification: The router will audit the code to ensure the baseline is correctly implemented as per the derivation and literature. Fallback: If the training is unstable or fails to converge, the router will ask /code to re-evaluate the baseline's learning rate and architecture, referencing the papers found by /general-search.\", \"todo_list\": [\"- [ ] /math: Derive the policy gradient theorem, clearly defining the score function and objective J(\u03b8).\", \"- [ ] /general-search: Find and summarize two seminal papers on variance reduction in policy gradients (e.g., actor-critic methods, GAE).\", \"- [ ] /code: Implement the REINFORCE algorithm with a learned value function baseline for the CartPole-v1 environment using PyTorch.\", \"- [ ] /code: Validate that the agent achieves an average return of >400 over 100 consecutive episodes and plot the learning curve.\", \"- [ ] /code: Instrument the training loop to log the empirical variance of the policy gradients per batch.\", \"- [ ] /router: Perform final QA check, ensuring the mathematical derivation, code, and literature summary are consistent and correct.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"stochastic-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the policy gradient theorem is complete, correct, and clearly explains why a baseline does not introduce bias.\", \"- The Python implementation of REINFORCE with a baseline successfully solves CartPole-v1, achieving an average return greater than 400 over 100 consecutive episodes.\", \"- The final report includes a plot of learning convergence (average reward vs. episodes) and correctly cites at least two relevant academic papers with their arXiv IDs.\", \"- The logged gradient variance is reported and shows a lower magnitude than would be expected without a baseline.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Mean of total rewards over the last 100 evaluation episodes. Success is defined as a value > 400.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Empirical variance of the policy gradient estimates per batch, tracked over training epochs. Used as a diagnostic for training stability.\"}}", "categories": ["math_first", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1303, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDevelop a K-SVD algorithm for dictionary learning in a compressed sensing context. Your goal is to optimize the dictionary for sparse representation of signals, benchmark its reconstruction error against a fixed wavelet dictionary, and analyze the convergence properties of the sparse coding stage using Orthogonal Matching Pursuit (OMP).\n", "completion": "{\"route_plan\": [\"/math(Derive the K-SVD update rules from the sparse representation objective function, proving that each step of the alternating minimization scheme monotonically decreases the objective, ensuring convergence to a local minimum.)\", \"/general-search(Find foundational papers on K-SVD and Orthogonal Matching Pursuit to verify the derived convergence properties and find pseudocode for a robust implementation. query=\\\"site:arxiv.org \\\\\\\"K-SVD algorithm\\\\\\\" \\\\\\\"dictionary learning\\\\\\\" convergence\\\", mode=both)\", \"/code(Implement K-SVD using Python, benchmarking reconstruction MSE against a fixed DCT basis. Validate the algorithm's stability by tracking the dictionary's condition number and eigenvalue spread over iterations.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation and derive the K-SVD algorithm. Then, the blue /general-search agent gathers authoritative literature to validate the derivations and inform the implementation. Finally, /code is used to create a working implementation, perform empirical validation against a baseline, and analyze the results, thus completing the theory-to-practice loop.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of K-SVD update rules and its convergence properties.\", \"A Python script implementing the K-SVD algorithm, the OMP sparse coding step, and the comparison benchmark.\", \"A summary report in Markdown containing plots of reconstruction error vs. iteration, a table comparing K-SVD to the DCT baseline, and visualizations of the learned dictionary atoms.\", \"A bibliography file with citations for at least two relevant arXiv papers with their IDs.\", \"A validation log file confirming that dictionary atoms maintain unit L2-norm and the objective function is non-increasing.\"], \"thinking_outline\": [\"1. Formulate the sparse representation problem and the associated dictionary learning objective function, defining the constraints on the dictionary and sparse codes.\", \"2. Derive the alternating minimization procedure for K-SVD, detailing the sparse coding step (using OMP) and the dictionary update step (using SVD).\", \"3. Implement the complete K-SVD algorithm and a benchmark using a fixed Discrete Cosine Transform (DCT) dictionary for reconstructing synthetic sparse signals from compressed measurements.\", \"4. Validate the implementation by checking the reconstruction Mean Squared Error (MSE) and verifying that the learned dictionary atoms maintain unit norm after each update.\", \"5. Analyze and report the performance, comparing the learned dictionary against the fixed one, and discuss the algorithm's convergence by plotting the objective function value over iterations.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent provides the core derivations. /general-search validates these derivations against published literature and finds implementation pseudocode, passing both to /code. The /code agent implements the algorithm and produces empirical results. The final package is sent to the router for QA. Verification: The code's objective function plot must match the monotonic decrease proven by /math. Fallback: If the objective increases, /code must debug the implementation, referencing the pseudocode from /general-search.\", \"todo_list\": [\"- [ ] /math: Derive the K-SVD update rules for both the dictionary (using SVD) and the sparse codes (using OMP).\", \"- [ ] /general-search: Find at least two key papers on K-SVD, focusing on algorithmic details and convergence proofs.\", \"- [ ] /code: Implement the K-SVD algorithm in Python with NumPy/SciPy.\", \"- [ ] /code: Implement the benchmark against a fixed Discrete Cosine Transform (DCT) dictionary.\", \"- [ ] /code: Generate plots for reconstruction MSE vs. iteration and final reconstructed signals.\", \"- [ ] /code: Validate that the learned dictionary atoms maintain unit L2-norm after each update step.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"The K-SVD implementation must achieve at least a 10% lower Mean Squared Error (MSE) on reconstructed test signals than the fixed DCT dictionary baseline.\", \"The final report must include a clear mathematical derivation of the K-SVD dictionary update step.\", \"The provided code must be runnable and include a script to reproduce the reported metrics and plots.\", \"A plot must demonstrate that the value of the optimization objective function is non-increasing across iterations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"Calculate the squared Frobenius norm of the difference between the original and reconstructed signal matrices, normalized by the number of elements: ||X - D*alpha||_F^2 / (num_signals * signal_length).\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"Calculate as 20 * log10(MAX_I) - 10 * log10(MSE), where MAX_I is the maximum possible signal value (e.g., 255 for 8-bit images).\"}}", "categories": ["math_first", "metrics_computation", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1319, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the combinatorial game 'Dawson's Kay'. I need a formal proof of the nim-sequence pattern using the Sprague-Grundy theorem, focusing on its periodicity. Subsequently, develop an efficient algorithm to determine the P/N positions for a given game state of size 'n', and provide a rigorous computational complexity analysis of this algorithm. The analysis must consider both time and space complexity in the context of dynamic programming or memoization.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:.edu 'Dawson's Kay' 'impartial game' 'Sprague-Grundy theorem' nim-value sequence complexity\\\", mode=rag)\", \"/math(Derive the recurrence for the nim-values (Grundy numbers) of Dawson's Kay based on the mex rule. Formulate a proof by induction for the observed 34-periodicity of the nim-sequence, ensuring the proof's complexity is well-bounded., includes cybersecurity)\", \"/code(Implement a Python algorithm using memoization to compute the nim-value for a heap of size n. Include runtime validation checks against a known database of the first 100 nim-values and analyze convergence properties of the computation.)\", \"/math(Perform a formal big-O analysis of the implemented algorithm's time and space complexity. Verify that the asymptotic bounds are consistent with theoretical expectations for solving impartial games via Grundy numbers and that the implementation avoids redundant subproblem computations., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on Dawson's Kay and the Sprague-Grundy theorem. This informs the /math agent's formal proof derivation. The /code agent then implements the derived mathematical recurrence, with a final /math step to formally analyze and verify the algorithm's computational complexity, ensuring a tight loop between theory and implementation.\", \"expected_artifacts\": [\"- A formal mathematical proof (LaTeX) of the nim-sequence pattern for Dawson's Kay.\", \"- A Python script implementing the nim-value calculation with memoization and unit tests.\", \"- A complexity analysis report detailing the time and space complexity of the algorithm.\", \"- A summary report citing at least two authoritative sources on combinatorial game theory.\", \"- A verification log confirming the implementation's output matches the theoretical sequence for n <= 100.\"], \"thinking_outline\": [\"1. Use search to gather literature on Dawson's Kay, focusing on its definition as a sum of games and its relation to the Sprague-Grundy theorem.\", \"2. Formulate the recurrence relation for the game's Grundy numbers (g-values) by applying the mex (Minimum Excluded value) rule to the set of possible moves.\", \"3. Prove the pattern and periodicity of the resulting g-sequence, likely using strong induction, referencing the established recurrence.\", \"4. Implement a dynamic programming or memoized recursion algorithm to compute g(n) and validate its output against published sequences for small n.\", \"5. Derive the precise time and space complexity of the implementation, ensuring the analysis accounts for the memoization table.\", \"6. Audit the final proof and complexity analysis for logical soundness and compare the algorithmic efficiency to theoretical limits.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The initial search results from /general-search provide the theoretical foundation for the /math agent's proof. The completed proof from /math defines the recurrence relation for the /code agent to implement. The implemented code is then handed back to the /math agent for a formal complexity verification. Router QA performs a final review of all artifacts. Fallback: If the proof in the first /math step is untenable, the router will re-engage /general-search for alternative proof strategies or known results.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize key papers on Dawson's Kay and its nim-sequence.\", \"- [ ] /math: Derive the nim-value recurrence and formally prove the sequence's periodicity.\", \"- [ ] /code: Implement the memoized algorithm for computing nim-values up to a given n.\", \"- [ ] /code: Add unit tests and runtime validation against a known sequence.\", \"- [ ] /math: Analyze and document the time and space complexity of the Python implementation.\", \"- [ ] /math: Verify that the implemented logic correctly reflects the derived mathematical recurrence.\", \"- [ ] /router: Perform final QA check on proof, code, and complexity analysis for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"sprague-grundy\"], \"acceptance_criteria\": [\"- The provided proof must be logically sound and correctly establish the nim-sequence's pattern for Dawson's Kay.\", \"- The Python code must execute correctly and compute the nim-value for n=100 within 1 second.\", \"- The complexity analysis must correctly state the asymptotic time and space bounds in big-O notation and be justified by the algorithm's structure.\", \"- All generated artifacts must be consistent, with the code implementing the proof's logic and the analysis reflecting the code's performance.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1326, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous derivation and JAX implementation of the reverse-time stochastic differential equation (SDE) for score-based generative models. Start from the forward process SDE, derive the reverse-time SDE using the Fokker-Planck equation, and then implement a numerical solver like Euler-Maruyama. Your implementation must verify that the learned score function correctly approximates the gradient of the log-density of the perturbed data distribution and track the FID score during training on a simple dataset like MNIST.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time SDE Fokker-Planck\\\", mode=web)\", \"/math(Derive the reverse-time SDE from the forward process SDE and the associated Fokker-Planck equation, explicitly showing the role of the score function, \\\\nabla_x \\\\log p_t(x), and tracking convergence criteria.)\", \"/code(Implement a JAX-based score network and an Euler-Maruyama solver for the derived reverse-time SDE. Train the diffusion model on MNIST, tracking the FID score and score-matching loss with CUDA acceleration., using Python)\", \"/math(Analyze the numerical stability and discretization error of the implemented Euler-Maruyama solver, and verify that the discrete update rule is a consistent approximation of the continuous-time SDE derivation., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with literature review via the blue /general-search agent to ground the task in established theory. The /math agent then performs the core theoretical derivation. This derivation is passed to the /code agent for a concrete JAX implementation. A final call to the /math agent verifies the numerical integrity of the implementation, ensuring the code correctly reflects the theory.\", \"expected_artifacts\": [\"A PDF document with the full mathematical derivation of the reverse-time SDE from the Fokker-Planck equation.\", \"A Python script containing the JAX implementation of the score model, SDE solver, and training loop.\", \"A metrics.json file containing the final FID score and a log of the training loss.\", \"A verification report from the final /math step confirming the numerical stability of the SDE solver.\", \"A bibliography citing at least two foundational arXiv papers on score-based SDEs.\"], \"thinking_outline\": [\"1. Research the theoretical connection between forward SDEs, the Fokker-Planck partial differential equation, and the resulting reverse-time SDE as described in the foundational literature on score-based generative modeling.\", \"2. Formally derive the reverse-time SDE, clearly stating all assumptions and showing how the score function, \\\\nabla_x \\\\log p_t(x), arises from the derivation.\", \"3. Implement a numerical integrator (e.g., Euler-Maruyama) for the derived SDE in JAX, alongside a neural network to approximate the score function using a denoising score-matching objective.\", \"4. Train the model on the MNIST dataset, systematically tracking the FID score on a held-out set and the training loss to monitor convergence.\", \"5. Validate the correctness of the implementation by checking the numerical stability of the solver and auditing the code to ensure the discretization scheme accurately reflects the derived continuous-time equations.\", \"6. Verify that the final generated samples qualitatively resemble the training data distribution and that the FID score has reached a reasonable value.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers for the math agent's derivation. The resulting equations from /math are the specification for the /code agent's implementation. The /code agent's implementation is then passed back to /math for a formal numerical analysis. Verification: The final /math analysis must confirm the code's consistency with the initial derivation. Fallback: If the derivation is intractable, /general-search for alternative but equivalent formulations; if the code is unstable, revert to a simpler ODE-based solver if applicable.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on SDE-based diffusion models (verify results)\", \"- [ ] /math: Produce a self-contained, step-by-step derivation of the reverse-time SDE.\", \"- [ ] /code: Implement the SDE solver and score network in JAX.\", \"- [ ] /code: Train the model on MNIST until the validation loss plateaus and compute the final FID score.\", \"- [ ] /math: Write a formal analysis of the implemented solver's numerical properties.\", \"- [ ] /math: Verify that the JAX implementation of the SDE update step correctly discretizes the derived continuous-time equation.\", \"- [ ] router QA: Review all artifacts for correctness, consistency, and adherence to acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"score-based-modeling\", \"stochastic-differential-equations\", \"generative-ai\"], \"acceptance_criteria\": [\"- The mathematical derivation must be rigorous and correctly link the forward SDE, Fokker-Planck equation, and reverse-time SDE.\", \"- The JAX implementation must successfully train, and the final FID score on MNIST must be below 20.\", \"- The numerical verification report must confirm that the solver is stable and a first-order accurate discretization of the derived SDE.\", \"- The final report must cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1328, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating model compression techniques for large language models and need a deep dive into Tensor-Train (TT) decomposition. Specifically, I want to understand the theoretical underpinnings of applying TT decomposition to a model's embedding layer. Can you provide a formal derivation of the TT-SVD and Alternating Least Squares (ALS) algorithms for this task, analyze the convergence properties with respect to the TT-ranks, and then develop a verified Python implementation to empirically study the trade-off between compression ratio and downstream perplexity on a benchmark dataset?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org tensor train decomposition for neural network compression \\\"alternating least squares\\\" convergence analysis\\\", mode=rag)\", \"/math(Derive the Alternating Least Squares (ALS) update rules for Tensor-Train decomposition of a target weight matrix, analyzing the theoretical convergence rate and its dependence on the matrix eigenvalue spectrum.)\", \"/code(Implement a Tensor-Train decomposed linear layer in PyTorch, leveraging CUDA for acceleration, and include a Bayesian optimization routine for hyperparameter tuning of the TT-ranks to minimize validation perplexity., using Python)\", \"/math(Verify the numerical stability of the TT-ALS implementation by analyzing the condition number of intermediate matrices and comparing the empirical computational complexity against theoretical Chebyshev polynomial bounds.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative literature from arXiv. This informs the /math agent's formal derivation of the core algorithm. The /code agent then implements this theory for empirical testing. A final /math call serves as a crucial verification step, ensuring the code's numerical behavior aligns with the theoretical model, closing the loop between theory and practice.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Tensor-Train ALS update equations and convergence analysis.\", \"- A Python script implementing the TT-decomposed layer and a training/evaluation harness.\", \"- A summary report in Markdown with plots showing the Pareto frontier of compression ratio versus model perplexity.\", \"- A verification document confirming the alignment between the theoretical complexity/convergence and empirical measurements.\", \"- A bibliography file with at least two cited arXiv preprints.\"], \"thinking_outline\": [\"1. Use /general-search to find seminal papers on Tensor-Train decomposition for deep learning, focusing on methods like TT-SVD and TT-ALS for weight matrix approximation.\", \"2. Formulate the optimization problem for approximating a dense weight matrix with a TT-matrix and formally derive the iterative update rules for the Alternating Least Squares (ALS) solver.\", \"3. Implement the TT-decomposed layer and the ALS training algorithm within the PyTorch framework, ensuring the implementation is numerically stable and can be integrated into a standard model architecture.\", \"4. Design and execute an experiment to train a language model with the compressed embedding layer, systematically varying the TT-ranks to map out the trade-off between parameter count and validation perplexity.\", \"5. Verify that the empirical convergence behavior of the implemented ALS solver aligns with the theoretical rate derived in the mathematical analysis.\", \"6. Validate the final implementation by comparing its performance against a non-decomposed baseline model and document the findings in a comprehensive report.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses these to produce a formal derivation -> /code implements the derivation and runs experiments -> /math receives the code and experimental logs to verify numerical properties against the theory -> router QA (verification: code's empirical convergence matches math's theoretical bounds; fallback: if mismatch, return to /math to check derivation or /code to debug implementation).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on Tensor-Train decomposition for compressing neural networks.\", \"- [ ] /math: Derive the complete TT-ALS update equations, including initialization via TT-SVD, and provide a proof sketch for its local convergence.\", \"- [ ] /code: Implement the TT-decomposed layer in PyTorch, compatible with autograd.\", \"- [ ] /code: Write a script to train a model with the TT-layer, varying TT-ranks and logging compression ratio and perplexity.\", \"- [ ] /math: Verify that the parameter count calculation in the code matches the theoretical formula for a TT-matrix.\", \"- [ ] /math: Analyze the empirical convergence plots from the code and check for consistency with the derived theoretical rate.\", \"- [ ] /code: Generate final plots for the trade-off curve and format the results into a metrics JSON file.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"model-compression\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for TT-ALS must be complete and mathematically sound.\", \"- The Python code must be runnable and reproduce the key findings, including the compression-perplexity trade-off curve.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\", \"- The verification artifact must explicitly confirm or deny the alignment between theoretical and empirical convergence rates.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1329, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project in computational physics and need to solve a large, sparse linear system on a GPU. Could you explain the convergence properties of the Jacobi method, especially for diagonally dominant matrices, and then provide a simple CUDA implementation to demonstrate its behavior?\n", "completion": "{\"route_plan\": [\"/math(Derive the sufficient condition for the convergence of the Jacobi method, focusing on strictly diagonally dominant matrices and its connection to the spectral radius of the iteration matrix. Explain the role of eigenvalue analysis in this proof.)\", \"/general-search(query=\\\"site:developer.nvidia.com OR site:arxiv.org introductory CUDA sparse matrix vector multiplication tutorial Jacobi\\\", mode=web)\", \"/code(Implement the Jacobi method in Python using CuPy for GPU acceleration. Test it on a sample sparse, diagonally dominant system, plot the residual norm to visualize convergence, and validate the result against a known solution.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation for convergence. It then uses the blue /general-search agent to find practical, high-authority examples of the core GPU operation (sparse matrix-vector multiplication) in CUDA. Finally, /code synthesizes the theory and practical examples into a working, verifiable implementation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Jacobi method's convergence condition.\", \"A Python script (`jacobi_cuda.py`) implementing the solver using CuPy.\", \"A PNG plot (`convergence_plot.png`) showing the L2 norm of the residual at each iteration.\", \"A verification report in `report.md` comparing the CUDA output against a NumPy baseline solution.\", \"A list of at least two cited URLs or arXiv IDs for CUDA implementation patterns.\"], \"thinking_outline\": [\"1. Formally define the Jacobi method as an iterative process for solving the linear system Ax=b by decomposing A into D+L+U.\", \"2. Derive the iterative update rule and prove that a sufficient condition for convergence is that the matrix A is strictly diagonally dominant, which guarantees the spectral radius of the iteration matrix is less than 1.\", \"3. Research common CUDA patterns for sparse matrix-vector multiplication (SpMV), which is the most computationally intensive step of each Jacobi iteration.\", \"4. Implement the Jacobi solver using a CUDA-accelerated library like CuPy, constructing a sample sparse, diagonally dominant matrix to ensure convergence.\", \"5. Verify the implementation's correctness by comparing its final solution vector for a small system against the highly accurate result from a trusted library like `numpy.linalg.solve`.\", \"6. Plot the L2 norm of the residual vector at each iteration to visually confirm the exponential decay predicted by the mathematical theory.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math defines the conditions (diagonally dominant matrix) required for the /code implementation to succeed. The /general-search agent provides CUDA-specific implementation patterns to guide the /code agent. The final code artifact is passed to router QA for verification against the theoretical convergence criteria and baseline results. Fallback: If the code diverges, re-verify the matrix properties in /math and debug the CUDA kernel logic in /code.\", \"todo_list\": [\"- [ ] Use /math to derive the convergence criteria for the Jacobi method based on matrix properties (verify results)\", \"- [ ] Use /general-search to find authoritative CUDA implementation patterns for key linear algebra operations.\", \"- [ ] Use /code to implement the Jacobi solver in Python with CuPy.\", \"- [ ] In the /code step, generate a plot of the residual norm versus iteration count.\", \"- [ ] Perform a final verification by comparing the CUDA result to a NumPy baseline for a known problem.\", \"- [ ] Submit all artifacts for router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"cuda\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly proves that strict diagonal dominance is a sufficient condition for Jacobi convergence.\", \"- The Python/CuPy script executes without errors and produces a solution vector for a sample 100x100 sparse system.\", \"- The generated plot must show a monotonically decreasing residual norm, trending towards zero.\", \"- The final solution vector from the CUDA code must be within a relative tolerance of 1e-5 compared to the solution from `numpy.linalg.solve`.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1338, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Alternating Least Squares (ALS) update rules for a Canonical Polyadic (CP) decomposition of a 4D convolutional weight tensor. Implement this in PyTorch to reduce model parameter count and computational complexity, then validate its convergence and benchmark against a standard Conv2D layer on CIFAR-10.\n", "completion": "{\"route_plan\": [\"/math(Derive the closed-form ALS update rule for a rank-R CP decomposition of a 4D tensor, analyzing the computational complexity of the forward pass and relating it to the tensor's eigenvalue properties.)\", \"/code(Implement a PyTorch layer for the CP-decomposed convolution using the derived ALS rules. Validate the implementation by checking for numerical convergence on a synthetic tensor and benchmark performance on CIFAR-10., using Python)\", \"/general-search(query=\\\"site:arxiv.org canonical polyadic decomposition convolutional neural network compression\\\", mode=web)\"], \"route_rationale\": \"The route follows a theory-to-practice-to-context pattern. /math is used first to establish the theoretical foundation by deriving the necessary update equations and analyzing complexity. /code then translates this theory into a practical, verifiable implementation. Finally, the blue /general-search agent is tasked with finding peer-reviewed literature to contextualize and justify the approach.\", \"expected_artifacts\": [\"A PDF document with the full mathematical derivation of the ALS update rules for CP tensor decomposition, including complexity analysis.\", \"A Python script (`cp_conv.py`) containing the PyTorch implementation of the CP-decomposed convolutional layer and the training/evaluation loop.\", \"A JSON file (`results.json`) reporting the final metrics, including parameter counts, FLOPs, and test accuracy for both baseline and decomposed models.\", \"A Markdown file (`report.md`) summarizing the methodology, results, and a list of at least two relevant academic citations from arXiv.\"], \"thinking_outline\": [\"1. Formulate the optimization problem for CP decomposition of a 4D convolutional kernel tensor as a sum of rank-one tensors.\", \"2. Derive the closed-form update rules for each factor matrix using the Alternating Least Squares (ALS) method, which involves tensor matricization and the Khatri-Rao product.\", \"3. Implement the derived ALS updates within a custom PyTorch layer and validate the numerical correctness of the tensor operations (e.g., matricization, Khatri-Rao product) against a trusted library like `tensorly` on a small, synthetic tensor.\", \"4. Integrate the decomposed layer into a simple CNN architecture and train it on the CIFAR-10 dataset, comparing its training dynamics to a non-decomposed baseline.\", \"5. Benchmark the decomposed model's parameter count, FLOPs, and final test accuracy against the original baseline model to produce a conclusive trade-off analysis.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math is the specification for the /code agent. The empirical results from /code are then contextualized by the literature found by /general-search. The router performs a final QA check. Verification: The empirical parameter reduction from /code must match the formula derived by /math. Fallback: If the model from /code fails to converge, task /math to investigate potential stability issues in the ALS derivation.\", \"todo_list\": [\"- [ ] /math: Derive the ALS update rules for each of the four factor matrices of a 4D tensor.\", \"- [ ] /math: Provide the formula for the parameter count and FLOPs of the CP-decomposed convolutional layer.\", \"- [ ] /code: Implement the CP-decomposed layer in PyTorch.\", \"- [ ] /code: Validate the implementation's tensor operations against the `tensorly` library for a small, known tensor to check for correctness.\", \"- [ ] /code: Train and evaluate both baseline and decomposed models on CIFAR-10, saving the results.\", \"- [ ] /general-search: Find two seminal papers on using CP/Tucker decomposition for CNN compression on arXiv.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"model-compression\"], \"acceptance_criteria\": [\"The mathematical derivation for the ALS update rules must be complete and correct, showing all steps involving matricization and the Khatri-Rao product.\", \"The Python implementation must achieve a parameter reduction of at least 5x compared to the baseline while maintaining test accuracy within a 3% margin on CIFAR-10.\", \"The final report must cite at least two relevant arXiv papers with IDs that discuss tensor decomposition for neural network compression.\", \"The code's custom tensor operations must pass a numerical validation check against a reference implementation like `tensorly`.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1348, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the conditions for identifying higher-order saddle points in a non-convex minimax optimization problem, specifically for a simplified Generative Adversarial Network (GAN) objective function. Analyze the Hessian of the Lagrangian to characterize these points, and then develop a Python simulation using JAX to numerically find and verify such a point, showing its distinct eigenvalue signature compared to local minima or maxima.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org GAN loss landscape higher-order saddle point Hessian analysis minimax optimization\\\", mode=both)\", \"/math(Derive the necessary and sufficient conditions for a k-th order saddle point on a GAN's Lagrangian, focusing on the Hessian eigenvalue spectrum and its implications for adversarial training dynamics.)\", \"/code(Implement a JAX-based optimization to locate a saddle point for a 2D Gaussian mixture GAN, tracking Hessian eigenvalue evolution to validate the adversarial convergence properties near the critical point., using Python)\", \"/math(Verify the numerically computed eigenvalue spectrum from the JAX simulation against the derived theoretical conditions, confirming the saddle point's order and stability characteristics.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on GAN stability and Hessian analysis. This informs the /math agent's formal derivation of saddle point conditions. The /code agent then implements a numerical experiment based on these derivations. A final /math step is crucial for verifying that the experimental results from the code align with the initial theoretical predictions, closing the loop.\", \"expected_artifacts\": [\"* A LaTeX document containing the full mathematical derivation of the conditions for higher-order saddle points in the specified GAN objective.\", \"* A Python script (JAX) implementing the GAN, the optimization algorithm, and the Hessian eigenvalue computation.\", \"* A JSON file (`results/metrics.json`) reporting the final eigenvalues of the located critical point and the convergence error.\", \"* A summary report in Markdown (`report.md`) explaining the findings, including plots of the loss landscape and eigenvalue evolution, and citing at least two relevant papers.\", \"* Verification output log (`verification.log`) showing the explicit comparison between theoretical and numerical eigenvalues.\"], \"thinking_outline\": [\"1. Research the theoretical framework for analyzing saddle points in non-convex, non-concave minimax games, focusing on definitions involving the Hessian of the game's Lagrangian.\", \"2. Formulate the Lagrangian for a simplified GAN objective (e.g., two 1D Gaussian distributions) and derive its first and second-order optimality conditions (gradient and Hessian matrix).\", \"3. Define the precise criteria for a higher-order saddle point based on the Hessian's eigenvalue signature (i.e., the number of positive, negative, and near-zero eigenvalues).\", \"4. Implement a numerical optimization algorithm (e.g., Gradient Descent Ascent) in JAX to find a critical point of the GAN objective function.\", \"5. Validate that the located critical point's numerically computed Hessian eigenvalue spectrum matches the theoretical signature of a higher-order saddle point derived in step 3.\", \"6. Audit the convergence behavior near the saddle point and discuss the implications of such structures on the stability and performance of adversarial training.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: numerical eigenvalues from /code must match the signature derived by the first /math agent; fallback: if mismatch, /code agent retries optimization with different hyperparameters and /math agent double-checks derivation assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Hessian analysis in GANs (verify results)\", \"- [ ] /math: Derive the Lagrangian's Hessian and the eigenvalue conditions for a k-th order saddle point.\", \"- [ ] /code: Implement the simplified GAN model and optimization loop in JAX.\", \"- [ ] /code: Add functionality to compute the Hessian and its eigenvalues at any given point.\", \"- [ ] /math: Receive the numerical eigenvalues from the /code agent.\", \"- [ ] /math: Verify that the numerical eigenvalues align with the theoretical signature for a saddle point.\", \"- [ ] /code: Generate plots of the loss landscape and eigenvalue trajectory during optimization.\", \"- [ ] Router QA: Review all artifacts for correctness, consistency, and adherence to acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"saddle-point-optimization\", \"hessian-analysis\", \"adversarial-learning\"], \"acceptance_criteria\": [\"* The mathematical derivation correctly identifies the Hessian eigenvalue conditions for a higher-order saddle point.\", \"* The Python simulation successfully converges to a critical point that is not a local minimum for either player.\", \"* The numerically computed eigenvalues of the converged point's Hessian match the theoretically derived signature for a saddle point within a small tolerance (e.g., epsilon < 1e-4).\", \"* The final report includes at least two citations to relevant arXiv preprints or peer-reviewed articles.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1352, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous mathematical analysis comparing the stability of persistence diagrams derived from point-cloud data sampled from a learned manifold. Specifically, I want to investigate the Gromov-Hausdorff distance between the underlying manifold and its learned representation, and how this bounds the bottleneck distance between their respective persistence homology barcodes. Please derive the theoretical bounds and then implement a simulation using Mapper or UMAP to empirically validate the convergence properties of the derived inequality, referencing recent arXiv preprints on geometric deep learning.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org stability theorem persistent homology Gromov-Hausdorff bottleneck distance manifold learning\\\", mode=rag)\", \"/math(Derive the formal proof bounding bottleneck distance as a function of Gromov-Hausdorff distance, ensuring the proof's convergence criteria are clearly stated for the arXiv context.)\", \"/code(Implement a Python simulation using an autoencoder to learn a manifold, compute persistence diagrams with giotto-tda, and track the bottleneck distance against the reconstruction error, potentially using CUDA for acceleration and analyzing the primary eigenvalue.)\", \"/math(Verify the simulation's numerical results against the derived theoretical bounds, analyzing asymptotic convergence and quantifying any discrepancies between theory and practice., includes cybersecurity)\"], \"route_rationale\": \"The plan begins with the blue /general-search agent to gather state-of-the-art theoretical foundations. This output is essential for the /math agent to construct a rigorous and relevant proof. The /code agent then translates the abstract mathematics into a concrete computational experiment for empirical validation. A final /math step provides critical cross-verification, ensuring the theoretical model accurately describes the computational results.\", \"expected_artifacts\": [\"A formal mathematical proof in LaTeX format detailing the stability bound derivation.\", \"Python source code for the manifold learning and Topological Data Analysis simulation.\", \"A summary report including plots of bottleneck distance vs. manifold reconstruction error.\", \"A verification document comparing theoretical bounds to empirical simulation results.\", \"A literature review citing at least two relevant arXiv preprints with their IDs.\"], \"thinking_outline\": [\"1. Establish the theoretical background on persistence homology, bottleneck distance, and Gromov-Hausdorff distance using authoritative sources.\", \"2. Formulate and prove a theorem that bounds the bottleneck distance between the persistence diagrams of a true manifold and its learned approximation as a function of their Gromov-Hausdorff distance.\", \"3. Design and implement a computational experiment to simulate this process: sample from a known manifold (e.g., a torus), train a neural network to learn its structure, and compute persistence diagrams for both.\", \"4. Verify that the implemented algorithms for persistence homology and manifold learning are numerically stable and correct.\", \"5. Empirically validate the derived theoretical bound by plotting the computed bottleneck distance against the approximation error across different sampling sizes and model parameters.\", \"6. Audit the final report to ensure the proof, code, and empirical results are consistent and rigorously presented.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (proof derivation) -> /code (simulation implementation) -> /math (verification of results). The math agent uses search results to build the proof. The code agent implements the mathematical constructs. A final math agent verifies that the code's output aligns with the proof's predictions. Fallback: If verification fails, the code agent must debug the simulation, focusing on numerical stability issues or incorrect parameterization.\", \"todo_list\": [\"- [ ] /general-search: Find recent arXiv papers on the stability of persistence homology with respect to Gromov-Hausdorff perturbations (verify results)\", \"- [ ] /math: Derive the formal proof bounding bottleneck distance using Gromov-Hausdorff distance, citing sources from the search.\", \"- [ ] /code: Implement the manifold learning (Autoencoder) and persistence homology computation (Ripser/Giotto-TDA) pipeline in Python.\", \"- [ ] /code: Generate plots showing the relationship between manifold approximation error and bottleneck distance.\", \"- [ ] /math: Verify that the empirical convergence rate observed in the simulation aligns with the derived theoretical bound.\", \"- [ ] /code: Document the simulation code and package dependencies in a `README.md` file.\", \"- [ ] Router QA: Review all artifacts for consistency between the mathematical proof, code implementation, and final report.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"algebraic-topology\"], \"acceptance_criteria\": [\"The provided mathematical proof must be rigorous, self-contained, and logically sound.\", \"The Python code must be executable and reproduce the plots and metrics presented in the final report.\", \"The empirical results must clearly validate or challenge the derived theoretical bound, with statistical significance discussed.\", \"The report must cite at least two relevant arXiv preprints on TDA stability or geometric deep learning.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1354, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the detailed balance condition for the Metropolis-Hastings algorithm and prove its ergodic convergence for a Bayesian logistic regression posterior. Implement this MCMC sampler and validate its stationary distribution against a known ground truth using the Kolmogorov-Smirnov test.\n", "completion": "{\"route_plan\": [\"/math(Formally derive the detailed balance condition for the Metropolis-Hastings algorithm and outline the proof structure for establishing ergodicity and convergence to the target posterior distribution in a Bayesian model.)\", \"/general-search(query=\\\"site:columbia.edu site:arxiv.org Metropolis-Hastings ergodicity proof Markov chain mixing time\\\", mode=web)\", \"/code(Implement the Metropolis-Hastings sampler in Python for a Bayesian logistic regression model. Validate the sampler's output against a known posterior by performing a Kolmogorov-Smirnov test to check for convergence.)\"], \"route_rationale\": \"The plan starts with /math to establish the core theoretical foundation and proof structure. It then hands off to the 'blue' /general-search agent to find authoritative references for the ergodicity proof, which can be complex. Finally, /code is used to implement the algorithm and, crucially, to empirically validate the theoretical convergence properties derived in the first step.\", \"expected_artifacts\": [\"A LaTeX document with the full mathematical proof of detailed balance and convergence.\", \"A Python script implementing the Metropolis-Hastings sampler for Bayesian logistic regression.\", \"A summary report including Kolmogorov-Smirnov test statistics and plots comparing the sampled posterior to the target distribution.\", \"A bibliography file with citations for at least two academic sources.\", \"A JSON file containing the final calculated metrics.\"], \"thinking_outline\": [\"1. Formally define a Markov chain and the properties of its stationary distribution.\", \"2. Derive the detailed balance condition and demonstrate how the Metropolis-Hastings acceptance probability satisfies it, ensuring the target distribution is the stationary distribution.\", \"3. Outline the proof that the chain is ergodic (irreducible and aperiodic) under common choices for the proposal distribution, which guarantees convergence.\", \"4. Formulate the Bayesian logistic regression model, specifying the likelihood and prior distributions to define the unnormalized target posterior.\", \"5. Implement the sampler, generate a sufficiently long chain of samples, and validate the empirical distribution of the samples against an analytically tractable or high-fidelity reference posterior using the specified statistical tests.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: code output matches theoretical distribution via K-S test; fallback: if K-S test fails, /code agent must debug sampler implementation or proposal distribution parameters).\", \"todo_list\": [\"- [ ] /math: Derive the detailed balance condition and sketch the proof of ergodicity.\", \"- [ ] /general-search: Find two high-quality references (e.g., from arXiv) on MCMC convergence proofs.\", \"- [ ] /code: Implement the MCMC sampler for Bayesian logistic regression with a Gaussian proposal.\", \"- [ ] /code: Write a validation module using the Kolmogorov-Smirnov test against a known target distribution.\", \"- [ ] /router: Review the mathematical proof for correctness and the code for implementation fidelity and validation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"mcmc-methods\", \"bayesian-inference\", \"stochastic-processes\"], \"acceptance_criteria\": [\"- The mathematical proof correctly derives the detailed balance condition and outlines the necessary conditions for convergence.\", \"- The Python implementation successfully generates samples from the posterior of a Bayesian logistic regression model on a synthetic dataset.\", \"- The Kolmogorov-Smirnov test p-value is greater than 0.05, indicating the sampled distribution is statistically indistinguishable from the target distribution.\", \"- At least two relevant academic papers are cited in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1362, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the logical error rate for the [[7,1,3]] Steane code under a depolarizing noise channel with probability *p*. I need a formal derivation of the threshold theorem's scaling behavior and a Monte Carlo simulation in Python to verify the analytical results, including confidence interval analysis from the bootstrap method.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org [[7,1,3]] Steane code logical error rate depolarizing channel threshold theorem\\\", mode=both, includes cybersecurity)\", \"/math(Derive the first-order approximation for the logical error rate of the Steane code under a depolarizing channel, analyzing the combinatorial complexity and establishing a convergence criterion.)\", \"/code(Implement a Monte Carlo simulation in Python with NumPy/SciPy for the Steane code, simulating single-qubit depolarizing errors and syndrome measurement. Use bootstrap resampling for confidence interval estimation on the logical error rate.)\", \"/math(Compare the analytical derivation of the logical error rate against the empirical results from the simulation, computing the discrepancy and verifying that it falls within the estimated bootstrap confidence interval.)\"], \"route_rationale\": \"The \\\"blue\\\" general-search agent first curates foundational literature on the Steane code and its error threshold. The /math agent then uses these sources to perform the analytical derivation. The /code agent implements a simulation to generate empirical data for validation. Finally, a second /math call performs a rigorous statistical comparison between the theoretical and simulated results.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the logical error rate.\", \"A Python script implementing the Monte Carlo simulation of the Steane code.\", \"A JSON file containing the simulated logical error rates at various physical error rates, along with their confidence intervals.\", \"A verification report comparing the analytical and simulated results, including a plot.\", \"A bibliography citing at least two authoritative papers from arXiv.\"], \"thinking_outline\": [\"1. Research the stabilizer formalism for the [[7,1,3]] Steane code and the mathematical model of the single-qubit depolarizing channel.\", \"2. Derive the probability of uncorrectable errors leading to a logical error, focusing on the lowest-order error events that defeat the correction procedure.\", \"3. Implement the simulation: encoding a logical qubit, applying the depolarizing error channel to each physical qubit, extracting syndromes, and applying corrections based on a lookup-table decoder.\", \"4. Validate the simulation's components by checking that the syndrome values for known single-qubit Pauli errors match theoretical expectations.\", \"5. Run the simulation across a range of physical error probabilities *p* to collect statistics on logical error rates, using bootstrap resampling to estimate confidence intervals.\", \"6. Audit the final analytical derivation against the simulation results, ensuring the theoretical curve lies within the 95% confidence intervals of the simulated data points, especially in the low-p regime.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (verification) -> router QA (verification: Cross-check if the code's error model matches the derivation's assumptions; fallback: If mismatch, send back to /math with simulation parameters for a refined derivation.)\", \"todo_list\": [\"- [ ] /general-search: Find 2-3 key papers on the Steane code threshold and logical error rate analysis.\", \"- [ ] /math: Derive the leading-order polynomial for the logical error rate P_L as a function of physical error rate p.\", \"- [ ] /code: Implement the simulation framework for the Steane code, including encoding, error channel, and decoding.\", \"- [ ] /code: Implement the data collection loop and bootstrap resampling logic for confidence intervals.\", \"- [ ] /math: verify the analytical formula against the low-p limit of the simulation data.\", \"- [ ] /code: verify the correctness of the syndrome lookup table for all single-qubit X, Y, and Z errors.\", \"- [ ] /math: Generate a final report comparing the analytical curve and simulated data points with error bars.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-error-correction\", \"fault-tolerant-computing\", \"monte-carlo-simulation\"], \"acceptance_criteria\": [\"The analytical derivation for the logical error rate must be mathematically sound and clearly explained.\", \"The Python simulation must be runnable and correctly reproduce known behavior for simple error cases.\", \"The final report's plot must show the analytical curve passing through the 95% confidence intervals of the simulated data points for low physical error rates.\", \"The solution must cite at least two relevant arXiv preprints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1423, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Variational Quantum Eigensolver (VQE) for the LiH molecular Hamiltonian at an interatomic distance of 1.546 \u00c5. Please derive the gradient expressions using the parameter shift rule for a hardware-efficient RyRz ansatz. Then, implement this in Python to find the ground state energy, ensuring the optimization process tracks convergence to within chemical accuracy and validates the final eigenvalue against known FCI results.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational quantum eigensolver LiH 'parameter shift rule' hardware-efficient ansatz\\\", mode=both)\", \"/math(Derive the analytical gradient for the expectation value of the LiH Hamiltonian with respect to the RyRz ansatz parameters using the parameter shift rule, detailing the resulting eigenvalue expressions and convergence conditions.)\", \"/code(Implement the VQE algorithm using PennyLane, constructing the LiH Hamiltonian, the RyRz ansatz, and a custom gradient-descent optimizer based on the derived parameter shift rule. Track the energy eigenvalue at each iteration., using Python)\", \"/math(Analyze the numerical convergence data from the code execution, plot the energy vs. iteration, and verify that the final computed ground state energy is within the chemical accuracy threshold of the known FCI eigenvalue for LiH.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on VQE for LiH. This grounds the theoretical work. /math is then used to perform the core analytical derivation of the quantum gradients. /code translates this mathematical formalism into a working simulation. The final /math step serves as a crucial verification, comparing the empirical results from the code against theoretical benchmarks to validate the entire workflow.\", \"expected_artifacts\": [\"- A PDF document containing the full mathematical derivation of the parameter shift rule for the specified ansatz and Hamiltonian.\", \"- A Python script (`vqe_lih.py`) implementing the complete VQE simulation.\", \"- A JSON file (`results.json`) logging the optimization history and final ground state energy eigenvalue.\", \"- A final report (`report.md`) summarizing the methodology, results, and comparison against literature values, including citations.\", \"- A log file (`verification.log`) detailing the numerical check of the analytic gradient against a finite-difference approximation.\"], \"thinking_outline\": [\"1. Research state-of-the-art VQE implementations for the LiH molecule, focusing on common hardware-efficient ansatz structures and optimization techniques documented on arXiv.\", \"2. Formulate the LiH molecular Hamiltonian in the second quantization representation and map it to a multi-qubit Pauli operator using the Jordan-Wigner transformation.\", \"3. Formally derive the analytical gradient of the Hamiltonian's expectation value with respect to each ansatz parameter using the parameter shift rule.\", \"4. Implement the VQE algorithm, including the Hamiltonian, the ansatz, and the custom gradient-based optimizer. Execute the simulation to find the ground state energy.\", \"5. Validate that the computed ground state energy is within chemical accuracy (1.6 x 10^-3 Hartree) of the established Full Configuration Interaction (FCI) benchmark value.\", \"6. Audit the code's gradient implementation by comparing its output for a sample set of parameters against a numerical finite-difference approximation to verify correctness.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers (arXiv IDs) to the math agent. The math agent provides the gradient derivations to the code agent. The code agent produces numerical results (JSON log) for the final math agent to analyze. Verification: The final analysis must confirm the code's output aligns with theoretical benchmarks. Fallback: If convergence fails or accuracy is not met, the code agent will be re-invoked with adjusted hyperparameters (learning rate, shot count) based on recommendations from the analysis step.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on VQE for LiH, focusing on ansatz design and the parameter shift rule.\", \"- [ ] /math: Derive the gradient expressions for the RyRz ansatz applied to the LiH Hamiltonian.\", \"- [ ] /code: Implement the VQE algorithm in PennyLane, including the Hamiltonian, ansatz, and optimizer.\", \"- [ ] /code: Add logging to capture the energy at each optimization step and save it to `results.json`.\", \"- [ ] /math: Numerically verify the implemented gradient calculation against a finite-difference approximation for a random parameter vector.\", \"- [ ] /math: Analyze the final energy from `results.json` and compare it to the FCI benchmark, confirming it is within chemical accuracy.\", \"- [ ] /router: Perform final QA, ensuring all artifacts are present, consistent, and meet the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-machine-learning\", \"variational-quantum-eigensolver\", \"computational-chemistry\"], \"acceptance_criteria\": [\"- The computed ground state energy for LiH must be within 1.6 x 10^-3 Hartree of the known FCI value.\", \"- The analytical gradient implementation must pass a numerical verification check against a finite-difference approximation with a tolerance of 1e-5.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\", \"- The code must be runnable and produce the specified JSON artifact with the optimization history.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1425, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nExplain the quadratic complexity of the self-attention mechanism in transformers. I need a formal derivation of the complexity, a simple Python implementation to validate it, and a summary of how FlashAttention improves computational efficiency for long-sequence econometrics.\n", "completion": "{\"route_plan\": [\"/math(Derive the computational complexity of the standard self-attention mechanism, showing why it is quadratic in the sequence length (N). Detail the matrix multiplications and their dimensions to prove the O(N^2 * d) complexity, paying attention to the Bayesian interpretation of attention weights.)\", \"/code(Implement a basic self-attention layer in Python using NumPy to empirically verify the quadratic complexity. Profile the execution time for varying sequence lengths and plot the results, ensuring any potential CUDA kernel launch overhead is considered in the analysis.)\", \"/general-search(query=\\\"site:arxiv.org \\\\\\\"FlashAttention\\\\\\\" tiled attention CUDA kernel fusion\\\", mode=web)\"], \"route_rationale\": \"/math is used for the formal theoretical derivation. /code provides empirical validation of the theory through profiling. The blue /general-search agent is then used to find authoritative research on specific optimization techniques that address the bottleneck identified by the first two steps.\", \"expected_artifacts\": [\"A formal mathematical derivation of self-attention's O(N^2 * d) complexity.\", \"A Python script implementing a naive self-attention mechanism and a plot visualizing its quadratic runtime.\", \"A summary report on FlashAttention, citing at least two arXiv papers with their IDs.\", \"A metrics file (metrics.json) reporting the primary and secondary metrics.\"], \"thinking_outline\": [\"1. Formally define the self-attention mechanism using matrix notation for Query (Q), Key (K), and Value (V).\", \"2. Derive the computational complexity by analyzing the matrix multiplication steps, specifically the Q @ K.T operation which results in an N x N attention matrix.\", \"3. Implement this mechanism in Python and design an experiment to measure runtime as the sequence length N increases.\", \"4. Validate that the empirical runtime measurements from the experiment fit a quadratic curve, confirming the theoretical derivation.\", \"5. Research FlashAttention to understand how kernel fusion and tiling reduce memory I/O, the primary bottleneck for large N.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The formal derivation from /math provides the theoretical hypothesis. /code tests this hypothesis empirically. The performance bottleneck observed in /code motivates the research in /general-search. Verification: The empirical complexity plot from /code must match the quadratic relationship derived in /math. Fallback: If they do not match, re-run /code with corrected profiling logic or re-audit the /math derivation for errors in dimensionality.\", \"todo_list\": [\"- [ ] /math: Derive the O(N^2 * d) complexity of the self-attention mechanism.\", \"- [ ] /code: Implement naive self-attention and plot its runtime vs. sequence length to show quadratic scaling.\", \"- [ ] /general-search: Research the core principles of FlashAttention and its performance benefits.\", \"- [ ] Verification: Compare the empirical plot from /code against the theoretical complexity from /math to ensure they align.\", \"- [ ] Router QA: Submit all artifacts for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"transformers\", \"computational-complexity\", \"model-optimization\"], \"acceptance_criteria\": [\"The mathematical derivation correctly identifies the Q @ K.T matrix multiplication as the source of O(N^2) complexity.\", \"The Python script's output plot clearly shows a curve that fits a quadratic function with an R-squared value > 0.98.\", \"The research summary correctly explains FlashAttention's use of kernel fusion and tiling to reduce HBM read/writes, citing at least two valid arXiv papers.\", \"The final report is well-structured, integrating the theory, empirical results, and research findings into a cohesive explanation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1470, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the full update rules for a dictionary learning algorithm, specifically the Method of Optimal Directions (MOD), by solving the associated non-convex optimization problem with an L0-norm sparsity constraint. Implement this in Python, validate its convergence on synthetic data generated from a known dictionary, and contextualize its performance by relating the learned dictionary's properties, like mutual coherence, to the recovery guarantees in compressed sensing theory under the Restricted Isometry Property (RIP).\n", "completion": "{\"route_plan\": [\"/math(Derive the update rules for the Method of Optimal Directions (MOD) dictionary learning algorithm by formulating the problem with a Frobenius norm data fidelity term and an L0-norm sparsity constraint. Analyze the alternating minimization convergence properties and the problem's Lagrangian.)\", \"/code(Implement the MOD algorithm and an Orthogonal Matching Pursuit (OMP) sparse coding step in Python. Validate on synthetic data, tracking mean squared reconstruction error against a known ground-truth dictionary. Compare performance with an ADMM-based approach for the sparse coding stage.)\", \"/general-search(query=\\\"site:arxiv.org site:ieee.org Method of Optimal Directions K-SVD compressed sensing Restricted Isometry Property mutual coherence\\\", mode=both, includes graphcut)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation and derive the algorithm's update equations. This is followed by /code for empirical implementation and validation of the derived theory. Finally, the blue /general-search agent is used to gather foundational literature, contextualize the results within the broader field of compressed sensing, and provide justification for the theoretical claims.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Method of Optimal Directions (MOD) update rules.\", \"A Python script implementing MOD and a sparse coding solver, with functions for generating synthetic data and evaluating performance.\", \"A plot of the signal reconstruction MSE versus iteration number, demonstrating algorithm convergence.\", \"A summary report citing at least two foundational papers on dictionary learning and compressed sensing, discussing concepts like RIP and mutual coherence.\"], \"thinking_outline\": [\"1. Formulate the dictionary learning problem as an optimization task aiming to minimize reconstruction error under a sparsity constraint for the coefficients.\", \"2. Derive the two-step alternating minimization procedure for the Method of Optimal Directions (MOD): first, the sparse coding step (e.g., using OMP), and second, the dictionary update step via a closed-form solution.\", \"3. Implement the complete MOD algorithm in Python, including a sparse solver and a data generation routine. Track reconstruction error and dictionary recovery metrics.\", \"4. Validate the implementation by ensuring the reconstruction error monotonically decreases and the learned dictionary converges towards the ground truth synthetic dictionary.\", \"5. Audit the Python code to verify that the dictionary update and sparse coding steps precisely match the mathematical formulas derived in the first step.\", \"6. Synthesize the empirical results with theoretical concepts from compressed sensing, explaining how properties of the learned dictionary affect sparse recovery performance.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent provides the exact update equations required by the /code agent. The /code agent generates empirical results (convergence plots, final dictionary) that need theoretical context from the /general-search agent. The final package is audited by the router. Verification: The router will cross-reference the math derivation with the code implementation. Fallback: If the code fails to converge, re-engage the /math agent to check the derivation for potential stability issues.\", \"todo_list\": [\"- [ ] /math: Derive the Lagrangian and obtain the closed-form dictionary update solution for MOD.\", \"- [ ] /code: Implement the MOD training loop and the OMP algorithm for the sparse coding step.\", \"- [ ] /general-search: Find foundational papers on MOD/K-SVD and the Restricted Isometry Property.\", \"- [ ] /code: Generate synthetic signals and run the algorithm, plotting the reconstruction MSE per iteration.\", \"- [ ] /math: Verify that the implemented dictionary update in the Python script is an exact match for the derived pseudo-inverse solution.\", \"- [ ] /code: Validate that the average sparsity of the learned coefficients meets the target sparsity constraint `k`.\", \"- [ ] /router: Review all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\", \"convex-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation for the MOD dictionary update step must be correct and complete.\", \"- The Python implementation must converge on synthetic data, achieving a final signal reconstruction MSE below a specified threshold (e.g., 1e-6).\", \"- The final report must cite at least two relevant academic papers (with arXiv or DOI links) and correctly define the Restricted Isometry Property (RIP).\", \"- The learned dictionary must successfully recover the ground-truth dictionary used to generate the synthetic data, as measured by atom-wise similarity.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1501, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the time-reversal stochastic differential equation for a variance-preserving forward process used in score-based generative modeling. Subsequently, implement a deep neural network in PyTorch to approximate the score function for a 2D Swiss roll distribution. Finally, use the trained score network to simulate the reverse SDE and generate samples, validating the convergence of the generated distribution to the target via the Wasserstein-1 distance.\n", "completion": "{\"route_plan\": [\"/math(Derive the time-reversal SDE from the corresponding Fokker-Planck equation for a given forward diffusion process, ensuring the final drift term explicitly incorporates the score function and accounts for Bayesian inverse principles.)\", \"/general-search(query=\\\"site:arxiv.org score-based generative model SDE implementation pytorch stability euler-maruyama\\\", mode=both)\", \"/code(Implement a score-matching network and a reverse-time SDE solver in PyTorch to generate samples from a 2D dataset, validating the model's sample quality and convergence using Wasserstein distance and FID score., using Python)\"], \"route_rationale\": \"/math is used first to establish the core theoretical equations for the time-reversal SDE. The blue /general-search agent then finds state-of-the-art implementation details and numerical stability tricks from arXiv. Finally, /code implements the derived theory and validates it empirically, using the findings from the literature search.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the time-reversal SDE.\", \"A Python script implementing the score network, training loop, and SDE sampler.\", \"A report summarizing the experimental results, including plots of the generated samples over time and a graph of the Wasserstein distance metric.\", \"A bibliography file citing at least two relevant arXiv preprints.\", \"A verification log confirming the alignment between the mathematical derivation and the code's loss function.\"], \"thinking_outline\": [\"1. Start by formulating the forward SDE and its corresponding Fokker-Planck equation which governs the evolution of the probability density p_t(x).\", \"2. Derive the time-reversal SDE by applying Anderson's theorem or by directly reversing the time parameter in the Fokker-Planck equation, expressing the reverse drift in terms of the score function, \u2207_x log p_t(x).\", \"3. Construct a neural network architecture and a denoising score matching loss function in PyTorch to approximate this score function for all time t.\", \"4. Implement a numerical SDE solver, like Euler-Maruyama, to simulate the reverse-time SDE using the trained score network.\", \"5. Validate the implementation by generating samples and computing the Wasserstein-1 distance between the generated and true data distributions to check for convergence.\", \"6. Verify that the learned score field qualitatively matches the true score field of the target distribution as t -> 0.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (implementation patterns) -> /code (implementation & validation) -> router QA (verification: code loss matches derivation, metric convergence is demonstrated; fallback: if code fails to converge, re-evaluate numerical solver stability with /general-search).\", \"todo_list\": [\"- [ ] /math: Derive the time-reversal SDE, clearly showing the relationship between the reverse drift and the score function.\", \"- [ ] /general-search: Find at least two key papers on practical implementation of SDE-based diffusion models, noting details on noise schedules and solver choices.\", \"- [ ] /code: Implement the score-matching network and training loop in PyTorch.\", \"- [ ] /code: Implement the reverse-time SDE sampler using the trained score network.\", \"- [ ] /code: Add instrumentation to compute and log the Wasserstein distance metric during evaluation.\", \"- [ ] /math: Verify that the implemented denoising score matching loss in the Python code correctly corresponds to the theoretical objective function.\", \"- [ ] router: Perform final QA review, checking for consistency between the derivation, code, and final report artifacts.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"generative-modeling\", \"stochastic-calculus\", \"diffusion-models\"], \"acceptance_criteria\": [\"The mathematical derivation of the time-reversal SDE must be complete and self-contained.\", \"The Python implementation must successfully train and generate visually plausible samples from the 2D Swiss roll distribution.\", \"The final report must show a decreasing trend in the Wasserstein-1 distance, indicating convergence of the model distribution to the target.\", \"Code must be runnable and adhere to the reproducibility contract.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1522, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm tackling a problem about counting non-intersecting paths on a large grid graph with arbitrary obstacles, a classic IOI-style challenge. A standard dynamic programming approach explodes combinatorially. Could we use a more advanced mathematical framework, possibly drawing from algebraic topology or statistical mechanics, to construct a transfer matrix whose determinant or trace gives the path count? I need a formal proof of this method's correctness, analyzing its computational complexity, and relating it to the graph's topological structure.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org non-intersecting lattice paths 'Kasteleyn matrix' 'FKT algorithm' transfer matrix method\\\", mode=rag, includes cybersecurity)\", \"/math(Formalize the problem by defining the grid graph and path configurations. Construct the transfer matrix using principles from combinatorial topology, analyzing its eigenvalue spectrum to determine the computational complexity.)\", \"/code(Implement the transfer matrix construction and the DP path-counting algorithm in Python using NumPy/SciPy. Add runtime validation to handle complex obstacle geometries and prepare for potential CUDA acceleration.)\", \"/math(Develop a rigorous proof of correctness, formally linking the determinant of the constructed matrix to the number of valid path configurations. Verify the implemented complexity aligns with the theoretical analysis., includes cybersecurity)\"], \"route_rationale\": \"The route starts with the blue general-search agent to survey existing literature on advanced path-counting algorithms like the FKT algorithm, which is essential for this advanced topic. The first /math agent then lays the theoretical groundwork. The /code agent implements this theory for empirical validation. The final /math agent provides the formal proof and verifies the theoretical-practical consistency, ensuring a robust and well-supported solution.\", \"expected_artifacts\": [\"* A formal mathematical proof detailing the construction of the transfer matrix and its connection to counting non-intersecting paths.\", \"* A Python script implementing the transfer-matrix dynamic programming algorithm.\", \"* A verification report comparing the algorithm's output against a brute-force enumeration on small, tractable grid graphs.\", \"* A summary of relevant literature, citing at least two arXiv preprints on lattice path counting or the FKT algorithm.\"], \"thinking_outline\": [\"1. Research the connection between counting perfect matchings (dimer coverings) on planar graphs and matrix determinants, focusing on the FKT algorithm and Kasteleyn matrices.\", \"2. Formalize the problem by representing the grid graph with obstacles and defining the state space for the dynamic programming approach in terms of the boundary path configurations.\", \"3. Derive the structure of the transfer matrix by applying principles from combinatorial topology, relating matrix entries to local path transitions and topological constraints imposed by the non-intersecting rule.\", \"4. Validate the transfer matrix construction by checking its determinant on simple graphs (e.g., a small grid without holes) where the number of paths is known through combinatorial formulas.\", \"5. Formulate and write the proof of correctness, showing that the desired count is equivalent to the determinant of a Kasteleyn-like matrix derived from the graph.\", \"6. Audit the final complexity analysis against the implemented algorithm's performance to ensure the theoretical bounds accurately reflect the empirical runtime scaling.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: code output is checked against mathematical derivation and brute-force results; fallback: if verification fails, /math agent re-evaluates the transfer matrix construction).\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on the FKT algorithm, Kasteleyn matrices, and transfer-matrix methods for path counting on lattices.\", \"- [ ] /math: Define the problem formally and derive the transfer matrix recurrence based on the graph's local connectivity.\", \"- [ ] /code: Implement the algorithm in Python, including functions to generate grid graphs with obstacles and compute the path count.\", \"- [ ] /code: Develop a separate brute-force enumeration solver for small graphs to use as a ground truth for verification.\", \"- [ ] /math: Write the formal proof of correctness, connecting the matrix determinant to the combinatorial counting problem.\", \"- [ ] /math: Verify that the implemented code logic correctly reflects the mathematical derivation of the Kasteleyn matrix.\", \"- [ ] /router: Perform final QA review of the proof, code implementation, and verification report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"dynamic-programming\", \"graph-theory\", \"combinatorial-topology\"], \"acceptance_criteria\": [\"* The mathematical proof must be rigorous, self-contained, and correctly apply concepts from graph theory and linear algebra to prove the counting formula.\", \"* The Python code must pass all unit tests, correctly matching the output of a brute-force solver on a predefined set of 10+ small grid graphs.\", \"* The complexity analysis must provide a correct and tight asymptotic bound on the algorithm's runtime as a function of grid dimensions.\", \"* The solution must cite at least two relevant academic papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1539, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for the Alternating Direction Method of Multipliers (ADMM) to enforce group lasso sparsity on the attention weight matrices of a Vision Transformer. The objective is to minimize cross-entropy loss subject to an L1/L2 norm constraint on groups of attention heads. Please provide a full derivation of the augmented Lagrangian, the primal and dual variable updates, and validate the convergence properties with a JAX implementation, ensuring the solution respects the non-convexity of the base problem. The analysis should consider the convergence rate and the impact of the penalty parameter rho on model performance, especially regarding adversarial robustness.\n", "completion": "{\"route_plan\": [\"/math(Derive the augmented Lagrangian and the ADMM update steps for group lasso regularization on a transformer's attention weights. Analyze the convergence conditions for this non-convex problem, considering the Hessian eigenvalue spectrum and the choice of the penalty parameter.)\", \"/code(Implement the ADMM optimization loop in JAX for a Vision Transformer, applying the derived update rules. Validate the implementation against a baseline SGD optimizer, tracking cross-entropy loss, sparsity level, and AUPRC under PGD adversarial attacks., using Python)\", \"/general-search(query=\\\"site:arxiv.org ADMM non-convex optimization deep learning group lasso transformer sparsity convergence\\\", mode=web)\"], \"route_rationale\": \"/math is used to derive the core optimization theory. /code is used for empirical validation of the mathematical derivation and performance analysis. The blue /general-search agent is used to find authoritative literature on arXiv to contextualize the theoretical and empirical results.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the ADMM update rules for the specified constrained optimization problem.\", \"A Python script using JAX implementing the ADMM optimizer for a Vision Transformer, including data loading and evaluation logic.\", \"A summary report in Markdown analyzing the convergence behavior, sparsity patterns, and model robustness, with at least 2 citations to relevant arXiv papers.\", \"A JSON file containing the final training and evaluation metrics, including loss, accuracy, sparsity ratio, and AUPRC under adversarial attack.\", \"Verification output from the code showing that the implemented primal/dual updates match the mathematical derivation's structure.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem, defining the cross-entropy loss objective and the group lasso L1/L2 norm constraint for the transformer attention heads.\", \"2. Construct the augmented Lagrangian for this problem and derive the iterative update equations for the primal variables (network weights, auxiliary sparsity variables) and the dual variable (Lagrange multiplier) based on the ADMM framework.\", \"3. Implement the derived ADMM algorithm in JAX, integrating it into the training loop of a Vision Transformer model.\", \"4. Validate the implementation by numerically checking the Karush-Kuhn-Tucker (KKT) conditions at convergence for a small-scale problem to ensure the optimization is finding a valid local minimum.\", \"5. Conduct experiments to compare the ADMM-trained sparse model against a dense baseline, evaluating performance, sparsity, and adversarial robustness.\", \"6. Audit the results against findings from relevant literature on non-convex ADMM and structured pruning to contextualize the outcomes.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The formal derivation from /math provides the specification for the /code implementation. The empirical results from /code are then compared against the literature found by /general-search. (Verification: /code output must match /math derivation structure; fallback: if implementation diverges, /math must re-check derivation for potential errors). The router performs a final QA check on all artifacts.\", \"todo_list\": [\"- [ ] /math: Formulate the augmented Lagrangian for the group-lasso constrained Vision Transformer problem.\", \"- [ ] /math: Derive the closed-form or proximal solutions for the primal and dual variable updates in the ADMM scheme.\", \"- [ ] /code: Implement the ADMM update logic as a custom optimizer in JAX.\", \"- [ ] /code: Set up the training and evaluation pipeline to measure sparsity and adversarial robustness (AUPRC).\", \"- [ ] /math: Verify that the implemented JAX code for the proximal operators correctly reflects the mathematical derivations.\", \"- [ ] /code: Validate the convergence of the algorithm by monitoring primal and dual residuals.\", \"- [ ] /general-search: Find and summarize two key papers from arXiv on applying ADMM to non-convex neural network optimization.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"deep-learning-theory\", \"structured-sparsity\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ADMM updates must be complete and self-contained.\", \"- The JAX implementation must successfully run and produce a model with a final sparsity ratio greater than 50%.\", \"- The final report must include convergence plots (primal/dual residuals), a comparison of metrics against a dense baseline, and cite at least two relevant arXiv papers.\", \"- The primary metric (AUPRC under attack) for the sparse model must not degrade by more than 15% relative to the dense baseline.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1559, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a high-fidelity Monte Carlo simulation in Python to analyze the performance of the 2D toric code. The simulation must model a Pauli depolarizing noise channel and implement a minimum-weight perfect matching (MWPM) decoder to correct errors. The primary objective is to numerically estimate the error correction threshold by plotting the logical error rate as a function of the physical error rate for various code distances (e.g., L=3, 5, 7) and identifying the crossover point. The implementation should be optimized for performance to handle a large number of simulation shots.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org toric code simulation minimum weight perfect matching decoder depolarizing noise threshold\\\", mode=both, includes cybersecurity)\", \"/math(Formalize the toric code stabilizer measurements, define the mapping from error syndromes to a graph for MWPM decoding, and derive the Bayesian expression for edge weights under depolarizing noise with complexity analysis.)\", \"/code(Implement the toric code simulation in Python using NumPy, including lattice creation, noise application, syndrome extraction, graph construction, and integration with a library like PyMatching for the MWPM algorithm. Ensure code includes convergence checks.)\", \"/math(Analyze the Monte Carlo simulation output. Apply statistical methods to fit the logical error rate curves from the arxiv papers and calculate the error threshold with credible intervals, checking for statistical convergence.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey authoritative literature from arXiv on toric code simulation, establishing a solid theoretical and algorithmic foundation. The first /math agent then translates this theory into a precise mathematical model for the decoder. Next, the /code agent implements this model in an efficient simulation. A final /math agent is required for rigorous statistical analysis of the simulation's output data to derive the primary metric (the error threshold). This sequence ensures the implementation is grounded in both theory and correct statistical practice.\", \"expected_artifacts\": [\"* A final report in Markdown format (`report.md`) detailing the mathematical model of the toric code, the MWPM decoding algorithm, and the statistical analysis of the simulation results.\", \"* A Python script (`simulation.py`) containing the complete, commented implementation of the Monte Carlo simulation.\", \"* A set of plots (`logical_error_rate_plots.png`) showing logical error rate vs. physical error rate for different code distances, clearly indicating the estimated error threshold.\", \"* A JSON file (`metrics.json`) containing the final calculated error threshold value, its confidence interval, and the raw data points.\", \"* A verification log (`verification.log`) documenting the validation of the MWPM decoder on small, known cases and the convergence checks for the Monte Carlo runs.\"], \"thinking_outline\": [\"1. Research and define the toric code model, including its stabilizers, logical operators on a 2D lattice, and the properties of the Pauli depolarizing noise channel.\", \"2. Formulate the error correction procedure as a graph problem: map the error syndromes (violated stabilizers) to vertices in a graph and derive the edge weights based on the probability of error chains connecting them.\", \"3. Implement the full Monte Carlo simulation loop: initialize the code, apply random Pauli errors, measure syndromes, construct the matching graph, and run the minimum-weight perfect matching (MWPM) algorithm to find a correction operator.\", \"4. Validate the implementation by checking if the combined error and correction operator is a stabilizer. Verify logical error by checking if the resulting operator commutes with all stabilizers but anticommutes with a logical operator.\", \"5. Execute sweeps across a range of physical error rates and for multiple code distances (e.g., L=3, 5, 7), ensuring sufficient Monte Carlo shots for statistical convergence.\", \"6. Post-process the collected data to plot logical error rates, identify the threshold where curves for different distances cross, and perform a finite-size scaling analysis to refine the threshold estimate and audit its statistical uncertainty.\"], \"handoff_plan\": \"/general-search -> /math (verification: mathematical formalism must align with canonical definitions from cited papers) -> /code (verification: implementation must pass unit tests for syndrome generation and graph construction based on the math spec; fallback: simplify noise model or consult /math for clarification) -> /math (verification: statistical analysis methods must be appropriate for the data distribution; fallback: rerun simulation with more shots if convergence is poor) -> router QA (final review of all artifacts for consistency and correctness).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on toric code threshold estimation with MWPM decoders.\", \"- [ ] /math: Derive the mathematical mapping from a set of Pauli errors to the syndrome graph, including the formula for edge weights.\", \"- [ ] /code: Implement the toric code lattice, depolarizing noise model, and syndrome extraction logic in Python.\", \"- [ ] /code: Integrate the `PyMatching` library to solve the MWPM problem and apply the correction.\", \"- [ ] /math: Verify that the implemented graph construction in the code correctly reflects the mathematical derivation.\", \"- [ ] /code: Run the full simulation sweep and generate the raw data for plotting.\", \"- [ ] /math: Analyze the raw data to compute the error threshold and its confidence interval, creating the final plots.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"monte-carlo-simulation\"], \"acceptance_criteria\": [\"* The Python simulation code must be executable and reproduce the results presented in the report.\", \"* The final report must correctly derive the MWPM decoding formalism and cite at least two relevant arXiv preprints.\", \"* The estimated error threshold must be in the expected range for the 2D toric code with depolarizing noise (~15.5%), as established in the literature.\", \"* The generated plots must clearly show the logical error rate decreasing with code size for physical error rates below the threshold, and increasing for rates above it.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1564, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nImplement the REINFORCE algorithm with a learned value function as a control variate to reduce gradient variance. Analyze its convergence properties on CartPole-v1 and provide a formal derivation of the policy gradient theorem.\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient theorem and show how a baseline, such as a state-value function, can be introduced as a control variate to analyze its effect on convergence without biasing the gradient estimate.)\", \"/general-search(query=\\\"site:arxiv.org REINFORCE with baseline convergence analysis policy gradient control variate\\\", mode=rag)\", \"/code(Write a Python implementation of the REINFORCE algorithm with a learned value function baseline using PyTorch. Train it on CartPole-v1, ensuring the code tracks total reward per episode for convergence analysis and reports final metrics.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation by deriving the policy gradient theorem. Then, the blue /general-search agent is tasked with finding recent, high-authority literature on arXiv to inform a modern implementation. Finally, /code is used to translate the theory and research into a practical, working implementation and evaluate its performance.\", \"expected_artifacts\": [\"A PDF document containing the formal mathematical derivation of the policy gradient theorem and the unbiased baseline proof.\", \"A Python script (`reinforce_baseline.py`) implementing the agent and training loop.\", \"A PNG image (`learning_curve.png`) plotting the total reward per training episode.\", \"A summary report (`report.md`) explaining the results and citing at least two relevant papers.\", \"A JSON file (`metrics.json`) with the final performance metrics.\"], \"thinking_outline\": [\"1. Formulate the objective function for policy optimization in an episodic reinforcement learning setting, defining it as the expected total reward.\", \"2. Derive the policy gradient theorem using the log-derivative trick, which provides a computable expression for the gradient of the objective function.\", \"3. Introduce an arbitrary baseline function (a control variate) into the gradient estimator and prove that it does not introduce bias, specializing the proof for the state-value function.\", \"4. Implement the REINFORCE algorithm with a neural network-based value function baseline in Python, ensuring proper synchronization of the policy and value networks.\", \"5. Train the agent on the CartPole-v1 environment and validate successful learning by observing a monotonically increasing trend in the smoothed episode reward curve, indicating convergence.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: The final code's performance must match or exceed benchmarks found by /general-search, and the derivation from /math must be correctly reflected in the loss functions. fallback: If performance is poor, /code agent must revisit the implementation of the value function loss or learning rates, referencing the literature from /general-search.)\", \"todo_list\": [\"- [ ] /math: Derive the policy gradient theorem using the log-derivative trick.\", \"- [ ] /math: Prove that subtracting a state-dependent baseline (control variate) from the returns yields an unbiased gradient estimator.\", \"- [ ] /general-search: Find and summarize two key papers on variance reduction techniques for the REINFORCE algorithm.\", \"- [ ] /code: Implement the policy and value function networks using PyTorch.\", \"- [ ] /code: Write the main training loop for the agent in the CartPole-v1 environment.\", \"- [ ] /code: Validate that the average reward over 100 episodes surpasses the environment's solving threshold.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"control-theory\"], \"acceptance_criteria\": [\"- The provided mathematical derivation of the policy gradient theorem is correct and logically sound.\", \"- The implemented agent successfully solves the CartPole-v1 environment, achieving an average reward of at least 475 over the final 100 episodes.\", \"- The final report includes a clearly labeled learning curve plot and correctly cites at least two relevant arXiv papers with their IDs.\", \"- The `metrics.json` file is correctly populated with the specified primary and secondary metrics.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1590, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the robustness of topological features in the latent space of a variational autoencoder. Can you provide a detailed mathematical derivation of the Stability Theorem for persistence diagrams, specifically relating the bottleneck distance to the Gromov-Hausdorff distance between point clouds? Furthermore, I need a Python simulation using the GUDHI library to empirically validate this theorem on a perturbed Swiss roll manifold, demonstrating how small perturbations in the data lead to correspondingly small changes in the persistence diagram. The analysis should include a Bayesian confidence interval on the stability ratio.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org persistent homology stability theorem bottleneck distance gromov-hausdorff\\\", mode=rag, includes cybersecurity)\", \"/math(Derive the stability theorem for persistence diagrams, formally bounding the bottleneck distance by the Gromov-Hausdorff distance between point sets. The proof should consider the convergence properties of Vietoris-Rips filtrations and reference Bayesian estimation concepts for the final analysis.)\", \"/code(Implement a Python simulation using scikit-learn to generate a Swiss roll manifold, apply controlled noise, and use the GUDHI library to construct Vietoris-Rips complexes. Compute the persistence diagrams and the resulting bottleneck distances, ensuring the code can handle eigenvalue analysis for a potential dimensionality reduction step.)\", \"/math(Analyze the simulation's output by plotting the bottleneck distance against the perturbation magnitude. Formulate and calculate a Bayesian confidence interval for the stability ratio to verify that the empirical results align with the derived theoretical bound.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative literature on the stability theorem. This theoretical grounding is passed to /math for a formal derivation. Next, /code implements the derived concepts in a simulation for empirical validation. Finally, /math is used again to perform a rigorous statistical analysis of the simulation results, connecting the empirical evidence back to the initial theory.\", \"expected_artifacts\": [\"A formal mathematical derivation of the persistence diagram stability theorem in a LaTeX document.\", \"A well-documented Python script (`stability_simulation.py`) that generates the manifold, computes persistence diagrams, and validates the theorem.\", \"A set of plots showing the persistence diagrams for perturbed and unperturbed data, and a scatter plot of bottleneck distance vs. perturbation level.\", \"A summary report in Markdown (`report.md`) that interprets the results, compares them against the theory, and includes at least two citations to relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research the formal statement and proof structure of the Stability Theorem for persistence diagrams, focusing on the role of bottleneck and Gromov-Hausdorff distances.\", \"2. Formulate a step-by-step derivation of the theorem, clearly defining all terms like filtrations, homology groups, and interleaving distances.\", \"3. Design a computational experiment to test the theorem: generate a known manifold, apply controlled perturbations, and compute the relevant topological and geometric distances.\", \"4. Implement the experiment in Python, ensuring correct usage of libraries for manifold generation (scikit-learn) and topological computation (GUDHI).\", \"5. Validate the implementation by checking intermediate outputs, such as the Betti numbers of the original manifold, to ensure they match theoretical expectations.\", \"6. Verify that the final empirical results (the ratio of bottleneck to Gromov-Hausdorff distance) are consistent with the derived theoretical upper bound.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results provide the theoretical foundation for the /math derivation. The completed derivation from /math defines the core logic and bounds that the /code agent must implement. The simulation output from /code is then passed to the final /math agent for statistical analysis and comparison with the initial theory. Verification: The /code agent must confirm its implementation of bottleneck distance aligns with the mathematical definition from /math. Fallback: If simulation results contradict the theory, the router will ask /math to re-check the derivation's assumptions and /code to audit the perturbation and distance calculation logic.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on the stability of persistence diagrams.\", \"- [ ] /math: Write the full derivation of the stability theorem, referencing the sourced papers.\", \"- [ ] /code: Implement the Swiss roll manifold generation and perturbation functions.\", \"- [ ] /code: Implement the Vietoris-Rips filtration and persistence diagram computation using GUDHI.\", \"- [ ] /math: Verify that the code's implementation of Gromov-Hausdorff and bottleneck distances is mathematically sound.\", \"- [ ] /code: Run the full simulation, calculate the stability ratio across multiple noise levels, and generate the required plots.\", \"- [ ] /math: Perform the final analysis, calculate the Bayesian confidence interval, and write the summary report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"computational-topology\"], \"acceptance_criteria\": [\"The mathematical derivation correctly proves that the bottleneck distance is bounded by the Gromov-Hausdorff distance.\", \"The Python script runs without error and produces plots demonstrating that as data perturbation increases, the bottleneck distance increases in a controlled manner.\", \"The final report shows that the empirically computed stability ratio is consistently less than or equal to 1, aligning with the theoretical bound.\", \"The report includes at least two citations to peer-reviewed papers or arXiv preprints on topological data analysis.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1648, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for a variance-preserving diffusion process. Then, implement a predictor-corrector sampler in JAX, specifically using the Euler-Maruyama method for the predictor and Langevin MCMC for the corrector. Your implementation must track the FID score and analyze the convergence behavior of the score matching objective with respect to the Lipschitz constant of the learned score network.\n", "completion": "{\"route_plan\": [\"/math(Derive the reverse-time SDE from the forward diffusion process using Fokker-Planck and Bayes' rule, ensuring the resulting score function's dependence on the data distribution gradient is explicit for later convergence analysis.)\", \"/general-search(query=\\\"site:arxiv.org score-based generative models predictor-corrector sampler euler-maruyama langevin dynamics convergence\\\", mode=rag)\", \"/code(Implement the derived predictor-corrector sampler in JAX, tracking FID metrics against a CIFAR-10 baseline. The score network must be a U-Net, and the implementation should leverage CUDA acceleration for training., using Python)\"], \"route_rationale\": \"The route begins with /math to establish the core theoretical foundation by deriving the reverse-time SDE. Next, the blue /general-search agent is tasked with gathering state-of-the-art implementation details and convergence proofs from recent literature. Finally, /code synthesizes the mathematical theory and research into a high-performance JAX implementation for empirical validation via FID metrics.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the reverse-time SDE.\", \"A Python script (`sampler.py`) containing the JAX implementation of the predictor-corrector sampler and the score-matching training loop.\", \"A `report.md` file summarizing the FID score evolution, convergence analysis, and citing at least two relevant arXiv papers.\", \"A `verification_log.txt` detailing the consistency check between the mathematical derivation and the code's numerical behavior.\", \"A JSON file (`metrics.json`) with the final FID score and training loss curves.\"], \"thinking_outline\": [\"1. Formally derive the reverse-time SDE from the forward process definitions, clearly stating all assumptions (e.g., Gaussian noise).\", \"2. Research state-of-the-art predictor-corrector sampling techniques for score-based models, focusing on Euler-Maruyama and Langevin dynamics updates.\", \"3. Implement the score-matching training loop and the predictor-corrector sampler in JAX, ensuring the code is well-structured and vectorized for GPU execution.\", \"4. Validate the numerical stability of the SDE solver implementation by checking for exploding gradients or NaN values during initial test runs.\", \"5. Train the model on a standard dataset (e.g., CIFAR-10) and empirically evaluate the model's performance by tracking the FID score over training epochs.\", \"6. Verify that the final reported FID score is consistent with published benchmarks for similar model architectures and sampling techniques.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /code agent must verify that its implementation of the discretized SDE update rules matches the formal derivation from the /math agent. Fallback: If the derivation is ambiguous or contains typos, the /math agent will be re-invoked to provide a corrected version before the /code agent proceeds with implementation.\", \"todo_list\": [\"- [ ] /math: Derive the reverse-time SDE for a variance-preserving diffusion process.\", \"- [ ] /general-search: Find recent papers on predictor-corrector samplers (Euler-Maruyama, Langevin) for score-based models.\", \"- [ ] /code: Implement the U-Net score network and the score-matching training loop in JAX.\", \"- [ ] /code: Implement the predictor-corrector sampler based on the mathematical derivation.\", \"- [ ] /math: Verify the numerical update rules in the JAX code align perfectly with the derived SDE discretization.\", \"- [ ] /code: Train the model on CIFAR-10, generate samples, and compute the final FID score.\", \"- [ ] /router: Perform final QA, checking for consistency across the derivation, code, and final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"score-based-modeling\", \"stochastic-differential-equations\", \"generative-ai\"], \"acceptance_criteria\": [\"The mathematical derivation of the reverse-time SDE must be complete and formally correct.\", \"The JAX implementation must successfully train without numerical instability (NaNs) and generate coherent image samples from random noise.\", \"The final report must include a plot of FID score vs. training steps and cite at least two relevant arXiv papers with IDs.\", \"The reported FID score on CIFAR-10 must be below a reasonable threshold for this class of model (e.g., < 10.0).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1655, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the continuous-time stochastic differential equation representing Bayesian posterior sampling via Langevin dynamics for a neural network. Discretize this using the Euler-Maruyama method and implement it in JAX. Validate the sampler's convergence and compare the quality of its uncertainty estimates against a mean-field variational inference baseline on a regression task, analyzing both posterior predictive distributions and expected calibration error.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org stochastic gradient langevin dynamics bayesian deep learning sde posterior sampling\\\", mode=web)\", \"/math(Derive the Langevin SDE for a Bayesian neural network posterior, p(w|D), and its Euler-Maruyama discretization. Specify the drift and diffusion terms and analyze the conditions for convergence to the target distribution.)\", \"/code(Implement the SDE sampler in JAX for a BNN on a regression task. Also implement a mean-field VI baseline. Track Expected Calibration Error and predictive log-likelihood, ensuring the Bayesian model uncertainty is quantifiable., using Python)\", \"/math(Analyze the simulation output. Compute Gelman-Rubin diagnostics to assess SDE sampler convergence and use bootstrap methods to generate a confidence interval for the ECE metric comparison between the two methods.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather seminal papers on SDE-based Bayesian inference. /math is then used for the rigorous theoretical derivation based on the literature. /code operationalizes this theory through a JAX implementation for empirical validation. A final /math step is required for the statistical analysis of the simulation results, assessing convergence and comparing model performance rigorously.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full derivation of the Langevin SDE and its discretization.\", \"- A Python script (`sde_bnn.py`) implementing the SDE sampler and the VI baseline, with dependencies.\", \"- A results report in Markdown (`results.md`) containing plots of posterior predictive distributions, convergence diagnostics (R-hat plots), and a table comparing performance metrics.\", \"- A bibliography file (`references.bib`) with citations for at least 3 relevant arXiv papers.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of representing Bayesian posterior distributions as the stationary distribution of a stochastic differential equation, focusing on Langevin dynamics.\", \"2. Formally derive the specific SDE whose stationary distribution corresponds to the posterior of a neural network's weights, and then derive its discrete-time Euler-Maruyama approximation for computational implementation.\", \"3. Implement both the SDE-based sampler and a standard mean-field variational inference (MFVI) baseline in JAX to create a controlled comparison.\", \"4. Validate the SDE sampler's implementation by running multiple chains and computing the Gelman-Rubin convergence diagnostic.\", \"5. Empirically compare the quality of uncertainty quantification between the SDE and MFVI methods using the Expected Calibration Error (ECE) metric on a held-out test set.\", \"6. Verify that the final report correctly interprets the results and that the derived theory aligns perfectly with the code's logic.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides foundational papers to the /math agent, which verifies their applicability before starting the derivation. The /math agent hands off the final discretized equations to the /code agent, who implements them with assertions to check for numerical stability. The /code agent outputs raw metric logs and plot data, which the final /math agent analyzes. A final QA check by the router ensures all artifacts are consistent. Fallback: If SDE convergence fails, the /code agent will report this and the final /math agent will focus on diagnosing the cause (e.g., step size, gradient noise).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 3+ key papers on Stochastic Gradient Langevin Dynamics (SGLD) for BNNs.\", \"- [ ] /math: Produce a self-contained derivation of the SDE and its discrete-time approximation.\", \"- [ ] /code: Implement the SDE sampler for a small BNN in JAX.\", \"- [ ] /code: Implement a comparable Mean-Field Variational Inference baseline.\", \"- [ ] /math: Verify that the core update rule in the JAX code is a correct implementation of the derived Euler-Maruyama step.\", \"- [ ] /code: Run experiments, generate posterior predictive plots, and save ECE and R-hat metric data to a JSON file.\", \"- [ ] /math: Analyze the generated metrics, perform convergence analysis, and write the final comparison.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-differential-equations\", \"langevin-dynamics\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Langevin SDE and its discretization is complete and correct.\", \"- The JAX code is runnable, well-documented, and reproduces the key metrics reported in the final analysis.\", \"- The final report clearly visualizes the difference in posterior uncertainty between the SDE and VI methods and provides a valid Gelman-Rubin statistic (R-hat < 1.1) confirming sampler convergence.\", \"- At least 3 relevant papers from arXiv are cited correctly.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1718, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Lagrangian for training a neural network subject to a spectral norm constraint on its weight matrices, which approximates a Lipschitz constraint for adversarial robustness. Formulate the Karush-Kuhn-Tucker (KKT) conditions for this constrained optimization problem and propose an Alternating Direction Method of Multipliers (ADMM) algorithm to solve it. Please validate the convergence of the ADMM implementation on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/math(Derive the Lagrangian and Karush-Kuhn-Tucker (KKT) conditions for a neural network loss function constrained by the spectral norm of its weight matrices. Analyze the resulting dual problem for ADMM formulation, considering the eigenvalue decomposition.)\", \"/general-search(query=\\\"site:arxiv.org ADMM spectral norm constraint deep learning convergence\\\", mode=web)\", \"/code(Implement the ADMM algorithm in PyTorch to solve the constrained optimization problem. The implementation must track primal/dual residuals to monitor convergence and validate the spectral norm constraint using power iteration after each epoch., using Python)\"], \"route_rationale\": \"The route starts with /math to establish the formal mathematical framework (Lagrangian, KKT) for the constrained optimization problem. The blue /general-search agent then gathers state-of-the-art literature on ADMM convergence for this specific application. Finally, /code implements and empirically validates the derived algorithm, ensuring the theoretical constraints are met in practice.\", \"expected_artifacts\": [\"- A PDF document containing the full mathematical derivation of the Lagrangian, KKT conditions, and ADMM update rules.\", \"- A Python script (`admm_optimizer.py`) implementing the ADMM algorithm for a simple neural network.\", \"- A summary report analyzing the convergence plots (primal/dual residuals) and citing at least two relevant papers.\", \"- A verification log file (`verification.log`) showing the final spectral norms of the weight matrices are below the constraint threshold.\"], \"thinking_outline\": [\"1. Formulate the primal optimization problem: minimizing the loss function subject to the spectral norm constraint on each weight matrix.\", \"2. Derive the augmented Lagrangian for this problem and subsequently derive the update rules for the primal variables (weights), dual variables, and the ADMM steps.\", \"3. Investigate the Karush-Kuhn-Tucker (KKT) conditions to understand the properties of the optimal solution.\", \"4. Implement the ADMM algorithm in Python, including a function to project matrices onto the spectral norm ball.\", \"5. Verify that the implemented weight updates correctly follow the derived mathematical formulas and that the primal and dual residuals converge to zero.\", \"6. Validate the final model by numerically computing the spectral norm of the learned weight matrices to ensure the constraints are satisfied within a small tolerance.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: code implementation of ADMM updates must match the mathematical derivation from /math; fallback: if convergence issues arise in /code, revisit the ADMM penalty parameter selection based on literature from /general-search).\", \"todo_list\": [\"- [ ] /math: Derive the augmented Lagrangian and the corresponding ADMM update equations for weights and dual variables.\", \"- [ ] /math: Analyze the KKT conditions for the primal problem.\", \"- [ ] /general-search: Find and summarize at least two key papers from arXiv on using ADMM for spectral norm regularization in deep learning.\", \"- [ ] /code: Implement the ADMM optimizer in PyTorch, including primal and dual variable updates.\", \"- [ ] /code: Add monitoring for primal/dual residuals and a callback to check the spectral norm constraint satisfaction during training.\", \"- [ ] /math: Verify that the update rules implemented in the Python code from /code are mathematically equivalent to the derived equations.\", \"- [ ] /code: Validate the convergence of the algorithm on a synthetic dataset and plot the residuals.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"lagrangian-methods\", \"adversarial-robustness\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Lagrangian and ADMM updates must be complete and correct.\", \"- The Python implementation must converge on the provided synthetic task, with primal and dual residuals approaching zero.\", \"- The final trained model's weight matrices must have spectral norms less than or equal to the specified constraint, verified numerically in the logs.\", \"- The report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1720, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the gradient approximation for the bilevel optimization problem in differentiable neural architecture search (DARTS) using a finite-difference Hessian-vector product. Then, analyze its convergence properties by framing the search space as a simplicial complex and discuss how topological data analysis could regularize the search process to avoid degenerate architectures, referencing tensor calculus for the gradient formulation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'differentiable architecture search' bilevel optimization 'hessian approximation' 'topological data analysis' automl\\\", mode=web)\", \"/math(Derive the second-order gradient approximation for DARTS using tensor notation. Formulate the topological regularization term based on persistent homology of the architecture's computation graph, analyzing potential bayesian interpretations and convergence criteria.)\", \"/code(Implement the topologically-regularized gradient computation in JAX. Profile the complexity of the Betti number calculation on a toy search space and ensure the implementation is CUDA-compatible for future scaling., using Python)\", \"/math(Formally prove that the topological regularizer penalizes architectures with high cyclomatic complexity, linking the penalty to the graph's eigenvalue spectrum. Verify the proof aligns with the JAX implementation's observed behavior on degenerate graphs.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on DARTS and topological data analysis in ML. The first /math agent then handles the core theoretical derivation. /code is used to create a tangible, verifiable implementation of the novel gradient formulation. A final /math agent provides a formal proof and verification step, connecting the theoretical properties to the code's behavior, ensuring a robust and validated result.\", \"expected_artifacts\": [\"A PDF document containing the complete mathematical derivation of the topologically-regularized gradient, using LaTeX.\", \"A Python script (JAX) implementing the gradient computation for a toy search space.\", \"A final report summarizing the methodology, results, and citing at least 3 relevant research papers with arXiv IDs.\", \"A formal proof verifying that the regularizer penalizes degenerate architectures.\", \"A metrics.json file reporting the GradientApproximationError and TopologicalComplexity for a test case.\"], \"thinking_outline\": [\"1. Research the foundational bilevel optimization problem in DARTS and its standard second-order gradient approximation.\", \"2. Formulate the architecture search space as a simplicial complex and derive a suitable topological regularization term using persistent homology (e.g., penalizing short-lived topological features or high Betti numbers).\", \"3. Combine the standard DARTS update with the topological regularizer, deriving the complete gradient update rule using clear tensor calculus notation.\", \"4. Implement this novel gradient calculation in JAX for a small, well-defined search space and validate the numerical outputs against a brute-force finite-difference calculation.\", \"5. Formally analyze the regularizer's effect, and verify that it assigns a higher penalty to architectures known to be degenerate (e.g., those dominated by skip connections), thus improving the search landscape.\", \"6. Synthesize all derivations, code, and experimental validation into a comprehensive report.\"], \"handoff_plan\": \"/general-search (literature) -> /math (derivation) -> /code (implementation) -> /math (proof & verification). The /math agent's derivation must be used as the formal specification for the /code agent. The final /math agent must receive the code from the /code agent to formally verify its logic. Fallback: If the derivation is intractable, the /math agent will notify the router, which may pivot to a simpler first-order approximation with a request for re-routing.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on DARTS (arXiv:1806.09055) and applications of TDA in deep learning.\", \"- [ ] /math: Produce a self-contained PDF deriving the regularized gradient update rule.\", \"- [ ] /code: Implement the gradient computation in JAX, including functions for building the cell's computation graph and calculating its Betti numbers.\", \"- [ ] /code: [Verification] Add unit tests comparing the Hessian-vector product implementation against JAX's automatic differentiation for a simple quadratic function.\", \"- [ ] /math: [Verification] Formally prove that the regularization term is zero for a simple feed-forward chain and non-zero for a graph with parallel skip-connections.\", \"- [ ] /math: Generate plots illustrating the effect of the regularizer on a simple 2D loss landscape.\", \"- [ ] Router: Perform final QA review, checking for consistency between the derivation, code, and final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-architecture-search\", \"differentiable-automl\", \"topological-data-analysis\"], \"acceptance_criteria\": [\"The mathematical derivation of the regularized gradient must be complete, free of logical errors, and use consistent notation.\", \"The JAX implementation must pass all provided unit tests and correctly compute the gradient for a predefined toy architecture.\", \"The final report must clearly explain the method, present the verification results, and cite at least 3 relevant arXiv papers.\", \"The GradientApproximationError metric must be below a threshold of 1e-4 on the provided test case.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1721, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the convergence guarantees for Stochastic Gradient Langevin Dynamics (SGLD) and contrast them with the Metropolis-Adjusted Langevin Algorithm (MALA) for a non-convex posterior distribution, as encountered in Bayesian neural networks. Please implement both samplers to estimate the posterior over weights for a Bayesian logistic regression model on a synthetic dataset, and empirically validate the theoretical convergence rates by analyzing trace plots and autocorrelation functions.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Stochastic Gradient Langevin Dynamics convergence non-convex posterior vs MALA\\\", mode=both)\", \"/math(Derive the continuous-time Langevin diffusion process and its Euler-Maruyama discretization for both SGLD and MALA, focusing on conditions for convergence in Wasserstein distance for a Bayesian model.)\", \"/code(Implement SGLD and MALA in Python using JAX to sample from the posterior of a Bayesian logistic regression model, incorporating CUDA and tracking effective sample size and autocorrelation metrics.)\", \"/math(Analyze the empirical trace plots and autocorrelation functions from the code output to validate the derived theoretical convergence rates, performing a statistical comparison of the two Bayesian samplers.)\"], \"route_rationale\": \"The blue general-search agent first grounds the task in authoritative literature. The first /math call is for the theoretical derivation based on these sources. The /code agent then provides an empirical testbed for the theory. The final /math call synthesizes the theoretical and empirical results, validating the findings.\", \"expected_artifacts\": [\"A LaTeX document providing the mathematical derivations of convergence for SGLD and MALA.\", \"A self-contained Python script implementing the Bayesian model, both samplers, and the analysis.\", \"A directory of generated plots including trace plots and autocorrelation functions for both samplers.\", \"A JSON file containing the computed metrics (ESS, Autocorrelation Time, Wall-clock time).\", \"A bibliography section in the report citing at least two relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Research the theoretical underpinnings of SGLD and MALA, specifically their convergence properties for non-convex posteriors using authoritative sources.\", \"2. Formally derive the discrete-time update rules for both algorithms from their continuous-time Langevin diffusion counterparts, stating all assumptions.\", \"3. Implement the samplers and a Bayesian logistic regression model, ensuring the data generation and model setup are reproducible.\", \"4. Execute the simulation to collect posterior samples and compute diagnostic metrics like effective sample size (ESS) and autocorrelation time.\", \"5. Validate that the implementation correctly reflects the derived mathematical equations by performing code review and unit testing critical components.\", \"6. Audit the empirical results to verify if they support the theoretical convergence claims derived earlier and synthesize the findings in a final report.\"], \"handoff_plan\": \"/general-search -> /math (verification: derivations must cite and use concepts from sourced papers) -> /code (verification: implementation must exactly match the derived equations from the /math agent) -> /math (verification: analysis must use the specific metrics generated by the /code agent) -> router QA (verification: final report must synthesize all artifacts consistently; fallback: re-run /code with corrected parameters if empirical results starkly contradict theory).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on SGLD and MALA convergence theory for non-convex Bayesian posteriors.\", \"- [ ] /math: Derive the update equations and state the key theorems and assumptions for convergence from the sourced literature.\", \"- [ ] /code: Implement the Bayesian logistic regression model and both the SGLD and MALA samplers in Python using JAX.\", \"- [ ] /code: Instrument the simulation to log trace plots, autocorrelation, and effective sample size (ESS) metrics to a JSON file.\", \"- [ ] /math: Verify that the implemented update rules in the code exactly match the derived mathematical formulas.\", \"- [ ] /code: Verify numerical stability and sampler convergence on a small-scale problem before running the full experiment.\", \"- [ ] /math: Analyze the generated plots and metrics to compare the empirical performance against the theoretical guarantees.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-processes\", \"mcmc\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly identifies the assumptions for convergence (e.g., Lipschitz smoothness of the gradient) for both samplers.\", \"- The Python implementation is runnable and produces trace plots and autocorrelation plots that clearly show the samplers exploring the posterior distribution.\", \"- The final analysis correctly links the empirical ESS and autocorrelation results to the theoretical convergence rates discussed in the derivation.\", \"- The report includes citations for at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1727, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a physics-informed neural network (PINN) to solve the 2D incompressible Navier-Stokes equations for flow past a cylinder. Please derive the composite loss function, incorporating the PDE residuals and the divergence-free constraint using an Euler-Lagrange formulation. Then, implement this in JAX, verifying the convergence properties and comparing the predicted velocity and pressure fields against established benchmark data.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org physics-informed neural networks Navier-Stokes incompressibility constraint Euler-Lagrange\\\", mode=web)\", \"/math(Derive the variational form and composite loss for the Navier-Stokes PINN, explicitly showing the Euler-Lagrange equations that justify the penalty term for the divergence-free constraint and analyzing the theoretical convergence rate.)\", \"/code(Implement the PINN in JAX, using automatic differentiation for the PDE residuals. Train the model on the flow-past-a-cylinder problem, tracking the L2 residual and divergence norm metrics, ensuring numerical stability via adaptive learning rates., using Python)\", \"/math(Verify the numerical results from the JAX code by auditing the convergence plots against the theoretical error bounds derived earlier and checking the pointwise divergence of the predicted velocity field to ensure the incompressibility constraint is met.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art formulations for PINNs in fluid dynamics. This informs the /math agent's formal derivation of the loss function. The /code agent then translates this mathematical model into a working implementation. A final /math call is crucial for verification, ensuring the computational results are consistent with the theoretical framework.\", \"expected_artifacts\": [\"- A LaTeX document with the full derivation of the Navier-Stokes PINN loss function.\", \"- A Python script (`main.py`) implementing the JAX model and training loop.\", \"- A final report (`report.pdf`) containing visualizations of the velocity/pressure fields, convergence plots, and a discussion comparing results to benchmarks.\", \"- A `metrics.json` file containing the final L2 PDE residual and divergence norm.\", \"- A `citations.bib` file with at least two relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Research current state-of-the-art PINN architectures and loss formulations for solving the incompressible Navier-Stokes equations, paying special attention to how the divergence-free constraint is handled.\", \"2. Formulate the problem mathematically by deriving the strong and weak forms of the PDE, leading to a composite loss function that includes terms for the momentum equations, boundary conditions, and the incompressibility constraint.\", \"3. Implement the neural network architecture and the custom loss function in Python using JAX for efficient automatic differentiation and JIT compilation.\", \"4. Train the model using a suitable optimizer and sampling strategy for collocation points within the domain and on its boundaries.\", \"5. Validate the trained model by computing the L2 norm of the PDE residual and the divergence of the velocity field across the domain. Compare the predicted flow fields with known results from literature or CFD simulations.\", \"6. Audit the empirical convergence rate observed during training against the theoretical guarantees or bounds discussed in the researched literature to check for consistency.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (verification) -> router QA. The /math agent's derivation of the loss function provides the exact specification for the /code agent. The /code agent's output (metrics, plots) is then handed back to the /math agent to verify consistency with the initial theoretical model. Fallback: If the code's convergence metrics diverge or fail to meet a threshold, the task reverts to the /math agent to reconsider the loss formulation or penalty weights.\", \"todo_list\": [\"- [ ] /general-search: Find >=2 recent arXiv papers on PINNs for Navier-Stokes.\", \"- [ ] /math: Derive the complete loss function, including PDE residuals and the incompressibility constraint penalty.\", \"- [ ] /code: Implement the PINN architecture and training loop in JAX.\", \"- [ ] /code: Generate plots for velocity/pressure fields and convergence curves.\", \"- [ ] /math: Verify that the implemented loss in the code correctly matches the mathematical derivation.\", \"- [ ] /math: Validate the final divergence norm is below the specified tolerance (e.g., 1e-4).\", \"- [ ] /router: Perform a final quality assurance check on all artifacts and metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"partial-differential-equations\", \"computational-fluid-dynamics\"], \"acceptance_criteria\": [\"- The mathematical derivation of the loss function must be self-contained and correct.\", \"- The JAX implementation must run without errors and reproduce the reported metrics within a 5% tolerance.\", \"- The final L2 PDE residual must be less than 1e-3, and the L2 norm of the velocity divergence must be less than 1e-4.\", \"- The report must include visualizations that are qualitatively consistent with the canonical 'flow past a cylinder' benchmark problem.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1736, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the rate-distortion function for a generative language model under a differential privacy constraint, specifically modeling the privacy leakage as a distortion measure. Implement a variational autoencoder (VAE) to empirically estimate this bound, using mutual information as a regularizer, and analyze the resulting text samples for semantic coherence versus memorization of sensitive training data. The analysis should consider the impact of adversarial watermarking on the information bottleneck.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org rate-distortion theory differential privacy generative models VAE adversarial watermarking\\\", mode=rag)\", \"/math(Derive the rate-distortion-privacy function R(D) for a Bernoulli source under a differential privacy constraint, treating privacy leakage as a distortion measure and using Bayesian inference for the posterior.)\", \"/code(Implement a VAE in PyTorch with a mutual information estimator for the information bottleneck, tracking AUPRC for a membership inference attack and applying adversarial watermarking. Use CUDA for acceleration., using Python)\", \"/math(Analyze the empirical results from the VAE, verifying the convergence of the evidence lower bound (ELBO) and comparing the estimated R(D) curve against the theoretical derivation. Assess robustness against adversarial attacks.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on this interdisciplinary topic. Next, /math is used to establish the theoretical foundation by deriving the core privacy-utility function. This is followed by /code to implement a model that empirically tests this theory. The final /math step is crucial for analysis and for validating that the empirical results from the code align with the initial theoretical derivation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the rate-distortion-privacy function in a LaTeX document.\", \"Annotated Python source code for the VAE, training loop, and evaluation metrics.\", \"A summary report in Markdown including plots of the empirical R(D) curve, privacy leakage (AUPRC), and text quality scores.\", \"A bibliography with at least 2 cited arXiv papers with their IDs.\", \"A verification log confirming the alignment between the mathematical derivation and the code's objective function.\"], \"thinking_outline\": [\"1. Research foundational papers connecting rate-distortion theory, differential privacy, and variational autoencoders for generative models.\", \"2. Formulate the optimization problem by defining a distortion measure based on differential privacy leakage (e.g., from a membership inference adversary) and formally derive the corresponding rate-distortion function R(D).\", \"3. Implement a VAE architecture that approximates this R(D) trade-off, using a tractable mutual information estimator as a regularizer to control the information bottleneck.\", \"4. Design and execute experiments to train the model across a range of rate-distortion trade-offs, generating the empirical R(D) curve and evaluating text quality versus privacy leakage.\", \"5. Verify that the implemented loss function correctly reflects the derived mathematical objective and validate that the model training converges by monitoring the ELBO.\", \"6. Audit the generated text samples for evidence of training data memorization and assess the model's robustness to privacy attacks.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (analysis) -> router QA. The initial derivation from /math is the specification for the /code implementation. The empirical results from /code are then passed to the second /math agent for theoretical validation. Verification: The second /math agent must confirm the code's loss function matches the first agent's derivation. Fallback: If the derivation is intractable, the /math agent will request a more focused query from /general-search for alternative formulations. If the code fails to converge, the /code agent will flag the derivation for review by the /math agent.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 key papers from arXiv on information-theoretic privacy for generative models.\", \"- [ ] /math: Produce a self-contained derivation of the rate-distortion-privacy function R(D).\", \"- [ ] /code: Implement the VAE model, mutual information estimator, and membership inference attack for AUPRC evaluation.\", \"- [ ] /math: Verify that the loss function implemented in the provided code is a valid objective for the derived R(D) problem.\", \"- [ ] /code: Execute the training script to generate the empirical R(D) curve, saving model checkpoints and metric logs.\", \"- [ ] /math: Analyze the generated plots and metrics, comparing the empirical results against the theoretical bounds from the derivation.\", \"- [ ] /router: Perform final QA, checking for consistency across the derivation, code, and final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"rate-distortion-theory\"], \"acceptance_criteria\": [\"The mathematical derivation of the R(D) function must be self-consistent and clearly define the distortion measure in terms of privacy leakage.\", \"The implemented VAE must demonstrate stable training convergence (monitored via ELBO) and produce a non-trivial empirical R(D) curve showing a trade-off between privacy and utility.\", \"The final report must include the R(D) plot, AUPRC scores over varying levels of compression, and correctly cite at least 2 relevant arXiv papers with IDs.\", \"The verification log must explicitly confirm that the code's loss function mathematically corresponds to the derived objective.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"Generate a 2D plot of a text quality metric (e.g., BLEU or semantic similarity) on the y-axis versus privacy leakage (AUPRC of a membership inference attack) on the x-axis. A superior result is a curve that is higher and to the left.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"Plot the Evidence Lower Bound (ELBO) value per training epoch. The curve must show convergence to a stable value, indicating successful model training.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_computation", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1746, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the posterior predictive distribution for a Gaussian Process with a Mat\u00e9rn kernel, and then implement a Bayesian optimization loop using the Expected Improvement acquisition function. Focus on the mathematical formalism of the GP updates and provide a Python implementation that validates this theory for a simple 1D function. Finally, research and discuss the computational complexity implications and potential mitigation strategies, like state-space representations, drawing from recent literature on scalable Bayesian methods.\n", "completion": "{\"route_plan\": [\"/math(Derive the posterior predictive mean and covariance for a Gaussian Process with a Mat\u00e9rn kernel. Also, formulate the Expected Improvement acquisition function, ensuring the Bayesian update rules for hyperparameters are clearly articulated to track convergence.)\", \"/general-search(query=\\\"site:jmlr.org OR site:arxiv.org gaussian process state-space representation computational complexity bayesian optimization\\\", mode=rag)\", \"/code(Implement a Bayesian optimization loop in Python using JAX and NumPy. Validate the derived Mat\u00e9rn GP posterior updates and EI acquisition function on a benchmark like the Forrester function, measuring convergence rate and the model's complexity.)\"], \"route_rationale\": \"The plan starts with /math to establish the core theoretical foundation. The blue general-search agent is then tasked with finding state-of-the-art literature on scalability to inform the analysis. Finally, /code is used to create a tangible implementation that validates the mathematical derivations and provides empirical performance data.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the GP posterior predictive distribution and the Expected Improvement acquisition function.\", \"A Python script implementing the Bayesian optimization loop, including visualizations of the surrogate model and acquisition function over iterations.\", \"A summary report comparing the computational complexity of standard GPs versus state-space representations, citing at least two relevant research papers.\", \"A metrics file (`metrics.json`) reporting the convergence rate and final discovered minimum of the benchmark function.\"], \"thinking_outline\": [\"1. Formulate the Gaussian Process model, specifying the prior (mean and Mat\u00e9rn covariance function).\", \"2. Derive the analytical expressions for the posterior predictive mean and variance at new points, conditioned on observed data.\", \"3. Derive the formula for the Expected Improvement (EI) acquisition function based on the posterior distribution.\", \"4. Validate the mathematical derivations by implementing the GP regression and Bayesian optimization loop in Python and testing it on a known 1D function.\", \"5. Research state-space representations of GPs as a method to mitigate the cubic computational complexity of standard GP regression.\", \"6. Verify that the implementation's convergence behavior aligns with theoretical expectations and that the research findings on complexity are accurately reported.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivations from /math provide the exact formulas required by /code. The literature from /general-search provides context for the complexity analysis in the final report. The /code agent's implementation serves as a validation of the /math agent's work. Verification: The router will check that the numerical output from the code for a test case matches the analytical formula from the math derivation. Fallback: If verification fails, the task is routed back to /math to check for errors in the derivation.\", \"todo_list\": [\"- [ ] /math: Derive the posterior predictive equations for a Gaussian Process with a Mat\u00e9rn kernel.\", \"- [ ] /math: Formulate the Expected Improvement (EI) acquisition function.\", \"- [ ] /general-search: Find authoritative papers on state-space representations for scalable GPs and their computational complexity.\", \"- [ ] /code: Implement the Bayesian optimization loop using the derived GP update rules and EI.\", \"- [ ] /code: Validate the implementation on a 1D benchmark function (e.g., Forrester) and generate convergence plots.\", \"- [ ] /math: Verify that the Python implementation's numerical results for the posterior mean and variance match the analytical derivations for a small, fixed dataset.\", \"- [ ] /code: Audit the complexity of the implemented matrix inversion and compare it to the theoretical O(N^3) complexity.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-processes\", \"bayesian-optimization\", \"gaussian-processes\"], \"acceptance_criteria\": [\"The mathematical derivation for the GP posterior predictive distribution must be complete and correct.\", \"The Python implementation must successfully find the global minimum of the Forrester function within a budget of 20 iterations, with the final reported value being within 1e-4 of the true minimum.\", \"The final report must correctly identify the O(N^3) complexity of standard GPs and summarize the O(N) complexity of state-space methods, citing at least two relevant papers with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Cumulative difference between the function value at the queried point and the true minimum value over iterations. Lower is better.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Average time in seconds to perform one full Bayesian optimization step (update model, find max of acquisition function). Measures computational cost.\"}}", "categories": ["math_first", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1748, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the error threshold for the toric code under depolarizing noise using a minimum-weight perfect matching decoder. Then, implement a simulation to verify this threshold and compare its performance against a simple convolutional neural network decoder, analyzing the logical error rate convergence and citing relevant arXiv literature on ML-based quantum decoders.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org toric code threshold 'minimum weight perfect matching' decoder 'neural network' convergence\\\", mode=both)\", \"/math(Formulate the stabilizer group for the toric code and derive the reduction of its decoding problem to a minimum-weight perfect matching on a syndrome graph, analyzing its computational complexity and Bayesian inference properties.)\", \"/code(Implement a CUDA-accelerated Python simulation of the toric code with depolarizing noise, building both an MWPM decoder using Blossom V and a baseline federated learning-style CNN decoder to compare logical error rates.)\", \"/math(Analyze the simulation data using bootstrap methods to establish a confidence interval for the error threshold, and verify the convergence of the logical error rate against the derived theoretical bounds.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the work in authoritative arXiv literature on toric code decoders. This research informs the /math agent's formal derivation of the MWPM decoding algorithm. The /code agent then implements this theory in a simulation to generate empirical data. A final /math step is crucial for statistical analysis and verification of the simulation results against the initial theoretical model.\", \"expected_artifacts\": [\"- A LaTeX-formatted report detailing the mathematical derivation of the MWPM decoder for the toric code.\", \"- A Python script implementing the toric code simulation, noise model, and both the MWPM and CNN decoders.\", \"- A plot comparing the logical error rate versus physical error rate for both decoders, clearly indicating the code threshold.\", \"- A bibliography with at least two cited arXiv preprints on quantum error correction decoders.\", \"- A verification summary confirming that the simulated MWPM threshold aligns with known literature values.\"], \"thinking_outline\": [\"1. Research the toric code's stabilizer formalism, the principles of minimum-weight perfect matching (MWPM) decoding, and recent applications of neural networks as quantum decoders.\", \"2. Formally derive the mapping from the toric code's error syndrome on a 2D lattice to an abstract graph problem suitable for an MWPM solver.\", \"3. Implement the complete simulation pipeline: toric code state preparation, application of depolarizing noise, syndrome extraction, and the two decoding algorithms (MWPM and CNN).\", \"4. Execute simulations across a sweep of physical error probabilities to gather statistics on the logical error rate for various code distances.\", \"5. Validate the simulation's correctness by comparing the empirical threshold obtained from the MWPM decoder against the well-established theoretical value (~10.3%) from the literature.\", \"6. Verify that the final analysis report accurately presents the mathematical derivations, simulation results, and a cogent comparison of the two decoder performances.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The literature from /general-search provides the foundation for the /math derivation. This formal model is the specification for the /code implementation. The raw data from /code is then handed back to /math for statistical analysis and verification. (verification: cross-check simulated threshold against literature; fallback: if simulation diverges significantly, re-audit the /math derivation and /code implementation for logical errors).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on toric code thresholds and ML-based decoders, focusing on arXiv.\", \"- [ ] /math: Derive the mapping from stabilizer measurements to the MWPM graph representation.\", \"- [ ] /code: Implement the toric code lattice, depolarizing noise model, and syndrome extraction logic.\", \"- [ ] /code: Implement both the MWPM (using a library like PyMatching) and a baseline CNN decoder.\", \"- [ ] /math: Verify the theoretical derivation aligns with the logic in the primary literature.\", \"- [ ] /code: Add assertions to validate the syndrome graph construction for simple, known error patterns.\", \"- [ ] /math: Plot logical vs. physical error rate curves and compute the threshold from the data.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"fault-tolerance\"], \"acceptance_criteria\": [\"- The mathematical derivation of the MWPM decoding algorithm for the toric code must be rigorous and correct.\", \"- The Python simulation must correctly implement the toric code, depolarizing noise model, and both decoders, and be runnable.\", \"- The plotted threshold for the MWPM decoder must be within 5% of the established literature value (approx. 10.3%).\", \"- The final report must cogently compare the performance of the two decoders and cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as the ratio of decoding failures to the total number of Monte Carlo trials for a given physical error rate `p` and code distance `d`.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Compute as the absolute percentage difference between the simulated MWPM threshold and the theoretical literature value of ~10.3%.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1800, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive and implement a differentiable neural architecture search (NAS) pipeline for a small convolutional cell. Start by formulating the search space as a categorical distribution over operations, then apply the Gumbel-Softmax reparameterization to create a continuous relaxation. Formulate the bi-level optimization problem where architectural parameters and network weights are updated iteratively. Implement this in PyTorch, run the search on CIFAR-10, and validate the convergence of the architectural parameters. Finally, extract and train the discovered discrete architecture from scratch to verify its performance.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org differentiable architecture search DARTS gumbel-softmax reparameterization bi-level optimization automl\\\", mode=both)\", \"/math(Derive the continuous relaxation of the categorical choice over operations using the Gumbel-Softmax trick. Formulate the bi-level AutoML optimization problem with architectural parameters \u03b1 and network weights w, ensuring the gradient expressions for \u03b1 account for the inner loop's convergence.)\", \"/code(Implement the differentiable NAS search loop in PyTorch with CUDA support. Build a small search space for a convolutional cell and run the search on CIFAR-10, tracking validation AUPRC and architectural parameter entropy as key metrics., using Python)\", \"/math(Verify the correctness of the implemented gradient approximation for the architectural parameters by comparing the automatic differentiation result with a manually derived finite difference approximation. Analyze the stability conditions for the bi-level optimization loop, referencing convergence theorems.)\", \"/code(Extract the final discrete architecture by applying the argmax operator to the learned architectural parameters. Train this architecture from scratch on CIFAR-10 to validate its performance. Plot the search-phase and training-phase learning curves, and audit the final model's complexity., using Python)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on DARTS and Gumbel-Softmax. Then, /math is used to formalize the theory. This theory is handed to /code for implementation. A second /math call is crucial for verifying the complex gradient calculations within the implementation. Finally, a second /code call validates the end-to-end result by training the discovered architecture, providing a complete and verified solution.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Gumbel-Softmax relaxation for NAS and the bi-level optimization objective.\", \"A documented Python script implementing the differentiable architecture search and validation pipeline using PyTorch.\", \"A final report in Markdown format containing visualizations of architectural parameter convergence, final training curves, and performance metrics.\", \"A bibliography file with citations for at least two relevant academic papers with their arXiv IDs.\", \"A verification log detailing the outcome of the gradient check between the analytical derivation and the implementation.\"], \"thinking_outline\": [\"1. Conceptualize the discrete nature of the neural architecture search space and identify why it is non-differentiable, preventing standard gradient-based optimization.\", \"2. Derive the mathematical formulation for relaxing the discrete, categorical choice of operations into a continuous space using the Gumbel-Softmax reparameterization trick, and formulate the complete bi-level optimization problem.\", \"3. Implement the search algorithm in PyTorch, carefully separating the updates for network weights (inner loop) and architectural parameters (outer loop).\", \"4. Verify the analytical gradient derivation against the implementation's automatic differentiation results to ensure the core optimization mechanism is correct and stable.\", \"5. Execute the search to discover a candidate architecture, then decode it into a discrete network and train it from scratch to validate its true performance, comparing it against a baseline.\", \"6. Analyze the final results, including the performance gap between the proxy search model and the final evaluated model, and discuss potential failure modes like performance collapse.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> /code -> router QA. The search results inform the mathematical derivation. The derivation from /math is the specification for the /code implementation. The second /math call audits the gradients in the first /code block. The final /code block validates the architecture produced by the entire pipeline. (Verification: Gradient alignment between math derivation and code implementation; Fallback: If gradients mismatch, re-derive the math and simplify the code implementation for debugging).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on DARTS and Gumbel-Softmax for differentiable NAS.\", \"- [ ] /math: Derive the continuous relaxation and the bi-level optimization objective.\", \"- [ ] /code: Implement the NAS search loop, search space, and data loaders for CIFAR-10.\", \"- [ ] /math: Formulate the finite difference approximation for the architecture gradients.\", \"- [ ] /code: Implement the gradient check by comparing the finite difference result to PyTorch's autograd.\", \"- [ ] /math: Verify that the derivation from the first math step aligns with the implementation's gradient computation.\", \"- [ ] /code: Implement the final training and evaluation pipeline for the discovered discrete architecture.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"automl\", \"neural-architecture-search\", \"differentiable-programming\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Gumbel-Softmax relaxation and bi-level objective must be complete and correct.\", \"- The Python code must execute without errors, successfully completing both the search and final evaluation phases.\", \"- The discovered architecture, when trained from scratch, must achieve at least 85% validation accuracy on CIFAR-10.\", \"- The gradient verification step must show that the analytical and numerical gradients are aligned within a small tolerance.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1804, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm developing a SE(3)-equivariant graph neural network to learn a Hamiltonian for N-body systems, specifically for coarse-grained protein dynamics. Can you derive the continuous-time update equations for node embeddings based on a message-passing framework that conserves total energy and momentum, then implement a proof-of-concept in PyTorch Geometric to simulate a simple system and validate it against a classical integrator like Velocity Verlet? I need to ensure the learned potential energy surface is smooth.\n", "completion": "{\"route_plan\": [\"/general-search(Find foundational arXiv papers on SE(3)-equivariant GNNs for Hamiltonian mechanics to inform the theoretical derivation, focusing on symplectic integrators and total energy conservation., mode=web)\", \"/math(Derive the continuous-time Hamiltonian equations and their discretization into a symplectic, SE(3)-equivariant message-passing scheme for the GNN, ensuring the learned potential energy is a smooth function.)\", \"/code(Implement the GNN integrator in PyTorch Geometric with CUDA support, simulating a small molecular system to track energy drift and momentum conservation metrics against a Velocity Verlet baseline., using Python)\", \"/math(Numerically verify the simulation output by analyzing the total energy drift over 1000 steps, checking for compliance with the theoretical conservation laws and auditing momentum calculations., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature, which is essential for a research-level task. This informs the /math agent's formal derivation of the model. The /code agent then implements this theoretical model. Finally, a second /math call provides a rigorous numerical verification, closing the loop by checking if the implementation's empirical behavior matches the derived theory.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Hamiltonian GNN update equations demonstrating energy and momentum conservation.\", \"A Python script implementing the GNN and a simulation loop for a small molecular system using PyTorch Geometric.\", \"A plot comparing the GNN-based trajectory and energy conservation against a Velocity Verlet integrator.\", \"A summary of at least two relevant arXiv papers with their corresponding IDs.\", \"A numerical verification report detailing the energy and momentum drift analysis.\"], \"thinking_outline\": [\"1. Research state-of-the-art SE(3)-equivariant GNNs for learning Hamiltonian dynamics and energy-conserving message passing schemes.\", \"2. Formulate the Hamiltonian for an N-body system and derive the continuous-time dynamics for node features (positions, momenta) that preserve physical symmetries and conserved quantities.\", \"3. Discretize the derived equations to create a learnable, symplectic GNN-based integrator.\", \"4. Implement this GNN integrator and validate its SE(3)-equivariance properties through numerical tests on rotated/translated input systems.\", \"5. Simulate a simple system and audit the model's long-term energy conservation against a baseline Velocity Verlet integrator, plotting the total energy over time.\", \"6. Verify that the learned force field (negative gradient of the potential energy) is consistent with ground truth forces for a known potential.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The blue search agent provides foundational papers to the math agent for derivation. The resulting equations serve as a specification for the code agent. The code agent produces simulation data, which is passed to the second math agent for numerical verification. Router QA performs a final check for consistency. Fallback: If numerical verification fails, the code agent must debug the implementation, or the first math agent must revisit the discretization scheme.\", \"todo_list\": [\"- [ ] /general-search: Find SOTA papers on equivariant GNNs for Hamiltonian systems and energy conservation (site:arxiv.org).\", \"- [ ] /math: Derive the SE(3)-equivariant message-passing update rules for a Hamiltonian GNN that conserves energy.\", \"- [ ] /code: Implement the derived model in PyTorch Geometric, including a simulation loop for a 3-body system.\", \"- [ ] /code: Run simulation and generate plots comparing GNN trajectory and energy drift versus a Velocity Verlet baseline.\", \"- [ ] /math: Verify the implementation's numerical stability and check that total energy drift is below a 1e-4 threshold.\", \"- [ ] /math: Audit the momentum conservation of the integrator by verifying near-zero drift in the simulation output.\", \"- [ ] /router: Perform final QA, ensuring the derived theory, code implementation, and verification results are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"hamiltonian-mechanics\"], \"acceptance_criteria\": [\"- The mathematical derivation must explicitly show how the chosen integration scheme conserves energy and momentum.\", \"- The Python implementation must pass a numerical SE(3) equivariance test with an error tolerance < 1e-6.\", \"- The GNN model's total energy drift over a 1000-step simulation must be at least one order of magnitude smaller than a non-symplectic forward Euler integrator.\", \"- The final report must cite at least two relevant arXiv papers with IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1856, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nImplement the K-SVD algorithm for dictionary learning and compare its performance against the Method of Optimal Directions (MOD) for a compressed sensing signal recovery task. I need a formal derivation of the update rules for both algorithms, a Python implementation that uses Orthogonal Matching Pursuit (OMP) for the sparse coding stage, and an analysis of how dictionary coherence impacts the reconstruction signal-to-noise ratio (SNR) for signals with varying sparsity levels (k). The analysis should also consider computational complexity and convergence behavior.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:ieeexplore.ieee.org K-SVD dictionary learning convergence analysis Method of Optimal Directions complexity\\\", mode=rag)\", \"/math(Derive the alternating minimization update rules for K-SVD and MOD, including the OMP sparse coding stage and the dictionary update stage. Analyze the theoretical convergence properties and computational complexity of each iteration.)\", \"/code(Implement K-SVD and MOD in Python with NumPy/SciPy. Generate synthetic sparse signals, apply a random Gaussian sensing matrix, and evaluate reconstruction SNR. The code must track dictionary coherence and runtime, including a Fourier basis for initialization.)\", \"/math(Analyze the empirical results from the code execution. Formally connect the observed reconstruction SNR and convergence speed to the derived theoretical properties, dictionary coherence metrics, and the eigenvalue distribution of the dictionary atoms' Gram matrix.)\"], \"route_rationale\": \"The plan begins with the 'blue' /general-search agent to gather state-of-the-art context on algorithm convergence and complexity. This informs the /math agent's formal derivation. The /code agent then implements these verified derivations. A final /math step provides a rigorous analysis, linking empirical results back to the initial theory, creating a closed loop of theory, implementation, and analysis.\", \"expected_artifacts\": [\"- A formal mathematical derivation of K-SVD and MOD update rules in a PDF document.\", \"- A Python script (`k_svd_vs_mod.py`) implementing both algorithms and the comparison experiment.\", \"- A JSON file (`metrics.json`) containing reconstruction SNR, dictionary coherence, and timing data for different sparsity levels.\", \"- A summary report (`report.md`) interpreting the results, including plots of SNR vs. sparsity and a discussion on algorithm convergence, citing at least two foundational papers.\", \"- Verification output log confirming the monotonic decrease of the objective function during training.\"], \"thinking_outline\": [\"1. Formulate the dictionary learning problem as a constrained optimization problem: minimizing reconstruction error under a sparsity constraint.\", \"2. Derive the two-stage, alternating minimization procedures for both K-SVD (OMP + SVD update) and MOD (OMP + pseudo-inverse update).\", \"3. Implement the complete pipeline: synthetic data generation (sparse signals, sensing matrix), dictionary learning loop for both algorithms, and signal reconstruction.\", \"4. Validate the implementation by instrumenting the code to log the objective function's value after each iteration and verifying that it is monotonically non-increasing.\", \"5. Systematically run experiments by varying the signal sparsity level (k) and measure the impact on final reconstruction SNR and dictionary coherence.\", \"6. Audit the final report to ensure the empirical findings (e.g., K-SVD's superior performance for a fixed atom-update budget) are correctly justified by the initial mathematical derivations and cited literature.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (analysis) -> router QA. The initial search provides foundational papers for the /math agent. The resulting derivations are the specification for the /code agent. The code's output (metrics, logs) is then passed to the second /math agent for final analysis. Verification: The router will check that the function signatures and data structures in the /code agent's implementation directly correspond to the variables and update rules in the /math agent's derivation. Fallback: If the code's objective function does not decrease, halt and return to the /math agent to re-verify the derivation correctness.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on K-SVD (Aharon et al., 2006) and MOD (Engan et al., 1999), focusing on convergence proofs and complexity analysis.\", \"- [ ] /math: Produce a self-contained PDF with the full derivation for both K-SVD and MOD update rules.\", \"- [ ] /code: Write a Python script to implement the full experiment, ensuring it saves all specified metrics to `metrics.json`.\", \"- [ ] /code: Add runtime assertions to check for matrix singularity and NaN values during dictionary updates.\", \"- [ ] /math: Verify that the Python implementation logic in the dictionary update step correctly matches the derived pseudo-inverse and SVD operations.\", \"- [ ] /math: Author the final analysis, generating plots and connecting the empirical data back to the theory of sparse representations.\", \"- [ ] /router: Perform final QA check, ensuring all artifacts are present and the acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The Python script must be executable and reproduce the results reported in `metrics.json` and `report.md`.\", \"- The final report must correctly identify the computational complexity of each algorithm and explain why K-SVD typically yields better dictionaries than MOD.\", \"- The submitted code must include a function to calculate the dictionary's mutual coherence, and this metric must be reported.\", \"- The derivations must be mathematically sound and clearly explain the role of the SVD in the K-SVD update step.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1860, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the stability of persistent homology for manifold learning. Please provide a full mathematical derivation of the stability theorem for persistence diagrams with respect to the bottleneck distance, bounded by the Gromov-Hausdorff distance between point clouds. Subsequently, implement a Python simulation using a witness complex construction on a noisy torus point cloud to empirically validate this theorem, analyzing its computational complexity and convergence properties against a standard Vietoris-Rips filtration.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org stability of persistence diagrams Gromov-Hausdorff bottleneck distance witness complex manifold learning\\\", mode=web)\", \"/math(Derive the stability theorem for persistence diagrams, formalizing the inequality between bottleneck distance and Gromov-Hausdorff distance. Reference the convergence and eigenvalue properties of the underlying topological spaces.)\", \"/code(Implement a Python simulation using the Gudhi library to generate a noisy torus, construct a witness complex filtration, and compute its persistent homology. Validate the stability by plotting bottleneck vs. Gromov-Hausdorff distances for varying noise levels., includes cybersecurity)\", \"/math(Analyze the computational complexity of the witness complex construction versus the Vietoris-Rips complex, referencing the number of simplices and relating the theoretical bounds to the empirical runtime convergence observed in the simulation.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature. This informs the /math agent for the formal derivation. The /code agent then provides empirical validation of the mathematical theory. A final call to the /math agent connects the empirical performance back to theoretical complexity analysis, ensuring a rigorous math-code-math verification loop.\", \"expected_artifacts\": [\"- A LaTeX document detailing the complete proof of the persistence diagram stability theorem.\", \"- A documented Python script (`tda_simulation.py`) that performs the simulation, including data generation, witness complex construction, and persistence computation.\", \"- A JSON file (`metrics.json`) containing the measured bottleneck and Gromov-Hausdorff distances for each simulation run.\", \"- A set of PNG plots visualizing the persistence diagrams and the relationship between the two distance metrics.\", \"- A bibliography in the report citing at least two relevant arXiv pre-prints.\"], \"thinking_outline\": [\"1. Research the foundational Isometry and Stability Theorems for persistent homology, focusing on the works by Cohen-Steiner, Edelsbrunner, and Harer.\", \"2. Formulate and write a rigorous proof connecting the Gromov-Hausdorff distance between two metric spaces to the bottleneck distance between their corresponding persistence diagrams.\", \"3. Implement a Python simulation to sample points from a known manifold (e.g., a 2-torus), add controlled levels of noise, and construct a witness complex for both the original and noisy point clouds.\", \"4. Validate the implementation by ensuring the Betti numbers computed for the noise-free point cloud match the known topology of the torus (\u03b2\u2080=1, \u03b2\u2081=2, \u03b2\u2082=1).\", \"5. Verify the stability theorem empirically by computing the bottleneck and Gromov-Hausdorff distances across multiple noise levels and plotting them to confirm the derived inequality holds.\", \"6. Conclude with a formal analysis of the computational complexity of the witness complex construction in the simulation and contrast it with the known exponential complexity of the Vietoris-Rips complex.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent for the proof. The math agent's derivation provides the theoretical basis for the code agent's simulation. The code agent's empirical results (metrics, runtimes) are then handed back to the math agent for the final complexity analysis. Verification: The router will check if the empirical results from /code align with the bounds derived by /math. Fallback: If a mismatch occurs, the task is routed back to /math to re-examine the proof's assumptions and to /code to debug the distance metric implementations.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on persistence diagram stability (e.g., Cohen-Steiner, Edelsbrunner, Harer) and witness complexes.\", \"- [ ] /math: Derive the proof for the stability of persistence diagrams under the bottleneck distance, citing sourced literature.\", \"- [ ] /code: Implement the simulation in Python to generate a noisy point cloud from a torus manifold and build a witness complex using the Gudhi library.\", \"- [ ] /code: Compute the persistence diagrams and measure the bottleneck and Gromov-Hausdorff distances.\", \"- [ ] /math: Verify that the implemented bottleneck distance calculation in the code aligns with the formal definition used in the proof.\", \"- [ ] /code: Validate the stability theorem by running simulations with varying noise levels and plotting the distances to check the bound.\", \"- [ ] /math: Analyze the computational complexity of the witness complex and compare it to the theoretical complexity of Vietoris-Rips, using data from the simulation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"persistent-homology\"], \"acceptance_criteria\": [\"- The mathematical derivation of the stability theorem is complete, correct, and logically sound.\", \"- The Python code runs without errors, is well-documented, and the simulation's results for the noise-free case correctly identify the Betti numbers of the torus.\", \"- The final report includes a plot that empirically demonstrates that the bottleneck distance is bounded by the Gromov-Hausdorff distance across various noise levels.\", \"- The complexity analysis correctly contrasts the witness and Vietoris-Rips constructions with reference to both theory and empirical runtimes.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"diagnostic\": \"Betti Number Accuracy: 'Percentage of correctly identified Betti numbers (\u03b2\u2080, \u03b2\u2081, \u03b2\u2082) for the noise-free manifold compared to ground truth [1, 2, 1]. Should be 100%.'\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1874, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the connection between the temperature parameter in the softmax function of a multi-head attention mechanism and the rate-distortion trade-off in self-supervised representation learning. Can you formalize this using the information bottleneck principle and empirically validate how temperature affects feature invariance and redundancy reduction, referencing principles from Bayesian information criteria and measuring convergence with AUPRC?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org information bottleneck self-supervised attention softmax temperature bayesian interpretation\\\", mode=web)\", \"/math(Derive the rate-distortion function for a masked attention head, treating the softmax temperature as a Lagrange multiplier controlling the channel capacity. Analyze the convergence properties of this formulation under a Fourier basis.)\", \"/code(Implement a Siamese network in PyTorch using CUDA to test the derived theory, training on a contrastive loss. Track feature covariance matrix collapse and downstream classification AUPRC as a function of softmax temperature., using Python)\", \"/math(Validate the empirical results from the code by checking if the observed eigenvalue distribution of the feature covariance matrix aligns with the theoretical predictions for different temperatures. Establish a confidence interval for the AUPRC change.)\"], \"route_rationale\": \"The blue general-search agent will first gather foundational literature. The /math agent will then formalize the theory. The /code agent will empirically validate the derived hypotheses. A final /math call will cross-verify the empirical findings against the theoretical model, ensuring rigorous alignment.\", \"expected_artifacts\": [\"A formal mathematical derivation connecting attention softmax temperature to the rate-distortion trade-off via the information bottleneck principle.\", \"A Python script (PyTorch) implementing a self-supervised learning model to empirically test the derived theory, including data loading and training loops.\", \"A report summarizing the findings, including plots of feature covariance matrices and AUPRC scores versus temperature, with citations to at least 2 relevant arXiv papers.\", \"A verification document confirming the alignment between the theoretical eigenvalue distribution of the covariance matrix and the empirical results.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation by framing the attention mechanism within the Information Bottleneck (IB) principle, identifying the mutual information terms for compression and prediction.\", \"2. Formulate the Lagrangian for the IB objective and derive the analytical form of the optimal encoder, showing its equivalence to a softmax function where the temperature parameter acts as the trade-off parameter \u03b2.\", \"3. Implement a self-supervised learning model (e.g., VICReg or Barlow Twins) where the encoder uses a transformer attention block. Systematically vary the attention softmax temperature during training.\", \"4. Evaluate the trained representations by measuring two key metrics: a) redundancy reduction (e.g., off-diagonal elements of the covariance matrix) and b) invariance (e.g., loss value or downstream task performance like AUPRC).\", \"5. Validate that the empirical trade-off curve between redundancy and invariance, as controlled by temperature, matches the theoretical rate-distortion curve derived in step 2.\", \"6. Check the spectral properties of the learned feature covariance matrix and verify that its decay aligns with the theoretical predictions about dimensionality collapse at extreme temperature values.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides papers to the math agent for derivation. The derivation informs the code agent's implementation of the SSL model. The code agent's empirical results (metrics, covariance matrices) are passed back to the math agent for formal verification against the theory. (Verification: Eigenvalue distribution of empirical covariance matches theory; Fallback: If mismatch, /code agent re-runs with corrected hyperparameters or /math agent refines the theoretical model).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on information bottleneck theory and its application to deep learning, specifically attention and self-supervised learning.\", \"- [ ] /math: Derive the Lagrangian for the information bottleneck objective and show the relationship between the softmax temperature and the rate-distortion trade-off parameter.\", \"- [ ] /code: Implement the Siamese network in PyTorch, ensuring the attention module's temperature is a configurable hyperparameter.\", \"- [ ] /code: Generate plots for AUPRC and covariance matrix off-diagonal sums as a function of temperature across multiple runs.\", \"- [ ] /math: Verify that the eigenvalue decay of the empirical feature covariance matrix aligns with the derived theoretical predictions for information collapse.\", \"- [ ] /math: Create a confidence interval for the observed change in AUPRC to ensure statistical significance.\", \"- [ ] router: QA review of all artifacts, checking for consistency between derivation, code, and final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"self-supervised-learning\", \"information-theory\", \"attention-mechanisms\"], \"acceptance_criteria\": [\"Pass: The mathematical derivation correctly identifies the softmax temperature as the Lagrange multiplier in the information bottleneck objective.\", \"Pass: The submitted Python code is executable and reproduces the plots showing a clear trade-off between feature redundancy and downstream AUPRC as temperature is varied.\", \"Pass: The final report includes at least two citations to relevant arXiv pre-prints and the theoretical verification of empirical covariance matrices is mathematically sound.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1904, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Lagrangian dual for a neural network weight pruning problem formulated as minimizing L1 norm subject to a spectral norm constraint on the weight matrices and an upper bound on the Frobenius norm of the change. How do the Karush-Kuhn-Tucker (KKT) conditions inform the optimal pruning mask and the dual variables' interpretation in this context of adversarial robustness?\n", "completion": "{\"route_plan\": [\"/math(Formulate the Lagrangian, analogous to an Euler-Lagrange setup, for the L1-norm minimization problem under spectral and Frobenius norm constraints. Derive the KKT optimality conditions, paying close attention to the primal/dual feasibility and the Hessian eigenvalue structure for second-order checks.)\", \"/general-search(query=\\\"site:arxiv.org lagrangian relaxation spectral norm pruning adversarial robustness convergence\\\", mode=rag)\", \"/code(Implement an ADMM solver in Python with JAX to find the optimal pruning mask based on the derived KKT conditions. Profile the solver's convergence on a synthetic dataset, track the spectral norm constraint violation, and enable CUDA acceleration.)\"], \"route_rationale\": \"The route starts with /math to establish the core theoretical foundation via formal derivation. This is followed by the blue /general-search agent to ground the derivation in existing literature and find justification for the chosen constraints. Finally, /code is used to empirically validate the theory by implementing a numerical solver that targets the derived optimality conditions.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Lagrangian, dual problem, and KKT conditions in a LaTeX-formatted PDF.\", \"A Python script implementing an ADMM solver for the constrained pruning problem.\", \"A summary report in Markdown that interprets the results and cites at least two relevant research papers.\", \"A JSON file containing the final metrics, including constraint violation and iteration count.\"], \"thinking_outline\": [\"1. Formulate the primal optimization problem for constrained weight pruning, clearly defining the objective function and all inequality constraints.\", \"2. Construct the Lagrangian function and derive the corresponding dual problem by minimizing over the primal variables.\", \"3. State and interpret the full Karush-Kuhn-Tucker (KKT) conditions (primal feasibility, dual feasibility, complementary slackness, and stationarity) for this specific problem.\", \"4. Validate the derived KKT conditions by checking against known results from authoritative literature found via search.\", \"5. Implement an iterative solver (e.g., ADMM) in Python to find a numerical solution that satisfies the derived KKT conditions.\", \"6. Verify that the numerical solution from the code converges and respects the primal constraints to within a specified tolerance on a synthetic test case.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent produces the formal derivation. Keywords from this derivation inform the /general-search agent's query. The /code agent implements the solver based on the derivation from /math, using patterns or insights from papers found by /general-search. The final artifacts are passed to the router for a full quality assurance check. Verification: Code's termination conditions must match the KKT stationarity condition. Fallback: If the code fails to converge, the KKT derivation is re-audited by /math for potential errors.\", \"todo_list\": [\"- [ ] /math: Derive the complete Lagrangian and Karush-Kuhn-Tucker (KKT) conditions for the constrained L1 minimization problem.\", \"- [ ] /general-search: Find and summarize at least two authoritative arXiv papers on spectral norm regularization in the context of network pruning or adversarial robustness.\", \"- [ ] /code: Implement the ADMM solver in Python (JAX) to find the optimal pruning mask according to the derived theory.\", \"- [ ] /code: Generate convergence plots and a final metrics JSON file from a run on a synthetic dataset.\", \"- [ ] /math: Verify that the stationarity condition derived in the KKT analysis aligns with the solver's fixed-point update rule in the /code implementation.\", \"- [ ] /code: Audit the Python implementation to confirm that numerical constraint checks are correctly implemented and reported.\", \"- [ ] router: Perform final QA review of all artifacts (derivation, code, report) against the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"deep-learning\", \"lagrangian-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation of the KKT conditions must be complete, logically sound, and self-contained.\", \"- The Python script must execute without errors on the provided synthetic data, converge, and produce all specified output artifacts.\", \"- The final report must correctly cite at least two relevant arXiv papers with IDs, explicitly connecting their theory to the derivation.\", \"- The primary metric (Constraint Violation) in the output `metrics.json` must be below a tolerance of 1e-5.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1923, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the convergence properties of Alternating Least Squares (ALS) for Canonical Polyadic (CP) tensor decomposition under non-negativity constraints. Please derive the update rules using a projected gradient or multiplicative update approach, implement it, and then validate its performance on a synthetic dataset, specifically focusing on how the latent factor collinearity impacts the convergence rate and reconstruction fidelity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org non-negative tensor factorization ALS convergence guarantees collinearity\\\", mode=web)\", \"/math(Derive the Karush-Kuhn-Tucker conditions for the constrained CP decomposition loss function, leading to projected gradient update rules for each factor matrix, considering the convergence criteria from the Bayesian literature.)\", \"/code(Implement the derived non-negative ALS algorithm in Python with NumPy/TensorLy, including a synthetic data generator with tunable factor collinearity, and track Mean Squared Error and factor matrix condition number per iteration.)\", \"/math(Analyze the empirical convergence data from the code; formally verify if the observed convergence rate aligns with the theoretical bounds for ill-conditioned problems discussed in the sourced arXiv literature and check eigenvalue spread.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find foundational papers on the convergence of constrained tensor decomposition. /math is then used to formalize the algorithm's derivation based on these papers. /code provides an empirical implementation for validation. A final /math step is crucial for the advanced task of verifying that the empirical results align with the established theoretical guarantees, closing the loop between theory and practice.\", \"expected_artifacts\": [\"- A summary of relevant literature on non-negative tensor decomposition and its convergence properties, with citations.\", \"- A formal mathematical derivation of the ALS update rules under non-negativity constraints.\", \"- A Python script implementing the algorithm and experiments, packaged as a runnable Jupyter notebook.\", \"- A final report (PDF) containing plots of convergence and an analysis comparing empirical results to theoretical bounds.\", \"- A JSON file containing the final metrics from the experimental runs.\"], \"thinking_outline\": [\"1. Survey the academic literature to identify established convergence theorems and common algorithmic approaches (e.g., HALS, MU) for non-negative CP decomposition.\", \"2. Formulate the constrained optimization problem and formally derive the iterative update rules for each factor matrix using Lagrange multipliers or a similar method.\", \"3. Implement the derived algorithm in Python, alongside a data generation process that allows for precise control over the ground-truth factors' collinearity.\", \"4. Execute a series of experiments, measuring reconstruction error and iterations to convergence across different levels of collinearity.\", \"5. Verify that the implementation's numerical behavior (e.g., decrease in loss function) is consistent with the mathematical derivation.\", \"6. Validate the final report's conclusions by checking if the observed relationship between collinearity and convergence speed matches predictions from the literature identified in step 1.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The /general-search results (papers on convergence) are handed to /math to ground the derivation. The formal derivation from /math is then handed to /code as the specification for implementation. /code's empirical results are handed back for final theoretical validation. (Verification: Derivation must match literature before coding begins; empirical convergence must be monotonic decreasing. Fallback: If derivation is intractable, revert to /general-search for alternative algorithms like multiplicative updates.)\", \"todo_list\": [\"- [ ] /general-search: Find \u22652 seminal arXiv papers on the convergence of ALS for non-negative CP/PARAFAC.\", \"- [ ] /math: Derive the update equations for non-negative ALS based on the KKT conditions.\", \"- [ ] /code: Implement the algorithm and a synthetic data generator to control factor matrix collinearity.\", \"- [ ] /code: Run experiments and log reconstruction error and convergence iterations against collinearity.\", \"- [ ] /math: Formally verify that the implementation's loss curve decreases monotonically.\", \"- [ ] /math: Audit the empirical convergence rate against the theoretical bounds from the literature survey.\", \"- [ ] /router: Perform final QA review on all artifacts, ensuring the report correctly links theory and empirical findings.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"numerical-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation of the update rules must be complete and correct.\", \"- The Python implementation must be runnable and reproduce the plots shown in the final report.\", \"- The final report must contain a clear comparison between empirical convergence rates and theoretical bounds cited from at least two papers.\", \"- The primary metric (MSE) must be below 1e-5 for low-collinearity synthetic data.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1955, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the algorithmic complexity of determining winning strategies in partisan octal games played on directed acyclic graphs (DAGs). Derive the recurrence for computing Grundy values (g-numbers) under partisan constraints, implement a dynamic programming solution to find P-positions, and analyze its time complexity in terms of graph properties. Cross-validate the implementation against known results for simpler impartial games, referencing state-of-the-art literature on computational game theory.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org partisan combinatorial games complexity OR \\\\\\\"octal games\\\\\\\" on graphs PSPACE\\\", mode=rag)\", \"/math(Formalize the partisan octal game rules on a DAG and derive the generalized Grundy value recurrence relation, considering the computational complexity and potential finance market modeling implications.)\", \"/code(Implement a Python dynamic programming solver using memoization to compute the g-values for all nodes in a given DAG, including runtime checks for acyclicity and tracking complexity metrics.)\", \"/math(Formally prove the time complexity of the implemented algorithm in terms of vertices and edges, and verify its correctness by auditing the code against the derived recurrence on a non-trivial example graph., includes numpy)\"], \"route_rationale\": \"The blue agent (/general-search) first gathers foundational literature on partisan game complexity. /math then formalizes the problem by deriving the necessary recurrence. /code implements this mathematical model as a verifiable algorithm. A final /math step provides formal verification and complexity analysis, ensuring theoretical rigor.\", \"expected_artifacts\": [\"A formal proof of the recurrence relation for partisan Grundy values on a DAG.\", \"A well-commented Python implementation of the dynamic programming solver.\", \"A complexity analysis report (PDF) detailing the time and space bounds.\", \"A verification document showing the manual calculation for a small graph and the corresponding program output.\", \"A bibliography with at least two cited arXiv papers on computational game theory.\"], \"thinking_outline\": [\"1. Research existing literature on the complexity of partisan games and specifically octal games on graphs to understand the state-of-the-art and identify relevant theorems (e.g., PSPACE-completeness).\", \"2. Formally define the game rules and derive the specific recurrence relation for computing the Grundy numbers (g-values) for this partisan variant, distinguishing it from the standard impartial case.\", \"3. Implement the derived recurrence relation using a dynamic programming approach with memoization in Python to solve for the g-value of any position in a given directed acyclic graph.\", \"4. Verify the correctness of the implementation by running it on a small, non-trivial graph (e.g., 5-7 nodes) and manually tracing the g-value computation to check for agreement.\", \"5. Analyze the asymptotic time and space complexity of the algorithm, expressing it in terms of the number of vertices and edges of the input graph, and validate this analysis against empirical runtime measurements.\"], \"handoff_plan\": \"/general-search -> /math (verification: ensure cited papers are relevant to partisan games, not just impartial Nim; fallback: broaden search to general combinatorial game theory) -> /code (verification: check if algorithm signature matches the derived recurrence; fallback: request clarification from /math on the recurrence form) -> /math (verification: audit code logic against the formal proof; fallback: debug code with specific counterexamples) -> router QA.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on the complexity of partisan games or generalized geography.\", \"- [ ] /math: Derive the recurrence for the partisan Grundy values based on the game rules.\", \"- [ ] /code: Implement the DP solver for computing g-values on a DAG.\", \"- [ ] /code: Add runtime asserts to verify the input graph is acyclic.\", \"- [ ] /math: Construct a small test case graph and manually compute its g-values.\", \"- [ ] /math: Verify the code's output matches the manual computation for the test case.\", \"- [ ] /math: Provide a formal Big-O complexity proof for the implemented algorithm.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"dynamic-programming\"], \"acceptance_criteria\": [\"The provided Python code correctly computes the Grundy value for any node in a given DAG representation.\", \"The mathematical derivation of the recurrence relation is sound and clearly explained.\", \"The time complexity analysis is proven formally and matches the O(V+E) expectation for DP on a DAG.\", \"At least two relevant academic papers (e.g., from arXiv) are cited in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1957, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to synthesize a functional program for list sorting, specified using first-order logic with inductive definitions. Can you devise a neural-symbolic framework that combines a sequence-to-sequence model for sketching the program structure with a symbolic solver (like an SMT solver) to fill in the details and formally verify the generated program against the specification? The synthesis must handle higher-order functions and demonstrate convergence guarantees based on Bayesian principles.\n", "completion": "{\"route_plan\": [\"/math(Formalize the inductive synthesis problem in higher-order logic and derive the Bayesian evidence lower bound (ELBO) objective for the neural sketch generator, considering the complexity of the symbolic solver's search space and ensuring convergence.)\", \"/code(Implement the neural-symbolic synthesizer in Python using PyTorch for the seq2seq sketcher and the Z3 SMT solver for verification. Validate the synthesizer on benchmark problems, tracking synthesis success rate and ensuring causal consistency in the generated code.)\", \"/general-search(query=\\\"site:arxiv.org neural-symbolic program synthesis inductive logic programming Bayesian deep learning\\\", mode=web)\"], \"route_rationale\": \"The /math tool is used first to establish the theoretical foundation and learning objective. This formal specification is then passed to the /code tool for a robust implementation and empirical validation. Finally, the blue /general-search agent is tasked with gathering contemporary research from authoritative sources like arXiv to contextualize the work and justify the chosen approach.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Bayesian neural-symbolic synthesis framework in a PDF document.\", \"A Python implementation of the synthesizer, including the neural and symbolic components, with a requirements.txt file.\", \"A verification report in markdown format showing that the synthesized program passes all logical specifications for a benchmark suite.\", \"A literature review section in the report with citations and summaries for at least 2 relevant arXiv preprints.\"], \"thinking_outline\": [\"1. Formalize the program synthesis problem using higher-order logic and define the search space for the symbolic solver.\", \"2. Derive the Bayesian learning objective (ELBO) for the neural component, connecting the likelihood of a program sketch to the probability of its successful verification by the SMT solver.\", \"3. Implement the sequence-to-sequence model for program sketching and integrate it with the Z3 SMT solver via its Python API to create a closed-loop system.\", \"4. Validate the entire pipeline on a set of formal specifications for list manipulation algorithms, primarily focusing on sorting and its variants.\", \"5. Verify that the synthesized programs are functionally correct by running them against the formal specifications and check that the theoretical convergence claims hold empirically during training.\", \"6. Compile a final report summarizing the theory, implementation, validation results, and relevant literature found.\"], \"handoff_plan\": \"/math agent provides the formal derivations and learning objective -> /code agent implements the system based on these derivations and generates synthesis results with a verification log -> /general-search agent finds contextual literature. The code implementation's correctness is verified against the math agent's derivations. All artifacts are passed to the router for final QA. Fallback: If the code fails to verify against the mathematical model, return to the /math agent to refine the theory.\", \"todo_list\": [\"- [ ] /math: Formalize the synthesis problem using higher-order logic and derive the Evidence Lower Bound (ELBO).\", \"- [ ] /math: Define the interface between the neural sketcher's output representation and the symbolic solver's input constraints.\", \"- [ ] /code: Implement the sequence-to-sequence neural sketch generator in PyTorch.\", \"- [ ] /code: Integrate the neural model with the Z3 SMT solver for program completion and formal verification.\", \"- [ ] /code: Run experiments on benchmark list-sorting specifications and log the synthesis success rate and timing metrics.\", \"- [ ] /math: Verify that the implemented learning objective in the code correctly matches the mathematical derivation from the theory stage.\", \"- [ ] /general-search: Find and summarize at least two key papers on neural-symbolic methods from arXiv, including their IDs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic\", \"program-synthesis\", \"theorem-proving\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Bayesian learning objective must be complete, correct, and self-contained.\", \"- The implemented Python code must successfully synthesize a correct sorting program from a formal first-order logic specification.\", \"- The final report must cite at least two relevant arXiv papers with IDs on neural-symbolic synthesis or inductive programming.\", \"- The primary metric (Synthesis Success Rate) must be above 80% on the designated benchmark task.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1988, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nPlease provide a formal derivation for the convergence proof of a non-convex Alternating Direction Method of Multipliers (ADMM) variant applied to sparse dictionary learning. I'm particularly interested in the sufficient conditions on the augmented Lagrangian penalty parameter, rho, and how to verify the proof against an empirical implementation. Can you also provide a JAX implementation to validate the theoretical convergence rate on a synthetic dataset?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org non-convex ADMM convergence sparse dictionary learning augmented lagrangian\\\", mode=both)\", \"/math(Derive the full convergence proof for the non-convex ADMM, establishing sufficient conditions on the augmented Lagrangian penalty parameter and analyzing the complexity of each iteration.)\", \"/code(Implement the non-convex ADMM algorithm in JAX for sparse dictionary learning, tracking primal/dual residual convergence metrics. The implementation must include synthetic data generation for reproducibility., using Python)\", \"/math(Analyze the empirical convergence rate from the JAX implementation against the derived theoretical bounds. Validate the alignment and check if Chebyshev's inequality applies to bound the dual variable updates.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on non-convex ADMM convergence. This informs the /math agent's formal proof derivation. The /code agent then implements the algorithm for empirical validation. Finally, the /math agent returns to connect the theoretical proof with the empirical results, providing a comprehensive and verified solution.\", \"expected_artifacts\": [\"A PDF report containing the complete mathematical derivation of the convergence proof, formatted in LaTeX.\", \"A Python script (`admm_sparse_coding.py`) using JAX to implement the non-convex ADMM algorithm and the validation experiment.\", \"A JSON file (`results.json`) logging the primal/dual residuals and empirical convergence rates per iteration.\", \"A literature review summary in the report, citing at least two key arXiv papers with their IDs.\", \"A verification log documenting the consistency check between the theoretical proof and the empirical results.\"], \"thinking_outline\": [\"1. Survey foundational and recent literature on convergence guarantees for non-convex ADMM, particularly in the context of matrix factorization or sparse coding.\", \"2. Formulate the sparse dictionary learning problem as a non-convex optimization problem and derive the corresponding ADMM update steps for the dictionary and sparse codes.\", \"3. Construct the convergence proof, rigorously identifying all necessary assumptions (e.g., Lipschitz continuity, coercivity) and deriving the sufficient conditions on the penalty parameter `rho` to guarantee convergence to a critical point.\", \"4. Implement the ADMM algorithm in JAX, ensuring the code accurately reflects the derived update equations. Generate a synthetic dataset with a known ground-truth dictionary to test the implementation.\", \"5. Validate that the empirical convergence behavior (e.g., decay of primal and dual residuals) is consistent with the theoretical rate derived in the proof. Plot theoretical bounds against empirical performance.\", \"6. Audit the final proof for logical soundness and verify that the code's numerical behavior does not violate any of the proof's core assumptions.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (proof derivation) -> /code (implementation) -> /math (empirical validation) -> router QA (final review). Verification: The final /math step explicitly validates the code's output against the initial proof's claims. Fallback: If empirical results contradict the proof, the task is routed back to the first /math agent to re-audit assumptions and derivations.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 key papers on non-convex ADMM convergence proofs.\", \"- [ ] /math: Formulate the problem and derive the step-by-step convergence proof based on the literature.\", \"- [ ] /code: Implement the ADMM algorithm, sub-problem solvers, and synthetic data generation in JAX.\", \"- [ ] /code: Instrument the code to log primal and dual residuals at each iteration.\", \"- [ ] /math: Verify that the assumptions made in the proof (e.g., on step sizes, problem structure) are correctly implemented in the code.\", \"- [ ] /math: Plot empirical results against the derived theoretical bounds and analyze any discrepancies.\", \"- [ ] /code: Validate that the numerical precision and stopping criteria are sufficient for observing the theoretical convergence behavior.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"convex-optimization\", \"admm\", \"machine-learning-theory\"], \"acceptance_criteria\": [\"- The mathematical proof must be rigorous, complete, and clearly state all assumptions and conditions.\", \"- The provided JAX code must be executable and reproduce the numerical results presented in the analysis.\", \"- The report must show that the empirical convergence rate is consistent with the derived theoretical bounds.\", \"- The solution must cite at least two relevant arXiv pre-prints with proper IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 1991, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous analysis of Canonical Polyadic (CP) decomposition using the Alternating Least Squares (ALS) algorithm for representation learning. Please provide a full mathematical derivation of the ALS update rules, leveraging the Khatri-Rao product for a compact formulation. Follow this with a proof sketch for the local convergence of ALS, discussing the role of tensor rank and ill-posedness. Finally, implement this in Python to validate the theory on a synthetic tensor with known low-rank structure, comparing the reconstruction error and the quality of the learned factor embeddings against a standard PCA baseline on a downstream task.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Canonical Polyadic decomposition Alternating Least Squares convergence Khatri-Rao product\\\", mode=web)\", \"/math(Derive the CP-ALS update equations for each factor matrix using matrix calculus and the Khatri-Rao product. Analyze the convergence properties and formulate a proof sketch for local convergence under specific uniqueness conditions., includes cybersecurity)\", \"/code(Implement the CP-ALS algorithm in Python using NumPy. Create a synthetic tensor with a known low-rank structure and adversarial noise. Measure reconstruction error and track the convergence rate of the loss function. Also implement a PCA baseline for comparison.)\", \"/math(Audit the Python implementation against the derived equations. Specifically, verify that the matrix inversions and Khatri-Rao product operations in the code correctly implement the derived update rules for ensuring numerical stability and proper convergence.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on CP-ALS theory. /math then takes this input to perform the core theoretical derivations and proofs. Next, /code translates the mathematical formulation into a practical Python implementation and experiment. A final /math step is crucial for verification, ensuring the code's logic perfectly aligns with the derived mathematical theory, which is a key requirement of the math_plus_code variant.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF report detailing the mathematical derivation of the CP-ALS update rules and the convergence proof sketch.\", \"- A Python script (`.py`) containing the full implementation of the CP-ALS algorithm, the synthetic data generator, the PCA baseline, and the experimental validation.\", \"- A JSON file (`metrics.json`) reporting the final reconstruction error, factor match score, and downstream task accuracy for both CP-ALS and PCA.\", \"- A set of plots (`.png`) visualizing the convergence of the reconstruction error over iterations and comparing the learned representations.\", \"- A bibliography with citations for at least two relevant academic papers from arXiv.\"], \"thinking_outline\": [\"1. Review foundational literature on CP decomposition, the ALS algorithm, and its convergence properties, particularly theorems involving the Khatri-Rao product and Kruskal's uniqueness conditions.\", \"2. Formally derive the ALS update rule for a single factor matrix by fixing the others, formulating the problem as a linear least squares problem, and solving the normal equations.\", \"3. Implement the CP-ALS algorithm in Python, including a function to construct a synthetic tensor from known factor matrices and a function to compute the reconstruction error.\", \"4. Validate the implementation by checking for monotonic decrease in the reconstruction error on the synthetic tensor and verifying that the algorithm can recover the ground-truth factors in a low-noise setting.\", \"5. Design and run an experiment to compare the representations learned by CP-ALS with those from PCA on a synthetic downstream classification task, measuring and reporting accuracy.\", \"6. Check the mathematical derivation against canonical sources and verify that the core computational steps in the Python code (e.g., the pseudoinverse calculation) are a direct and correct implementation of the derived formulas.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers (e.g., Kolda & Bader, 2009) to the math agent. The math agent produces a formal derivation (PDF), which the code agent implements. The code agent hands its script back to the math agent for a logic audit. (Verification: The math agent confirms the Python code's update step matches the derived closed-form solution. Fallback: If a discrepancy is found, the code agent must revise the implementation and resubmit for verification). Finally, all artifacts are passed to the router for a holistic quality check.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on CP-ALS convergence and applications.\", \"- [ ] /math: Produce a full derivation of the CP-ALS update rules using Khatri-Rao product notation.\", \"- [ ] /math: Formulate and prove local convergence conditions for the algorithm.\", \"- [ ] /code: Implement the CP-ALS algorithm and the synthetic data generation process in a Python script.\", \"- [ ] /code: Run the comparison experiment against PCA and generate all required plots and metric files.\", \"- [ ] /math: Verify that the core logic in the Python script's update loop correctly implements the derived normal equations.\", \"- [ ] /code: Add comments to the code linking the implementation back to specific equations in the math derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"numerical-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation for the ALS update must correctly use the Khatri-Rao product and solve the resulting least-squares problem.\", \"- The implemented Python code must show a monotonically decreasing reconstruction error on the synthetic dataset over iterations.\", \"- The final report must demonstrate that the representations learned via CP-ALS lead to higher accuracy on the downstream task than the PCA baseline.\", \"- The bibliography must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2012, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project in computational imaging and need to understand the basics of the Alternating Direction Method of Multipliers (ADMM). Can you walk me through the derivation of the ADMM update steps for a simple Lasso regression problem and provide a Python implementation to show its convergence behavior?\n", "completion": "{\"route_plan\": [\"/math(Derive the augmented Lagrangian for the Lasso problem in consensus form and find the update rules for the primal and dual variables, analyzing the conditions for convergence of the ADMM algorithm.)\", \"/code(Implement the ADMM solver for Lasso in Python, using the derived update rules. Create a synthetic dataset and plot the convergence of the primal and dual residuals to demonstrate the algorithm's performance.)\", \"/general-search(query=\\\"site:stanford.edu/~boyd/papers/admm/ 'Distributed Optimization and Statistical Learning' Lasso econometrics applications\\\", mode=web)\"], \"route_rationale\": \"/math is used for the formal derivation of the ADMM update equations. /code is then used to create a practical implementation and visualize the algorithm's behavior. The blue /general-search agent is tasked with finding a canonical reference to validate the derivation and provide context on applications.\", \"expected_artifacts\": [\"A PDF document (`derivation.pdf`) containing the complete mathematical derivation using LaTeX.\", \"A commented Python script (`admm_lasso.py`) that implements the ADMM solver for Lasso.\", \"A set of PNG images (`convergence_plots.png`) showing the primal and dual residuals decreasing over iterations.\", \"A brief markdown report (`report.md`) summarizing the findings and citing at least two authoritative sources.\", \"A JSON file (`metrics.json`) reporting the final primal and dual residual norms.\"], \"thinking_outline\": [\"1. Formulate the standard Lasso problem (L1-regularized least squares) and rewrite it in the 'consensus' form required for ADMM by introducing a splitting variable.\", \"2. Write down the augmented Lagrangian for this consensus formulation.\", \"3. Derive the iterative update rules for each variable (the primal variables 'x' and 'z', and the dual variable 'u') by minimizing the augmented Lagrangian with respect to each variable one at a time.\", \"4. Implement the complete algorithm in Python, paying close attention to the soft-thresholding operator for the L1-norm update step.\", \"5. Validate the implementation by running it on synthetic data and verifying that the primal and dual residuals converge to a value close to zero, as predicted by theory.\", \"6. Cross-reference the derived update equations with an authoritative source, such as the monograph by Boyd et al., to check for correctness.\"], \"handoff_plan\": \"/math derives the equations -> /general-search finds a canonical reference -> /math verifies its derivation against the reference (fallback: re-derive if a discrepancy is found) -> /code implements the verified equations and generates plots -> router QA (verification: plots show clear convergence and residual norms in `metrics.json` are below a small epsilon; fallback: flag the code implementation for debugging).\", \"todo_list\": [\"- [ ] Use /math to derive the ADMM update equations for the Lasso problem.\", \"- [ ] Use /general-search to find the canonical Boyd et al. ADMM monograph and another relevant paper.\", \"- [ ] Verify that the derivation from /math matches the formulation in the reference materials.\", \"- [ ] Use /code to implement the verified algorithm, generate synthetic data, and produce convergence plots.\", \"- [ ] Submit all artifacts (derivation, code, plots, report, metrics) for final router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"convex-optimization\", \"admm\", \"lasso-regression\"], \"acceptance_criteria\": [\"The mathematical derivation of the ADMM updates is correct, clearly explained, and matches the standard formulation.\", \"The Python code is well-commented and successfully converges on a generated synthetic dataset.\", \"The generated plots clearly illustrate the convergence of both primal and dual residuals toward zero.\", \"The final report cites at least two authoritative sources on ADMM, including Boyd's monograph, with correct formatting.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2013, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for dictionary learning using the Method of Optimal Directions (MOD). Formulate the problem as a non-convex optimization with a sparsity-inducing L1-norm penalty on the coefficients. Then, implement this in Python, validate its convergence on a synthetic dataset of sparse signals, and find literature connecting the learned dictionary's coherence to the Restricted Isometry Property (RIP) for compressed sensing recovery guarantees.\n", "completion": "{\"route_plan\": [\"/math(Derive the alternating minimization update rules for the Method of Optimal Directions, analyzing the Lagrangian for both the sparse coding and dictionary update steps and discussing convergence criteria.)\", \"/code(Implement the MOD algorithm in Python using NumPy. Validate the implementation by plotting reconstruction error vs. iteration on a synthetic dataset, ensuring the sparsity constraint is met via Orthogonal Matching Pursuit and checking eigenvalue stability of the dictionary atoms.)\", \"/general-search(query=\\\"site:arxiv.org Method of Optimal Directions K-SVD dictionary learning convergence proof coherence Restricted Isometry Property\\\", mode=web)\"], \"route_rationale\": \"/math is needed for the formal derivation of the MOD update rules. /code is required to implement the algorithm and empirically verify the theoretical convergence properties. The 'blue' /general-search agent will then be tasked with finding seminal papers that provide theoretical justification and link dictionary properties like coherence to compressed sensing performance.\", \"expected_artifacts\": [\"A LaTeX-formatted derivation of the MOD algorithm's update rules.\", \"A Python script implementing MOD and generating a synthetic dataset for testing.\", \"A plot of the Mean Squared Error (MSE) of signal reconstruction versus algorithm iteration.\", \"A summary report citing at least two papers on dictionary learning and its connection to compressed sensing theory.\", \"Verification output showing the final dictionary's coherence value.\"], \"thinking_outline\": [\"1. Formulate the dictionary learning problem as a non-convex optimization with an L0-norm pseudo-norm regularizer for the sparse codes.\", \"2. Derive the alternating minimization update steps for the Method of Optimal Directions (MOD), one for the sparse codes (e.g., using OMP) and one for the dictionary atoms.\", \"3. Implement the MOD algorithm in Python, including a synthetic data generation process with a known ground-truth dictionary.\", \"4. Validate the implementation by running it on the synthetic data and plotting the reconstruction error to check for convergence.\", \"5. Verify that the mathematical derivation of the dictionary update step correctly corresponds to the implemented code, particularly the pseudo-inverse operation.\", \"6. Search for and synthesize literature connecting the coherence of the learned dictionary to the Restricted Isometry Property (RIP) and its implications for signal recovery.\"], \"handoff_plan\": \"/math (derivation) -> /code (implementation & empirical validation). /code output (plots, coherence value) and /math derivation are handed to /general-search for contextualization and finding theoretical guarantees. Final artifacts from all tools are passed to router QA. Verification: /code must numerically match /math's theoretical convergence expectations. Fallback: If convergence fails in /code, revisit the /math derivation for potential errors in the update rule logic.\", \"todo_list\": [\"- [ ] /math: Derive the update rules for the dictionary D and sparse codes X in the MOD algorithm, starting from the objective function min ||Y - DX||_F^2 s.t. ||x_i||_0 <= k for all i.\", \"- [ ] /code: Implement the MOD algorithm using NumPy/SciPy, including a sparse coding step with Orthogonal Matching Pursuit (OMP).\", \"- [ ] /code: Generate a synthetic dataset with a known ground-truth dictionary and sparse signals, and plot the reconstruction MSE per iteration.\", \"- [ ] /math: Verify that the matrix operations in the /code implementation for the dictionary update step correctly implement the derived pseudo-inverse solution.\", \"- [ ] /code: Verify the final learned dictionary by calculating its coherence and comparing it to theoretical bounds.\", \"- [ ] /general-search: Find and summarize at least two key papers (e.g., from Elad, Aharon, Bruckstein) that discuss MOD or K-SVD and their relation to compressed sensing.\", \"- [ ] /router: Perform final QA check on all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"The mathematical derivation of MOD update rules must be complete and correct.\", \"The Python implementation must converge on a synthetic dataset, demonstrated by a monotonically decreasing reconstruction error plot.\", \"The literature summary must correctly cite at least two relevant academic papers with arXiv IDs or DOIs, explaining the link between dictionary coherence and the RIP.\", \"The implemented code must pass a verification check against the mathematical derivation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2024, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the connection between score-based generative modeling and the Fokker-Planck equation on a Riemannian manifold. Specifically, analyze how the manifold's curvature tensor influences the convergence properties of the reverse-time SDE and the spectral properties of the diffusion process operator. Provide a proof for the de Rham-Hodge decomposition of the score field.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative model riemannian manifold Fokker-Planck\\\", mode=both)\", \"/math(Derive the Fokker-Planck equation for a diffusion process on a Riemannian manifold and relate its generator to the Laplace-Beltrami operator, considering the influence of Bayesian priors.)\", \"/code(Implement a toy diffusion model in Python using JAX on a sphere to visualize the score field, validate the derived dynamics, and leverage CUDA acceleration for sampling.)\", \"/math(Prove the de Rham-Hodge decomposition for the score field, connecting it to the Helmholtz decomposition, and analyze the Hessian eigenvalue spectrum's dependence on manifold curvature.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on manifold diffusion. This informs the core theoretical derivation in /math. A /code step provides an empirical validation of the theory. The final /math step tackles the advanced proof requested in the prompt, building upon the initial derivation and code insights.\", \"expected_artifacts\": [\"A formal mathematical proof detailing the derivation of the Fokker-Planck equation on a Riemannian manifold and the de Rham-Hodge decomposition of the score field.\", \"Python code implementing a toy diffusion model on a 2-sphere, including visualization of the learned score field.\", \"A summary report citing at least two relevant arXiv papers with IDs, explaining the theoretical results and simulation outcomes.\", \"A verification log confirming the consistency between the mathematical derivation and the numerical simulation's behavior.\"], \"thinking_outline\": [\"1. Establish the mathematical formalism for Stochastic Differential Equations (SDEs) on Riemannian manifolds, defining the drift and diffusion terms with respect to the manifold's metric tensor.\", \"2. Derive the forward and reverse-time Fokker-Planck equations corresponding to the manifold SDE, explicitly showing the dependency on the Ricci curvature.\", \"3. Validate the derivation by specializing the general manifold case to the Euclidean case and checking for consistency with established results for standard diffusion models.\", \"4. Formulate and prove the de Rham-Hodge decomposition of the score vector field, identifying the conservative (gradient) and solenoidal (curl) components.\", \"5. Implement a numerical simulation for a toy example (e.g., diffusion on a 2-sphere) to visualize the score field and empirically verify that the learned score aligns with the gradient of the log-density.\", \"6. Audit the proof for logical soundness and ensure all assumptions, such as manifold smoothness and boundary conditions, are explicitly stated.\"], \"handoff_plan\": \"/general-search -> /math (verification: check if sourced papers provide a sufficient basis for derivation) -> /code (verification: implement derived equations and check for numerical stability; fallback: simplify manifold to Euclidean space if unstable) -> /math (verification: ensure proof uses definitions consistent with initial derivation) -> router QA (verification: all artifacts are consistent and acceptance criteria are met; fallback: re-route to /math for proof correction).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on diffusion models on manifolds, focusing on Fokker-Planck and score matching.\", \"- [ ] /math: Derive the Fokker-Planck equation and its generator for a Riemannian diffusion process.\", \"- [ ] /code: Implement a toy diffusion model on a sphere in Python (JAX) to simulate the derived dynamics.\", \"- [ ] /math: Formulate and prove the de Rham-Hodge decomposition for the score field.\", \"- [ ] /math: Verify that the Euclidean specialization of the manifold derivation matches standard diffusion theory.\", \"- [ ] /code: Validate the implementation by comparing the learned score field against the true gradient of the target log-density.\", \"- [ ] /router: Perform final QA review, checking proof correctness, code functionality, and artifact consistency.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"differential-geometry\", \"stochastic-calculus\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Fokker-Planck equation correctly incorporates the Laplace-Beltrami operator and curvature terms.\", \"- The proof of the de Rham-Hodge decomposition is logically sound and complete.\", \"- The Python simulation successfully learns a score field that qualitatively matches the gradient of the target density on the sphere.\", \"- The final report cites at least two relevant arXiv pre-prints.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2033, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a physics-informed neural network (PINN) to solve the 2D steady-state incompressible Navier-Stokes equations for a lid-driven cavity problem. Please compare the convergence properties of a soft vs. hard enforcement of Dirichlet boundary conditions and detail the formulation of the residual-based loss function using automatic differentiation for the momentum and continuity equations. How does the weighting of the loss components affect the final solution's physical plausibility and convergence rate?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org physics informed neural network navier-stokes loss function convergence bayesian\\\", mode=web)\", \"/math(Derive the strong and weak forms of the 2D Navier-Stokes PDE residuals and formulate the composite loss function, incorporating Euler-Lagrange principles for the variational form and considering potential Fourier feature mappings for input encoding.)\", \"/code(Implement the PINN in JAX with CUDA support, defining the network architecture, the composite loss from the math derivation, and an Adam optimizer. Ensure the training loop tracks convergence metrics and logs PDE residuals., using Python)\", \"/math(Verify the correctness of the automatic differentiation implementation by comparing computed gradients for the PDE residual against a symbolic differentiation result for a simplified case.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey current SOTA literature on PINN loss functions for fluid dynamics. This informs the /math agent's formal derivation of the PDE residual and composite loss. The /code agent then implements this mathematical model. A final /math call is crucial for verifying that the code's automatic differentiation correctly computes the complex gradients specified in the derivation, preventing implementation bugs.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Navier-Stokes residual and the composite PINN loss function for both soft and hard boundary constraints.\", \"- A documented Python script in JAX or PyTorch implementing the PINN solver for the lid-driven cavity problem.\", \"- A summary report comparing convergence behavior, including plots of the loss curve, predicted velocity/pressure fields, and residual heatmaps.\", \"- A verification log confirming the consistency between the mathematical derivation and the code's gradient computation.\", \"- A bibliography with at least two cited arXiv papers on PINNs for fluid dynamics.\"], \"thinking_outline\": [\"1. Formulate the 2D steady-state incompressible Navier-Stokes equations (continuity and momentum) in their differential residual form, suitable for a PINN loss function.\", \"2. Design a multi-output neural network architecture that takes spatial coordinates (x, y) as input and outputs the velocity components (u, v) and pressure (p).\", \"3. Derive the composite physics-informed loss function, representing a weighted sum of the mean squared error of the PDE residuals at interior collocation points and the mean squared error at the boundary condition points.\", \"4. Implement the complete PINN model and training loop, leveraging automatic differentiation to compute all required partial derivatives for the PDE residuals from the network's output.\", \"5. Validate the trained model by computing the L2 norm of the PDE residual on a fine grid of test points and visually inspecting the predicted velocity streamlines for physical plausibility.\", \"6. Audit the final solution's adherence to the incompressibility constraint (div(v) \u2248 0) across the domain.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results provide foundational papers for the mathematical derivation. The code agent implements the precise formulation from the math agent. A final math verification step audits the code's gradient calculations. Fallback: If the /code agent's loss stagnates unexpectedly, it flags a potential issue in the loss formulation, triggering a review by the /math agent.\", \"todo_list\": [\"- [ ] /general-search: Find >=2 recent, highly-cited arXiv papers on PINN architectures and loss functions for Navier-Stokes equations.\", \"- [ ] /math: Formally derive the PDE residual for the 2D steady-state Navier-Stokes equations and define the composite loss function.\", \"- [ ] /code: Implement the PINN architecture and training loop in JAX, using the derived loss function.\", \"- [ ] /math: Verify that the gradients computed via automatic differentiation in the code match a symbolic derivation for a simplified test case.\", \"- [ ] /code: Train the model until the validation loss plateaus, saving model checkpoints and generating plots for velocity, pressure, and loss history.\", \"- [ ] /code: Write a script to evaluate the final model's mean PDE residual on a 256x256 grid of test points.\", \"- [ ] router: Perform a final QA review of all artifacts, checking for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"partial-differential-equations\", \"computational-fluid-dynamics\"], \"acceptance_criteria\": [\"- The implemented PINN must achieve a final mean PDE residual below 1e-4 on the validation collocation points.\", \"- The final report must include visualizations of the velocity field (streamlines) and pressure field that are qualitatively consistent with known results for the lid-driven cavity problem.\", \"- The solution must satisfy the incompressibility constraint, with the mean absolute divergence of the velocity field being less than 1e-3.\", \"- The report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2072, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive the Trust Region Policy Optimization (TRPO) surrogate objective function from the policy gradient theorem with a value function baseline. Please provide the full Lagrangian derivation for the KL-divergence constrained optimization problem and then implement it in Python to solve a classic control theory environment like Pendulum-v1. The implementation must use the conjugate gradient method to approximate the Fisher-vector product, not a simpler first-order method, and show proof of convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Trust Region Policy Optimization' 'natural policy gradient' Lagrangian dual\\\", mode=web)\", \"/math(Derive the TRPO surrogate objective with a state-dependent baseline from first principles. Formulate the optimization problem with a KL-divergence constraint and derive its Lagrangian, analyzing the convergence criteria for the conjugate gradient step.)\", \"/code(Implement the TRPO algorithm in Python with PyTorch for the Pendulum-v1 control theory environment. The implementation must include a conjugate gradient solver for the Hessian-vector products and log the KL-divergence for convergence analysis.)\", \"/math(Audit the Python code's numerical stability. Verify that the implemented conjugate gradient updates correctly approximate the solution to the Lagrangian subproblem and that the empirical KL-divergence aligns with the theoretical trust region constraint.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational academic papers on TRPO. These papers inform the /math agent's rigorous derivation of the core algorithm. The /code agent then translates this mathematical framework into a working Python implementation. A final /math step is crucial for verification, ensuring the code's numerical behavior correctly reflects the derived theory.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the TRPO algorithm from the policy gradient theorem, including the Lagrangian formulation.\", \"- A Python script implementing TRPO for the Pendulum-v1 environment, with the conjugate gradient solver clearly separated.\", \"- A final report (PDF) summarizing the experimental results, including plots of mean episode return and KL-divergence over training steps.\", \"- A list of citations including at least two arXiv papers on policy optimization.\"], \"thinking_outline\": [\"1. Review foundational literature on policy gradients, natural policy gradients, and Trust Region Policy Optimization (TRPO) to establish the theoretical basis.\", \"2. Formally derive the surrogate objective function for TRPO, showing how the KL-divergence constraint is incorporated using a Lagrangian multiplier to ensure policy stability.\", \"3. Implement the complete TRPO algorithm, paying special attention to the efficient computation of the Fisher-vector product using the conjugate gradient method.\", \"4. Validate the implementation by running experiments on a standard control theory benchmark (e.g., Pendulum-v1) and plotting the average return and the average KL-divergence per policy update.\", \"5. Check that the empirical KL-divergence consistently remains near the target delta value, verifying that the trust region constraint is effectively enforced by the algorithm.\", \"6. Audit the final results to confirm that the observed learning stability and performance are consistent with published TRPO benchmarks.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The initial search provides papers for the formal derivation. The code agent implements this derivation. The final math agent audits the code against the theory, checking numerical outputs. Fallback: If the verification step fails (e.g., KL-divergence is not constrained), the task is returned to the /code agent with specific feedback for debugging the conjugate gradient or line search implementation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on TRPO and natural policy gradients (e.g., Schulman et al.).\", \"- [ ] /math: Provide a step-by-step derivation of the TRPO objective function and its Lagrangian formulation.\", \"- [ ] /code: Implement the TRPO agent, including actor and critic networks, GAE, and the conjugate gradient solver.\", \"- [ ] /code: Train the agent in the specified environment and generate plots for mean return and KL-divergence.\", \"- [ ] /math: Verify that the average KL-divergence reported by the code stays within a small epsilon of the target trust region delta.\", \"- [ ] /math: Numerically check the correctness of a single conjugate gradient step against a known analytical solution for a small quadratic.\", \"- [ ] /router: Perform final QA, ensuring the mathematical derivation, code implementation, and empirical results are consistent and well-documented.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"constrained-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation must correctly formulate the TRPO problem using Lagrange multipliers and derive the dual function.\", \"- The final agent's average return over the last 10 episodes must meet or exceed the established benchmark for TRPO on Pendulum-v1.\", \"- The generated plot of KL-divergence must show that the per-update divergence stays consistently close to the hyperparameter delta.\", \"- The code must implement the conjugate gradient algorithm, not a first-order approximation like gradient descent, for the policy update.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2088, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a variational inference framework for a latent stochastic differential equation (SDE) model. Please derive the continuous-time evidence lower bound (ELBO) by applying Girsanov's theorem to define the variational posterior over trajectories. Then, implement this in Python using a suitable SDE solver like the Stochastic Runge-Kutta method, focusing on the numerical stability of the KL-divergence computation between the prior and posterior SDEs. The goal is a Bayesian deep learning approach for generative time-series modeling.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference latent stochastic differential equations Girsanov theorem bayesian\\\", mode=rag)\", \"/math(Derive the evidence lower bound (ELBO) for a latent SDE using a change of measure via Girsanov's theorem. Detail the resulting KL-divergence term between the prior and approximate posterior path measures for this Bayesian model.)\", \"/code(Implement the Neural SDE model and the variational inference training loop in Python with torchsde. Use an SRK solver, track ELBO convergence, and compute posterior predictive log-likelihood (PPL) on a held-out dataset.)\", \"/math(Verify the correctness of the KL-divergence term's implementation by comparing the automatic differentiation gradients from the code with the analytically derived gradients for a simplified linear SDE case.)\"], \"route_rationale\": \"The blue general-search agent first gathers foundational literature on variational SDEs. /math then formally derives the objective function (ELBO). /code implements this complex Bayesian model and training procedure. A final /math step provides crucial verification by cross-checking the analytical derivation against the computational graph's gradients, ensuring implementation correctness.\", \"expected_artifacts\": [\"- A PDF report containing the full mathematical derivation of the continuous-time ELBO using Girsanov's theorem.\", \"- A well-commented Python script implementing the variational latent SDE model.\", \"- A JSON file logging the ELBO and posterior predictive log-likelihood during training.\", \"- A verification summary confirming the alignment between the analytical and numerical gradients.\", \"- A bibliography with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Research state-of-the-art variational inference techniques for continuous-time Bayesian models, specifically focusing on Neural SDEs and the application of Girsanov's theorem.\", \"2. Formally derive the evidence lower bound (ELBO) for a latent SDE, clearly defining the prior and variational posterior processes and the resulting KL-divergence over path measures.\", \"3. Implement the model architecture in Python, defining the neural networks for the drift and diffusion terms of the SDEs and selecting an appropriate numerical SDE solver.\", \"4. Construct the variational training objective in code, ensuring the KL-divergence term is computed correctly and is numerically stable.\", \"5. Validate the code's correctness by creating a simplified test case (e.g., an Ornstein-Uhlenbeck process) and verifying that the implementation's gradients match the analytical gradients.\", \"6. Audit the final model's performance by analyzing the ELBO convergence curve and the qualitative nature of samples drawn from the generative model.\"], \"handoff_plan\": \"/general-search (papers) -> /math (derivation) -> /code (implementation) -> /math (verification). The final verification step is critical; if the numerical gradients do not match the analytical ones, the fallback is to debug the /code implementation of the KL-divergence term, potentially reverting to the /math agent to re-check the derivation for typos.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key arXiv papers on variational inference for Neural SDEs (verify results)\", \"- [ ] /math: Produce a full derivation of the ELBO for a latent SDE model.\", \"- [ ] /code: Implement the Neural SDE architecture, SDE solver, and data loaders.\", \"- [ ] /code: Implement the variational objective function and the training loop, including metric logging.\", \"- [ ] /math: Formulate and execute the verification plan comparing numerical and analytical gradients.\", \"- [ ] /code: Run the final experiment, generate plots for the report, and save the metrics JSON.\", \"- [ ] /router: Perform final QA review of all artifacts for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-differential-equations\", \"generative-models\"], \"acceptance_criteria\": [\"- The mathematical derivation in the report correctly applies Girsanov's theorem to derive the KL divergence between path measures.\", \"- The Python code runs without errors, and the reported ELBO shows a clear, non-trivial improvement over at least 50 training epochs.\", \"- The verification report explicitly shows the comparison between numerical and analytical gradients, with a relative error below a small threshold (e.g., 1e-4).\", \"- The final report includes citations for at least two relevant arXiv papers, with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Maximize this value on the training set. Report the final converged value and plot its curve over epochs.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Evaluate on a held-out test set after training. Higher is better. Compute by averaging the log-probability of test data under the posterior predictive distribution.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2101, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the convergence properties of low-rank LoRA-based fine-tuning for Vision Transformers. Derive the Hessian eigenvalue spectrum for the attention update matrix under a Frobenius norm constraint and propose a novel regularizer based on Bayesian principles to improve generalization. Implement this in PyTorch and validate its performance against standard AdamW on a CIFAR-100 benchmark.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'LoRA' 'Hessian eigenvalue' 'Bayesian regularization' transformer convergence\\\", mode=rag)\", \"/math(Derive the Hessian for the LoRA update matrices based on the literature, formulate the Bayesian regularizer, and prove its expected effect on the eigenvalue spectrum and model convergence.)\", \"/code(Implement the proposed regularized LoRA layer and a training loop in PyTorch using CUDA for a Vision Transformer on CIFAR-100. Add logging for accuracy, loss, and empirical Hessian eigenvalues., using Python)\", \"/math(Analyze the empirical training logs from the code step, using bootstrap methods to generate a confidence interval for the performance gain and validate alignment with the theoretical convergence proof.)\"], \"route_rationale\": \"The blue /general-search agent is needed first to survey the latest arXiv literature on LoRA Hessian analysis. /math will then formalize these insights into a mathematical derivation. /code is required to implement the resulting algorithm and run experiments. A final /math step is crucial for verifying the alignment between theoretical convergence bounds and empirical results.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the Hessian and the Bayesian regularizer.\", \"A Python script implementing the regularized LoRA fine-tuning for a Vision Transformer.\", \"A JSON file containing training logs, final accuracy metrics, and Hessian eigenvalue distributions.\", \"A summary report citing at least 3 relevant arXiv papers and comparing theoretical vs. empirical convergence.\"], \"thinking_outline\": [\"1. Review current literature on low-rank adaptation (LoRA) and spectral properties of neural network Hessians.\", \"2. Formulate the optimization problem for LoRA fine-tuning and derive the Hessian of the attention update matrices with respect to the low-rank factors.\", \"3. Propose a Bayesian regularizer based on the derived Hessian's spectral properties to encourage faster convergence and better generalization.\", \"4. Implement the custom regularizer within a PyTorch training loop for a Vision Transformer and validate the implementation's numerical stability and correctness against a baseline.\", \"5. Execute training runs on CIFAR-100, collecting performance metrics and empirical Hessian eigenvalue data.\", \"6. Verify that the empirical convergence rate and Hessian spectrum align with the theoretical bounds predicted by the mathematical analysis.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: final /math step validates that empirical results from /code align with initial /math derivation; fallback: if divergence is found, /code agent must re-audit implementation for numerical errors or incorrect gradient computation).\", \"todo_list\": [\"- [ ] /general-search: Find recent arXiv papers on LoRA, Hessian spectral analysis, and Bayesian regularization for transformers.\", \"- [ ] /math: Derive the Hessian for the LoRA update matrices and formulate the optimization objective with the new Bayesian regularizer.\", \"- [ ] /code: Implement the proposed regularized LoRA layer and training loop in PyTorch for a Vision Transformer on CIFAR-100.\", \"- [ ] /code: Add logging to track accuracy, loss, and empirical Hessian eigenvalues during training.\", \"- [ ] /math: Verify that the derived theoretical convergence properties are consistent with the literature found by /general-search.\", \"- [ ] /math: Analyze the empirical training logs from /code to validate alignment with the theoretical convergence proof.\", \"- [ ] router: Perform final QA review of all artifacts, checking for consistency between derivation, code, and results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"llm-optimization\", \"spectral-theory\", \"bayesian-deep-learning\"], \"acceptance_criteria\": [\"The final report must include a complete mathematical derivation of the Hessian and the proposed Bayesian regularizer.\", \"The provided Python code must successfully run and achieve at least a 2% accuracy improvement over the baseline AdamW fine-tuning on the CIFAR-100 validation set.\", \"The analysis must demonstrate a clear link between the theoretical convergence bounds and the empirical results, supported by plots of the Hessian eigenvalue spectrum.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2116, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a safe reinforcement learning agent for a robotic manipulator. Please formulate the policy optimization problem as a Constrained Markov Decision Process (CMDP) and derive the Lagrangian dual. Then, research and implement a primal-dual algorithm, like Lagrangian-PPO, ensuring the cumulative safety cost remains below a specified threshold. The implementation should include a formal proof of convergence for the dual variable update under specific assumptions about the value function approximator.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org constrained markov decision process primal-dual reinforcement learning convergence\\\", mode=rag)\", \"/math(Derive the Lagrangian dual for the CMDP objective function, incorporating a cost-based safety constraint and referencing the expected reward from the chosen control theory policy on arXiv.)\", \"/code(Implement a primal-dual PPO algorithm in PyTorch for the CMDP, tracking safety constraint violations and reward convergence with CUDA support., using Python)\", \"/math(Prove the convergence of the Lagrangian multiplier's gradient ascent update, assuming Lipschitz continuity of the value function and a bounded eigenvalue for the Fisher Information Matrix, informed by our Bayesian confidence interval.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to find authoritative literature on CMDP primal-dual methods. This grounds the subsequent theoretical work in /math, where the core Lagrangian problem is derived. /code then implements the derived algorithm. A final call to /math is required to construct the formal convergence proof, a key user requirement that depends on both the theory and the specific implementation.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the Lagrangian dual for the CMDP objective.\", \"Python source code for the primal-dual safe RL agent, including environment setup and training scripts.\", \"A formal proof of convergence for the Lagrangian multiplier update, including all necessary lemmas and assumptions.\", \"A summary report citing at least two relevant arXiv papers on primal-dual methods for CMDPs.\", \"A verification log confirming that numerical gradients in the code match the analytical gradients from the derivation.\"], \"thinking_outline\": [\"1. Formulate the safe RL problem as a Constrained Markov Decision Process (CMDP), defining state, action, transition dynamics, reward function, and a safety cost function.\", \"2. Derive the Lagrangian relaxation of the CMDP's objective function, identifying the primal and dual variables and formulating the KKT conditions for optimality.\", \"3. Implement the chosen primal-dual algorithm (e.g., Lagrangian-PPO), ensuring the code structure separates the policy update (primal step) from the Lagrangian multiplier update (dual step).\", \"4. Verify that the implementation's gradient calculations for both policy and multiplier updates correctly reflect the derived Lagrangian objective.\", \"5. Construct the convergence proof for the dual variable update, clearly stating all assumptions (e.g., learning rates, function approximation properties) and showing the multiplier converges to a value that enforces the safety constraint.\", \"6. Validate the final implementation against a simple benchmark environment to demonstrate that the agent maximizes rewards while satisfying the safety cost threshold.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: code implementation must match the derived Lagrangian dual update rule; fallback: if convergence proof fails, /general-search for alternative assumptions or proof techniques.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on primal-dual algorithms for CMDPs from arXiv.\", \"- [ ] /math: Formulate the CMDP objective and derive its Lagrangian dual, clearly defining all terms.\", \"- [ ] /code: Implement the primal-dual policy optimization algorithm in PyTorch.\", \"- [ ] /code: Add logging for reward, safety cost, and the Lagrangian multiplier's value over training epochs.\", \"- [ ] /math: Construct the convergence proof for the dual variable update based on the implemented algorithm.\", \"- [ ] /math: Verify that the analytical gradients derived for the Lagrangian match the numerical gradients computed in the Python implementation.\", \"- [ ] /code: Validate the agent's behavior in a benchmark environment, ensuring safety constraints are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"constrained-optimization\", \"control-theory\"], \"acceptance_criteria\": [\"Pass: The submitted LaTeX document contains a complete and correct derivation of the Lagrangian dual.\", \"Pass: The Python code successfully trains an agent that respects the predefined safety cost constraint (cumulative cost < threshold) in a benchmark environment.\", \"Pass: The convergence proof is mathematically sound and its assumptions are clearly stated and justified.\", \"Fail: The agent violates the safety constraint, or the proof of convergence is invalid.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2123, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and evaluate a structured variational inference method for a Bayesian neural network. Specifically, I want to use a matrix-variate normal posterior approximation for weights in each layer to capture inter-parameter correlations, rather than the typical mean-field assumption. Please derive the Evidence Lower Bound (ELBO), implement the model, and compare its convergence and predictive uncertainty against a standard diagonal Gaussian (mean-field) VI baseline on a benchmark dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org structured variational inference 'matrix-variate normal' Bayesian neural network\\\", mode=both)\", \"/math(Derive the Evidence Lower Bound (ELBO) for a Bayesian neural network using a matrix-variate normal posterior over weights, ensuring the KL-divergence term is tractable and analyzing convergence conditions.)\", \"/code(Implement the derived structured Bayesian VI model and a mean-field baseline in Pyro, evaluating on a classification task to compare posterior predictive accuracy and AUPRC metrics., using Python)\", \"/math(Verify the reparameterization trick gradient implementation for the matrix-variate family via numerical differentiation and analyze the eigenvalue spectrum of the estimated covariance to diagnose convergence issues.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to ground the approach in established literature from arXiv. The first /math agent then formalizes the objective function (ELBO) based on the research. The /code agent implements this formal model and a baseline for empirical comparison. A final /math agent is used for rigorous verification of the custom gradient implementation and deeper analysis of the model's learned parameters, ensuring correctness and stability.\", \"expected_artifacts\": [\"- A complete mathematical derivation of the ELBO for the matrix-variate posterior.\", \"- A Python project in a git repository implementing both the structured and mean-field VI models.\", \"- A comparative analysis report in PDF format with plots showing ELBO convergence, predictive accuracy, and uncertainty calibration (e.g., reliability diagrams).\", \"- A `references.bib` file citing at least three relevant arXiv papers.\", \"- A verification script output confirming the analytical gradients match numerical approximations.\"], \"thinking_outline\": [\"1. Research established literature on structured variational inference, specifically using matrix-variate normal posteriors for weights in Bayesian neural networks.\", \"2. Formulate the ELBO for the BNN with the specified structured posterior, carefully deriving the analytical form of the KL-divergence between the matrix-variate variational distribution and the prior.\", \"3. Implement the reparameterization trick for the matrix-variate normal distribution to enable low-variance gradient estimation for the variational parameters.\", \"4. Validate the correctness of the custom gradient implementation using numerical differentiation checks (e.g., finite differences) within the chosen deep learning framework.\", \"5. Train both the structured and mean-field models on a suitable benchmark dataset (e.g., MNIST/CIFAR-10), systematically comparing their ELBO convergence, predictive performance, and quality of uncertainty estimates.\", \"6. Audit the trained structured model's covariance matrices to check for potential pathologies like rank collapse or numerical instability that might affect performance.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The handoff from /code to the second /math is critical for verification; the /code agent must provide access to the gradient computation functions. Verification failure triggers a fallback: the /math agent will instruct the /code agent to add regularization to the covariance matrices or simplify the variational family and rerun.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 3-5 seminal papers on matrix-variate posteriors in Bayesian neural networks from arXiv.\", \"- [ ] /math: Produce a LaTeX document with the full derivation of the ELBO and its gradients with respect to the variational parameters.\", \"- [ ] /code: Implement the Bayesian neural network layers for both mean-field and matrix-variate posteriors in Pyro/PyTorch.\", \"- [ ] /code: Write a training and evaluation script to compare the models on the MNIST dataset, tracking metrics.\", \"- [ ] /math: Develop and run a gradient checking script to verify the reparameterization trick implementation in the code.\", \"- [ ] /code: Generate all plots specified in the expected artifacts, including ELBO curves and reliability diagrams.\", \"- [ ] /router: Review all generated artifacts against the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"- The provided mathematical derivation of the ELBO for the matrix-variate case must be complete and correct.\", \"- The implemented code must be runnable and successfully train both models, with the structured model achieving a final ELBO value greater than or equal to the mean-field baseline.\", \"- The final report must contain plots demonstrating superior (or equivalent) performance and a tighter ELBO for the structured VI approach.\", \"- The repository must contain unit tests verifying the sampling and log-probability calculations for the custom matrix-variate distribution.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2124, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive the update rules for a gauge-equivariant graph neural network that learns Hamiltonian dynamics for molecular systems. The model must preserve the symplectic structure of phase space to ensure long-term energy conservation. Please provide a formal proof of its symplectic nature by analyzing the Jacobian of the learned flow map and discuss its connection to Lie group equivariance and Noether's theorem. Analyze the convergence properties of this custom integrator.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org gauge equivariant graph neural networks Hamiltonian dynamics symplectic integrators Lie algebra\\\", mode=both)\", \"/math(Derive the GNN update equations preserving the Poisson bracket structure of the Hamiltonian system. Prove the resulting flow is a symplectic map and analyze the convergence criteria for the numerical integration scheme.)\", \"/code(Implement a JAX prototype of the symplectic GNN integrator using CUDA for acceleration. Include functionality to track total system energy and momentum over simulation time, adding runtime validation checks for Hamiltonian conservation., using Python)\", \"/math(Verify the implemented code's conservation properties by analyzing the numerical Jacobian's deviation from a symplectic matrix and comparing the model's long-term energy drift against a baseline Verlet integrator. Check for numerical stability near eigenvalue degeneracies.)\"], \"route_rationale\": \"The 'blue' /general-search agent is used first to gather state-of-the-art literature on equivariant GNNs and Hamiltonian mechanics. /math is then used for the core theoretical derivation and proof. /code translates the derived mathematics into a functional JAX prototype. A final /math step provides rigorous numerical verification of the implementation against the theoretical guarantees, a critical step for physics-based models.\", \"expected_artifacts\": [\"- A PDF document containing the full mathematical derivation of the symplectic GNN, including the proof of symplecticity and the convergence analysis.\", \"- A Python script (`symplectic_gnn.py`) with the JAX implementation of the model.\", \"- A verification report (`verification_report.md`) summarizing the numerical analysis, including plots of energy drift over time compared to a baseline.\", \"- A bibliography file (`references.bib`) with at least 3 citations from arXiv.\", \"- A JSON file (`metrics.json`) reporting the final energy drift and computational throughput.\"], \"thinking_outline\": [\"1. Research existing literature on Hamiltonian Neural Networks, SE(3)-equivariant GNNs, and the application of geometric deep learning to physical simulations, focusing on symplectic and energy-conserving properties.\", \"2. Formulate the molecular system's Hamiltonian, defining the canonical coordinates and the Poisson bracket structure on the system's phase space manifold.\", \"3. Derive the GNN's update equations by parameterizing a canonical transformation that approximates the time-evolution operator, ensuring the learned map is symplectic by construction.\", \"4. Prove that the Jacobian of the one-step network update is a symplectic matrix, thus guaranteeing the preservation of phase space volume and the Hamiltonian's invariants.\", \"5. Validate the theoretical derivation by implementing the model and running a long-term simulation of a benchmark molecular system (e.g., a chain of interacting particles).\", \"6. Audit the numerical results to verify that the total energy drift remains bounded and is significantly lower than that of a non-symplectic baseline like a standard Runge-Kutta integrator.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation) -> /math (numerical verification) -> router QA (verification: Cross-reference the implemented update rule in the JAX code against the derived equations in the PDF; fallback: If inconsistent, return to /math for correction of the derivation or /code for implementation fixes).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22653 key papers on equivariant GNNs for Hamiltonian systems from arXiv.\", \"- [ ] /math: Produce the complete derivation of the symplectic GNN update rules and the associated proofs.\", \"- [ ] /code: Implement the derived model in JAX, ensuring it can run on a GPU via CUDA.\", \"- [ ] /code: Add logging to track the Hamiltonian (total energy) at each simulation step.\", \"- [ ] /math: Perform the numerical analysis on the output of the code, generating plots of energy drift.\", \"- [ ] /math: Verify that the implemented JAX functions align with the derived mathematical formalism from the first /math step.\", \"- [ ] /router: Review all artifacts for consistency, correctness, and adherence to the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"geometric-deep-learning\", \"molecular-dynamics\", \"hamiltonian-mechanics\"], \"acceptance_criteria\": [\"- The provided mathematical proof of symplecticity must be complete and mathematically rigorous.\", \"- The JAX implementation must correctly implement the equations from the derivation and run without errors.\", \"- The verification report must show that the model's total energy drift over a 1000-step simulation is at least an order of magnitude smaller than a non-symplectic baseline (e.g., RK4).\", \"- The final report must cite at least 3 relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2165, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on a sparse attention mechanism for a transformer model and need to formulate it as a constrained optimization problem. My goal is to maximize the Shannon entropy of the attention distribution to encourage exploration, subject to an L1-norm budget for sparsity and a Lipschitz continuity constraint on the attention score function to ensure robustness. Please derive the complete Lagrangian, state the Karush-Kuhn-Tucker (KKT) conditions for optimality, and then search for recent arXiv papers discussing similar non-convex optimization approaches with convergence guarantees.\n", "completion": "{\"route_plan\": [\"/math(Formulate the Lagrangian for maximizing Shannon entropy of an attention distribution under L1-norm sparsity and Lipschitz continuity constraints. Derive the full set of Karush-Kuhn-Tucker (KKT) conditions for this non-convex optimization problem, paying close attention to the subgradient for the L1 term and the dual variables associated with the inequality constraints and potential convergence issues.)\", \"/general-search(query=\\\"site:arxiv.org non-convex constrained optimization convergence guarantees sparse attention Lipschitz\\\", mode=both)\", \"/code(Implement a Python function using PyTorch to solve the derived optimization problem numerically via a projected gradient descent or ADMM approach. The function should take attention logits as input and output the optimized sparse attention weights, while tracking convergence and constraint violation metrics like the L1 norm.)\"], \"route_rationale\": \"The task first requires a formal mathematical derivation of the optimization problem, making /math the ideal starting point. To address the user's request for recent literature on convergence, the 'blue' general-search agent is tasked with querying arXiv via /general-search. Finally, /code is needed to implement a numerical solver based on the mathematical formulation, providing a practical solution.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the Lagrangian and KKT conditions.\", \"A Python script implementing the numerical solver with PyTorch.\", \"A summary report citing at least 2 relevant arXiv papers on non-convex optimization for sparse models.\", \"A verification log comparing the numerical solution's optimality conditions against the derived KKT conditions.\", \"A plot showing the convergence of the objective function and constraint violations over solver iterations.\"], \"thinking_outline\": [\"1. Define the objective function (Shannon entropy) and the constraints (L1-norm budget, Lipschitz continuity) mathematically.\", \"2. Construct the Lagrangian function by introducing Lagrange multipliers for each constraint.\", \"3. Derive the Karush-Kuhn-Tucker (KKT) conditions, including stationarity (using subgradients for the L1-norm), primal feasibility, dual feasibility, and complementary slackness.\", \"4. Validate the derived KKT conditions for a simple, low-dimensional case to ensure correctness before implementation.\", \"5. Research and select an appropriate numerical optimization algorithm (e.g., Projected Gradient Descent, ADMM) suitable for this non-convex, constrained problem, referencing findings from the literature search.\", \"6. Implement the solver and verify that the final solution approximately satisfies the KKT conditions, checking for convergence and constraint satisfaction.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: /code agent verifies that the numerical solution satisfies the KKT conditions derived by /math, within a small tolerance; fallback: if convergence fails, /code agent flags the issue and requests /math to check for convexity assumptions or /general-search to find alternative algorithms.)\", \"todo_list\": [\"- [ ] /math: Formulate the constrained optimization problem: objective function (Shannon entropy) and constraints (L1 norm, Lipschitz).\", \"- [ ] /math: Construct the Lagrangian and derive the complete KKT optimality conditions.\", \"- [ ] /general-search: Find >=2 recent arXiv papers on convergence for non-convex optimization in sparse deep learning models.\", \"- [ ] /code: Implement a numerical solver (e.g., projected gradient descent) in PyTorch for the derived problem.\", \"- [ ] /code: Instrument the solver to track objective value and constraint violation metrics during optimization.\", \"- [ ] /math: Verify that the KKT derivation correctly handles the non-differentiable L1-norm via subgradients.\", \"- [ ] /code: Validate that the implemented solver's output for a test case approximately satisfies the KKT conditions from the /math derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"lagrangian-multipliers\", \"deep-learning-optimization\"], \"acceptance_criteria\": [\"The mathematical derivation of the Lagrangian and KKT conditions must be complete and mathematically sound.\", \"The Python implementation must converge on a test case and produce sparse attention weights that satisfy the L1 and Lipschitz constraints within a 1e-4 tolerance.\", \"The final report must cite at least two relevant arXiv papers with IDs and briefly summarize their applicability.\", \"The verification log must show that the numerical solution's KKT residuals are below a predefined threshold.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2166, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating non-stationary Gaussian Processes for Bayesian optimization. I need to derive a spectral mixture kernel whose spectral density is a mixture of Student's t-distributions, not Gaussians. Please provide the full derivation starting from Bochner's theorem, prove the resulting kernel is positive definite, and then implement it in JAX to model a function with heavy-tailed characteristics. The analysis must include a verification of the kernel's empirical eigenvalue decay against theoretical Fourier properties.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Bochner's theorem Gaussian Process spectral mixture kernel heavy-tailed spectral density\\\", mode=rag)\", \"/math(Derive the closed-form expression for a covariance kernel whose spectral density is a mixture of Student's t-distributions. Use Bochner's theorem and Fourier analysis to formally prove the resulting kernel is positive definite and real-valued for any valid Bayesian hyperparameter set.)\", \"/code(Implement the derived Student's t-spectral mixture kernel in JAX. Construct a Gaussian Process regression model, and train it on synthetic data exhibiting heavy-tailed noise. Track Negative Log Marginal Likelihood convergence and compute kernel matrix eigenvalue spectra., using Python)\", \"/math(Verify the numerical stability of the JAX implementation by analyzing the condition number of the computed kernel matrix. Validate that the empirical eigenvalue decay from the code aligns with the heavy-tailed properties of the theoretical Fourier dual derived earlier.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the task in existing literature on spectral kernels. Next, /math handles the core theoretical derivation and proof. Then, /code implements the derived mathematical object and runs experiments. Finally, a second /math call verifies the implementation's numerical properties against the theory, ensuring a closed loop of validation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Student's t-spectral mixture kernel, including the proof of positive definiteness, delivered as a LaTeX document.\", \"A Python script using JAX that implements the kernel, trains a Gaussian Process model, and generates plots of the model fit and eigenvalue spectrum.\", \"A summary report in Markdown comparing the theoretical properties of the kernel with the empirical results from the code, including convergence plots and spectral analysis.\", \"A verification log detailing the numerical stability checks and the alignment between theoretical and empirical eigenvalue decay.\", \"A JSON file containing the final model performance metrics (NLML, MSE) and hyperparameters.\"], \"thinking_outline\": [\"1. Review Bochner's theorem and the theory of characteristic functions to understand the relationship between a valid covariance kernel and its spectral density as a Fourier pair.\", \"2. Derive the characteristic function corresponding to the Student's t-distribution, which will be a key component of the kernel, likely involving modified Bessel functions.\", \"3. Formulate the full covariance function by taking the inverse Fourier transform of the mixture of t-distribution densities and prove its positive definiteness.\", \"4. Implement the derived kernel in code, paying close attention to numerical stability, especially when computing the kernel matrix for training the Gaussian Process.\", \"5. Verify that the empirical eigenvalues of the computed kernel matrix exhibit a power-law decay, which is the expected behavior for a process with a heavy-tailed spectral density, and check this against the theoretical derivation.\", \"6. Validate the entire pipeline by confirming the GP model successfully learns the target function and that the final metrics are within expected ranges for Bayesian models.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA (final review). The math derivation from the first /math agent must provide a clear, implementable formula for the /code agent. The /code agent's output (empirical eigenvalues) is the direct input for the second /math agent's verification task. Fallback: If the derivation is not numerically stable, the /math agent will be re-invoked to propose a stabilized approximation before handing off to /code.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on spectral mixture kernels and Bochner's theorem.\", \"- [ ] /math: Derive the Student's t-spectral mixture kernel and prove its properties.\", \"- [ ] /code: Implement the kernel and GP regression model in JAX.\", \"- [ ] /code: Generate synthetic data and run the GP model, logging metrics.\", \"- [ ] /math: Receive the empirical kernel matrix from /code.\", \"- [ ] /math: Verify the numerical properties and eigenvalue decay of the implementation against the theory.\", \"- [ ] /router: Perform QA check on all artifacts, ensuring consistency between derivation, code, and verification report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-processes\", \"gaussian-processes\"], \"acceptance_criteria\": [\"- The mathematical proof of positive definiteness for the derived kernel must be complete and logically sound.\", \"- The JAX implementation must run without numerical errors and achieve a lower Negative Log Marginal Likelihood than a standard RBF kernel on the provided heavy-tailed dataset.\", \"- The verification report must quantitatively show that the empirical eigenvalue decay of the kernel matrix aligns with the power-law behavior expected from the theoretical derivation.\", \"- All code must be documented and include unit tests for the kernel implementation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2169, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the complete coordinate ascent variational inference (CAVI) update equations for a Bayesian Latent Dirichlet Allocation (LDA) model. Start from the Evidence Lower Bound (ELBO) formulation for this specific generative model, showing all steps for the optimal variational parameters \u03b3 and \u03c6. Then, implement the CAVI algorithm in Python to validate the derivation. The implementation should monitor ELBO convergence on a synthetic corpus generated from known Dirichlet priors and multinomial distributions.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:jmlr.org site:arxiv.org Latent Dirichlet Allocation Blei 2003 'variational inference'\\\", mode=both)\", \"/math(Derive the Evidence Lower Bound (ELBO) and the CAVI update equations for the variational parameters \u03b3 (topic proportions) and \u03c6 (topic assignments) for a Bayesian LDA model, ensuring mathematical rigor and referencing convergence properties.)\", \"/code(Implement the derived CAVI algorithm in Python using NumPy. Generate a synthetic corpus, apply the variational updates iteratively, and include runtime validation to track the ELBO at each step to confirm convergence.)\", \"/math(Verify that the Python implementation of the ELBO calculation and the \u03c6/\u03b3 update rules precisely matches the formal mathematical derivation. Audit the logic for potential off-by-one errors or incorrect application of the digamma function., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find the canonical literature on LDA and variational inference, providing a solid theoretical foundation. The /math agent is then used to perform the core, complex derivation of the update equations. Next, the /code agent translates these equations into a working simulation to test the theory. A final /math step is crucial for verification, ensuring the code is a faithful implementation of the derived mathematics, which is a common failure point in such tasks.\", \"expected_artifacts\": [\"A PDF document containing the full LaTeX derivation of the ELBO and CAVI update equations.\", \"A Python script (`cavi_lda.py`) that implements the algorithm and can be executed to reproduce the results.\", \"A plot (`elbo_convergence.png`) showing the value of the ELBO on the y-axis versus the iteration number on the x-axis, demonstrating monotonic increase.\", \"A `report.md` file summarizing the results, including the final converged ELBO value and citing the foundational papers.\", \"A `metrics.json` file containing the primary and secondary metric values.\"], \"thinking_outline\": [\"1. Formulate the complete probabilistic graphical model for Latent Dirichlet Allocation, defining all random variables, parameters, and their dependencies.\", \"2. Write down the expression for the Evidence Lower Bound (ELBO) specific to the LDA model, using the mean-field assumption that the variational distribution q(\u03b8, z, \u03b2) factorizes.\", \"3. Derive the coordinate ascent update equation for the per-word topic assignment variational parameter \u03c6 by taking the functional derivative of the ELBO with respect to it and setting to zero.\", \"4. Derive the update equation for the per-document topic proportion variational parameter \u03b3 using the same optimization procedure.\", \"5. Validate the implementation by checking for monotonic convergence of the ELBO on a small, synthetic dataset where the ground truth parameters are known.\", \"6. Verify that the implemented update equations in the Python code are an exact match to the formulas derived in the mathematical steps.\"], \"handoff_plan\": \"/general-search ( foundational papers) -> /math (derivation) -> /code (implementation) -> /math (verification). The code agent's output is audited by the math agent against the initial derivation. Fallback: If verification fails, the math agent provides specific corrections and sends the task back to the /code agent for revision.\", \"todo_list\": [\"- [ ] /general-search: Find and download the original Blei et al. 2003 JMLR paper on LDA and a comprehensive tutorial on Variational Inference.\", \"- [ ] /math: Produce the full derivation for the CAVI update rules for \u03c6 and \u03b3, starting from the LDA model's joint distribution and the ELBO.\", \"- [ ] /code: Write a Python script to implement the CAVI algorithm, including synthetic data generation and ELBO tracking.\", \"- [ ] /code: Generate a plot demonstrating that the ELBO is monotonically increasing over iterations.\", \"- [ ] /math: Audit the Python code, comparing the implemented update logic line-by-line against the derived equations.\", \"- [ ] /math: Verify the ELBO calculation in the code matches the mathematical formula derived earlier.\", \"- [ ] /router: Review all artifacts for correctness, clarity, and fulfillment of acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-statistics\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"- Pass: The submitted mathematical derivation for the CAVI updates is complete and correct.\", \"- Pass: The Python code executes without errors and produces a plot showing the ELBO monotonically increasing to a converged value.\", \"- Pass: The final report correctly cites at least two authoritative sources on LDA or Variational Inference, with full arXiv or DOI identifiers.\", \"- Fail: The ELBO value decreases or oscillates during execution.\", \"- Fail: The implemented update equations in the code do not match the mathematical derivation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2179, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to understand the mathematical foundations of regularization in deep learning. Can you explain how the Lagrange multiplier method relates to L2 weight decay and provide a simple Python example that demonstrates this convergence?\n", "completion": "{\"route_plan\": [\"/math(Derive the solution for a simple linear regression model with an L2 norm constraint on the weights using the Euler-Lagrange equation, forming the Lagrangian and finding the gradient.)\", \"/code(Implement a Python script using NumPy to solve this regularized regression problem on synthetic data, visualizing the weight norm and loss convergence to validate the math.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation of L2 regularization as a constrained optimization problem via Lagrange multipliers. This derivation is then handed to /code to provide an empirical verification on a toy problem, demonstrating that the theoretical concept holds in practice and visualizing the outcome.\", \"expected_artifacts\": [\"A step-by-step mathematical derivation of the L2-regularized loss function using Lagrange multipliers, formatted in LaTeX.\", \"A self-contained Python script implementing the solution on a synthetic dataset.\", \"A plot generated by the script showing the effect of the regularization parameter on the learned model weights.\", \"A verification summary in the final report confirming that the L2 norm of the weights in the coded solution is constrained as predicted by the derivation.\"], \"thinking_outline\": [\"1. Formulate a simple learning problem (e.g., linear regression) as an unconstrained optimization problem minimizing mean squared error.\", \"2. Introduce an L2 norm constraint on the model's weights, transforming it into a constrained optimization problem.\", \"3. Construct the Lagrangian function for this problem by introducing a Lagrange multiplier \u03bb.\", \"4. Derive the optimal weights by finding the stationary points of the Lagrangian (i.e., setting its gradients to zero).\", \"5. Verify that the derived solution is equivalent to the standard L2 weight decay term (\u03bb||w||\u00b2) commonly seen in deep learning loss functions.\", \"6. Implement the solution in code and check that as the regularization strength increases, the L2 norm of the learned weights decreases.\"], \"handoff_plan\": \"/math -> /code -> router QA (verification: The code agent will validate that the final weight vector's L2 norm from the simulation behaves as predicted by the math agent's derivation for different lambda values. fallback: If the code's output contradicts the derivation, the math agent will be asked to re-check the gradient calculations.)\", \"todo_list\": [\"- [ ] /math: Derive the Lagrangian for L2-constrained linear regression and find the optimal weight solution.\", \"- [ ] /code: Implement the derived solution and create a synthetic dataset for testing.\", \"- [ ] /code: Generate a plot showing the trade-off between training loss and the L2 norm of the weights as the regularization parameter changes.\", \"- [ ] Verification: Confirm that the Python implementation's weight norm shrinks as the lambda hyperparameter increases.\", \"- [ ] Router QA: Review all artifacts for correctness, clarity, and adherence to the prompt.\"], \"difficulty\": \"introductory\", \"tags\": [\"constrained-optimization\", \"lagrange-multipliers\", \"regularization\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies the method of Lagrange multipliers to a least-squares objective with an L2 norm constraint.\", \"- The Python code runs without errors and produces a plot clearly illustrating the effect of L2 regularization.\", \"- The final report explicitly connects the Lagrange multiplier (\u03bb) from the derivation to the weight decay hyperparameter in the code.\", \"- The L2 norm of the final weights, as reported in the metrics file, is inversely correlated with the regularization strength.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2181, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating Bayesian inference for continuous-time models and need to formulate and implement a variational inference scheme for Neural Stochastic Differential Equations (SDEs). Please derive the Evidence Lower Bound (ELBO) for a latent SDE governed by an It\u00f4 process, and then implement this in JAX. The implementation should compare the efficacy of a simple mean-field Gaussian posterior versus a more expressive amortized normalizing flow posterior on a synthetic dataset, analyzing convergence speed and posterior accuracy with respect to the theoretical Fokker-Planck dynamics.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference for neural stochastic differential equations Girsanov theorem ELBO\\\", mode=rag)\", \"/math(Derive the Evidence Lower Bound (ELBO) for a Bayesian Neural SDE using It\u00f4 calculus, ensuring the derivation accounts for stochastic convergence criteria and Girsanov's theorem.)\", \"/code(Implement the variational inference loop in JAX, comparing a mean-field Gaussian posterior against an amortized normalizing flow, tracking both ELBO and AUPRC on a synthetic control theory dataset., using Python)\", \"/math(Verify the analytical KL-divergence term from the ELBO derivation against a numerical Monte Carlo estimate generated by the JAX code to ensure alignment between theory and implementation.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on variational posteriors for Neural SDEs. This informs the /math agent's formal derivation of the ELBO. The /code agent then implements this derivation for empirical analysis. A final /math call serves as a crucial verification step, comparing the theoretical derivation to the empirical results from /code, ensuring correctness before finalizing the report.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the SDE ELBO, referencing Girsanov's theorem.\", \"- A commented JAX Python script implementing the Bayesian Neural SDE training and evaluation loop.\", \"- A summary report in Markdown comparing the performance (final ELBO, convergence rate) of the two posterior families.\", \"- A literature review file listing at least two relevant post-2020 arXiv papers with IDs and summaries.\", \"- A verification log showing the percentage difference between the analytical KL-divergence and its numerical estimate.\"], \"thinking_outline\": [\"1. Conduct a literature survey to identify modern approaches for defining variational posteriors over the parameters of Neural SDEs, focusing on methods that ensure tractable KL-divergence terms.\", \"2. Formally derive the Evidence Lower Bound (ELBO) for a general latent SDE model. This involves applying It\u00f4 calculus and Girsanov's theorem to handle the change of measure between the prior and approximate posterior processes.\", \"3. Implement the derived model in JAX, including an SDE solver (e.g., Euler-Maruyama or SRK) and the variational inference training loop for both a mean-field and a normalizing flow posterior.\", \"4. Train the models on a synthetic dataset (e.g., a noisy Ornstein-Uhlenbeck process) and log the ELBO, its components (expected log-likelihood and KL-divergence), and posterior parameter statistics over training epochs.\", \"5. Validate the implementation by checking for ELBO convergence and ensuring the learned posterior distributions capture the true data-generating process dynamics.\", \"6. Verify the correctness of the KL-divergence derivation by comparing the analytical formula's output with a high-sample Monte Carlo estimate computed from the implemented posterior and prior SDEs.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The 'blue' search agent provides foundational papers to the /math agent for derivation. The resulting equations are handed to the /code agent for implementation. The /code agent's numerical outputs (e.g., Monte Carlo KL estimates) are passed back to the /math agent for a verification check against the analytical formula. All artifacts are then passed to the router for a final quality assurance review. Fallback: If the derivation and implementation mismatch exceeds a 5% tolerance, the task is routed back to the /math agent to re-check the application of Girsanov's theorem.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 key papers on variational inference for Neural SDEs.\", \"- [ ] /math: Produce a complete, step-by-step LaTeX derivation of the SDE ELBO.\", \"- [ ] /code: Implement the Bayesian Neural SDE model and training logic in JAX.\", \"- [ ] /code: Add instrumentation to track ELBO, KL divergence, and posterior variance during training.\", \"- [ ] /math: Verify the analytical KL derivation aligns with the numerical estimate from the code's Monte Carlo sampler.\", \"- [ ] /code: Generate plots comparing the learned dynamics and posterior distributions for both approximation methods.\", \"- [ ] /router: Review all artifacts for correctness, clarity, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-deep-learning\", \"stochastic-differential-equations\", \"variational-inference\"], \"acceptance_criteria\": [\"- The mathematical derivation must correctly apply Girsanov's theorem and clearly define the final variational objective function.\", \"- The JAX implementation must execute without errors and demonstrate a monotonically increasing ELBO on the provided synthetic dataset.\", \"- The final report must contain at least two citations to arXiv papers published after 2020, with corresponding IDs.\", \"- The verification step must show a discrepancy of less than 5% between the analytical and numerically estimated KL-divergence values.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2193, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the connection between the geometry of the loss landscape and generalization in deep neural networks. Specifically, I want to formally derive the relationship between the spectral norm of the Hessian matrix of the loss function and PAC-Bayes generalization bounds. Can you help me formulate the proof, implement a numerical experiment using power iteration on a ResNet-18 to track the top eigenvalue during training, and analyze how its evolution correlates with the generalization gap?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian eigenspectrum generalization PAC-Bayes sharpness-aware minimization\\\", mode=both)\", \"/math(Derive a PAC-Bayes generalization bound that explicitly incorporates the Hessian's spectral norm as a measure of flatness, analyzing convergence criteria and the role of the learning rate in controlling the top eigenvalue.)\", \"/code(Implement power iteration in PyTorch to estimate the top Hessian eigenvalue for a ResNet-18 on CIFAR-10, logging the eigenvalue and generalization gap each epoch. Ensure CUDA device placement and determinism., using Python)\", \"/math(Verify that the empirical correlation between the final eigenvalue and the generalization gap is positive and statistically significant, cross-referencing the derived Bayesian bound to interpret the magnitude of the effect.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on Hessian analysis and generalization. This informs the formal proof derivation in /math. The /code tool is then used for empirical validation of the theory. A final /math step provides theoretical interpretation and verification of the numerical results against the derived bounds.\", \"expected_artifacts\": [\"- A formal proof in LaTeX connecting the Hessian's spectral norm to a PAC-Bayes generalization bound.\", \"- A Python script implementing the numerical experiment and eigenvalue estimation.\", \"- A plot visualizing the top Hessian eigenvalue and the training/test loss curves over epochs.\", \"- A summary report (.md) synthesizing the theory, empirical results, and including at least two arXiv citations.\", \"- A verification log confirming the alignment between the mathematical derivation and the code's output.\"], \"thinking_outline\": [\"1. Use /general-search to survey literature connecting loss landscape curvature (via the Hessian eigenspectrum) to generalization in deep learning, focusing on seminal works by Hochreiter & Schmidhuber, Keskar et al., and PAC-Bayes analyses.\", \"2. Formulate a rigorous mathematical derivation linking the maximum eigenvalue (spectral norm) of the Hessian to a well-defined generalization bound, clearly stating all assumptions.\", \"3. Develop a Python implementation using automatic differentiation and the power method to efficiently estimate the Hessian's maximum eigenvalue for a trained neural network without materializing the full Hessian matrix.\", \"4. Conduct an experiment training a standard CNN architecture, periodically calculating the max eigenvalue, and tracking the generalization gap (test error - train error).\", \"5. Validate that the empirical results align with the theoretical prediction: specifically, check that training runs resulting in solutions with larger maximum eigenvalues exhibit poorer generalization.\", \"6. Synthesize all findings into a cohesive report that integrates the theoretical proof, the experimental methodology, the empirical results (including plots), and relevant citations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results provide the theoretical foundation for the /math derivation. The completed derivation provides the formula and hypothesis for the /code implementation. The numerical output from /code is then handed back to /math for theoretical validation and interpretation. Fallback: If the derivation in the first /math step is untenable, the plan reverts to /general-search for alternative theoretical frameworks. If the code fails verification, it is returned to the /code agent with a specific error report from the /math agent.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Hessian spectral analysis and PAC-Bayes bounds for deep networks.\", \"- [ ] /math: Formulate and write the formal derivation connecting spectral norm to the generalization bound.\", \"- [ ] /code: Implement the power iteration method for top eigenvalue estimation in PyTorch.\", \"- [ ] /code: Run the training and analysis experiment on CIFAR-10 and generate the required plots.\", \"- [ ] /math: Verify that the code's output (positive correlation between lambda_max and generalization gap) is consistent with the derived theory.\", \"- [ ] /general-search: Collect and format at least two citations from arXiv with full IDs.\", \"- [ ] router: Perform final QA review on the proof, code, plots, and summary report to ensure coherence and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"matrix-calculus\", \"spectral-analysis\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The submitted mathematical proof must be rigorous, with all steps clearly justified.\", \"- The Python code must be executable and successfully reproduce the plot showing the evolution of the top Hessian eigenvalue against the generalization gap.\", \"- The final report must correctly conclude a positive correlation between the Hessian spectral norm and the generalization gap, supported by both the proof and the empirical data.\", \"- The report must cite at least two relevant academic papers from arXiv, including their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2210, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a GPU-accelerated block Lanczos algorithm in CUDA for finding the lowest few eigenvalues of a large, sparse, symmetric positive-definite matrix arising from a finite element discretization. The focus is on maintaining orthogonality among the Lanczos vectors to prevent ghost eigenvalues, possibly using selective re-orthogonalization. The implementation should be benchmarked against a standard library solver like ARPACK (via SciPy) to demonstrate performance gains and numerical stability.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org parallel block lanczos algorithm selective reorthogonalization GPU CUDA\\\", mode=both)\", \"/math(Derive the block Lanczos algorithm with selective re-orthogonalization based on the Paige-Parlett-van der Vorst criteria. Formulate the matrix operations for GPU parallelism, focusing on sparse matrix-vector products and the eigenvalue extraction from the resulting block-tridiagonal matrix., includes optimality)\", \"/code(Implement the derived algorithm in Python using CuPy/Numba for CUDA acceleration. The code must handle CSR sparse matrix formats, track convergence criteria, and include thorough profiling hooks to measure kernel execution times.)\", \"/math(Analyze the convergence behavior of the implementation. Formulate a verification procedure to check the residual norm `||Ax - \u03bbx||` for each computed eigenpair and compare the Ritz values against known ground truth or a high-precision solver to validate accuracy.)\"], \"route_rationale\": \"The plan starts with a literature survey via the 'blue' /general-search agent to ground the approach in current research. /math is then used to formalize the algorithm's derivation. /code translates this mathematical specification into a high-performance GPU implementation. A final /math step is crucial for rigorous verification of the numerical results, ensuring the implementation is not just fast but also correct.\", \"expected_artifacts\": [\"A PDF report containing the full mathematical derivation of the re-orthogonalized block Lanczos algorithm.\", \"Python source code for the GPU-accelerated solver and benchmarking scripts.\", \"A performance comparison plot (wall-clock time vs. matrix dimension) against the SciPy baseline.\", \"A verification report detailing the residual norms and eigenvalue accuracy for test matrices.\", \"A bibliography with at least two cited arXiv papers on parallel Lanczos methods.\"], \"thinking_outline\": [\"1. Research state-of-the-art selective re-orthogonalization techniques for the Lanczos algorithm on parallel architectures to mitigate the effects of finite-precision arithmetic.\", \"2. Formulate the complete mathematical derivation of the block Lanczos algorithm, explicitly detailing the re-orthogonalization logic and the process for solving the projected eigenvalue problem.\", \"3. Implement the algorithm using a CUDA-enabled Python library (e.g., CuPy), paying close attention to efficient memory management and kernel launch configurations for sparse matrix operations.\", \"4. Develop a rigorous benchmark suite comparing the implementation's speed and accuracy against a trusted CPU-based solver like `scipy.sparse.linalg.eigsh` on a set of representative sparse matrices.\", \"5. Verify the computed eigenvalues by checking the residual norms and comparing them against the output of the baseline solver to ensure numerical stability and correctness.\", \"6. Audit the source code against the mathematical derivation to ensure all steps, especially the re-orthogonalization conditions, are implemented faithfully.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides papers to the math agent for derivation. The math agent's derivation is the specification for the code agent. The code agent's implementation is then handed back to the math agent for formal verification. (Verification: Residual norms must be below 1e-8. Fallback: If verification fails, the code agent must debug the implementation, focusing on the Gram-Schmidt orthogonalization steps.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize two key papers on GPU-accelerated Lanczos methods with re-orthogonalization.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the block Lanczos algorithm with selective re-orthogonalization.\", \"- [ ] /code: Implement the algorithm in Python with CuPy, ensuring it can process sparse matrices in CSR format.\", \"- [ ] /code: Create a benchmarking script that measures wall-clock time and compares results against `scipy.sparse.linalg.eigsh`.\", \"- [ ] /math: Verify the implementation's output by calculating the residual `||Ax - \u03bbx||` for each eigenpair.\", \"- [ ] /math: Verify that the code's logic for re-orthogonalization matches the conditions laid out in the mathematical derivation.\", \"- [ ] /router: Perform final QA check on all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"sparse-matrices\"], \"acceptance_criteria\": [\"- The implemented GPU solver must demonstrate at least a 5x speedup over the SciPy CPU baseline for matrices with dimensions greater than 10,000x10,000.\", \"- The relative error of the computed eigenvalues must be less than 1e-8 when compared to the reference solver.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\", \"- The code must be well-documented and include a README with instructions for reproducing the benchmark results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2253, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and analyze a Chebyshev polynomial preconditioner for the Conjugate Gradient method to solve large, sparse, ill-conditioned symmetric positive-definite linear systems, specifically for a CUDA-accelerated heat equation simulation. The focus is on deriving the optimal polynomial degree based on Lanczos eigenvalue estimates and comparing the convergence rate against theoretical bounds for Krylov subspace methods.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org OR site:.edu Chebyshev polynomial preconditioner CUDA Lanczos eigenvalue estimation Krylov convergence bounds\\\", mode=both)\", \"/math(Derive the algorithm for constructing a Chebyshev preconditioner, including the Lanczos method for estimating the extremal eigenvalue range [\u03bb_min, \u03bb_max] of the system matrix A, and derive the theoretical convergence rate improvement.)\", \"/code(Implement a Preconditioned Conjugate Gradient (PCG) solver using Python and CuPy for CUDA execution. The implementation must include the Lanczos eigenvalue estimation and the application of the Chebyshev polynomial preconditioner.)\", \"/math(Verify the numerical stability and convergence of the implemented PCG solver by comparing the empirical iteration count against the theoretical bounds derived from the estimated eigenvalue spectrum and condition number.)\"], \"route_rationale\": \"The plan starts with literature review via the blue /general-search agent to ground the work in current research. /math is then used for the formal derivation of the preconditioning and convergence theory. /code translates this theory into a high-performance CUDA implementation. A final /math step is crucial for verifying that the code's empirical performance aligns with the derived theoretical guarantees, closing the validation loop.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Chebyshev preconditioner and its theoretical convergence factor.\", \"- A documented Python source file implementing the PCG solver using CuPy for GPU execution.\", \"- A performance report (.md) containing a plot comparing iteration counts and wall-clock time for standard CG vs. PCG.\", \"- A list of at least 2 cited arXiv preprints or peer-reviewed articles.\", \"- A verification summary confirming the alignment between empirical and theoretical convergence rates.\"], \"thinking_outline\": [\"1. Review literature on Chebyshev preconditioning for Krylov subspace methods on parallel architectures to identify best practices for eigenvalue estimation.\", \"2. Formulate the mathematical derivation for the preconditioner, detailing the application of the three-term recurrence for the Chebyshev polynomials and the Lanczos algorithm for spectrum estimation.\", \"3. Implement the complete PCG solver in a CUDA-enabled Python environment, ensuring the preconditioner and the main solver are executed on the GPU.\", \"4. Construct a suitable test case, such as the matrix derived from a finite difference discretization of the 2D Poisson equation, which is known to be ill-conditioned.\", \"5. Validate the implementation by running the solver on the test case and comparing the observed number of iterations to convergence against the theoretical prediction.\", \"6. Audit the CUDA implementation to check for performance bottlenecks, such as excessive device-host memory transfers or non-coalesced memory access patterns.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. Verification: The final /math agent must receive the eigenvalues estimated by /code and the theoretical bounds from the first /math agent to perform its check. Fallback: If empirical convergence deviates by >15% from theory, flag for router review and potentially re-run /math derivation with relaxed assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent papers on GPU-accelerated polynomial preconditioners.\", \"- [ ] /math: Provide a step-by-step derivation of the Chebyshev preconditioning algorithm and its convergence factor.\", \"- [ ] /code: Implement the PCG solver and the preconditioner in CuPy, including docstrings and type hints.\", \"- [ ] /code: Add logging to output estimated eigenvalues, iteration count, and residual norm at each step.\", \"- [ ] /math: Receive solver logs from /code and produce a verification report comparing empirical vs. theoretical convergence.\", \"- [ ] /math: verify that the derivation correctly uses the estimated eigenvalue bounds in the polynomial scaling.\", \"- [ ] /router: Review all artifacts for correctness, consistency, and adherence to the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"krylov-methods\"], \"acceptance_criteria\": [\"- The implemented PCG solver must converge faster (fewer iterations and lower wall time) than the standard CG solver on a test matrix with a condition number > 1000.\", \"- The final report must include a plot visualizing the convergence history (residual norm vs. iteration) for both methods.\", \"- The verification artifact must show that the empirical iteration count is within 15% of the theoretical prediction.\", \"- The code must be executable and produce repeatable results given the provided reproducibility settings.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2268, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for a variance-preserving (VP) diffusion process. Subsequently, implement a predictor-corrector sampler in PyTorch with CUDA acceleration, combining an Euler-Maruyama predictor with Langevin MCMC correction steps. Validate the sampler's convergence by tracking the Fr\u00e9chet Inception Distance (FID) and ensure the energy-based model interpretation is consistent with the derived score function from arXiv sources.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models stochastic differential equations reverse-time SDE\\\", mode=both)\", \"/math(Derive the reverse-time SDE for a variance-preserving diffusion process, starting from the forward SDE dx = f(x,t)dt + g(t)dw. Clearly define the score function and its relationship to the drift term of the reverse SDE, referencing key convergence theorems from arXiv papers.)\", \"/code(Implement a PyTorch-based predictor-corrector sampler using CUDA for a 2D toy dataset. The predictor should use Euler-Maruyama, and the corrector should apply several steps of Langevin MCMC. Track FID as the primary convergence metric., using Python)\", \"/math(Verify that the gradient of the log-density (score) implemented in the Langevin MCMC corrector step is mathematically equivalent to the score function derived in the initial derivation. Check for consistency in scaling factors and noise schedules related to the energy function.)\"], \"route_rationale\": \"/general-search (blue agent) is used first to locate foundational arXiv papers on score-based generative modeling with SDEs. /math then uses these sources to perform the core theoretical derivation. /code translates this mathematical framework into a practical PyTorch implementation with specific samplers and metrics. A final /math call is crucial for verification, ensuring the code's logic precisely matches the derived theory, preventing subtle implementation bugs.\", \"expected_artifacts\": [\"- A LaTeX document containing the full derivation of the reverse-time SDE.\", \"- A Python script (`sampler.py`) with the PyTorch implementation of the predictor-corrector sampler.\", \"- A JSON file (`metrics.json`) reporting the FID score over sampling iterations.\", \"- A markdown file (`verification_report.md`) confirming the mathematical equivalence between the derived score and the implemented gradient.\", \"- A `references.bib` file with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Research foundational literature on score-based generative models and their connection to stochastic differential equations (SDEs).\", \"2. Formally derive the reverse-time SDE from the forward process, explicitly defining the score function \u2207x log p_t(x).\", \"3. Implement the predictor-corrector sampling algorithm, separating the predictor (discretization of the reverse SDE) and corrector (Langevin MCMC using the score function).\", \"4. Validate the implementation by running it on a simple distribution and tracking the FID score to ensure convergence towards the target data distribution.\", \"5. Verify the mathematical consistency between the score function derived in step 2 and the gradient function used in the Langevin corrector step of the code. Audit the noise schedules.\", \"6. Consolidate all findings, derivations, code, and verification results into the specified artifacts.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (verification). The blue search agent provides arXiv papers to the math agent for the derivation. The resulting SDE equations are the primary input for the code agent. The implemented code (specifically the score network's output) is then handed back to the math agent for a formal verification against the initial derivation. Fallback: If verification fails, the code agent must debug the implementation of the score function's gradient.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on score-based models via SDEs (e.g., by Song, Ermon).\", \"- [ ] /math: Complete the derivation of the reverse-time SDE for the VP-SDE case.\", \"- [ ] /code: Implement the Predictor (Euler-Maruyama) and Corrector (Langevin MCMC) steps in PyTorch.\", \"- [ ] /code: Instrument the code to log FID scores at regular sampling intervals.\", \"- [ ] /math: Verify the implemented score function in the code matches the theoretical derivation from the first math step.\", \"- [ ] /code: Write a small test suite to check the shapes and dtypes of tensors throughout the sampling process.\", \"- [ ] Router QA: Review all artifacts for consistency, correctness, and adherence to acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-modeling\"], \"acceptance_criteria\": [\"- The final LaTeX derivation of the reverse-time SDE must be mathematically sound and complete.\", \"- The implemented sampler must demonstrate a decreasing FID score over time, indicating convergence.\", \"- The verification report must explicitly confirm the equivalence between the derived and implemented score functions.\", \"- The code must be runnable on a CUDA-enabled device and produce the specified `metrics.json` output.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2344, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the barren plateau phenomenon in Variational Quantum Eigensolvers. Can you provide a detailed analysis for a hardware-efficient ansatz, specifically deriving the analytical expression for the gradient variance as a function of the number of qubits? Please also develop a numerical simulation using the H2 molecular Hamiltonian to demonstrate how this vanishing gradient impedes convergence and affects the final ground state energy estimation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org barren plateau theory 'hardware-efficient ansatz' 'gradient variance' 2-design\\\", mode=both)\", \"/math(Derive the expression for the variance of the cost function's partial derivative, Var[\u2202C/\u2202\u03b8_k], for a 2-design ansatz, linking its exponential decay to the Hilbert space dimension and Hamiltonian eigenvalue distribution.)\", \"/code(Write a Python script using Qiskit to implement a VQE algorithm for the H2 molecular Hamiltonian. Use a hardware-efficient ansatz and track the cost function convergence and gradient norm across optimization steps.)\", \"/math(Analyze the numerical convergence data from the simulation, specifically validating that the observed gradient norm decay rate aligns with the derived exponential variance scaling and Hamiltonian properties.)\"], \"route_rationale\": \"The blue general-search agent first gathers authoritative literature on barren plateaus and 2-designs. The first /math agent then performs the formal derivation based on this research. Next, the /code agent implements a concrete numerical experiment to observe the phenomenon. Finally, a second /math agent provides a crucial verification step by comparing the theoretical derivation against the empirical simulation results.\", \"expected_artifacts\": [\"A formal mathematical derivation of the gradient variance in VQE, formatted in LaTeX.\", \"A documented Python script (`vqe_h2_simulation.py`) implementing the VQE simulation using Qiskit.\", \"A summary report in Markdown including plots of the cost function convergence and gradient norm decay.\", \"A bibliography section in the report citing at least two relevant arXiv preprints.\", \"A verification document comparing the theoretically predicted gradient decay with the numerically observed results.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of barren plateaus in variational quantum algorithms, focusing on the role of unitary 2-designs and hardware-efficient ansaetze.\", \"2. Formulate the mathematical problem for the variance of the cost function gradient, Var[\u2202C/\u2202\u03b8], using principles from random matrix theory and properties of the Hamiltonian.\", \"3. Execute the formal derivation, clearly showing the exponential decay of the variance with the number of qubits.\", \"4. Implement the VQE simulation for a specific, well-understood problem (H2 molecule) to generate empirical data on convergence behavior.\", \"5. Verify that the numerical results from the simulation, particularly the scaling of the gradient norm, quantitatively match the predictions from the mathematical derivation.\", \"6. Validate the final calculated ground state energy of the H2 molecule against established benchmarks from full configuration interaction (FCI) calculations to confirm the simulation's physical accuracy.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (simulation) -> /math (verification) -> router QA. Verification: The final math agent explicitly checks if the code's output (gradient decay rate) matches the first math agent's derivation. Fallback: If there is a mismatch, the /code agent is re-invoked with corrected parameters or optimizer settings based on feedback from the verification step.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on barren plateaus, focusing on McClean et al., 2018, and literature on unitary 2-designs.\", \"- [ ] /math: Derive the gradient variance expression, showing all intermediate steps and assumptions related to the ansatz and Hamiltonian.\", \"- [ ] /code: Implement the VQE simulation for H2 at its equilibrium bond distance (0.74 \u00c5) using a RyRz ansatz and the SPSA optimizer.\", \"- [ ] /math: Plot the log of the gradient norm vs. number of qubits/layers and perform a linear fit to extract the decay rate for comparison with theory.\", \"- [ ] /code: Add a validation check to compare the final VQE energy with the known FCI ground state energy for H2.\", \"- [ ] /math: Verify that the derivation's scaling factor aligns with the empirically fitted decay rate from the simulation data.\", \"- [ ] /router: Perform final QA review of all artifacts, checking for consistency between derivation, code, and final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"variational-algorithms\", \"hamiltonian-simulation\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly shows that the gradient variance decays exponentially with the number of qubits.\", \"- The VQE simulation code successfully runs and produces plots demonstrating optimization stagnation (a plateau) for deeper circuits.\", \"- The final report cites at least 2 relevant arXiv papers with their IDs.\", \"- The calculated ground state energy for the H2 molecule at equilibrium distance is within 0.01 Hartree of the known FCI value.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2345, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on a Bayesian machine learning project and need to derive and implement a Stochastic Variational Inference (SVI) algorithm for a sparse Gaussian Process model with inducing points. Please provide the full derivation for the Evidence Lower Bound (ELBO), analyze the convergence criteria based on the Robbins-Monro conditions for the stochastic optimization, and then implement the SVI optimizer in Python using JAX to model a non-stationary finance dataset, ensuring the posterior uncertainty is correctly quantified.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org stochastic variational inference gaussian process inducing points Hensman\\\", mode=web)\", \"/math(Derive the Evidence Lower Bound (ELBO) for the sparse Gaussian Process model, incorporating the prior over inducing points and the variational posterior distribution, ensuring the expression is amenable to minibatch-based stochastic gradient estimation.)\", \"/code(Implement the SVI algorithm for the sparse GP in JAX, using a non-stationary kernel suitable for finance data. Profile the convergence rate and validate the posterior uncertainty quantification against a synthetic baseline., using Python)\", \"/math(Formally verify that the implemented stochastic gradients match the derived ELBO derivatives and analyze the expected posterior predictive distribution's behavior in data-sparse regions.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find the canonical academic papers on SVI for GPs, establishing a solid theoretical foundation. This research is handed to the math agent to perform the core derivation. The resulting mathematical expressions are then passed to the code agent for a robust JAX implementation. Finally, the math agent is re-engaged to verify the implementation's correctness against the theory and analyze the model's output, ensuring a closed loop of verification.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the ELBO and its stochastic gradients.\", \"A Python script with the JAX implementation of the SVI for sparse Gaussian Processes.\", \"A results summary in Markdown, including plots of the model's convergence (ELBO vs. iteration) and posterior predictive distribution on a test set.\", \"A list of at least two cited arXiv papers that informed the derivation and implementation.\", \"A verification report confirming the numerical correctness of the implemented gradients.\"], \"thinking_outline\": [\"1. Use /general-search to retrieve foundational literature on sparse Gaussian Processes and Stochastic Variational Inference, focusing on the works by Titsias (2009) and Hensman et al. (2013).\", \"2. Following the literature, derive the Evidence Lower Bound (ELBO) for the specified model. This involves defining the variational distribution and integrating out the inducing points.\", \"3. Implement the GP model, the variational distribution, and the SVI optimization loop in JAX. Pay close attention to numerical stability and the parameterization of covariance matrices.\", \"4. Validate the implementation by performing a numerical gradient check, comparing the analytical gradients derived in step 2 with those computed via finite differences on a small data batch.\", \"5. Execute the model on a suitable non-stationary dataset, analyzing the convergence of the ELBO and the quality of the posterior predictive uncertainty.\", \"6. Verify that the final model's uncertainty estimates are well-calibrated, showing higher variance in regions of the input space with sparse or no training data.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The initial search results provide the theoretical basis for the math agent's derivation. This derivation becomes the formal specification for the code agent. The code agent's implementation is then handed back to the math agent for final verification against the theory. Fallback: If the math agent's derivation is not computationally tractable, the code agent will flag this and request a revised formulation, possibly prompting another search for alternative variational families.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on SVI for sparse GPs (e.g., Hensman et al., 2013).\", \"- [ ] /math: Derive the ELBO for a sparse GP with a variational Gaussian posterior over inducing points.\", \"- [ ] /math: Derive the stochastic gradients of the ELBO with respect to variational parameters and kernel hyperparameters.\", \"- [ ] /code: Implement the SVI optimization loop in JAX, using the derived gradients.\", \"- [ ] /code: Test the implementation on a synthetic dataset with known properties.\", \"- [ ] /math: verify the code's analytical gradients against numerical differentiation to ensure correctness.\", \"- [ ] /code: Run the final model on the target finance dataset and generate convergence and posterior predictive plots.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-processes\", \"gaussian-processes\"], \"acceptance_criteria\": [\"- The ELBO derivation in the LaTeX document must match established literature.\", \"- The JAX implementation must converge on a synthetic dataset, with the ELBO monotonically increasing (on average).\", \"- The final report must demonstrate well-calibrated uncertainty, with predictive standard deviation increasing in data-sparse regions.\", \"- The numerical gradient check verification report must show a relative error of less than 1e-4.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2371, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time Stochastic Differential Equation (SDE) for a variance-preserving diffusion process, and then implement a numerical solver in Python using the score function approximation. What is the connection to the probability flow ODE and how does this impact FID score?\n", "completion": "{\"route_plan\": [\"/math(Derive the reverse-time SDE from a forward variance-preserving diffusion process, showing how the score function, \u2207_x log p_t(x), emerges as the drift term. Analyze the convergence properties of the discretized solver.)\", \"/code(Implement a PyTorch-based numerical solver for the derived reverse-time SDE. Use a simple U-Net to model the score function for a 2D dataset and track the FID score during training as a primary metric., using Python)\", \"/general-search(query=\\\"site:arxiv.org score-based generative models probability flow ODE FID\\\", mode=rag, context=\\\"Find authoritative sources on the connection between the reverse-time SDE, the probability flow ODE, and its implications for sampling quality in diffusion models.\\\")\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation by deriving the core equations. This is followed by /code to translate the theory into a practical implementation and empirically validate the approach. Finally, the blue /general-search agent is used to gather peer-reviewed literature to contextualize the results and justify the connection between the SDE, its corresponding ODE, and generative quality metrics.\", \"expected_artifacts\": [\"A LaTeX-formatted document with the full derivation of the reverse-time SDE.\", \"A Python script implementing the SDE solver and a trained score model for a 2D dataset.\", \"A plot showing the evolution of the generated samples from noise to the target data distribution.\", \"A summary report citing at least two relevant arXiv papers connecting the SDE, ODE, and FID.\", \"A validation log file comparing the numerical stability of different step sizes for the solver.\"], \"thinking_outline\": [\"1. Formulate the forward variance-preserving SDE and its corresponding Fokker-Planck equation.\", \"2. Use time-reversal theory for diffusion processes to derive the reverse-time SDE, identifying the score function as the key component.\", \"3. Implement an Euler-Maruyama or a similar numerical scheme to discretize and solve the reverse-time SDE, using a neural network to approximate the score.\", \"4. Train the score network on a simple 2D dataset and generate samples, evaluating the process qualitatively and quantitatively.\", \"5. Validate the implementation by checking if the generated samples' distribution converges towards the target distribution as the number of discretization steps increases.\"], \"handoff_plan\": \"/math (derivation) -> /code (implementation & metrics) -> /general-search (context & citations) -> router QA (verification: check if code's solver matches the math derivation and if the report cites sources justifying the FID/ODE link; fallback: if the derivation is flawed, re-run /math with corrected assumptions).\", \"todo_list\": [\"- [ ] /math: Derive the reverse-time SDE for a variance-preserving diffusion process.\", \"- [ ] /math: Analyze the conditions for the stability of its numerical discretization.\", \"- [ ] /code: Implement the SDE solver using a U-Net for score approximation on a 2D toy dataset.\", \"- [ ] /code: Validate the solver's output by visually inspecting the generated sample manifold and tracking FID over training.\", \"- [ ] /general-search: Find and summarize two key papers on the relationship between score-based SDEs and probability flow ODEs.\", \"- [ ] /router: Perform final QA check, ensuring the mathematical derivation, code implementation, and literature summary are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-ai\"], \"acceptance_criteria\": [\"The mathematical derivation of the reverse-time SDE is correct and self-contained.\", \"The Python code successfully generates samples that visually match the target 2D data distribution.\", \"The final report correctly cites at least 2 relevant arXiv papers and explains the connection between the SDE, ODE, and sample quality.\", \"The FID score for the generated samples shows consistent improvement over the training process.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2414, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for the Method of Optimal Directions (MOD) for dictionary learning under an L0 sparsity constraint and a Frobenius norm reconstruction error. Implement this in Python using Orthogonal Matching Pursuit (OMP) for the sparse coding step. Analyze its convergence rate on a synthetic sparse signal dataset, comparing its performance against a fixed Discrete Cosine Transform (DCT) dictionary baseline using Peak Signal-to-Noise Ratio (PSNR) and support recovery accuracy. Critically, your derivation must explicitly handle the non-convex nature of the joint optimization problem via alternating minimization.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org OR site:ieeexplore.ieee.org 'Method of Optimal Directions' 'K-SVD' dictionary learning convergence analysis bayesian\\\", mode=rag)\", \"/math(Derive the MOD update rules for the dictionary D and sparse codes X by minimizing the Frobenius norm ||Y-DX||_F^2 subject to ||x_i||_0 <= k. Analyze the convergence guarantees using concepts from control theory and non-convex optimization.)\", \"/code(Implement the MOD algorithm in Python/JAX, including Orthogonal Matching Pursuit (OMP) for the sparse coding stage. Track reconstruction error and dictionary coherence. Add CUDA support for batch matrix operations to improve performance.)\", \"/math(Verify the implemented algorithm's convergence behavior against the theoretical derivation. Use Fourier analysis to characterize the spectral properties of the learned dictionary atoms and compare them against the known frequency content of the synthetic signals.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather authoritative literature on MOD and its convergence properties. Next, /math is used to perform the formal mathematical derivation of the algorithm's update rules. /code then implements this derived algorithm in a high-performance framework. A final /math step is crucial for verifying that the numerical behavior of the implementation aligns with the theoretical model and for performing deeper analysis of the results.\", \"expected_artifacts\": [\"- A PDF report containing the full mathematical derivation of the MOD algorithm's update steps.\", \"- A Python script (`mod_implementation.py`) containing the complete, commented implementation of MOD and the comparative analysis.\", \"- A JSON file (`results.json`) reporting the final PSNR, support recovery accuracy, and computation time for both MOD and the DCT baseline.\", \"- A PNG image (`convergence_plot.png`) visualizing the reconstruction error decrease per iteration for the MOD algorithm.\", \"- A verification summary detailing the consistency check between the mathematical derivation and the code's output on a test case.\"], \"thinking_outline\": [\"1. Research the foundational papers on dictionary learning, specifically Method of Optimal Directions (MOD) and K-SVD, to fully grasp the joint optimization problem over the dictionary and the sparse coefficients.\", \"2. Formulate the objective function with the L0 pseudo-norm sparsity constraint and derive the two-step update rules for the dictionary and sparse codes using an alternating minimization strategy.\", \"3. Implement the derived algorithm in Python, ensuring the sparse coding step (e.g., Orthogonal Matching Pursuit) is numerically stable and efficient.\", \"4. Validate the implementation on a small, synthetic dataset where the ground-truth dictionary and sparse signals are known, ensuring the algorithm recovers them with high fidelity.\", \"5. Design and execute a benchmark experiment comparing the learned dictionary's performance against a standard fixed basis (like DCT) on signal reconstruction tasks.\", \"6. Verify that the empirical convergence plot (reconstruction error vs. iteration) demonstrates the expected monotonic decrease and aligns with the theoretical convergence properties of alternating minimization schemes for this class of problems.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation) -> /math (verification & analysis) -> router QA (verification: code output matches derivation for a 1-step test case; fallback: if mismatch, return to /math to re-check derivation and /code to debug implementation logic).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on MOD (e.g., Engan et al.) and its convergence analysis, saving key PDFs.\", \"- [ ] /math: Derive the closed-form solution for the dictionary update step and outline the iterative algorithm for the sparse coding step.\", \"- [ ] /code: Implement the main MOD loop, calling a separate function for the OMP sparse coding subroutine.\", \"- [ ] /code: Create a synthetic data generator for sparse signals using a known dictionary.\", \"- [ ] /math: Verify that the implemented code's gradient calculations for a single dictionary update align with the derived mathematical expressions.\", \"- [ ] /code: Run the full experiment, generate the convergence plot, and save the final metrics to a JSON file.\", \"- [ ] /math: Audit the final plot and metrics to ensure they are consistent with convergence theory.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The provided mathematical derivation correctly uses alternating minimization to solve the non-convex objective function.\", \"- The Python implementation achieves a PSNR of at least 30 dB on the specified synthetic dataset, outperforming the DCT baseline.\", \"- The final report includes a convergence plot that clearly shows a monotonically decreasing reconstruction error across iterations.\", \"- The solution cites at least two authoritative papers on dictionary learning with valid arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2416, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Natural Policy Gradient (NPG) update rule by formulating policy improvement as a constrained optimization problem that maximizes expected advantage subject to a KL-divergence constraint. Prove that the Fisher Information Matrix (FIM) emerges as the metric tensor in this formulation. Then, implement NPG for a continuous control theory task and empirically analyze its convergence properties against vanilla policy gradient, focusing on the FIM's conditioning and its impact on the policy update's stability.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org natural policy gradient derivation kakade fisher information matrix kl divergence\\\", mode=web)\", \"/math(Derive the NPG update rule by setting up a Lagrangian to maximize the advantage function under a KL-divergence trust region. Explicitly show how the Fisher Information Matrix arises as the solution to this constrained optimization problem, ensuring convergence guarantees.)\", \"/code(Implement both vanilla Policy Gradient and Natural Policy Gradient in JAX for the Pendulum-v1 control theory environment. Track average return and the FIM's eigenvalue spectrum. Profile the computational overhead of the Fisher matrix inversion., using Python)\", \"/math(Analyze the empirical results from the code agent. Specifically, verify that the computed Fisher Information Matrix is positive semi-definite and analyze its condition number across training to audit the numerical stability of the NPG updates., includes cybersecurity, gradient)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on NPG. This informs the /math agent's formal derivation of the update rule. The /code agent then translates this mathematical theory into a practical JAX implementation for empirical testing. A final /math step is crucial for verification, analyzing the numerical properties of the implemented FIM to ensure the code's stability and correctness, closing the theory-practice loop.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF containing the complete mathematical derivation of the Natural Policy Gradient update rule from the constrained optimization problem.\", \"- A Python script implementing both NPG and vanilla PG, with clear comments and functions for computing the empirical Fisher Information Matrix.\", \"- A JSON file containing the logged metrics, including average return, KL divergence per step, and the condition number of the FIM over training epochs.\", \"- A summary report comparing the convergence plots of NPG vs. vanilla PG and discussing the findings from the numerical stability analysis of the FIM.\"], \"thinking_outline\": [\"1. Formulate policy improvement as an optimization problem: maximizing the surrogate advantage objective subject to a bound on the KL-divergence between the old and new policies.\", \"2. Use Lagrange multipliers to solve this constrained optimization problem, performing a first-order approximation of the objective and a second-order approximation of the KL-divergence constraint.\", \"3. Show that the Hessian of the KL-divergence is the Fisher Information Matrix (FIM), and derive the final NPG update rule involving the inverse of the FIM.\", \"4. Validate the derived update rule against established literature found via search (e.g., Kakade, 2002; Schulman et al., 2015).\", \"5. Implement the algorithm and verify the numerical stability of the FIM inversion by checking its condition number and eigenvalue distribution during training runs.\", \"6. Conclude by synthesizing the theoretical derivation and empirical results to explain why NPG often provides more stable and faster convergence than vanilla PG.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The initial search provides foundational papers for the math agent's derivation. The code agent implements the derived formula. The final math agent verifies the numerical properties of the code's output. Verification failure in the final math step (e.g., FIM is not positive semi-definite) would trigger a fallback to the code agent to debug the FIM computation logic.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Natural Policy Gradients and TRPO, focusing on derivations involving the Fisher Information Matrix.\", \"- [ ] /math: Complete the formal derivation of the NPG update rule using a KL-divergence trust region.\", \"- [ ] /code: Implement the NPG and a vanilla PG baseline agent in JAX.\", \"- [ ] /code: Add instrumentation to log average return, KL-divergence, and the FIM's condition number.\", \"- [ ] /math: Verify the analytical derivation of the Fisher inverse aligns with the computational approach in the code.\", \"- [ ] /math: Perform a numerical analysis on the logged metrics, particularly the FIM's properties, to validate stability.\", \"- [ ] /router: Review all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"policy-gradient\", \"information-geometry\", \"reinforcement-learning\"], \"acceptance_criteria\": [\"- The mathematical derivation must correctly show the FIM emerging from a second-order approximation of the KL-divergence constraint.\", \"- The implementation must run without numerical errors (e.g., non-invertible FIM) and produce learning curves for a standard control benchmark.\", \"- The final analysis must clearly demonstrate a performance difference between NPG and vanilla PG, linking it back to the theoretical properties of the FIM.\", \"- The project must cite at least two relevant academic papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2436, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project in reinforcement learning and need to understand the foundational policy gradient theorem. Can you derive it from first principles, explain the likelihood ratio trick, and then implement a basic REINFORCE algorithm to solve the CartPole-v1 control theory problem?\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient theorem using the log-derivative trick, defining the objective function J(\u03b8) and showing its gradient with respect to policy parameters \u03b8 for a classic control theory problem.)\", \"/code(Implement the REINFORCE algorithm in Python using PyTorch to solve the CartPole-v1 environment, plotting reward convergence and validating the implementation against a known baseline performance.)\", \"/general-search(query=\\\"site:neurips.cc REINFORCE algorithm sutton barto\\\" OR query=\\\"site:arxiv.org policy gradient methods survey\\\", mode=web)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation of the policy gradient theorem. This theory is then practically validated using /code to implement the REINFORCE algorithm. Finally, the blue /general-search agent is used to find canonical references, grounding the derivation and implementation in established academic literature.\", \"expected_artifacts\": [\"A formal mathematical derivation of the policy gradient theorem.\", \"A documented Python script implementing the REINFORCE algorithm for CartPole-v1.\", \"A plot showing the learning curve (average reward per episode) demonstrating policy improvement and convergence.\", \"A list of at least 2 cited papers on policy gradients with arXiv IDs or DOIs.\", \"A summary of the validation checks performed on the code's output, confirming it solves the environment.\"], \"thinking_outline\": [\"1. Define the reinforcement learning objective function J(\u03b8) for an episodic task, representing the expected total reward.\", \"2. Use the log-derivative trick (also known as the likelihood ratio trick) to derive the policy gradient \u2207\u03b8 J(\u03b8), clearly explaining each mathematical step.\", \"3. Translate the derived gradient expression into the pseudocode for the REINFORCE algorithm.\", \"4. Implement the REINFORCE algorithm in Python and apply it to the CartPole-v1 environment from the Gymnasium library.\", \"5. Validate the implementation by checking if the average reward over 100 consecutive episodes consistently surpasses the environment's solved threshold (e.g., a score of 195).\"], \"handoff_plan\": \"/math provides the theoretical derivation -> /code implements the algorithm based on the derivation -> /general-search finds supporting literature. The complete package is sent to router QA. Verification: The code's performance (reward convergence) must align with the theoretical expectation of policy improvement. Fallback: If the code fails to converge, re-audit the math derivation for errors and simplify the neural network architecture.\", \"todo_list\": [\"- [ ] /math: Derive the policy gradient theorem, showing all steps of the log-derivative trick.\", \"- [ ] /code: Implement the REINFORCE algorithm and generate a reward convergence plot for CartPole-v1.\", \"- [ ] /general-search: Collect 2+ foundational papers on policy gradients.\", \"- [ ] verification: Sanity-check that the implemented agent solves the CartPole-v1 environment per the acceptance criteria.\", \"- [ ] router QA: Submit all artifacts for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"deep-learning\"], \"acceptance_criteria\": [\"The mathematical derivation of the policy gradient theorem is complete, correct, and clearly explained.\", \"The Python code successfully trains an agent that solves the CartPole-v1 environment (average reward > 195 over 100 consecutive episodes).\", \"At least two relevant academic papers (e.g., from arXiv, NeurIPS) are cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2439, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nImplement a high-performance Python simulation of the surface code, a specific type of toric code, to demonstrate fault-tolerant quantum computation. Your simulation must model a circuit-level depolarizing noise channel, implement the stabilizer measurement circuits using the Stim toolkit, and decode the resulting syndromes using a Minimum-Weight Perfect Matching (MWPM) algorithm. Critically, analyze the logical error rate as a function of the physical error probability to numerically estimate the error threshold, ensuring your implementation handles both X and Z type errors and provides a robust convergence analysis.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org surface code circuit-level noise threshold minimum weight perfect matching stim convergence\\\", mode=both)\", \"/math(Derive the stabilizer group formalism for the surface code, including the eigenvalue properties of plaquette and star operators, and formulate the Bayesian inference problem that MWPM solves for syndrome decoding.)\", \"/code(Implement the surface code simulation in Python using Stim for circuit generation and PyMatching for decoding. The implementation must assess computational complexity and use bootstrap resampling to estimate sampling error in the logical error rates.)\", \"/math(Analyze the simulation output by plotting logical error rate vs. physical error rate for multiple code distances. Perform a finite-size scaling analysis to extract the error threshold and its confidence interval, potentially using Fourier methods for analyzing periodic boundary conditions.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather state-of-the-art literature on surface code thresholds and simulation methods. This informs the /math agent's formal derivation of the stabilizer physics. The /code agent then implements this theory in a high-performance simulation. Finally, the /math agent returns to perform a rigorous statistical analysis and verification of the simulation output.\", \"expected_artifacts\": [\"- A technical report in Markdown detailing the stabilizer formalism, the noise model, and the data analysis methodology.\", \"- A Python script (`surface_code_sim.py`) containing the complete, commented simulation and analysis code.\", \"- A PNG image (`threshold_plot.png`) plotting logical error rates versus physical error rates, clearly showing the threshold crossing.\", \"- A JSON file (`results.json`) containing the estimated error threshold, its confidence interval, and the raw data points.\", \"- A verification log documenting the alignment of mathematical derivations with the code's behavior on simple error cases.\"], \"thinking_outline\": [\"1. Review the literature to understand the stabilizer formalism of the surface code and the standard circuit-level depolarizing noise model.\", \"2. Formulate the mathematical mapping from a set of stabilizer measurement outcomes (a syndrome) to the most likely error chain using the principles of Minimum-Weight Perfect Matching.\", \"3. Implement the simulation using Stim to generate surface code circuits with injected noise, execute them to get measurement outcomes, and build the syndrome graph.\", \"4. Use the PyMatching library to solve the MWPM problem on the syndrome graph and determine the required correction.\", \"5. Validate the simulation's correctness by checking the logical error rate for various code distances and physical error probabilities, ensuring the curves cross at the expected threshold.\", \"6. Verify the final computed error threshold against established values from the literature (e.g., around 0.57% for this noise model) to confirm the simulation's accuracy.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides key papers and arXiv IDs. The math agent uses these to create a formal spec. The code agent implements the spec. The math agent analyzes the code's output data. Handoffs are verified at each stage. For example, before final analysis, the code's output on a simple, known error case is checked against the math agent's expected syndrome. If they mismatch (fallback), the implementation logic is revisited.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on surface code threshold estimation with MWPM decoders.\", \"- [ ] /math: Derive the stabilizer generators for a d x d surface code and the syndrome extraction logic.\", \"- [ ] /code: Implement the Stim circuit for a distance-d surface code with depolarizing noise.\", \"- [ ] /code: Integrate PyMatching to decode syndromes and determine logical errors.\", \"- [ ] /math: Develop the data analysis script to perform finite-size scaling and extract the threshold value.\", \"- [ ] /code: Verify that the decoder correctly identifies and corrects single-qubit X and Z errors on a small lattice.\", \"- [ ] /math: Verify that the scaling of logical error rates with code distance is consistent with theoretical expectations above and below the threshold.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-error-correction\", \"fault-tolerant-computing\", \"computational-physics\"], \"acceptance_criteria\": [\"- The Python simulation correctly reproduces the approximate error threshold for the surface code under circuit-level depolarizing noise (~0.57%) within a 5% relative tolerance.\", \"- The final report provides a clear mathematical derivation of the surface code stabilizers and the syndrome decoding graph.\", \"- The generated plot (`threshold_plot.png`) clearly displays a crossing point for the logical error rate curves of at least three different code distances (e.g., d=3, 5, 7).\", \"- The code is well-commented and includes unit tests for the syndrome-to-graph conversion logic.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2440, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to investigate the stability of second-order optimization methods for large language models. Specifically, derive a formal proof bounding the maximum stable learning rate for a Hessian-free optimizer by the spectral radius of the attention mechanism's Hessian. Also, implement this analysis in JAX to empirically validate the theoretical bound by tracking the Hessian eigenvalue and observing convergence divergence across a parameter grid.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org transformer hessian eigenvalue spectrum second-order optimization convergence\\\", mode=rag)\", \"/math(Derive the convergence proof bounding the learning rate based on the Hessian's maximum eigenvalue, leveraging spectral theory and concepts from the arXiv papers.)\", \"/code(Implement a Hessian-free optimizer in JAX/CUDA, using the Lanczos algorithm to estimate the max eigenvalue and validate the learning rate bound via a parameter sweep to test convergence., using Python)\", \"/math(Validate the empirical convergence thresholds from the CUDA code against the derived analytical bound, checking for discrepancies and analyzing the Hessian eigenvalue distribution.)\"], \"route_rationale\": \"The blue /general-search agent first gathers state-of-the-art literature on Hessian spectral analysis. The /math agent then uses these findings to construct a rigorous mathematical proof. Next, the /code agent implements the theory to provide empirical evidence. Finally, a second /math call verifies the consistency between the theoretical derivation and the experimental code output, closing the validation loop.\", \"expected_artifacts\": [\"A LaTeX document containing the formal proof and derivation of the learning rate bound.\", \"A Python script using JAX to implement the optimizer and validation experiment.\", \"A JSON file reporting the empirically determined maximum stable learning rate and the estimated maximum Hessian eigenvalue.\", \"A summary report citing at least two relevant arXiv papers with IDs.\", \"A verification log confirming the alignment of theoretical bounds and empirical results.\"], \"thinking_outline\": [\"1. Review recent literature on the spectral properties of Transformer Hessians and their relation to second-order optimization convergence.\", \"2. Formulate a precise theorem linking the maximum stable learning rate to the spectral radius (maximum eigenvalue) of the Hessian.\", \"3. Develop a formal mathematical proof for the theorem, clearly stating all assumptions and leveraging concepts from numerical analysis.\", \"4. Implement a numerical experiment to estimate the Hessian's maximum eigenvalue for a Transformer block and test the derived learning rate bound.\", \"5. Verify that the empirical results from the simulation align with the theoretical predictions from the proof within a defined tolerance.\", \"6. Audit any discrepancies between theory and practice, potentially relating them to numerical precision or simplifying assumptions in the proof.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: empirical results from /code must match theoretical bounds from the first /math step; fallback: if divergence exceeds threshold, /code agent must re-run with smaller numerical tolerance and /math agent must re-check proof assumptions.)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Transformer Hessian spectral analysis and Hessian-free methods.\", \"- [ ] /math: Formulate and derive the proof for the learning rate bound based on the literature.\", \"- [ ] /code: Implement the JAX experiment with Lanczos algorithm for eigenvalue estimation and run the parameter sweep.\", \"- [ ] /math: Receive empirical results from /code and formally verify them against the derived proof.\", \"- [ ] /code: Document the JAX implementation with clear comments on the numerical methods used.\", \"- [ ] /math: Finalize the LaTeX report including the verified proof and analysis of results.\", \"- [ ] router: Perform final QA check, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"transformer-optimization\", \"spectral-theory\", \"hessian-free-optimization\"], \"acceptance_criteria\": [\"The formal proof correctly derives the learning rate bound from first principles of optimization theory.\", \"The JAX implementation successfully reproduces the theoretical prediction, with empirical divergence occurring just above the predicted learning rate bound.\", \"The final report includes at least two citations to relevant arXiv preprints on Hessian analysis.\", \"The primary metric (LR_discrepancy) is below the specified threshold of 0.05.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2495, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a Physics-Informed Neural Network (PINN) to solve the 2D incompressible Navier-Stokes equations for a lid-driven cavity flow problem, focusing on moderately high Reynolds numbers where turbulent effects begin to emerge. Can you derive the full loss formulation, implement a JAX-based model, and critically evaluate its convergence properties and accuracy against recent Fourier Neural Operator (FNO) benchmarks from arXiv?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org physics-informed neural network Navier-Stokes convergence fourier neural operator benchmark\\\", mode=web)\", \"/math(Derive the PINN loss function for the 2D incompressible Navier-Stokes equations, including boundary conditions and the PDE residual. Ensure the derivation correctly handles the pressure-Poisson equation formulation for improved convergence.)\", \"/code(Implement the PINN in JAX, using automatic differentiation for the PDE residual. The model should target the lid-driven cavity benchmark and include callbacks for logging the L2 relative error and PDE residual norm during training. Use an ADAM optimizer with a curricular learning rate schedule., using Python)\", \"/math(Analyze the training results by plotting the convergence of the loss components. Verify that the final velocity and pressure fields qualitatively match expected physical behavior and quantitatively compare the L2 error against the benchmarks found on arxiv.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art benchmarks for PINN and FNO methods on the target problem. /math is then used for the rigorous mathematical formulation of the PDE loss. /code implements this formulation in a modern ML framework (JAX) suitable for scientific computing. A final /math step is crucial for verifying the physical plausibility of the results and performing a rigorous quantitative comparison against the literature.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the Navier-Stokes PINN loss function.\", \"A Python script (`main.py`) containing the full JAX implementation of the PINN solver.\", \"A summary report in Markdown (`report.md`) comparing the PINN's performance (L2 error, convergence speed) against FNO benchmarks, complete with citations.\", \"Generated plots (`velocity_field.png`, `loss_convergence.png`) visualizing the final flow field and training dynamics.\", \"A JSON file (`metrics.json`) with the final L2 relative error and mean PDE residual.\"], \"thinking_outline\": [\"1. Use /general-search to find at least two recent, highly-cited papers on arXiv comparing PINNs and FNOs for solving Navier-Stokes equations, noting their reported error metrics and hyperparameters.\", \"2. Formulate the strong form of the 2D incompressible Navier-Stokes equations and derive the corresponding PDE residual functions for velocity (u, v) and pressure (p).\", \"3. Construct the composite loss function as a weighted sum of the mean squared errors for the PDE residuals, boundary conditions, and any available initial/data conditions.\", \"4. Implement the neural network and the training loop in JAX, ensuring the gradients for the PDE residual are correctly computed via automatic differentiation.\", \"5. Validate the implementation by running a low-Reynolds number case first and verifying that the solution converges to a known steady-state result from literature.\", \"6. Audit the final high-Reynolds number simulation results for physical consistency and compare the L2 relative error directly against the benchmarks identified in the initial literature search.\"], \"handoff_plan\": \"/general-search (literature benchmarks) -> /math (loss derivation) -> /code (implementation) -> /math (analysis & verification) -> router QA. Verification: The /code agent must confirm that the implemented loss function components exactly match the /math derivation. Fallback: If the PDE residual does not decrease during training, the /code agent must halt, flag the divergence, and request a review of the loss term weighting or network architecture from the /math agent.\", \"todo_list\": [\"- [ ] /general-search: Find >=3 recent arXiv papers on PINNs/FNOs for Navier-Stokes.\", \"- [ ] /math: Formulate the complete loss function, including all boundary and PDE residual terms.\", \"- [ ] /code: Implement the JAX model and training loop for the lid-driven cavity problem.\", \"- [ ] /math: Verify that the automatic differentiation in the code correctly calculates the Laplacian and convective terms of the PDE residual.\", \"- [ ] /code: Run the simulation, train the model to convergence, and generate all specified artifacts (plots, metrics JSON).\", \"- [ ] /math: Analyze the final metrics and plots, comparing them against literature benchmarks and providing a conclusive summary.\", \"- [ ] /code: Verify that the generated metrics in `metrics.json` are consistent with the final values logged during the training run.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"navier-stokes\"], \"acceptance_criteria\": [\"- The final L2 relative error for the velocity field must be below 5e-3 for Reynolds number Re=1000.\", \"- The final report must cite at least 3 relevant arXiv papers with IDs, published within the last 3 years.\", \"- The `loss_convergence.png` plot must show a monotonic decrease in the PDE residual component of the loss.\", \"- The implementation must correctly reproduce the steady-state vortex structure for the lid-driven cavity benchmark.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2530, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a GPU-accelerated solver for the pressure-Poisson equation that arises in Lattice Boltzmann Method (LBM) simulations for incompressible fluid dynamics. Please derive the preconditioned Conjugate Gradient (CG) algorithm, implement it using Python with CUDA acceleration (e.g., via CuPy or Numba), and analyze its convergence properties based on the eigenvalue distribution of the system matrix. The analysis should compare the theoretical convergence rate against empirical results for a 2D lid-driven cavity flow problem.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org CUDA accelerated conjugate gradient lattice boltzmann preconditioner eigenvalue\\\", mode=both)\", \"/math(Derive the Preconditioned Conjugate Gradient algorithm for a symmetric positive-definite system. Analyze its convergence rate in terms of the condition number and eigenvalue distribution of the preconditioned matrix, referencing Chebyshev polynomial bounds.)\", \"/code(Implement the solver in Python using CuPy for CUDA acceleration. Profile its computational complexity against a CPU baseline, ensuring the convergence criterion is based on the L2-norm of the residual vector.)\", \"/math(Verify that the empirical convergence rate observed in the code's output plots aligns with the theoretical bounds derived from the matrix eigenvalue spectrum analysis. Audit the preconditioning effectiveness.)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature on GPU solvers for LBM. The math agent then provides the necessary theoretical foundation and convergence analysis. The code agent implements the high-performance solver based on this theory. A final math agent call verifies that the practical implementation behaves as predicted by the theory, closing the loop.\", \"expected_artifacts\": [\"- A LaTeX/PDF document with the full mathematical derivation of the Preconditioned Conjugate Gradient algorithm and its convergence analysis.\", \"- A Python script (`solver.py`) containing the GPU-accelerated implementation using CuPy.\", \"- A performance report (`report.md`) including convergence plots (residual norm vs. iteration) and a table comparing GPU vs. CPU wall-clock times.\", \"- A JSON file (`metrics.json`) containing the computed speedup and convergence rate.\", \"- A list of citations including at least two arXiv preprints with their IDs.\"], \"thinking_outline\": [\"1. Research state-of-the-art preconditioning techniques for Poisson-like equations arising in LBM and their suitability for GPU architectures.\", \"2. Formally derive the Preconditioned Conjugate Gradient (PCG) algorithm and establish the theoretical convergence rate as a function of the condition number of the preconditioned system matrix.\", \"3. Implement the PCG solver in Python using CuPy, focusing on minimizing host-device memory transfers by keeping all relevant vectors and matrices on the GPU.\", \"4. Construct the sparse system matrix corresponding to the 2D lid-driven cavity problem.\", \"5. Validate the GPU solver's numerical correctness by comparing its output against a trusted CPU-based implementation (e.g., `scipy.sparse.linalg.cg`) on a small-scale problem.\", \"6. Verify that the measured convergence rate from the GPU implementation matches the theoretical predictions from the mathematical derivation by analyzing the system's eigenvalue spectrum.\"], \"handoff_plan\": \"/general-search provides relevant papers -> /math uses these to formalize the algorithm and theory -> /code implements the algorithm -> /math verifies the code's empirical results against the theory -> router QA. Verification: The empirical convergence plot from /code must exhibit a slope consistent with the condition number-based bound from /math. Fallback: If verification fails, /code will re-examine the implementation for numerical precision issues, and /math will re-evaluate the assumptions in the convergence analysis.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 key papers on GPU-accelerated PCG for LBM.\", \"- [ ] /math: Produce the complete derivation of the PCG algorithm and its convergence theory.\", \"- [ ] /code: Write the CuPy-based GPU solver and a CPU baseline for comparison.\", \"- [ ] /code: Generate the system matrix for the test case and run benchmarks to produce performance data and convergence plots.\", \"- [ ] /math: Analyze the generated matrix's eigenvalues and verify that the code's convergence plot matches the theoretical rate.\", \"- [ ] /code: Consolidate all code, plots, and metrics into the specified output files.\", \"- [ ] /router: Perform final QA, ensuring all acceptance criteria are met and artifacts are present.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"cuda\", \"computational-fluid-dynamics\"], \"acceptance_criteria\": [\"- The final solver must be numerically correct, matching the output of `scipy.sparse.linalg.cg` to a tolerance of 1e-6.\", \"- The GPU implementation must demonstrate at least a 20x speedup over the baseline NumPy/SciPy CPU implementation for a sufficiently large grid size (e.g., 512x512).\", \"- The final report must contain a log-linear plot of the residual norm vs. iteration count, showing a clear linear descent.\", \"- The mathematical derivation must correctly link the convergence rate to the condition number of the preconditioned system.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2548, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the application of Canonical Polyadic (CP) tensor decomposition for learning disentangled representations in high-dimensional biomedical data. I need to formulate the optimization problem with a sparsity-inducing L1-norm regularizer on the factor matrices and derive the Alternating Direction Method of Multipliers (ADMM) update rules. Furthermore, I need a CUDA-accelerated implementation in JAX to benchmark its convergence rate and representation quality against Tucker decomposition on a simulated dataset, measuring reconstruction error and factor matrix sparsity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org ADMM L1 regularized Canonical Polyadic tensor decomposition convergence biomedical\\\", mode=web)\", \"/math(Derive the full ADMM update rules for L1-regularized CP decomposition based on the augmented Lagrangian, ensuring the convergence criteria for biomedical signal processing are considered.)\", \"/code(Implement the derived ADMM updates in JAX with CUDA acceleration, benchmarking convergence rate and reconstruction error on a simulated high-dimensional federated dataset., using Python)\", \"/math(Formally analyze the convergence properties of the implemented ADMM algorithm, verifying that the primal and dual residuals approach zero and checking the eigenvalue spectrum of the update matrices.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature from arXiv. This informs the /math agent's formal derivation of the ADMM updates. The /code agent then implements this derivation and runs benchmarks. A final call to the /math agent is required to formally analyze the convergence of the algorithm and verify the numerical results, ensuring theoretical soundness.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the ADMM update rules.\", \"- A Python script in JAX containing the CUDA-accelerated implementation of the algorithm.\", \"- A JSON file reporting benchmark metrics, including reconstruction error and sparsity over iterations.\", \"- A summary report (PDF) interpreting the results, with citations to at least 3 relevant arXiv papers.\", \"- A verification log comparing theoretical convergence rates with empirical results from the code.\"], \"thinking_outline\": [\"1. Review recent literature on regularized tensor decomposition methods, particularly ADMM applied to CP decomposition for high-dimensional data.\", \"2. Formulate the L1-regularized CP decomposition objective function and derive the complete ADMM update equations for each factor matrix and the dual variables.\", \"3. Implement the derived ADMM algorithm in JAX, including a CUDA backend for acceleration and a data simulation module that mimics biomedical data properties.\", \"4. Verify the correctness of the mathematical derivation by cross-referencing it with established literature and checking the Lagrangian's stationarity conditions.\", \"5. Validate the JAX implementation by comparing its output on a small, known problem against a reference solution, checking for numerical stability and correctness of the gradient updates.\", \"6. Execute the benchmark on a larger simulated dataset, analyze the convergence plots, reconstruction error, and final factor matrix sparsity to assess performance.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (analysis) -> router QA. Verification: The derivation from /math must be directly implemented by /code. The final analysis by /math must use the numerical outputs from /code to validate theoretical convergence. Fallback: If the derivation is flawed, the router will request a revision from /math before proceeding to /code.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least 3 recent arXiv papers on ADMM for L1-regularized tensor decomposition (verify results)\", \"- [ ] /math: Derive the complete ADMM update rules for the L1-regularized CP decomposition objective.\", \"- [ ] /code: Implement the derived algorithm in JAX, ensuring it can leverage CUDA for acceleration.\", \"- [ ] /code: Create a synthetic data generator and a benchmarking script to measure reconstruction error and sparsity.\", \"- [ ] /math: Formally analyze the convergence conditions for the derived ADMM updates.\", \"- [ ] /math: Verify that the numerical residuals produced by the /code agent's implementation converge as predicted by the theoretical analysis.\", \"- [ ] /code: Run the final benchmarks and generate plots for the summary report.\", \"- [ ] Router QA: Review all artifacts for correctness, consistency, and fulfillment of acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"admm-optimization\"], \"acceptance_criteria\": [\"- The LaTeX derivation of ADMM update rules must be mathematically sound and complete.\", \"- The JAX implementation must pass a unit test on a small-scale problem with a known solution.\", \"- The final benchmark report must show a reconstruction error below 1e-4 and a factor matrix sparsity of at least 70% on the specified synthetic dataset.\", \"- The summary report must correctly cite at least 3 relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2553, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the denoising score-matching objective for a variance-preserving stochastic differential equation (VP-SDE) from first principles. I need to see the connection to the Fokker-Planck equation and the time-reversal argument. Subsequently, implement a predictor-corrector sampler in JAX using these theoretical results to generate samples from a 2D Gaussian mixture model.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models by solving stochastic differential equations Song Ermon\\\", mode=web)\", \"/math(Derive the denoising score-matching loss for a continuous-time diffusion process defined by a VP-SDE. Explicitly show the connection to the reverse-time SDE and Fokker-Planck equation to justify the Bayesian objective.)\", \"/code(Implement a predictor-corrector sampler in JAX for a 2D GMM. Use Euler-Maruyama for prediction and Langevin MCMC for correction, ensuring numerical stability and tracking convergence via MMD., using Python)\", \"/math(Analyze the convergence properties of the implemented sampler. Verify that the discretization of the reverse SDE in the code aligns with the derived theoretical model, highlighting potential sources of error.)\"], \"route_rationale\": \"The blue /general-search agent first gathers foundational literature on score-based SDEs. This informs the /math agent's formal derivation of the core objective function. The /code agent then translates this mathematical framework into a practical JAX implementation. A final /math step provides crucial verification, ensuring the code's behavior is consistent with the derived theory.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the score-matching objective.\", \"A Python script (`sampler.py`) with the JAX implementation of the predictor-corrector sampler.\", \"A summary of at least two cited arXiv papers with their IDs and relevance.\", \"A PNG plot visualizing the evolution of generated samples converging to the target 2D GMM distribution.\", \"A verification report in Markdown comparing empirical MMD convergence to theoretical expectations.\"], \"thinking_outline\": [\"1. Research and synthesize the foundational theory connecting score-matching, diffusion processes, and stochastic differential equations, focusing on the work by Song et al.\", \"2. Formally derive the score-matching objective for a VP-SDE, starting from the forward process and deriving the corresponding reverse-time SDE. Explicitly use the Fokker-Planck equation or integration by parts in the derivation.\", \"3. Translate the derived reverse-time SDE into a discrete-time predictor-corrector sampling algorithm (e.g., Euler-Maruyama + Langevin MCMC) and implement it in JAX.\", \"4. Validate the implementation by running the sampler on a simple, known target distribution (2D GMM) and visually inspecting the quality of the generated samples.\", \"5. Verify the correctness of the derivation and implementation by comparing the empirical convergence rate (measured by MMD) against theoretical bounds.\", \"6. Consolidate all findings\u2014derivation, code, plots, and verification analysis\u2014into a final report.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA (verification: Cross-check MMD from code against theoretical convergence rates from math analysis; fallback: If divergence, ask /code to check numerical stability or /math to re-check discretization error assumptions.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers (e.g., Song & Ermon) on score-based generative modeling with SDEs.\", \"- [ ] /math: Produce a formal derivation of the VP-SDE score-matching objective, showing all intermediate steps.\", \"- [ ] /code: Implement the predictor-corrector sampler in JAX for a 2D GMM target.\", \"- [ ] /code: Generate plots showing the sampling trajectory and a final MMD convergence curve.\", \"- [ ] /math: Verify that the discrete-time update rules in the JAX code are a valid approximation of the continuous-time SDE derivation.\", \"- [ ] /math: Formulate the expected convergence behavior and compare it against the empirical MMD metric from the code.\", \"- [ ] router: Perform final QA check, ensuring all artifacts are present and the acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-modeling\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly links the forward SDE, reverse SDE, and the final score-matching objective.\", \"- The JAX implementation of the predictor-corrector sampler successfully generates samples that visually match the target 2D GMM distribution.\", \"- The final report includes the derivation, plots, and cites at least two relevant arXiv papers with IDs.\", \"- The verification step confirms that the empirical MMD decreases over sampling steps, consistent with convergence.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2567, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive and implement a spectral regularizer for the self-attention mechanism within a contrastive learning framework. The goal is to prevent dimensional collapse by encouraging a more uniform singular value distribution of the attention matrix. Analyze the regularizer's effect on the convergence properties and downstream classification accuracy using a Bayesian optimization approach for hyperparameter tuning. The derivation must leverage concepts from matrix calculus and information-theoretic principles related to entropy maximization.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org self-supervised learning attention collapse spectral regularization information bottleneck bayesian methods\\\", mode=web)\", \"/math(Derive the gradient for a spectral entropy regularizer on attention matrices. Analyze the convergence implications and the connection to the model's eigenvalue spectrum.)\", \"/code(Implement the spectral regularizer in a PyTorch SimCLR model for CIFAR-10. Use CUDA for acceleration and track downstream classification AUPRC. Integrate Bayesian optimization for tuning the regularization strength., using Python)\", \"/general-search(query=\\\"site:arxiv.org state-of-the-art representational collapse mitigation SSL benchmarks computational complexity\\\", mode=web)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on spectral properties of attention and SSL. The /math agent then uses this to produce a formal derivation. Next, the /code agent implements and validates the theory empirically. A final search contextualizes the results against SOTA benchmarks.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the spectral regularizer and its gradient.\", \"A Python script implementing the regularizer within a SimCLR framework, including the training and evaluation loop.\", \"A summary report including plots of singular value distributions, downstream task performance metrics, and a bibliography with at least 3 arXiv citations.\", \"A verification log confirming the analytical gradient matches a numerical approximation.\"], \"thinking_outline\": [\"1. Review literature on representational collapse in self-supervised learning and the spectral properties of attention matrices.\", \"2. Formulate a regularizer based on the entropy of the singular value distribution of the attention matrix to encourage a wider spectrum.\", \"3. Derive the gradient of this regularizer with respect to the attention weights using matrix calculus and the chain rule.\", \"4. Implement the regularizer in a PyTorch-based SSL model and design an experiment to measure its impact on representation quality.\", \"5. Validate the implementation by checking gradient correctness using numerical methods (e.g., finite differences) to ensure the derivation and code align.\", \"6. Analyze the experimental results, comparing them against a baseline without the regularizer, and audit the change in the attention matrices' singular value spectra.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA. The search agent provides foundational papers to the math agent. The math agent provides a formal derivation and gradient formula to the code agent. The code agent implements the model and provides empirical results, plots, and a verification log. router QA performs the final review. Verification: The analytical gradient from /math must match the numerical gradient computed in the /code implementation. Fallback: If the derivation is intractable, /math will flag it, and the router will initiate a new search for alternative, simpler regularizer formulations.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on attention collapse and spectral regularization in deep learning.\", \"- [ ] /math: Derive the spectral entropy regularizer and its gradient with respect to attention weights.\", \"- [ ] /code: Implement the SimCLR baseline and the proposed regularizer in PyTorch.\", \"- [ ] /code: Integrate a Bayesian optimization loop for hyperparameter tuning of the regularization strength.\", \"- [ ] /math: Verify the analytical gradient derivation aligns with the numerical gradient check from the implementation.\", \"- [ ] /code: Run experiments and generate plots for singular value entropy and downstream accuracy.\", \"- [ ] /general-search: Find SOTA benchmarks for SSL regularization to contextualize results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"attention-mechanisms\", \"self-supervised-learning\", \"spectral-methods\"], \"acceptance_criteria\": [\"The provided mathematical derivation must be complete and self-contained.\", \"The implemented code must reproduce the reported metrics and pass the gradient verification check.\", \"The final report must show a statistically significant improvement in either the downstream task accuracy or the singular value entropy metric compared to the baseline.\", \"The solution must cite at least three relevant academic papers from arXiv.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2603, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of online dictionary learning for sparse coding in compressed sensing. Please derive the update rules for an online dictionary learning algorithm, like the one proposed by Mairal et al., focusing on the stochastic gradient descent approach for the dictionary update and the LARS or ISTA algorithm for the sparse coding step. Then, implement this in Python, compare its convergence rate and reconstruction fidelity against a batch method like K-SVD on synthetic signals with known sparsity, and analyze the computational complexity trade-offs, particularly for processing high-dimensional biomedical data streams.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org online dictionary learning Mairal convergence analysis K-SVD complexity\\\", mode=web)\", \"/math(Derive the stochastic gradient update for the dictionary atoms and the LASSO subproblem solution using an ADMM formulation. Analyze the convergence conditions and the Hessian eigenvalue distribution under a biomedical signal model.)\", \"/code(Implement online dictionary learning and K-SVD in Python. Generate synthetic sparse signals using a Bernoulli-Gaussian model, track reconstruction MSE, and profile the computational complexity per epoch.)\", \"/math(Validate the empirical convergence rate from the simulation against theoretical bounds from literature. Audit the implementation's handling of the regularization parameter and its effect on the solution's sparsity and stability., includes graphcut)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather seminal papers on online dictionary learning and K-SVD. This literature informs the /math agent's formal derivation of the update equations and convergence analysis. The /code agent then implements these derivations for empirical validation and performance comparison. A final /math step is crucial for verification, ensuring the empirical results align with theoretical guarantees and auditing the code's logic.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Online Dictionary Learning algorithm's update rules in a PDF document.\", \"- A Python script (`main.py`) implementing both ODL and K-SVD, capable of reproducing the analysis.\", \"- A set of plots (`convergence.png`, `reconstruction_error.png`) comparing the performance of the two algorithms.\", \"- A summary report in Markdown (`report.md`) explaining the findings, including a complexity analysis and at least two citations to arXiv papers.\", \"- A JSON file (`metrics.json`) containing the final MSE and dictionary recovery scores for both methods.\"], \"thinking_outline\": [\"1. Formulate the dictionary learning problem as an empirical risk minimization problem with L1 regularization for the sparse codes and a constraint on the dictionary atoms (e.g., unit L2-norm).\", \"2. Derive the update rules for the online algorithm, which involves alternating between a sparse coding step (solving a LASSO problem) for a given signal and a dictionary update step using stochastic gradient descent.\", \"3. Implement the derived online algorithm and a standard batch algorithm (K-SVD) in Python, creating a testing framework with synthetic data generation.\", \"4. Validate the implementation by checking that the dictionary atoms remain normalized and that the sparse codes adhere to the specified sparsity level.\", \"5. Empirically evaluate and compare the convergence speed (reconstruction error vs. number of signals processed) and final dictionary quality (atom recovery rate) of both algorithms.\", \"6. Verify that the observed empirical convergence behavior is consistent with the theoretical guarantees discussed in the literature sourced from the initial search.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation & simulation) -> /math (verification of results against theory) -> router QA (final check for consistency between derivation, code, and report; fallback: if simulation contradicts theory, re-examine the derivation in /math).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize key papers on Online Dictionary Learning (Mairal et al.) and K-SVD, focusing on algorithmic descriptions and convergence proofs.\", \"- [ ] /math: Derive the complete update rules for Online Dictionary Learning based on the sourced literature.\", \"- [ ] /code: Implement the ODL algorithm and the K-SVD baseline algorithm in Python using NumPy/SciPy.\", \"- [ ] /code: Develop a simulation harness to generate synthetic data and run comparative experiments, logging metrics to a file.\", \"- [ ] /math: Verify that the implemented code's update steps are a correct translation of the mathematical derivation.\", \"- [ ] /math: Analyze the plots from the /code agent and formally compare the empirical convergence with the theoretical bounds.\", \"- [ ] /router: Perform final QA review, ensuring all artifacts are present and consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\", \"optimization-algorithms\"], \"acceptance_criteria\": [\"- The mathematical derivation for the ODL update rules must be complete and mathematically sound.\", \"- The Python implementation must run without errors and generate plots comparing ODL and K-SVD on reconstruction MSE over iterations.\", \"- The final report must correctly conclude which algorithm has better computational complexity for the streaming data scenario and cite at least two relevant academic papers with arXiv IDs.\", \"- The dictionary recovery rate for both algorithms on synthetic data must exceed 85% for a sufficiently low-noise setting.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2674, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous comparison between Persistent Homology via Vietoris-Rips complexes and Laplacian Eigenmaps for manifold learning. Specifically, derive the theoretical stability guarantees for persistence diagrams under Hausdorff noise using the bottleneck distance, and contrast this with the spectral convergence rates of the graph Laplacian. Please provide a simulation on a noisy Swiss roll dataset to empirically validate the theoretical convergence claims.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org persistent homology stability theorem bottleneck distance\\\" AND \\\"graph laplacian spectral convergence manifold learning\\\", mode=web)\", \"/math(Derive the proof for the Stability Theorem of persistence diagrams under a Bayesian noise model and analyze the spectral graph theory for Laplacian convergence, focusing on the Fiedler eigenvalue behavior.)\", \"/code(Implement in Python both Vietoris-Rips persistence and Laplacian Eigenmaps for a noisy manifold, using bootstrap resampling to estimate confidence interval plots for reconstruction error versus noise level.)\", \"/math(Formally verify the empirical convergence rates from the code against the theoretical bounds derived earlier, analyzing the computational complexity and applying Chebyshev's inequality to bound deviations.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather seminal papers on stability and convergence. The /math agent then formalizes these theories. The /code agent provides empirical validation through simulation. A final /math agent reconciles theory with empirical results, ensuring a complete, verified analysis.\", \"expected_artifacts\": [\"- A LaTeX document containing the mathematical derivations for both the Persistent Homology stability theorem and the spectral convergence of Laplacian Eigenmaps.\", \"- A Python script (`main.py`) that generates the noisy manifold, implements both algorithms, and produces comparison plots.\", \"- A final PDF report synthesizing the theoretical and empirical findings, including plots and at least two citations to academic papers.\", \"- A JSON file (`metrics.json`) reporting the final computed values for Manifold Reconstruction Error and Bottleneck Distance across different noise levels.\"], \"thinking_outline\": [\"1. Research and synthesize the foundational theorems governing the stability of persistent homology (Isometry and Stability Theorems) and the convergence of graph Laplacians to the Laplace-Beltrami operator.\", \"2. Formulate a rigorous mathematical derivation comparing the robustness of the two methods under a defined probabilistic noise model, contrasting the topological (bottleneck distance) and geometric (spectral) perspectives.\", \"3. Develop a Python simulation to generate point clouds from a known manifold (e.g., Swiss roll) with varying levels of noise and sample density.\", \"4. Implement both Persistent Homology (using Ripser/Gudhi) and Laplacian Eigenmaps (using scikit-learn) and apply them to the simulated data.\", \"5. Validate the implementation by checking that the persistence diagram of the clean data reflects the known topology (e.g., Betti numbers) of the manifold.\", \"6. Empirically measure and plot the performance metrics (Bottleneck Distance, Procrustes Error) as a function of noise, and verify that the observed decay rates align with the derived theoretical bounds.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides foundational papers to the /math agent. The /math agent's derivations serve as the formal specification for the /code agent. The /code agent's empirical results are then passed back to the final /math agent for theoretical verification. Verification: The router will check that the code implementation's core logic matches the mathematical formulas from the /math agent. Fallback: If empirical results diverge significantly from theory, the /code agent will be tasked to investigate potential implementation bugs or model assumption violations.\", \"todo_list\": [\"- [ ] /general-search: Find at least two key papers from arXiv on persistent homology stability and graph Laplacian convergence.\", \"- [ ] /math: Derive the stability theorem for persistence diagrams, clearly stating all assumptions.\", \"- [ ] /math: Derive the spectral convergence proof for the graph Laplacian.\", \"- [ ] /code: Implement the data generation pipeline for a noisy Swiss roll manifold.\", \"- [ ] /code: Implement wrappers for Persistent Homology and Laplacian Eigenmaps algorithms and the evaluation metrics.\", \"- [ ] /math: Verify that the empirical convergence plots from the code align with the derived theoretical bounds.\", \"- [ ] /math: Check that the Python code's mathematical operations correctly mirror the formal derivations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"spectral-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies the Stability Theorem for Persistence Diagrams, referencing the bottleneck distance.\", \"- The Python simulation correctly generates the specified noisy manifold and computes both PH and LE embeddings.\", \"- The final report compares the methods using both Bottleneck distance and Procrustes analysis, referencing at least 2 arXiv papers with IDs.\", \"- The empirical results plots clearly demonstrate the relationship between noise level and reconstruction error for both methods.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2676, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nPlease provide a formal derivation of the reverse-time stochastic differential equation (SDE) central to score-based generative modeling, starting from its corresponding forward-time Fokker-Planck equation. Subsequently, implement a 1D score-matching network using JAX to approximate the score function for a bimodal Gaussian mixture distribution. Your implementation should demonstrate the sampling process and analyze convergence properties by tracking the Fr\u00e9chet Inception Distance (FID).\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time SDE Fokker-Planck equation derivation\\\", mode=web)\", \"/math(Derive the reverse-time SDE for a Bayesian diffusion process, starting from the forward process Fokker-Planck equation, showing the emergence of the score function and its implications for model convergence.)\", \"/code(Implement a 1D score-matching network in JAX using an Euler-Maruyama solver for the reverse-time SDE. Track the FID score against a target bimodal distribution and ensure CUDA acceleration is utilized., using Python)\", \"/math(Verify the consistency of the implemented Euler-Maruyama discretization against the continuous-time SDE derivation, analyzing the local truncation error and its theoretical effect on the final FID metric.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on the SDE/Fokker-Planck connection. This context is handed to /math for the core theoretical derivation. /code then implements this theory in JAX. A final /math call is crucial for verifying that the code's discrete-time solver correctly approximates the continuous-time theory, a common failure point.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the reverse-time SDE in LaTeX format.\", \"- A JAX/Flax implementation of the 1D score-matching model and sampling loop.\", \"- A report in Markdown format containing visualizations of the sample distribution evolving over time and a plot of FID vs. training steps.\", \"- A JSON file containing the final FID score and a list of at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Use search to gather authoritative sources (e.g., papers by Song, Ermon, Ho) that detail the mathematical link between forward SDEs, their Fokker-Planck representation, and the corresponding reverse-time SDEs used in score-based generative models.\", \"2. Formally derive the reverse-time SDE, explicitly showing how the score of the data distribution, \u2207_x log p_t(x), arises from the time-reversal of the diffusion process.\", \"3. Implement a neural network in JAX to approximate this score function using denoising score matching. Implement a sampler based on a numerical SDE solver (e.g., Euler-Maruyama) for the derived reverse-time SDE.\", \"4. Validate the implementation by generating samples and comparing their distribution to the target bimodal Gaussian mixture. Compute the FID score as the primary metric for sample quality.\", \"5. Check the derivation by cross-referencing it with the papers found in step 1. Verify that the implemented discretization in the sampler is a first-order accurate approximation of the derived continuous-time SDE.\", \"6. Consolidate all findings\u2014derivation, code, plots, and metrics\u2014into a final report, ensuring all claims are supported by the generated evidence.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The /math agent uses arXiv papers from /general-search to ground the derivation. The /code agent implements the SDE from /math. The second /math agent audits the code's discretization against the derivation. Router QA performs a final check on FID metric and derivation correctness. Fallback: If FID fails to converge, the /code agent is re-triggered to debug the score network architecture or SDE solver step size.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on score-based models and the Fokker-Planck connection.\", \"- [ ] /math: Produce a step-by-step derivation of the reverse-time SDE, annotating each major step.\", \"- [ ] /code: Implement the score network and the Euler-Maruyama sampler in JAX.\", \"- [ ] /code: Train the model and generate samples, calculating the FID score periodically.\", \"- [ ] /math: Verify that the code's update rule is a valid discretization of the derived SDE.\", \"- [ ] /code: Create plots visualizing the learned distribution and the FID convergence curve.\", \"- [ ] /router: Perform final QA review, checking consistency between the derivation, implementation, and reported metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"score-based-modeling\", \"stochastic-calculus\"], \"acceptance_criteria\": [\"- The provided mathematical derivation correctly links the forward Fokker-Planck equation to the reverse-time SDE.\", \"- The JAX code runs without errors and generates samples from a distribution that qualitatively matches the target bimodal Gaussian.\", \"- The final reported FID score is below 50, indicating successful learning.\", \"- The final report includes properly formatted citations for at least two relevant arXiv papers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2683, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a GPU-accelerated preconditioned conjugate gradient solver in CUDA for a sparse linear system Ax=b arising from a finite element discretization of the Poisson equation. The preconditioner must be based on an incomplete Cholesky factorization, and I need a rigorous analysis of its convergence rate, specifically deriving bounds on the condition number reduction and relating it to the eigenvalue distribution of the preconditioned matrix M\u207b\u00b9A.\n", "completion": "{\"route_plan\": [\"/math(Derive the Preconditioned Conjugate Gradient (PCG) algorithm and analyze the spectral properties of the preconditioned matrix M\u207b\u00b9A, specifically focusing on how an incomplete Cholesky preconditioner affects the eigenvalue distribution and Chebyshev polynomial bounds on convergence.)\", \"/general-search(query=\\\"site:arxiv.org parallel incomplete cholesky factorization CUDA GPU sparse linear systems\\\", mode=web)\", \"/code(Implement the PCG solver in Python using CUDA (via CuPy) for a large sparse system from a Poisson discretization. Profile the kernel performance and track the convergence rate against the theoretical eigenvalue bounds.)\"], \"route_rationale\": \"The route starts with /math to establish the core theoretical foundation for the PCG algorithm and its convergence properties. The blue /general-search agent is then used to research state-of-the-art parallel algorithms for the incomplete Cholesky preconditioner, a specialized topic not covered by standard texts. Finally, /code implements the algorithm on the GPU to validate the theory and measure performance.\", \"expected_artifacts\": [\"A LaTeX document containing the full derivation of the PCG algorithm and the convergence analysis proof.\", \"A documented Python/CUDA source code file for the solver and a test harness.\", \"A performance report comparing theoretical vs. empirical convergence rates, including GPU kernel timings.\", \"A verification script that checks the final solution's residual norm `||Ax-b||\u2082` is below a specified tolerance.\", \"A bibliography with at least 2 relevant arXiv papers on parallel preconditioning.\"], \"thinking_outline\": [\"1. Derive the Preconditioned Conjugate Gradient (PCG) algorithm from first principles for a symmetric positive-definite system Ax=b.\", \"2. Theoretically analyze the convergence rate of PCG in terms of the condition number \u03ba(M\u207b\u00b9A). Derive the relationship between the eigenvalue distribution and the number of iterations required to reach a given tolerance using Chebyshev polynomials.\", \"3. Research and select an efficient parallel algorithm for incomplete Cholesky (IC) factorization suitable for sparse matrices on a GPU architecture.\", \"4. Implement the selected parallel IC factorization and the full PCG solver using CUDA. Test on a well-defined sparse matrix generated from a finite element discretization of the 2D Poisson equation.\", \"5. Validate the implementation by comparing the observed convergence rate against the theoretical bounds derived in step 2. Check the final residual norm `||Ax-b||\u2082` to ensure correctness.\", \"6. Audit the CUDA kernel performance, identifying any memory bandwidth or latency bottlenecks.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivations from /math provide the theoretical specifications. /general-search finds a suitable parallel algorithm for the implementation. /code builds and tests the solver. The final handoff to router QA involves a verification step where the code's empirical convergence is checked against the theoretical bounds from /math. Fallback: If convergence fails or is slow, revisit the preconditioner choice in /math or the parallel algorithm from /general-search.\", \"todo_list\": [\"- [ ] /math: Derive the PCG algorithm and the convergence bounds based on the eigenvalue spectrum of the preconditioned system.\", \"- [ ] /general-search: Find and summarize at least two high-authority sources from arXiv on parallel incomplete Cholesky factorization algorithms for GPUs.\", \"- [ ] /code: Implement the PCG solver and the parallel IC(0) preconditioner in Python with a CUDA backend (CuPy).\", \"- [ ] /code: Create a test harness using a sparse matrix from a 2D Poisson problem and measure convergence rate and wall-clock time.\", \"- [ ] /math: Verify that the empirical convergence rate from the code aligns with the derived Chebyshev bounds based on the estimated condition number.\", \"- [ ] /code: Verify the final solution by calculating the L2 norm of the residual vector `||b - Ax||\u2082` and ensuring it's below the specified tolerance.\", \"- [ ] router: Perform final QA check on the consistency of the theoretical report, code implementation, and performance results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"krylov-methods\", \"cuda\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly outlines the PCG algorithm and its convergence properties with respect to the preconditioned system's eigenvalues.\", \"- The CUDA implementation correctly solves a sparse linear system for a 1024x1024 grid Poisson problem to a residual tolerance of 1e-8.\", \"- The final report includes a plot comparing the theoretical convergence bound with the empirically observed residual norm reduction per iteration.\", \"- The solver implementation is numerically stable and free of race conditions.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2707, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the reverse-time stochastic differential equation (SDE) for a variance-preserving diffusion process, starting from the associated Fokker-Planck equation. Then, implement a score-based sampler in JAX that leverages this derivation to generate samples from a 2D distribution like a Swiss roll, ensuring numerical stability and tracking the model's convergence via the score-matching loss.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org score-based generative models reverse-time SDE Fokker-Planck\\\", mode=rag)\", \"/math(Derive the reverse-time SDE for a variance-preserving diffusion process, showing the connection to the score function \u2207x log p(x) via the Fokker-Planck equation, citing key results from arXiv.)\", \"/code(Implement a JAX-based score-matching diffusion model for a 2D Swiss roll dataset, using an Euler-Maruyama solver for the reverse SDE and tracking training loss and sample convergence with CUDA acceleration., using Python)\", \"/math(Verify that the implemented Euler-Maruyama discretization step in the JAX code is a first-order correct approximation of the derived continuous-time reverse SDE, analyzing potential sources of error and convergence issues.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find authoritative literature on score-based models. This informs the /math agent's formal derivation of the core SDE. The derived theory is then handed to the /code agent for a high-performance JAX implementation. A final verification step with the /math agent ensures the numerical method in the code correctly implements the derived SDE, closing the loop between theory and practice.\", \"expected_artifacts\": [\"A LaTeX document containing the full, annotated derivation of the reverse-time SDE.\", \"A JAX Python script implementing the score-based generative model, SDE sampler, and training loop.\", \"A final report in Markdown summarizing the findings, including plots of generated samples over time and a list of cited arXiv papers.\", \"A verification memo from the final math step confirming the correctness of the numerical implementation against the derivation.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation by reviewing the connection between forward diffusion processes, score functions (\u2207 log p), and reverse-time SDEs using authoritative sources.\", \"2. Formally derive the reverse-time SDE from the forward process SDE, likely using the Fokker-Planck equation as an intermediate step.\", \"3. Implement the score network and the numerical solver (e.g., Euler-Maruyama) for the derived reverse-time SDE in JAX for performance.\", \"4. Validate the implementation's correctness by training the model and generating samples, visually inspecting their quality as they evolve from noise to the target distribution.\", \"5. Verify the mathematical integrity of the code by explicitly checking that the SDE discretization in the sampler is a consistent, first-order approximation of the continuous-time equation derived earlier.\", \"6. Audit the model's convergence by monitoring the score-matching training loss and ensuring it decreases to a stable, low value.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: final /math step validates /code implementation against initial /math derivation; fallback: if discretization is incorrect, the /code agent must refactor the SDE solver based on /math feedback).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on score-based diffusion (e.g., by Song, Ermon) from arXiv.\", \"- [ ] /math: Perform the derivation of the reverse-time SDE, annotating each step with clear justifications.\", \"- [ ] /code: Implement the score network (e.g., a simple MLP) and the reverse SDE sampler in JAX.\", \"- [ ] /code: Train the model on a 2D Swiss roll dataset and generate visualizations of the sampling process.\", \"- [ ] /math: Formally verify that the code's update rule for the sampler is a consistent discretization of the derived SDE.\", \"- [ ] /math: Check the initial derivation for mathematical rigor and consistency with sources found by /general-search.\", \"- [ ] router: Review all artifacts for coherence, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"diffusion-models\", \"stochastic-calculus\", \"generative-modeling\"], \"acceptance_criteria\": [\"The mathematical derivation must be complete, rigorous, and correctly link the forward process to the reverse-time SDE via the score function.\", \"The JAX implementation must successfully train and generate samples that visually approximate the target 2D Swiss roll distribution.\", \"The final report must include at least 2 citations to relevant arXiv preprints, including their IDs.\", \"The verification memo must confirm that the discretization scheme implemented in the code matches the derived theory.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2719, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm tasked with developing a message-passing graph neural network to learn a Hamiltonian potential for N-body molecular dynamics simulations of liquid water. How can I formally derive the SE(3)-equivariant update rules to guarantee the conservation of total energy and momentum, and then implement this architecture in PyTorch Geometric, validating the long-term trajectory stability against a standard TIP3P force field?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org SE(3) equivariant graph neural network Hamiltonian molecular dynamics chemistry\\\", mode=both)\", \"/math(Derive the equivariant message-passing update rules for atomic state vectors under SE(3) transformations, ensuring the learned Hamiltonian potential function conserves total system energy and momentum, and detail the resulting force field calculation.)\", \"/code(Implement the SE(3)-equivariant GNN in PyTorch Geometric, using a symplectic Velocity Verlet integrator for the molecular dynamics simulation of a water box. Track key metrics like energy drift to analyze convergence and stability., using Python)\", \"/math(Formulate the statistical validation framework to compare the GNN simulation's long-term trajectory stability against a classical force field. Define the primary metric as total energy drift over a 1 nanosecond trajectory and secondary metrics including radial distribution function error., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the approach in state-of-the-art literature on equivariant GNNs. This informs the /math agent's formal derivation of the model. The /code agent then implements this specific mathematical model and runs the simulation. Finally, the /math agent returns to define and analyze the results using rigorous statistical metrics, ensuring the implementation meets the physics-based constraints of the problem.\", \"expected_artifacts\": [\"A PDF report detailing the mathematical derivation of the SE(3)-equivariant update rules and the Hamiltonian formalism used.\", \"A Python script implementing the GNN model, the MD simulation loop, and the analysis.\", \"A CSV file logging the time-series data of total energy for both the GNN and baseline model simulations.\", \"A summary of the literature review with citations and links to at least two relevant arXiv papers.\", \"A PNG image file containing plots that compare the radial distribution functions and total energy drift between the GNN and the baseline.\"], \"thinking_outline\": [\"1. Review literature on SE(3) or E(3) equivariant GNNs to understand how rotational and translational invariances are mathematically enforced in message-passing frameworks for molecular systems.\", \"2. Formulate the Hamiltonian for the molecular system and derive the corresponding force field as the negative gradient of the potential energy, which will be approximated by the GNN.\", \"3. Implement the GNN architecture, paying close attention to the construction of equivariant features (e.g., using spherical harmonics or tensor products) and ensuring the symplectic integrator is correctly implemented to minimize numerical energy drift.\", \"4. Design a robust validation experiment comparing the GNN's long-term trajectory stability against a well-established classical force field; verify that the simulation conserves energy over nanosecond-scale trajectories.\", \"5. Statistically analyze the simulation outputs, quantifying the deviation in structural properties (like radial distribution functions) and thermodynamic properties (like total energy).\", \"6. Validate the implementation by checking for numerical stability issues and audit the energy conservation logs for any systematic, non-physical drift.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides foundational papers to /math. /math provides the specific equations of motion and force field calculations to /code. /code produces simulation data logs, which are then analyzed by the second /math call. Verification: The router QA will check that the final energy drift metrics from the code match the targets defined by the /math agent. Fallback: If the code implementation shows significant energy drift, it will be handed back to /math to re-check the derivation of the symplectic integrator or force calculation.\", \"todo_list\": [\"- [ ] /general-search: Find at least 2 state-of-the-art papers from arXiv on equivariant GNNs for learning Hamiltonian potentials.\", \"- [ ] /math: Derive the SE(3) equivariant update rules and the force field calculation based on the literature.\", \"- [ ] /code: Implement the GNN model and simulation loop in PyTorch Geometric, using a Velocity Verlet integrator.\", \"- [ ] /code: Run a 1ns simulation for both the GNN and a TIP3P baseline model, logging total energy and atomic positions.\", \"- [ ] /math: Verify that the implemented force calculation in the code aligns with the derived mathematical formulation.\", \"- [ ] /math: Define the metrics for energy drift and radial distribution function (RDF) error.\", \"- [ ] /code: Generate plots comparing the GNN and baseline for energy drift and RDFs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"computational-chemistry\"], \"acceptance_criteria\": [\"- The mathematical derivation for SE(3) equivariance must be correct, self-contained, and consistent with the cited literature.\", \"- The implemented GNN model must demonstrate a mean total energy drift of less than 1% over a 1ns NVE simulation for a 128-molecule water box.\", \"- The final report must cite at least 2 relevant papers from arXiv, including their persistent identifiers.\", \"- The radial distribution function (O-O and O-H) from the GNN simulation must qualitatively match the baseline TIP3P model's RDF.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2743, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating network pruning techniques and need to formulate a weight sparsification problem for a ResNet block as a constrained optimization problem. Specifically, derive the iterative update rules using the dual ascent method for a Lagrangian relaxation that minimizes an L1 regularization term subject to a hard constraint on the squared Frobenius norm of the weight tensor, ensuring the budget does not exceed a predefined constant. How do the Karush-Kuhn-Tucker conditions inform the convergence properties of this specific non-convex problem?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Lagrangian relaxation ADMM neural network pruning convergence theory\\\", mode=web)\", \"/math(Formulate the primal and dual problems for L1 minimization with a Frobenius norm constraint. Derive the dual ascent update rules and analyze the KKT conditions for optimality, paying close attention to any Euler-Lagrange dependencies.)\", \"/code(Implement the derived dual ascent algorithm in PyTorch for a target weight tensor. The implementation must track primal/dual objectives, constraint violation, and use CUDA for acceleration to analyze convergence., using Python)\", \"/math(Verify that the terminal state of the implemented algorithm approximately satisfies the derived KKT conditions. Audit the empirical convergence rate against theoretical bounds found in the literature.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey modern literature on Lagrangian methods in deep learning, as this is a rapidly evolving field. This informs the /math agent's formal derivation of the optimization problem. The /code agent then implements this verified mathematical framework. A final /math step is crucial for verifying that the code's empirical behavior aligns with the theoretical derivation and KKT conditions, ensuring correctness.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Lagrangian dual and the dual ascent update rules in a LaTeX PDF.\", \"A Python script implementing the constrained optimization procedure.\", \"A summary report (Markdown) that includes plots of the primal/dual objective convergence, constraint violation, and cites at least two relevant arXiv papers.\", \"A JSON file containing final metrics: sparsity level and terminal constraint violation.\", \"A verification document confirming the approximate satisfaction of KKT conditions at the solution.\"], \"thinking_outline\": [\"1. Formulate the network weight sparsification task as a precise mathematical constrained optimization problem: minimize L1 norm subject to an upper bound on the Frobenius norm.\", \"2. Consult recent literature via /general-search to identify state-of-the-art approaches and theoretical convergence guarantees for applying dual ascent or ADMM to such non-convex problems.\", \"3. Derive the Lagrangian, the corresponding dual function, and the iterative update equations for both the primal variables (weights) and the dual variable (the Lagrange multiplier).\", \"4. Implement the algorithm and validate its correctness by checking if the primal iterates approach a sparse solution while the constraint violation converges to zero.\", \"5. Verify the final solution by plugging it back into the KKT conditions derived in step 3 to ensure they are satisfied within a small tolerance.\", \"6. Audit the empirical results against the theoretical claims found in the literature, noting any discrepancies in convergence speed or solution quality.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: The math agent first verifies the problem formulation from the literature. The code agent then implements the exact derivation from the math agent. The router finally checks if the code's output metrics satisfy the theoretical KKT conditions from the math derivation; fallback: If KKT conditions are not met, the router will re-engage the /math agent to check for potential issues with subgradient selection or step-size assumptions.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize \u22652 arXiv papers on Lagrangian methods for deep neural network pruning or regularization.\", \"- [ ] /math: Derive the complete dual ascent update rules for the specified L1 minimization problem with a Frobenius norm constraint.\", \"- [ ] /code: Implement the algorithm in PyTorch, ensuring it logs the primal objective, dual objective, and constraint violation at each iteration.\", \"- [ ] /math: Formulate the specific KKT conditions for this problem and write a verification function to check them.\", \"- [ ] /code: Execute the implementation on a sample tensor, generate convergence plots, and save the final metrics to a JSON file.\", \"- [ ] /math: verify the implementation's output by checking the KKT conditions and comparing the empirical constraint violation against the theoretical expectation of zero.\", \"- [ ] /router: Perform final QA review on all artifacts, ensuring the derivation, code, and report are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"lagrangian-multipliers\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies the method of Lagrange multipliers and explicitly states the KKT conditions for the problem.\", \"- The Python implementation converges to a solution where the Frobenius norm constraint violation is less than 1e-5.\", \"- The final report cites at least two relevant arXiv papers with their IDs.\", \"- The final weight tensor achieves a sparsity level of at least 50% (percentage of elements with magnitude < 1e-6).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2778, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the theoretical underpinnings of Trust Region Policy Optimization (TRPO), specifically focusing on the Kullback-Leibler divergence constraint as an information-geometric trust region. Implement a TRPO agent in PyTorch to solve a continuous control theory benchmark like MuJoCo's Hopper-v4, and critically analyze its performance against a vanilla policy gradient method, verifying the monotonic improvement guarantee empirically.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Trust Region Policy Optimization natural policy gradient convergence proof\\\", mode=rag)\", \"/math(Derive the TRPO objective function with the KL-divergence constraint, formalizing the connection to natural policy gradients and control theory principles to ensure theoretical convergence.)\", \"/code(Implement the TRPO algorithm in PyTorch, including the conjugate gradient solver for the Fisher-vector product and Generalized Advantage Estimation, targeting a continuous control environment., using Python)\", \"/math(Verify the correctness of the conjugate gradient implementation by auditing its matrix-free computation of the Fisher-vector product against the formal mathematical derivation., includes cybersecurity)\"], \"route_rationale\": \"The blue general-search agent first grounds the task in authoritative literature (e.g., Schulman et al.). The /math agent then formalizes the theory. The /code agent translates the theory into a working implementation. A final /math agent provides a crucial verification step, ensuring the complex numerical optimization routine in the code correctly reflects the derived mathematics.\", \"expected_artifacts\": [\"- A PDF report containing the full mathematical derivation of the TRPO algorithm.\", \"- A Python script (`trpo_agent.py`) with the complete, commented implementation.\", \"- A metrics file (`results.json`) and learning curve plots (`learning_curve.png`) from the training run.\", \"- A `references.bib` file citing at least two seminal papers on TRPO and natural policy gradients with arXiv IDs.\", \"- A verification log (`verification.md`) detailing the audit of the conjugate gradient solver.\"], \"thinking_outline\": [\"1. Ground the work by retrieving the seminal papers on TRPO and natural policy gradients, focusing on the theoretical guarantees of monotonic improvement.\", \"2. Formally derive the TRPO surrogate objective and the KL-divergence constraint, detailing the second-order Taylor expansion and approximation of the Fisher Information Matrix.\", \"3. Implement the TRPO agent, paying close attention to the efficient, matrix-free computation of the Fisher-vector product using the conjugate gradient algorithm and Generalized Advantage Estimation (GAE).\", \"4. Validate the implementation's performance by comparing the agent's learning curves (average return vs. episodes) against published benchmarks for the chosen environment.\", \"5. Audit the code's conjugate gradient solver and line search mechanism to verify they correctly implement the constrained optimization step derived mathematically.\", \"6. Conclude with a comparative analysis against a simpler REINFORCE implementation, discussing the empirical effects of the trust region on stability and convergence.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA (verification: All artifacts are consistent and criteria are met; fallback: If verification fails, re-assign the /code or /math derivation step with specific error feedback).\", \"todo_list\": [\"- [ ] /general-search: Find the original TRPO paper by Schulman et al. and a key paper on Natural Policy Gradients by Kakade.\", \"- [ ] /math: Derive the surrogate objective function and the KL-constrained optimization problem from the retrieved papers.\", \"- [ ] /code: Implement the TRPO agent in PyTorch, including the actor-critic networks, GAE, and the conjugate gradient solver.\", \"- [ ] /code: Set up the MuJoCo Hopper-v4 environment and training loop, logging average return and policy KL-divergence.\", \"- [ ] /math: Verify that the implemented conjugate gradient algorithm correctly solves the Ax=b system for the Fisher-vector product as specified in the derivation.\", \"- [ ] /code: Validate the agent's performance by running experiments and checking if the learning curve shows stable improvement.\", \"- [ ] router: Review all generated artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly links natural policy gradients to the TRPO objective and provides a step-by-step walkthrough of the constrained optimization.\", \"- The Python implementation successfully runs and demonstrates stable learning on the Hopper-v4 environment, achieving an average return comparable to published results.\", \"- The final report includes plots of the learning curve and correctly cites at least two foundational papers with their arXiv IDs.\", \"- The verification log confirms the alignment between the mathematical derivation of the conjugate gradient step and its software implementation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2784, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to explore the application of the Alternating Direction Method of Multipliers (ADMM) for inducing structured sparsity in neural networks under adversarial training constraints. Please derive the complete ADMM updates for an L1-regularized objective with an auxiliary variable, formulate the augmented Lagrangian, and then implement this in Python to validate the convergence of primal and dual residuals on a benchmark computer vision task.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org ADMM for adversarial deep learning L1 sparsity consensus optimization\\\", mode=both)\", \"/math(Derive the ADMM update steps for the augmented Lagrangian, detailing the proximal operator for L1 sparsity and the dual ascent update for the adversarial constraints from the causal model.)\", \"/code(Implement the derived ADMM solver in PyTorch, targeting a convolutional network. Track primal/dual residual convergence and model sparsity, and integrate a simple Bayesian uncertainty estimate for the penalty parameter., using Python)\", \"/math(Verify the implemented proximal operator's convergence properties by analyzing the Hessian eigenvalue spectrum and its relation to the chosen penalty parameter from a control theory perspective.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on ADMM for robust optimization. This informs the /math agent's formal derivation of the specific update equations. The /code agent then implements this derivation. A final verification step by the /math agent ensures the code's behavior aligns with the theory, particularly regarding convergence conditions, creating a robust validation loop.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the ADMM updates for the specified problem in a LaTeX-formatted PDF.\", \"- A Python script implementing the ADMM solver for a deep neural network.\", \"- A final report in Markdown summarizing the experimental setup, showing plots of primal/dual residual convergence, and analyzing the final model's sparsity and adversarial accuracy.\", \"- A bibliography file with citations for at least two relevant academic papers with arXiv IDs.\"], \"thinking_outline\": [\"1. Research the formulation of adversarial training as a constrained optimization problem and how ADMM can be applied, particularly in the consensus form.\", \"2. Explicitly define the objective function, constraints, and the augmented Lagrangian for the L1-regularized adversarial problem.\", \"3. Derive the closed-form or iterative solutions for each of the ADMM subproblems: the weight update (x-update), the auxiliary variable update (z-update), and the dual variable update (u-update).\", \"4. Implement the complete ADMM algorithm in a deep learning framework, including logic for calculating primal and dual residuals to monitor convergence.\", \"5. Validate the implementation by running experiments and checking that the residuals converge to a small tolerance, as predicted by the theory.\", \"6. Audit the resulting network to quantify the level of sparsity achieved and measure its classification accuracy under adversarial attack.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent. The math agent provides the formal derivations to the code agent. The code agent provides the implementation and empirical convergence data back to the math agent for theoretical verification. Verification step: The math agent audits the code's convergence plots against the expected rates from optimization theory. Fallback: If residuals diverge, the code agent is to flag the issue and pass control back to the math agent to re-check the derivation's assumptions, possibly requiring a new search for alternative ADMM formulations.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on ADMM for neural network optimization.\", \"- [ ] /math: Formulate the augmented Lagrangian and derive the ADMM update equations.\", \"- [ ] /code: Implement the ADMM solver and the training loop for a sample CNN architecture.\", \"- [ ] /code: Instrument the code to log primal and dual residuals at each iteration.\", \"- [ ] /math: Verify that the implemented proximal operator for the L1 norm is correct.\", \"- [ ] /code: Add assertions to the training loop to check for convergence of residuals below a tolerance threshold.\", \"- [ ] /math: Analyze the convergence results from the code agent and compare them with theoretical guarantees.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"adversarial-robustness\", \"admm\"], \"acceptance_criteria\": [\"- The mathematical derivation of the x, z, and u updates must be present and correct.\", \"- The Python implementation must run and produce a trained model.\", \"- The final report must contain plots demonstrating the convergence of both primal and dual residuals to a value below 1e-4.\", \"- The final trained model must exhibit at least 50% sparsity in the target layers while maintaining >90% accuracy on the clean test set.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2788, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Karush-Kuhn-Tucker (KKT) conditions for optimizing a deep neural network classifier subject to a spectral norm inequality constraint on its weight matrices, which is crucial for controlling its Lipschitz constant and ensuring adversarial robustness. Then, implement a projected gradient descent algorithm in PyTorch that enforces this constraint, and empirically validate its convergence and effect on model robustness using the CIFAR-10 dataset. I am particularly interested in the Lagrangian formulation and the analysis of the dual problem's convergence properties.\n", "completion": "{\"route_plan\": [\"/math(Derive the Lagrangian for a neural network loss function with a spectral norm inequality constraint on each weight matrix. Formulate the Karush-Kuhn-Tucker (KKT) conditions for this constrained optimization problem, focusing on the primal/dual variables, stationarity, and complementarity slackness equations for adversarial defense.)\", \"/general-search(query=\\\"site:arxiv.org spectral norm regularization adversarial robustness KKT conditions projected gradient descent convergence\\\", mode=both)\", \"/code(Implement a projected gradient descent optimizer in PyTorch that enforces the spectral norm constraint on a ResNet-18 model for CIFAR-10. Track the training loss, validation accuracy, and the largest eigenvalue of the weight matrices to validate convergence and constraint satisfaction. Compare adversarial robustness using a PGD attack against a baseline model., using Python)\"], \"route_rationale\": \"The plan starts with /math to establish the core theoretical foundation by deriving the KKT conditions for the constrained problem. This is followed by a targeted search with the 'blue' /general-search agent to gather supporting literature and canonical algorithmic approaches from arXiv. Finally, /code is used for empirical validation, implementing the derived theory in PyTorch to measure the real-world impact on model robustness and convergence.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the Lagrangian and KKT conditions.\", \"- A commented Python script (`main.py`) containing the PyTorch implementation of the constrained optimizer and the experimental setup.\", \"- A JSON file (`results.json`) containing final metrics: clean accuracy, adversarial accuracy, and max spectral norm for both baseline and constrained models.\", \"- A plot (`convergence.png`) showing training loss and max spectral norm over epochs for the constrained model.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem mathematically, defining the objective function (cross-entropy loss) and the inequality constraints (spectral norm of weight matrices \u2264 \u03b3).\", \"2. Construct the Lagrangian function by introducing Lagrange multipliers (dual variables) for each constraint. From this, derive the full Karush-Kuhn-Tucker (KKT) conditions: stationarity, primal feasibility, dual feasibility, and complementary slackness.\", \"3. Analyze the KKT stationarity condition to derive the update rule for the primal variables (weights), which motivates a projected gradient descent algorithm where the projection enforces the spectral norm constraint.\", \"4. Validate the derived projection operator against established methods found in the literature, ensuring correctness and computational efficiency (e.g., using power iteration or SVD).\", \"5. Implement the algorithm and execute the training pipeline, systematically logging metrics like loss, accuracy, the spectral norm of each constrained layer's weights, and robustness against a PGD adversarial attack.\", \"6. Verify that the final trained model's weight matrices satisfy the spectral norm constraint and that the empirical results (improved adversarial accuracy) align with the theoretical motivation.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides keywords and equations for /general-search to find canonical literature. The combination of the formal derivation and supporting papers informs the /code agent's implementation of the projected gradient descent algorithm. Router QA performs the final check. Verification: The code's projection step must match the KKT derivation. Fallback: If the code's validation shows divergence or constraint violation, re-engage /math to check the derivation for flaws.\", \"todo_list\": [\"- [ ] /math: Derive the Lagrangian and full KKT conditions for the specified problem.\", \"- [ ] /general-search: Find and summarize at least two key papers from arXiv on spectral norm regularization for adversarial defense.\", \"- [ ] /code: Implement the ResNet-18 model, baseline training loop, and the projected gradient descent optimizer in PyTorch.\", \"- [ ] /code: Run experiments for both baseline and constrained models, generating logs, metrics, and plots.\", \"- [ ] /math: Verify that the projection logic implemented in the Python code is a correct interpretation of the derived KKT conditions.\", \"- [ ] /code: Audit the experimental results to confirm the spectral norm constraint is met and adversarial accuracy is improved.\", \"- [ ] /router: Review all generated artifacts for correctness, consistency, and fulfillment of acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"deep-learning-theory\", \"adversarial-robustness\"], \"acceptance_criteria\": [\"- The mathematical derivation of the KKT conditions in the LaTeX document is complete, logically sound, and free of errors.\", \"- The final trained model's weight matrices all have a spectral norm less than or equal to the specified constraint \u03b3, as reported in `results.json`.\", \"- The constrained model demonstrates at least a 5% absolute improvement in adversarial accuracy against a PGD-10 attack compared to the unconstrained baseline.\", \"- The final report cites at least 2 relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2789, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to understand optimization in deep learning. Can you derive the Hessian matrix for a simple linear regression loss function and then use Python to compute its eigenvalues to show how they describe the curvature of the loss landscape?\n", "completion": "{\"route_plan\": [\"/math(Derive the gradient and Hessian matrix for the Mean Squared Error loss of a linear regression model, explaining how each eigenvalue relates to the principal curvatures.)\", \"/code(Implement a Python script using NumPy and JAX to compute the Hessian's eigenvalues for a sample dataset, and create a 3D plot to visualize the loss landscape and validate the convergence properties.)\"], \"route_rationale\": \"/math is used for the formal symbolic derivation of the Hessian, establishing the theoretical groundwork. /code is then used to provide empirical validation of the mathematical theory, computing the eigenvalues for a concrete example and visualizing the results to build intuition.\", \"expected_artifacts\": [\"* A markdown document with the complete mathematical derivation of the Hessian matrix for the specified loss function.\", \"* A Python script (`.py`) that generates a dataset, computes the Hessian and its eigenvalues, and produces a visualization.\", \"* A 3D plot (`.png`) of the loss landscape, annotated to show the directions corresponding to the eigenvectors.\", \"* A short report (`.md`) summarizing the findings and confirming the consistency between the derived theory and the computational experiment.\"], \"thinking_outline\": [\"1. Define the Mean Squared Error (MSE) loss function for a simple linear regression model (y = wx + b).\", \"2. Use matrix calculus to derive the gradient vector and the Hessian matrix of the MSE loss with respect to the parameters w and b.\", \"3. Explain the theoretical link: the eigenvalues of the Hessian represent the magnitude of curvature along the directions of the corresponding eigenvectors.\", \"4. Implement a Python script to compute the numerical Hessian and its eigenvalues at the minimum of the loss surface for a synthetic dataset.\", \"5. Verify that the visualization of the loss landscape aligns with the computed eigenvalues (e.g., a high eigenvalue corresponds to a steep valley).\"], \"handoff_plan\": \"/math agent provides the symbolic Hessian formula -> /code agent implements this formula numerically, computes eigenvalues, and generates visualizations -> router QA (verification: numerical eigenvalues must match expectations from the data's structure; fallback: if visualization is unclear, /code agent will regenerate with different viewing angles or color maps).\", \"todo_list\": [\"- [ ] /math: Derive the symbolic form of the Hessian for the MSE loss function.\", \"- [ ] /code: Implement the numerical computation of the Hessian and its eigenvalues for a sample dataset.\", \"- [ ] /code: Generate a 3D plot visualizing the loss landscape's curvature.\", \"- [ ] Verification: Cross-check that the visualized curvature in the plot aligns with the magnitude of the computed eigenvalues.\", \"- [ ] Router QA: Review all artifacts for correctness, clarity, and adherence to acceptance criteria.\"], \"difficulty\": \"introductory\", \"tags\": [\"matrix-calculus\", \"spectral-analysis\", \"optimization-landscape\"], \"acceptance_criteria\": [\"* The mathematical derivation of the Hessian must be correct and clearly explained.\", \"* The Python code must run without errors and correctly compute the eigenvalues using a standard library.\", \"* The final report must explicitly connect the magnitude of the computed eigenvalues to the steepness of the visualized loss landscape.\", \"* The visualization must clearly depict an elliptical or parabolic bowl shape characteristic of a convex loss surface.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2804, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the error threshold for the 5-qubit perfect code under a depolarizing noise model. Your analysis must include the derivation of the logical error probability as a function of the physical error rate `p`, and you must simulate its performance, validating the break-even point where the logical error rate equals `p`. The analysis should consider the eigenvalue spectrum of the error operators.\n", "completion": "{\"route_plan\": [\"/math(Derive the logical error probability for the 5-qubit perfect code under a depolarizing noise model, expressing it as a polynomial in the physical error rate p. Analyze the eigenvalue structure of the Pauli error operators to justify the model's assumptions and ensure convergence.)\", \"/code(Implement a Python simulation of the 5-qubit code with a depolarizing noise channel. Numerically calculate the error threshold by finding the physical error rate 'p' where the logical error rate equals 'p'. Analyze the simulation's time complexity and use Matplotlib to plot logical vs. physical error rates.)\", \"/general-search(query=\\\"site:arxiv.org 5-qubit quantum code error threshold depolarizing noise 'threshold theorem'\\\", mode=web)\"], \"route_rationale\": \"/math is first to establish the theoretical framework and derive the logical error probability polynomial. /code follows to numerically simulate the code's performance and validate the analytical findings from /math. Finally, the blue /general-search agent is used to find authoritative arXiv papers that corroborate the theoretical and simulated results, providing context and justification.\", \"expected_artifacts\": [\"A formal mathematical derivation of the logical error probability for the 5-qubit code under depolarizing noise.\", \"A Python script implementing the simulation, including the noise model, encoding, error correction, and decoding steps.\", \"A plot comparing the logical error rate to the physical error rate, visually identifying the error threshold.\", \"A summary report referencing at least two arXiv preprints that discuss quantum error correction thresholds.\", \"A verification log confirming the simulation's numerical threshold aligns with the derived analytical solution's break-even point.\"], \"thinking_outline\": [\"1. Define the 5-qubit perfect code's stabilizer generators and logical operators.\", \"2. Model the single-qubit depolarizing channel and derive the probability of single Pauli errors (I, X, Y, Z).\", \"3. Calculate the logical error probability P_L as a function of the physical error rate p by considering the lowest-weight error events that are uncorrectable.\", \"4. Solve for the error threshold by finding the non-trivial root of the equation P_L(p) = p.\", \"5. Validate the derived threshold by implementing a Monte Carlo simulation of the full error correction cycle and comparing the numerical crossover point with the analytical result.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent provides the analytical formula for logical error probability. The /code agent implements a simulation based on this formula and must verify that its numerical results for low `p` match the Taylor expansion of the formula. The /general-search agent finds papers to cross-reference both the formula and the simulated threshold value. Router QA performs a final check on consistency across all artifacts. Fallback: If simulation deviates significantly from theory, return to /math to re-check the derivation for higher-order error terms.\", \"todo_list\": [\"- [ ] /math: Derive the polynomial expression for the logical error rate P_L(p) for the 5-qubit code.\", \"- [ ] /math: Solve P_L(p) = p to find the theoretical error threshold.\", \"- [ ] /code: Implement the 5-qubit code's stabilizers, encoding, and decoding logic in Python.\", \"- [ ] /code: Simulate the effect of a depolarizing noise channel and sweep the physical error rate `p`.\", \"- [ ] /code: Validate the simulation by comparing logical error rates at low `p` against the first-order term from the /math derivation.\", \"- [ ] /general-search: Find and summarize at least two authoritative papers (e.g., from arXiv) on quantum error correction thresholds, specifically mentioning the 5-qubit code if possible.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"quantum-computing\", \"error-correction\", \"fault-tolerance\"], \"acceptance_criteria\": [\"The derived logical error probability P_L(p) is a correct polynomial in p, accounting for leading-order error terms.\", \"The Python simulation correctly implements the 5-qubit code and depolarizing noise model, and its output plot clearly shows the threshold where P_L \u2248 p.\", \"The reported error threshold from the simulation is within 5% of the analytically derived value.\", \"At least two relevant arXiv preprints on quantum error correction are cited and correctly summarized.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2848, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a neural-symbolic system for program synthesis. Specifically, I want to synthesize a Python function that correctly implements reversal on a singly-linked list. The system must take a formal specification in higher-order logic, use a neural-guided search to generate candidate programs, and then use an inductive theorem prover to formally verify both the functional correctness (post-conditions) and termination (via a well-founded ordering) of the synthesized code against its Hoare logic specification.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural program synthesis inductive theorem proving Hoare logic\\\", mode=both, includes numpy)\", \"/math(Formalize the list reversal specification using higher-order logic, define the inductive proof strategy for correctness, and establish a well-founded ordering for the termination proof under computational complexity constraints., includes numpy)\", \"/code(Implement a Python prototype for a neural-guided synthesizer for the recursive list reversal function. The implementation must include a verifier based on the derived Hoare logic and track convergence metrics for the search process.)\", \"/math(Verify the synthesized Python code against the formal specification. Audit the generated proof trace for soundness and completeness, ensuring it satisfies the denotational semantics and Bayesian inference model.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on neural-symbolic methods. This informs the /math agent's formalization of the problem. The /code agent then implements the synthesizer based on this formal groundwork. Finally, a second /math agent call is essential to formally verify the synthesized program, closing the reasoning loop and ensuring correctness.\", \"expected_artifacts\": [\"A formal specification of the list reversal problem in LaTeX.\", \"A documented Python script implementing the neural-guided synthesizer and verifier.\", \"A final report (PDF) detailing the methodology, the synthesized code, its formal proof trace, and performance metrics.\", \"A bibliography with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Research current neural-symbolic methods combining guided search with formal verification for program synthesis.\", \"2. Formulate the list reversal problem specification in higher-order logic, including preconditions, postconditions, and termination criteria based on a well-founded ordering on the list length.\", \"3. Implement the neural-guided search algorithm to synthesize a candidate Python program and a verifier module based on inductive proof rules.\", \"4. Validate the synthesized program by executing the verifier, which must generate a complete and formal proof trace.\", \"5. Audit the generated proof trace to check for logical soundness and completeness against the initial formal specification.\", \"6. Summarize the findings, analyze the efficiency of the synthesis process, and discuss limitations, citing all referenced literature.\"], \"handoff_plan\": \"/general-search -> /math (formalization) -> /code -> /math (verification) -> router QA (verification: router confirms the final proof trace from /math aligns with the synthesized code from /code; fallback: if verification fails, the /math agent provides a counterexample to the /code agent to refine the synthesis search).\", \"todo_list\": [\"- [ ] /general-search: Survey recent arXiv papers on neural-guided synthesis and inductive theorem proving.\", \"- [ ] /math: Produce a LaTeX document with the formal higher-order logic specification for verified list reversal.\", \"- [ ] /code: Implement the Python-based synthesizer and verifier modules.\", \"- [ ] /math: Verify the synthesized code and generate a formal proof trace.\", \"- [ ] /code: Validate the implementation's robustness with property-based tests for edge cases (e.g., empty lists, single-node lists).\", \"- [ ] /router: Perform final QA review, checking consistency between the formal spec, code, and final proof.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic\", \"theorem-proving\", \"program-synthesis\"], \"acceptance_criteria\": [\"- A formal specification in higher-order logic for the list reversal function is delivered.\", \"- The synthesized Python program is successfully generated and passes all verification checks against the formal specification.\", \"- The final report includes a complete, machine-checkable proof trace and cites at least 2 relevant arXiv papers with IDs.\", \"- The synthesizer successfully handles edge cases like empty lists and single-element lists.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Binary (1/0) indicating if a fully verified program was synthesized within the compute budget.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Integer count of steps taken by the guided search algorithm to find a valid program; lower is better.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2935, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nImplement an ADMM solver for the basis pursuit denoising (LASSO) problem. Derive the primal-dual updates and provide a rigorous analysis of the algorithm's convergence properties under standard assumptions for the rho parameter.\n", "completion": "{\"route_plan\": [\"/math(Derive the scaled-form ADMM update equations for the LASSO objective function, detailing the augmented Lagrangian and the proximal operator for the L1-norm. Analyze the conditions for guaranteed convergence.)\", \"/general-search(query=\\\"site:arxiv.org admm convergence analysis lasso boyd\\\", mode=web)\", \"/code(Implement the derived ADMM updates in Python using NumPy. Generate a synthetic sparse signal recovery problem and validate the implementation's correctness and convergence rate against scikit-learn's Lasso solver.)\"], \"route_rationale\": \"/math is required to formally derive the iterative update steps from first principles. The blue /general-search agent will then be used to find authoritative papers on ADMM convergence theory to support the analysis. Finally, /code will translate the mathematical formulation into an executable Python script and perform empirical validation.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF with the full derivation of the ADMM updates for LASSO.\", \"A well-commented Python script implementing the ADMM solver.\", \"A summary report citing at least two academic papers on ADMM theory.\", \"A plot comparing the convergence of the custom ADMM solver's objective value to the scikit-learn baseline.\", \"A JSON file containing the final primal/dual residuals and objective value.\"], \"thinking_outline\": [\"1. Formulate the LASSO problem as a constrained optimization problem suitable for ADMM by introducing a splitting variable.\", \"2. Construct the augmented Lagrangian for the split problem and derive the iterative update rules for the primal and dual variables.\", \"3. Research the theoretical conditions for ADMM convergence, particularly the role of the penalty parameter rho, using authoritative sources.\", \"4. Implement the algorithm and a synthetic data generation process in Python.\", \"5. Verify the implementation's correctness by comparing the final objective value and recovered sparse vector against the solution from a trusted library like scikit-learn.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (literature review) -> /code (implementation & validation) -> router QA (verification: residuals are below tolerance and final solution matches baseline; fallback: if validation fails, return to /code to debug proximal operator implementation).\", \"todo_list\": [\"- [ ] /math: Derive the ADMM update equations for LASSO.\", \"- [ ] /general-search: Find and summarize two key papers on ADMM convergence guarantees.\", \"- [ ] /code: Implement the ADMM solver using Python and NumPy.\", \"- [ ] /code: Create a synthetic dataset for a sparse recovery task.\", \"- [ ] /code: Validate the solver's output against scikit-learn's Lasso implementation and plot convergence.\", \"- [ ] /router: Review all artifacts for correctness, consistency, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"convex-optimization\", \"admm\", \"numerical-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation of the update rules must be correct and complete.\", \"- The Python implementation must converge on a synthetic problem, with primal and dual residuals tending to zero.\", \"- The final solution vector and objective value must match the output of `sklearn.linear_model.Lasso` within a 1e-4 tolerance.\", \"- The report must cite at least two relevant academic papers with their arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2936, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement an E(3)-equivariant graph neural network, like PaiNN or SchNet, to learn the potential energy surface for a small molecule such as ethanol from ab initio data. The core task is to integrate this GNN potential into a custom molecular dynamics simulation using a Velocity Verlet integrator and rigorously demonstrate long-term energy conservation over a nanosecond-scale NVE ensemble simulation, quantifying any energy drift.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org E(3) equivariant graph neural network molecular dynamics energy conservation PaiNN DimeNet\\\", mode=web)\", \"/math(Derive the Hamiltonian formulation for a molecular system and the conditions for a symplectic integrator like Velocity Verlet to conserve energy. Analyze the implications for the GNN's force-field prediction, especially concerning its continuity and gradient accuracy for long-term climate simulation stability.)\", \"/code(Implement an equivariant GNN in PyTorch Geometric to predict forces from atomic positions using CUDA. Train on an ethanol dataset and validate the force prediction implementation against analytical gradients of a simple potential like Lennard-Jones to check for convergence., using Python)\", \"/math(Analyze the MD trajectory generated by the GNN potential for energy drift over 1 nanosecond. Compute the total energy standard deviation and bootstrap confidence intervals for the drift rate, comparing it statistically to a classical force field baseline.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey SOTA equivariant GNN architectures. /math is then used to establish the theoretical physics foundation for energy conservation in MD simulations. /code is required for the core implementation and training of the GNN potential. Finally, /math is used again for a rigorous statistical analysis of the simulation output, which is crucial for validating the model's physical plausibility.\", \"expected_artifacts\": [\"- A final report document detailing the mathematical derivations, model architecture, training procedure, and analysis of results.\", \"- Python source code for the GNN model, the MD simulator, and training/analysis scripts, with dependencies specified.\", \"- A list of at least two cited arXiv papers on equivariant GNNs for molecular simulation.\", \"- Plots showing the training loss curve, a comparison of predicted vs. true forces, and the total energy of the system as a function of simulation time, clearly showing any drift.\", \"- A JSON file containing the final computed metrics, including energy drift rate and force MAE.\"], \"thinking_outline\": [\"1. Research current state-of-the-art E(3)-equivariant GNN architectures suitable for learning potential energy surfaces, focusing on models known for their accuracy and conservation properties.\", \"2. Formulate the mathematical basis for the simulation, deriving the equations for Hamiltonian dynamics and the Velocity Verlet integration scheme. Specify the analytical conditions under which the GNN-predicted forces must conserve total energy.\", \"3. Implement the selected GNN architecture and the MD simulation loop. Train the model on a provided dataset of ethanol configurations, energies, and forces.\", \"4. Verify the correctness of the force prediction by comparing the GNN's output on a known simple potential (e.g., two-atom Lennard-Jones) against its analytical gradient.\", \"5. Run a long-timescale NVE simulation using the trained GNN potential. Collect trajectory data, focusing on total energy at each timestep.\", \"6. Validate the physical plausibility of the model by analyzing the simulation trajectory for energy drift. Quantify the drift, compare it to a baseline, and conclude on the model's suitability for production MD.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides candidate models and papers to the math agent. The math agent derives the theoretical requirements, which are passed as specifications to the code agent. The code agent produces a trained model and simulation data, which the second math agent analyzes. Verification: The code agent must verify its force implementation against analytical gradients provided by the first math agent. Fallback: If energy drift is unacceptably high, the code agent must revisit the GNN architecture or training hyperparameters based on guidance from the math agent and initial research.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 SOTA equivariant GNNs for MD, noting their approaches to energy conservation.\", \"- [ ] /math: Provide the full derivation for the Velocity Verlet integrator and the energy conservation criterion for a potential-derived force field.\", \"- [ ] /code: Implement the chosen GNN architecture in PyTorch Geometric and train it on the provided ethanol dataset.\", \"- [ ] /code: Implement the NVE simulation loop using the trained GNN to compute forces.\", \"- [ ] /math: Verify that the implemented GNN force function is the negative gradient of its predicted energy (F = -\u2207U).\", \"- [ ] /math: Perform a statistical analysis of the energy drift from the simulation output, including confidence intervals.\", \"- [ ] /code: Generate plots for energy drift over time and force prediction accuracy.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-network\", \"molecular-dynamics\", \"physics-informed-ml\"], \"acceptance_criteria\": [\"- The final report must contain a complete mathematical derivation of the symplectic integration scheme used.\", \"- The implemented GNN must achieve a Mean Absolute Error (MAE) on atomic forces below 0.05 eV/\u00c5 on the test set.\", \"- The total energy drift over a 1 nanosecond NVE simulation must be less than 1e-4 eV/atom/ps.\", \"- The source code must be well-documented and include a requirements.txt file for reproducibility.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2958, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the dual function for a Constrained Markov Decision Process (CMDP) using Lagrangian relaxation. Then, implement a primal-dual algorithm to solve for a safe policy in a simple grid-world environment, ensuring the cost function's expectation remains below a specified threshold. Your derivation should reference concepts from Euler-Lagrange equations regarding optimality.\n", "completion": "{\"route_plan\": [\"/math(Derive the Lagrangian dual for a Constrained MDP using control theory principles, formulating the Karush-Kuhn-Tucker conditions for the optimal safe policy.)\", \"/general-search(query=\\\"site:arxiv.org primal-dual algorithm constrained markov decision process Lagrangian methods\\\", mode=rag)\", \"/code(Implement a primal-dual RL algorithm in Python/JAX for a grid-world CMDP, tracking convergence of the Lagrange multiplier and ensuring the safety constraint violation rate is below 5%.)\"], \"route_rationale\": \"The /math tool is first used to establish the core theoretical foundation via a formal derivation. The blue /general-search agent then grounds this theory in published research to guide implementation. Finally, the /code tool translates the mathematical formulation and literature insights into a working, verifiable simulation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the CMDP Lagrangian dual function and the resulting primal-dual update rules.\", \"A Python implementation of the primal-dual algorithm and the grid-world test environment.\", \"A summary report containing plots of episodic reward, episodic cost, and the Lagrange multiplier's evolution over training.\", \"A list of at least two cited arXiv papers on CMDPs or primal-dual RL methods.\", \"A final JSON file with evaluation metrics confirming the policy's safety.\"], \"thinking_outline\": [\"1. Formulate the problem as a Constrained Markov Decision Process (CMDP), clearly defining states, actions, rewards, costs, and the safety constraint.\", \"2. Apply the method of Lagrangian relaxation to incorporate the safety constraint into the objective function, forming the Lagrangian.\", \"3. Derive the primal-dual update rules by finding the gradients of the Lagrangian with respect to the policy parameters (primal) and the Lagrange multiplier (dual).\", \"4. Implement the algorithm and a suitable grid-world environment where certain states incur a high cost, representing unsafe regions.\", \"5. Validate the final trained policy by executing it for a sufficient number of episodes and verifying that the average accumulated cost remains below the predefined safety threshold.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent provides the core equations. /general-search uses keywords from the derivation to find state-of-the-art implementation patterns. /code builds the simulation based on both the derivation and literature. Verification: router QA checks if the empirical average cost from the /code output respects the safety threshold defined in the prompt. Fallback: If constraints are violated, the /code agent is instructed to re-run with adjusted learning rates for the dual variable.\", \"todo_list\": [\"- [ ] /math: Derive the Lagrangian dual and KKT conditions for the CMDP problem.\", \"- [ ] /general-search: Find two seminal papers on primal-dual methods for CMDPs and summarize their approaches.\", \"- [ ] /code: Implement the CMDP grid-world environment with designated safe and unsafe states.\", \"- [ ] /code: Implement the primal-dual policy optimization algorithm based on the math derivation.\", \"- [ ] /code: Validate that the trained policy's average cost is below the specified safety threshold via 100 simulation rollouts.\", \"- [ ] /router: Review the final report, code, and verification plots for correctness, clarity, and satisfaction of all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"constrained-optimization\", \"reinforcement-learning\", \"robotics-safety\"], \"acceptance_criteria\": [\"The mathematical derivation correctly formulates the Lagrangian dual for the CMDP and derives the appropriate gradient updates.\", \"The Python code successfully implements the primal-dual algorithm and runs to completion without errors.\", \"The final report includes plots demonstrating that the learned policy respects the safety constraint, with the average episodic cost being less than the predefined threshold.\", \"At least two relevant arXiv papers are cited in the final report with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2974, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous analysis of dictionary learning algorithms for sparse representations. Specifically, derive the update rules for the K-SVD algorithm, detailing the SVD-based dictionary atom update step. Then, implement K-SVD and a simpler baseline like Method of Optimal Directions (MOD) in Python. Compare their performance on reconstructing synthetic sparse signals from compressive measurements using Orthogonal Matching Pursuit (OMP) for the sparse coding stage. Evaluate based on reconstruction SNR, convergence speed, and final dictionary coherence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD dictionary learning algorithm Aharon Elad\\\", mode=both, includes cybersecurity)\", \"/math(Derive the K-SVD dictionary update step, focusing on the rank-1 approximation derived from the error matrix. Analyze the convergence properties and computational complexity compared to MOD.)\", \"/code(Implement K-SVD and MOD algorithms in Python with NumPy. Include an OMP implementation for the sparse coding step. Design an experiment to test reconstruction fidelity versus dictionary size and signal sparsity, tracking key finance-related metrics for synthetic time series.)\", \"/math(Formalize the comparison metrics: define Reconstruction SNR, dictionary coherence, and plot the convergence rate of the objective function. Relate the observed eigenvalue distribution of the learned dictionary Gram matrix to its performance.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather seminal papers on K-SVD for a strong theoretical foundation. /math is then used to formalize the derivation of the core algorithm. /code translates this theory into a working implementation and runs the comparative experiments. A final /math call is used to rigorously define the analysis metrics, ensuring a robust evaluation of the results.\", \"expected_artifacts\": [\"A PDF report containing the full mathematical derivation of the K-SVD update rules.\", \"A well-documented Python script (`ksvd_mod_comparison.py`) implementing K-SVD, MOD, and OMP.\", \"A directory of plots visualizing reconstruction SNR vs. sparsity and convergence curves.\", \"A JSON file (`metrics.json`) reporting the final performance metrics for both algorithms.\", \"A `references.bib` file with at least two cited papers from arXiv.\"], \"thinking_outline\": [\"1. Research the K-SVD and MOD algorithms, focusing on the original papers to understand the theoretical guarantees and update procedures.\", \"2. Formulate and derive the K-SVD dictionary atom update rule, showing how it minimizes the reconstruction error for a specific atom by performing a rank-1 approximation of the error matrix via SVD.\", \"3. Implement the full dictionary learning loop for both K-SVD and MOD, including a sparse coding subroutine like OMP. Validate the OMP implementation on a known dictionary and signal to ensure correctness.\", \"4. Design and execute a simulation to compare the algorithms. Generate synthetic sparse signals, apply a random measurement matrix to simulate compressed sensing, and then use the learned dictionaries to reconstruct them.\", \"5. Analyze the results by computing metrics like reconstruction SNR, dictionary coherence, and the number of iterations to convergence. Verify that the implementation's convergence behavior aligns with theoretical expectations.\", \"6. Summarize the findings in a structured report, presenting the derivations, methodology, results, and a concluding discussion on the trade-offs between K-SVD and MOD.\"], \"handoff_plan\": \"/general-search (arXiv papers) -> /math (derivation) -> /code (implementation & experiment) -> /math (metric formalization) -> router QA (verification: code output matches derivation's expected behavior, metrics are correctly calculated; fallback: if code fails verification, return to /math to check derivation for errors, then to /code for debugging).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize the original K-SVD and MOD papers from authoritative sources like arXiv or IEEE Xplore.\", \"- [ ] /math: Produce a step-by-step derivation of the K-SVD dictionary column update rule.\", \"- [ ] /code: Implement the K-SVD and MOD algorithms, including the OMP sparse coding stage, using Python and NumPy.\", \"- [ ] /code: Create a synthetic data generation pipeline for sparse signals and compressive measurements.\", \"- [ ] /math: Verify that the implemented K-SVD update step correctly minimizes the Frobenius norm of the error matrix as per the derivation.\", \"- [ ] /code: Run the comparative simulation and generate plots for SNR, coherence, and convergence.\", \"- [ ] /math: Verify that the final reported metrics in `metrics.json` are computed according to the formal definitions.\", \"- [ ] router QA: Review all artifacts for consistency, correctness, and adherence to the acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"The mathematical derivation for the K-SVD update rule must be complete and correct.\", \"The Python code must execute without errors and reproduce the expected behavior of K-SVD and MOD algorithms.\", \"The final report must clearly show that K-SVD achieves a higher reconstruction SNR than MOD under the specified experimental conditions.\", \"The project must cite at least two relevant academic papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 2989, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the computational boundaries of impartial games. Please provide a rigorous analysis of the Sprague-Grundy theorem, focusing on its application to determining winning positions. Also, detail the PSPACE-completeness proof for generalized geography. Finally, implement a dynamic programming algorithm in Python to compute the Grundy numbers (nim-values) for game positions represented by a given directed acyclic graph, and provide a formal complexity analysis of your implementation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:.edu 'combinatorial game theory' 'sprague-grundy theorem' 'generalized geography' pspace completeness reduction proof\\\", mode=both)\", \"/math(Formalize the Sprague-Grundy theorem for impartial adversarial games, defining nim-sum and Grundy numbers (g-values), and outlining the complexity reduction proof for generalized geography sourced from arxiv.)\", \"/code(Implement a Python algorithm using memoization to compute Grundy numbers for game positions represented as a directed acyclic graph, treating the graph as an adjacency list database and including comprehensive test cases.)\", \"/math(Analyze the time and space complexity of the Python implementation, formally proving its bounds for DAGs and contrasting this with the known PSPACE-hardness of adversarial games on general graphs.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather authoritative literature on combinatorial game theory and complexity classes. This informs the /math agent's formal derivation of the Sprague-Grundy theorem. The /code agent then implements the derived theory to create a concrete algorithm. Finally, the /math agent returns to analyze the computational complexity of the specific implementation, linking the practical algorithm back to the high-level complexity theory.\", \"expected_artifacts\": [\"- A LaTeX document detailing the Sprague-Grundy theorem, the recursive formula for Grundy numbers (nim-values), and a summary of the PSPACE-completeness proof for generalized geography.\", \"- A Python script (`grundy_calculator.py`) that computes nim-values for any given game represented as a DAG.\", \"- A verification report comparing the implemented algorithm's performance on test graphs against the theoretical time complexity.\", \"- A bibliography with at least two citations to seminal papers or books on the topic, including arXiv IDs or DOIs.\"], \"thinking_outline\": [\"1. Research the formal definitions of impartial games, the Sprague-Grundy theorem (including nim-sum and nim-values), and the reduction proofs establishing the PSPACE-completeness of games like generalized geography.\", \"2. Formulate the recursive definition of the Grundy number (g-number) for a game position as the Minimum Excluded value (mex) of the g-numbers of its reachable successor positions.\", \"3. Develop a dynamic programming or memoization-based algorithm in Python to compute the g-numbers for all nodes in a given directed acyclic game graph.\", \"4. Validate the algorithm's correctness by running it on several small, non-trivial game graphs with manually calculated or known nim-values, such as simple paths, forks, and joins.\", \"5. Analyze the time and space complexity of the Python implementation, expressing it in terms of the number of vertices (V) and edges (E) in the input graph.\", \"6. Verify that the theoretical complexity analysis (e.g., O(V+E) for a DAG) aligns with the PSPACE-hardness results, explaining why the algorithm is efficient for DAGs but the problem is hard for general graphs (which may contain cycles).\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers (e.g., from arXiv) to the math agent. The math agent provides a formal specification and recursive formula to the code agent. The code agent implements the algorithm and returns the source code and test outputs. The math agent then receives the code to perform a final complexity analysis. The router performs a final QA check on all artifacts for coherence and correctness. Fallback: If the math derivation is flawed, the code agent will flag an inconsistent spec and halt, returning control to the router to re-prompt the math agent.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Sprague-Grundy theorem and PSPACE-completeness of games.\", \"- [ ] /math: Formally derive the recursive definition of Grundy numbers and outline the PSPACE reduction.\", \"- [ ] /code: Implement the memoized recursion algorithm in Python for computing Grundy numbers on a DAG.\", \"- [ ] /code: Create a test suite with at least three distinct graph structures to validate the implementation.\", \"- [ ] /math: Perform a rigorous time and space complexity analysis of the implemented algorithm.\", \"- [ ] /math: Verify that the complexity derivation is consistent with the initial theoretical findings on PSPACE-hardness.\", \"- [ ] /code: Audit the Python implementation to ensure it correctly handles all graph edge cases and matches the mathematical spec.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"pspace-completeness\"], \"acceptance_criteria\": [\"- The mathematical explanation of the Sprague-Grundy theorem must be precise and the connection to nim-sum must be clearly articulated.\", \"- The Python implementation must correctly compute the Grundy numbers for all provided test cases, including a non-trivial DAG with at least 10 nodes.\", \"- The complexity analysis must correctly identify the algorithm's runtime as O(V+E) for DAGs and explain why this does not contradict the PSPACE-completeness of generalized games on cyclic graphs.\", \"- At least two relevant academic sources must be cited correctly, with links provided via DOI or arXiv ID.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3021, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for dictionary learning using the Alternating Direction Method of Multipliers (ADMM) for an L1-regularized sparse coding objective. Please implement this algorithm, analyzing its convergence properties on a synthetic dataset by tracking primal and dual residuals. Find benchmark papers on arXiv establishing theoretical convergence guarantees for similar ADMM formulations to validate our approach.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'dictionary learning' ADMM 'convergence guarantees' L1 regularization\\\", mode=rag)\", \"/math(Derive the augmented Lagrangian and the iterative update rules for both the dictionary and sparse codes based on the ADMM framework, ensuring the derivation accounts for non-smooth L1 norm convergence properties.)\", \"/code(Implement the derived ADMM updates in Python using NumPy/SciPy. Generate a synthetic signal dataset and visualize the algorithm's convergence by plotting primal and dual residuals against iteration number.)\", \"/math(Verify the implementation's correctness by checking if the computed residuals match their theoretical definitions from the ADMM literature, and confirm that the algorithm's eigenvalue spectrum of the dictionary update remains stable.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find foundational papers on arXiv, providing a theoretical basis. This is handed to /math to perform the core mathematical derivation. The resulting equations are then implemented by /code, which also generates empirical results. Finally, /math provides a crucial verification step, comparing the code's output (residuals) against the derived theory to ensure correctness.\", \"expected_artifacts\": [\"A formal mathematical derivation of the ADMM update rules for dictionary learning in a PDF document.\", \"A Python script implementing the algorithm, including data generation and convergence plotting.\", \"A summary report citing at least two relevant arXiv papers with IDs, comparing their theoretical convergence guarantees with the empirical results.\", \"A JSON file containing the final primal and dual residual values after convergence.\", \"Plots visualizing the learned dictionary atoms and the convergence of residuals over iterations.\"], \"thinking_outline\": [\"1. Use search to find seminal papers on ADMM applied to L1-regularized problems, specifically dictionary learning, to establish the problem formulation and theoretical convergence conditions.\", \"2. Formulate the augmented Lagrangian for the problem and derive the iterative update steps for the sparse codes (x-update), the dictionary (D-update), and the dual variable (u-update).\", \"3. Implement the derived algorithm in code, including a function to generate synthetic sparse signals from a ground-truth dictionary.\", \"4. Validate the implementation by running it on the synthetic data and plotting the primal and dual residuals per iteration, checking that they decay towards zero as predicted by theory.\", \"5. Verify the final learned dictionary against the ground-truth dictionary used for data generation, ensuring it has recovered the original atoms up to permutation and scaling.\", \"6. Synthesize the findings into a report, comparing the observed convergence rate with the theoretical guarantees found in the initial literature search.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers for the math agent's derivation. The code agent implements the derivation. The math agent then verifies the code's output against the theory. Verification failure (e.g., diverging residuals) triggers a fallback to the first /math step to re-check the derivation. Success hands all artifacts to the router for final review.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key arXiv papers on ADMM for dictionary learning.\", \"- [ ] /math: Derive the complete set of ADMM update equations and stopping criteria.\", \"- [ ] /code: Implement the algorithm and the synthetic data generation process.\", \"- [ ] /code: Generate plots for primal/dual residuals vs. iteration.\", \"- [ ] /math: Verify that the implementation's residual calculations align with the formal mathematical definitions from the derivation.\", \"- [ ] /math: Audit the convergence plots to confirm they meet theoretical expectations (i.e., residuals approach zero).\", \"- [ ] /router: Review all artifacts for correctness, completeness, and alignment with the user query.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"admm\"], \"acceptance_criteria\": [\"- The provided Python script must successfully execute and recover a known synthetic dictionary with a reconstruction error below 1e-4.\", \"- The final report must cite at least two arXiv papers by ID and explicitly connect their theoretical convergence analysis to the empirical results.\", \"- The mathematical derivation must be complete and self-contained, showing all steps for the augmented Lagrangian and subproblem solutions.\", \"- The generated plots must clearly show primal and dual residuals converging to a value below 1e-5.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3038, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want a step-by-step guide to understanding the Evidence Lower Bound (ELBO) in variational inference. Please derive the ELBO, explain its components like the expected log-likelihood and KL divergence, and then implement a simple Bayesian linear regression model in Python that optimizes the ELBO.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) from the definition of marginal log-likelihood. Show explicitly how it decomposes into an expected log-likelihood term and the Kullback-Leibler divergence between the approximate and true Bayesian posterior.)\", \"/general-search(query=\\\"site:arxiv.org tutorial 'Bayesian linear regression' 'variational inference' ELBO convergence\\\", mode=web)\", \"/code(Implement Bayesian linear regression using PyTorch. Define a variational family with learnable parameters for the weights, optimize the negative ELBO using stochastic gradient descent, and track convergence. The model should be tested on a synthetic dataset, not a simple Bernoulli trial., using Python)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation of the ELBO. Next, the blue /general-search agent is tasked with finding a canonical, well-explained example to guide the implementation. Finally, /code translates the theory and the example into a working program, providing a concrete demonstration of the concepts.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the ELBO.\", \"A Python script implementing Bayesian linear regression with variational inference, including data generation and model training.\", \"A plot showing the value of the ELBO increasing over training iterations.\", \"A summary of findings, including the final ELBO value and links to at least two cited sources.\", \"A verification report confirming the code's output matches expected behavior on a small, fixed dataset.\"], \"thinking_outline\": [\"1. Start by defining the core problem in Bayesian inference: computing the posterior distribution and the marginal likelihood (evidence).\", \"2. Introduce the concept of a simpler, parameterized variational distribution q(z|v) to approximate the true posterior p(z|x).\", \"3. Use Jensen's inequality on the log marginal likelihood, log p(x), to derive the Evidence Lower Bound (ELBO) and identify its two main interpretations (expected log-likelihood minus KL divergence, and log evidence minus KL divergence).\", \"4. Formulate the specific probabilistic model for Bayesian linear regression, defining priors for the weights and bias.\", \"5. Implement this model in code, defining the variational parameters and the ELBO as the loss function.\", \"6. Verify that the implemented ELBO calculation is numerically stable and that the training loop shows monotonic improvement on a simple, noise-free synthetic dataset.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the loss function formula. /general-search finds a reference implementation structure. The /code agent implements the model based on the formula and the reference structure. The final code and plots are passed to the router for QA. Verification: The code's ELBO value must increase over time. Fallback: If the ELBO diverges, the /code agent must revisit the mathematical derivation from /math to check for sign errors in the loss function implementation.\", \"todo_list\": [\"- [ ] Use /math to derive the ELBO and its components.\", \"- [ ] Use /general-search to find a reference tutorial for VI in Bayesian linear regression.\", \"- [ ] Use /code to write and test the Python implementation.\", \"- [ ] Verify that the ELBO loss converges during training on a synthetic dataset.\", \"- [ ] Bundle all artifacts (derivation, code, plot, citations) for final router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"variational-inference\", \"bayesian-methods\", \"probabilistic-models\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO is correct and clearly shows its relationship to the KL divergence.\", \"- The Python script successfully implements Bayesian linear regression, runs without errors, and uses a recognized framework like PyTorch or JAX.\", \"- The generated plot clearly shows the ELBO value increasing (or the negative ELBO decreasing) over training iterations, indicating successful optimization.\", \"- The final report includes at least two citations to authoritative sources on variational inference.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3046, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive and implement a distributed consensus ADMM variant for solving a group-lasso regularized logistic regression problem. Analyze its convergence rate under this non-smooth but convex composite objective, and empirically compare its performance against standard ADMM, focusing on the impact of the augmented Lagrangian parameter rho on communication overhead and solution accuracy in a federated setting.\n", "completion": "{\"route_plan\": [\"/math(Derive the update rules for a consensus ADMM formulation of group-lasso logistic regression, detailing the primal/dual variable updates and the block soft-thresholding proximal operator. Analyze the theoretical convergence conditions for this composite objective.)\", \"/general-search(query=\\\"site:arxiv.org convergence analysis consensus ADMM federated learning group lasso\\\", mode=rag)\", \"/code(Implement the derived consensus ADMM in Python using NumPy for a simulated federated learning bioinformatics workload. Measure convergence rate via residual norms, solution sparsity, and communication costs against a baseline centralized solver, validating the impact of the augmented Lagrangian parameter rho.)\"], \"route_rationale\": \"/math is used first to establish the theoretical foundation and derive the core algorithm's update steps. The blue /general-search agent is then tasked with gathering supporting literature on the convergence properties of this specific ADMM variant, crucial for contextualizing the results. Finally, /code implements and validates the derived theory against empirical performance metrics in a simulated environment.\", \"expected_artifacts\": [\"A LaTeX document (.pdf) containing the full mathematical derivation of the ADMM updates and convergence analysis.\", \"A Python script (.py) implementing the distributed consensus ADMM and the baseline solver.\", \"A JSON file (`metrics.json`) reporting the final primal/dual residuals, convergence iterations, and solution sparsity.\", \"A summary report (.md) including convergence plots, analysis of the parameter rho, and citations.\", \"A log file containing the output of the verification checks for residual norms.\"], \"thinking_outline\": [\"1. Formulate the global group-lasso regularized logistic regression problem as a consensus optimization problem suitable for the Alternating Direction Method of Multipliers (ADMM).\", \"2. Derive the scaled-form ADMM update equations for the local primal variables (local models), the global consensus variable, and the dual variables, paying special attention to the proximal operator for the group-lasso penalty.\", \"3. Implement the derived algorithm in Python, simulating a distributed environment. Track key metrics like the objective function value, primal and dual residuals, and solution sparsity per iteration.\", \"4. Validate the implementation's correctness by checking if the primal and dual residuals converge to a value below a small tolerance (e.g., 1e-6) on a synthetic dataset.\", \"5. Use the literature search to find established convergence rates for consensus ADMM on composite convex problems and verify that the empirical rates observed in the simulation are consistent with theory.\", \"6. Audit the final solution to ensure it satisfies the problem's optimality conditions approximately.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the specific algorithmic form to search for in the literature. /general-search provides theoretical convergence guarantees that /code will attempt to empirically validate. The final results from /code are passed to the router for a full-stack quality assurance check. Verification: The derivation from /math is checked against standard ADMM literature found by /general-search. The implementation in /code is verified against the derivation. Fallback: If the derivation is inconsistent with literature, /math is asked to revise. If the code fails to converge, debug against a simpler L2-regularized problem.\", \"todo_list\": [\"- [ ] /math: Derive the consensus ADMM update equations for group-lasso regularized logistic regression.\", \"- [ ] /general-search: Find >=2 recent arXiv papers on consensus ADMM convergence for composite objectives.\", \"- [ ] /code: Implement the consensus ADMM algorithm and a centralized baseline solver in Python.\", \"- [ ] /code: Generate plots comparing the convergence rate (residual norms vs. iteration) of the ADMM implementation against the baseline.\", \"- [ ] /math: Verify that the proximal operator implemented in the code correctly matches the derived block soft-thresholding function.\", \"- [ ] /code: Validate that the final primal and dual residuals are below a tolerance of 1e-5.\", \"- [ ] /router: Perform final QA review, ensuring consistency between the derivation, cited literature, and empirical results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"convex-optimization\", \"admm\", \"distributed-computing\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ADMM updates, including the group-lasso proximal operator, is complete and correct.\", \"- The Python implementation runs successfully and generates convergence plots where primal and dual residuals monotonically decrease to a pre-specified tolerance.\", \"- The final report cites at least two relevant arXiv papers with IDs and compares the empirical findings to the theoretical guarantees discussed in those papers.\", \"- The `metrics.json` file is successfully generated and contains valid numerical values for all specified metrics.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3095, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Fisher Information Metric for a single-head self-attention mechanism, treating the softmax output as a categorical distribution on the vertices of a simplex. Subsequently, analyze the spectral properties of this metric's Hessian, specifically its largest eigenvalue, and discuss its implications for the stability of contrastive self-supervised learning under InfoNCE loss. Provide a formal proof connecting the spectral norm to convergence guarantees.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org information geometry self-attention Fisher Information Matrix InfoNCE convergence\\\", mode=rag)\", \"/math(Derive the Fisher Information Metric for the attention distribution. Then, compute the Hessian of the InfoNCE loss with respect to the attention logits and analyze its maximum eigenvalue to establish local curvature properties., includes numpy)\", \"/code(Implement a PyTorch model to numerically compute the Hessian's spectral norm for a toy attention layer under InfoNCE loss. Add validation checks using finite differences and track convergence metrics for the simulation., using Python)\", \"/math(Finalize the formal proof connecting the bounded spectral norm of the Hessian to the linear convergence rate of the learning dynamics, citing foundational results on optimization from the arxiv search.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather seminal papers on information geometry in attention and InfoNCE. The core mathematical derivation and Hessian analysis are assigned to /math. /code is then used for numerical verification of the theoretical spectral properties. Finally, /math synthesizes these results into a formal proof, ensuring a rigorous and validated conclusion.\", \"expected_artifacts\": [\"A formal mathematical proof (LaTeX) deriving the Fisher Information Metric and connecting the Hessian's spectral norm to InfoNCE convergence.\", \"A Python script (PyTorch) for the numerical validation of the Hessian's largest eigenvalue.\", \"A summary report citing at least 2 relevant arXiv papers on information geometry and SSL.\", \"A verification log comparing the analytical Hessian derivation with the numerical results from the code.\"], \"thinking_outline\": [\"1. Establish the probabilistic model for the self-attention mechanism, defining the output distribution over the sequence tokens on a probability simplex.\", \"2. Derive the components of the Fisher Information Matrix (FIM) for this categorical distribution parameterized by the attention logits, using standard information geometry definitions.\", \"3. Compute the Hessian of the InfoNCE loss function with respect to the same logits and relate it to the FIM.\", \"4. Validate the analytical expression for the largest eigenvalue of the Hessian through a numerical simulation in Python, checking for consistency with the derived bounds.\", \"5. Construct a formal proof that a bounded spectral norm of this Hessian implies a local linear convergence rate for gradient descent, referencing relevant optimization theory.\", \"6. Audit the final proof against the initial problem statement and the numerical verification results to ensure correctness and completeness.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (numerical validation) -> /math (final proof) -> router QA (verification: Compare numerical eigenvalues from /code with the analytical bounds from /math; fallback: If mismatch >5%, re-run /math derivation with refined assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on Information Geometry of Transformers and InfoNCE loss theory.\", \"- [ ] /math: Derive the Fisher Information Metric and the Hessian of the InfoNCE loss.\", \"- [ ] /code: Implement numerical computation of the Hessian's spectral norm for a toy example.\", \"- [ ] /math: Verify that the analytical derivation from the first math step aligns with the numerical results from /code.\", \"- [ ] /math: Formulate the final proof connecting the spectral norm bound to convergence guarantees.\", \"- [ ] /code: Document the simulation setup and generate plots of eigenvalue distributions.\", \"- [ ] router: Perform final QA check, ensuring the proof, code, and citations meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-geometry\", \"self-supervised-learning\", \"attention-mechanism\"], \"acceptance_criteria\": [\"- The final report must contain a complete, step-by-step mathematical derivation of the Hessian.\", \"- The numerical eigenvalue computed by the Python script must match the analytically derived bounds within a 5% tolerance.\", \"- The proof must correctly apply concepts from optimization theory to establish convergence.\", \"- At least two relevant arXiv pre-prints must be cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3097, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and analyze a distributed consensus LASSO solver using the Alternating Direction Method of Multipliers (ADMM). Please derive the primal-dual updates for the augmented Lagrangian, establish the theoretical convergence guarantees under standard assumptions, and implement the algorithm in Python. The implementation should be benchmarked against a centralized solver on synthetic data, focusing on solution accuracy and convergence rate as a function of the penalty parameter rho, especially concerning the communication complexity in a federated setting.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org federated learning consensus ADMM convergence rate analysis\\\", mode=rag)\", \"/math(Derive the ADMM updates for the distributed LASSO problem, analyzing the theoretical convergence rate and per-iteration communication complexity.)\", \"/code(Implement the ADMM solver and a centralized LASSO baseline in Python, generate synthetic data from a sparse linear model, and measure convergence metrics.)\", \"/math(Verify the implementation's correctness by checking if the primal and dual residuals from the code's output logs converge to zero, cross-referencing the eigenvalue properties of the problem formulation., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with literature review via the blue /general-search agent to ground the theoretical work in established results. /math is then used to perform the formal derivation. /code implements the derived algorithm and evaluation harness. A final /math call verifies the implementation's numerical behavior against the theoretical convergence conditions, ensuring correctness.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the ADMM updates and the convergence analysis.\", \"A Python script (`admm_lasso.py`) implementing both the distributed ADMM solver and a centralized baseline for comparison.\", \"A summary report (`report.md`) with plots showing the convergence of primal/dual residuals and a comparison of the final objective values.\", \"A BibTeX file (`references.bib`) with citations for at least two relevant arXiv papers.\", \"A text file (`verification_log.txt`) showing the final primal and dual residual values are below a tolerance of 1e-6.\"], \"thinking_outline\": [\"1. Formulate the global consensus LASSO problem as the sum of local LASSO objectives subject to a consensus constraint, z_i = x, for all agents i.\", \"2. Construct the augmented Lagrangian for this formulation and derive the iterative ADMM updates for the local variables (x_i), the consensus variable (z), and the dual variables (u_i).\", \"3. Based on the literature review, adapt a formal proof to show that the algorithm converges to the optimal solution under standard assumptions on the data and penalty parameter.\", \"4. Implement the algorithm, simulating a distributed environment, and validate the correctness by ensuring the primal and dual residuals diminish with each iteration.\", \"5. Check the final objective value and sparsity pattern of the ADMM solution against a trusted centralized LASSO solver (e.g., from scikit-learn) on a generated dataset to verify correctness.\", \"6. Analyze and plot the convergence rate as a function of the ADMM penalty parameter rho to understand its impact on performance.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses these to create a formal derivation -> /code implements the derivation and generates numerical results -> /math verifies that the numerical residuals from the code match theoretical expectations. Router QA performs a final check on all artifacts. Fallback: If verification fails, /code will be rerun with more extensive logging to pinpoint the discrepancy with the /math derivation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on ADMM for consensus optimization, focusing on convergence proofs.\", \"- [ ] /math: Derive the complete ADMM update equations for the distributed LASSO problem based on the augmented Lagrangian.\", \"- [ ] /math: Formulate the convergence proof, clearly stating all assumptions.\", \"- [ ] /code: Implement the ADMM solver, a data generator, and a centralized baseline solver in Python.\", \"- [ ] /code: Run experiments to collect convergence data (primal/dual residuals) and final objective values.\", \"- [ ] /math: Verify that the primal and dual residuals produced by the /code agent converge to a value below 1e-6.\", \"- [ ] /code: Generate plots for the final report and compile all artifacts.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"admm\", \"convex-optimization\", \"primal-dual-methods\"], \"acceptance_criteria\": [\"- The implemented ADMM algorithm must converge, with both primal and dual residuals decreasing monotonically towards zero (final value < 1e-6).\", \"- The final objective value from the ADMM solver must match the value from the centralized scikit-learn solver within a relative tolerance of 1e-4.\", \"- The final report must cite at least two relevant arXiv papers with their IDs, discussing their relevance to the derivation.\", \"- The submitted code must be executable and reproduce the plots and metrics presented in the report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3104, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm planning an education module on combinatorial game theory and need a complete walkthrough of Nim's winning strategy. Can you explain the Sprague-Grundy theorem, derive the nim-sum using XOR, and provide a Python implementation to calculate the Grundy numbers (g-numbers) for a given game state to determine P/N positions? The complexity analysis is crucial.\n", "completion": "{\"route_plan\": [\"/math(Derive the Sprague-Grundy theorem for impartial games, focusing on the concept of nim-sum (bitwise XOR) to calculate Grundy numbers. Prove why a game state's g-number is zero if and only if it is a P-position, establishing the basis for the winning strategy's computational complexity.)\", \"/general-search(query=\\\"site:arxiv.org OR site:*.edu 'Sprague-Grundy theorem' impartial games algorithmic complexity education\\\", mode=both)\", \"/code(Write a Python script to compute the Grundy number (g-number) for any state in the game of Nim. The implementation must include a function to determine if a position is a winning (N) or losing (P) position based on the nim-sum and validate its correctness against known simple cases. Analyze the time complexity of the algorithm.)\"], \"route_rationale\": \"/math is used for the formal proof and derivation of the Sprague-Grundy theorem. The blue /general-search agent then finds academic references to support these theoretical claims. Finally, /code implements the derived algorithm in Python for practical validation and complexity analysis.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Sprague-Grundy theorem and the nim-sum property.\", \"A commented Python script implementing a solver for the game of Nim.\", \"A formal analysis of the time and space complexity of the implemented solver.\", \"A list of at least 2 citations from academic sources (e.g., arXiv, university course notes) supporting the theory.\", \"Verification output showing the Python code correctly identifies P/N-positions for several test cases.\"], \"thinking_outline\": [\"1. Define an impartial game and introduce the concepts of P-positions (previous player winning) and N-positions (next player winning).\", \"2. Formulate the Sprague-Grundy theorem, defining Grundy numbers (g-numbers) and the minimum excluded value (mex) function.\", \"3. Prove that the g-number of a sum of games is the nim-sum (bitwise XOR) of their individual g-numbers.\", \"4. Implement the g-number calculation and nim-sum logic in Python to build a solver for Nim.\", \"5. Validate the Python implementation by checking its output against manually computed g-numbers for small, simple Nim game states.\", \"6. Analyze the algorithmic complexity of the solver in terms of the number and size of the heaps.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: Python script's outputs for base cases must match the mathematical derivation's predictions for P/N positions; fallback: If a mismatch occurs, the /math agent will re-audit the proof for edge cases, and the /code agent will add more detailed logging to trace the g-number calculations.)\", \"todo_list\": [\"- [ ] Use /math to derive the Sprague-Grundy theorem and the nim-sum property.\", \"- [ ] Use /general-search to find two authoritative references on impartial game theory.\", \"- [ ] Use /code to implement a Python solver for Nim, including g-number calculation and complexity analysis.\", \"- [ ] Verify that the Python solver correctly identifies winning/losing positions for at least three distinct test cases.\", \"- [ ] Assemble all artifacts (derivation, code, citations, verification logs) into a final report.\", \"- [ ] Submit final package for router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"sprague-grundy-theorem\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly proves the nim-sum property based on the mex function.\", \"- The Python code correctly identifies the winning/losing status for Nim games with up to 5 heaps and 20 items per heap.\", \"- The complexity analysis correctly states the time complexity as a function of the number of heaps.\", \"- At least two relevant academic citations (from arXiv or .edu domains) are provided.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3112, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a physics-informed neural network (PINN) to solve the 2D incompressible Navier-Stokes equations for lid-driven cavity flow. The model must strictly enforce the divergence-free constraint on the velocity field. Please provide the complete mathematical derivation of the loss function, a JAX implementation using automatic differentiation for the PDE residuals, and a validation against established benchmark data.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org physics informed neural network incompressible navier-stokes divergence-free constraint formulations\\\", mode=web)\", \"/math(Derive the composite loss function for the Navier-Stokes PINN, including momentum equation residuals, continuity equation (incompressibility) residual, and Dirichlet boundary conditions, referencing the Euler-Lagrange energy functional.)\", \"/code(Implement the PINN architecture in JAX, including the custom loss function based on automatic differentiation for PDE residuals, and set up the training loop for the lid-driven cavity problem, ensuring convergence diagnostics are logged., using Python)\", \"/math(Formally verify the trained model's output by computing the discrete divergence of the velocity field across the domain and analyzing the convergence of its maximum absolute value towards zero., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art formulations for handling the incompressibility constraint in PINNs. /math is then used for the rigorous derivation of the governing equations and loss function. /code implements this mathematical model in JAX, a suitable framework for such scientific computing tasks. A final /math step is crucial for verifying that the numerical solution produced by the code adheres to the physical principles derived earlier.\", \"expected_artifacts\": [\"- A LaTeX/Markdown document detailing the mathematical derivation of the Navier-Stokes PINN loss function.\", \"- A fully commented Python script (`pinn_solver.py`) implementing the JAX-based solver.\", \"- A final report (`report.md`) summarizing the methodology, training results, and verification analysis, including plots of velocity/pressure fields and convergence metrics.\", \"- A `metrics.json` file containing the final mean squared error loss and the maximum divergence of the velocity field.\", \"- A bibliography with citations for at least two relevant academic papers.\"], \"thinking_outline\": [\"1. Research current best practices for formulating PINNs for the incompressible Navier-Stokes equations, paying close attention to methods for enforcing the divergence-free constraint.\", \"2. Formulate the partial differential equations (momentum and continuity) and boundary conditions for the 2D lid-driven cavity problem. Derive the composite loss function as the sum of squared residuals.\", \"3. Implement the neural network architecture, the physics-informed loss function using JAX's automatic differentiation, and the training procedure using the Adam optimizer followed by L-BFGS.\", \"4. Train the model on collocation points sampled from the domain and its boundaries until the loss converges.\", \"5. Validate the trained model by evaluating the divergence of the predicted velocity field across a fine grid within the domain and confirm it is close to zero everywhere.\", \"6. Verify the solution's physical plausibility by comparing the predicted velocity and pressure fields with established benchmark results from computational fluid dynamics literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers for the /math agent's derivation. The resulting loss function from /math is the specification for the /code agent. The code's numerical output is then handed back to the /math agent for formal verification against the divergence-free constraint. Verification failure at the final step triggers a fallback to the /code agent to debug the implementation or loss function.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on PINN formulations for incompressible fluid flow.\", \"- [ ] /math: Derive the complete loss function, clearly defining terms for momentum, continuity, and boundary conditions.\", \"- [ ] /code: Implement the PINN network architecture and the data sampling strategy for domain and boundary points.\", \"- [ ] /code: Implement the custom loss function using JAX's grad function and configure the training loop.\", \"- [ ] /math: Verify that the implemented loss components in the code correctly map to the derived PDE residuals.\", \"- [ ] /code: Train the model and generate plots for loss history, final velocity/pressure fields, and divergence map.\", \"- [ ] /math: Perform a final numerical verification of the incompressibility constraint on the output data.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"navier-stokes\"], \"acceptance_criteria\": [\"- The final mean squared error of the PDE residuals must be less than 1e-5.\", \"- The maximum absolute value of the divergence of the predicted velocity field (`|\u2207 \u00b7 u|`) across the validation grid must be less than 1e-4.\", \"- The generated velocity field plot must qualitatively show the primary vortex characteristic of lid-driven cavity flow.\", \"- The provided code must run and reproduce the reported metrics given the `repro` contract.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3140, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project on adversarial training and need to understand saddle points. Can you explain non-convex optimization by deriving the Hessian eigenvalue conditions for a saddle point and then creating a visualization for the function f(x, y) = x^3 - 3x - y^2?\n", "completion": "{\"route_plan\": [\"/math(Derive the first and second-order necessary conditions for identifying critical points of f(x, y) = x^3 - 3x - y^2, specifically focusing on the Hessian eigenvalue signature that defines a saddle point.)\", \"/code(Using Python with NumPy and Matplotlib, implement the function f(x, y) = x^3 - 3x - y^2, locate its critical points, and create a 3D surface plot with a contour plot to visually validate the saddle point's structure and local convergence behavior.)\", \"/general-search(query=\\\"site:distill.pub OR site:arxiv.org introductory saddle point analysis non-convex optimization visualization\\\", mode=rag)\"], \"route_rationale\": \"The /math agent is used first to establish the theoretical ground truth by deriving the properties of the function's critical points. The /code agent then provides empirical validation by implementing the function and generating a visualization that should match the mathematical theory. Finally, the blue /general-search agent is tasked with finding authoritative external references to contextualize the findings.\", \"expected_artifacts\": [\"A markdown file with the mathematical derivation of saddle point conditions and coordinates.\", \"A Python script (`saddle_point_visualization.py`) that generates the 3D plot.\", \"A PNG image (`saddle_point_plot.png`) showing the surface and contour plot.\", \"A bibliography file (`references.bib`) with at least two cited sources.\", \"A brief verification report confirming the code's output matches the mathematical derivation.\"], \"thinking_outline\": [\"1. Define a saddle point in the context of multivariable calculus and optimization, noting its relevance to non-convex problems.\", \"2. For the function f(x, y) = x^3 - 3x - y^2, compute the gradient and set it to zero to find all critical points.\", \"3. Compute the Hessian matrix for the function and evaluate it at each critical point.\", \"4. Analyze the eigenvalues of the Hessian at each critical point to classify them as local minima, maxima, or saddle points. A saddle point will have both positive and negative eigenvalues.\", \"5. Verify that the Python code implementation correctly identifies the same critical points and that the generated plot visually corresponds to the mathematical classification of the saddle point.\", \"6. Synthesize the derivation, visualization, and references into a coherent explanatory report.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math provides the ground truth coordinates and classification for the critical points. The /code agent implements this, and its output (plotted saddle point location) is verified against the math results. The /general-search agent provides supporting literature. The complete package is then audited by router QA. Fallback: If the code's visualization contradicts the mathematical derivation, the task is returned to the /math agent to re-check the Hessian analysis.\", \"todo_list\": [\"- [ ] /math: Derive the critical points and Hessian eigenvalue conditions for f(x, y) = x^3 - 3x - y^2.\", \"- [ ] /code: Implement the function, find critical points programmatically, and generate a 3D visualization.\", \"- [ ] /general-search: Find two authoritative sources on saddle points in optimization.\", \"- [ ] /verification: Cross-reference the critical points found by the /math and /code agents to ensure they match.\", \"- [ ] /router-qa: Assemble all artifacts and submit for final review and approval.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-matrix\"], \"acceptance_criteria\": [\"The mathematical derivation correctly identifies the coordinates of the saddle point at (1, 0) and the local maximum at (-1, 0).\", \"The Python script generates a 3D plot that clearly visualizes the saddle structure (curving up in one direction, down in another) at the derived coordinates.\", \"The final report includes at least two valid citations to authoritative sources on optimization or machine learning.\", \"The primary metric (Hessian Eigenvalue Product) is confirmed to be negative at the identified saddle point.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3143, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Knill-Laflamme conditions for a non-degenerate quantum error-correcting code from first principles, employing the operator-sum representation of a quantum channel. Furthermore, express the encoding isometry and error operators using tensor calculus notation and formally prove the connection between these conditions and the code's ability to correct errors up to a weight determined by its minimum distance. How does the projector onto the codespace relate to the Choi matrix of the recovery channel?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Knill-Laflamme conditions tensor network formulation stabilizer codes Choi matrix quantum recovery channel\\\", mode=rag)\", \"/math(Derive the Knill-Laflamme conditions for a [[k,n,d]] stabilizer code, analyzing the projector's eigenvalue spectrum and ensuring the convergence of the recovery fidelity under noise. Reference foundational arxiv papers on quantum channels.)\", \"/code(Implement a symbolic verification in Python using SymPy for the 7-qubit Steane code, representing the encoding isometry and Pauli error operators as matrices. The implementation should check the orthogonality conditions and confirm the error-correcting properties based on the derived mathematical constraints, considering computational complexity and potential for CUDA acceleration for larger codes.)\", \"/math(Formally audit the symbolic code's output against the derived theoretical conditions. Finalize the proof connecting the code distance to the weight of correctable errors, referencing concepts from control theory to model the fault-tolerant recovery process and chebyshev bounds on success probability.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather authoritative literature from arXiv. This informs the core theoretical work in /math, where the Knill-Laflamme conditions are derived. The /code tool is then used to create a concrete, symbolic verification of this abstract theory on a well-known code, acting as a sanity check. The final /math step validates the code's output and completes the formal proof, ensuring a tight loop between theory and verification.\", \"expected_artifacts\": [\"- A formal mathematical proof in LaTeX format detailing the derivation of the Knill-Laflamme conditions and their relation to code distance.\", \"- A commented Python script using SymPy that symbolically verifies the conditions for the 7-qubit Steane code.\", \"- A bibliography file citing at least two relevant arXiv preprints with their IDs.\", \"- A verification report summarizing the alignment between the theoretical derivation and the symbolic code's output.\"], \"thinking_outline\": [\"1. Define the quantum error correction framework using quantum channels, Kraus operators {E_a}, and the encoding isometry V from the logical Hilbert space to the physical codespace.\", \"2. Derive the necessary and sufficient Knill-Laflamme conditions, P E_a^\u2020 E_b P = c_{ab} P, where P is the projector onto the codespace, by requiring the existence of a valid recovery operation.\", \"3. Validate this derivation against foundational texts and papers to ensure correctness and standard notational conventions.\", \"4. Express the encoding map V and the projector P = V V^\u2020 using tensor notation and implement a symbolic representation for the [[7,1,3]] Steane code.\", \"5. Use the symbolic implementation to explicitly compute the matrices c_{ab} for all single-qubit Pauli errors and check that they satisfy the conditions for a valid QECC.\", \"6. Verify that the symbolic computation results align perfectly with the theoretical predictions from the formal derivation, confirming the implementation's correctness.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation that serves as a specification for the code agent. The code agent returns a symbolic computation and script which the math agent then formally verifies. Fallback: If the code verification fails, the task is routed back to the /code agent with specific error feedback from the /math agent for debugging.\", \"todo_list\": [\"- [ ] /general-search: Identify and retrieve seminal papers on Knill-Laflamme conditions and stabilizer codes from arXiv.\", \"- [ ] /math: Formulate the problem and provide a step-by-step derivation of the QECC conditions.\", \"- [ ] /code: Develop the Python/SymPy script to model the Steane code's operators.\", \"- [ ] /code: Execute the script to compute the coefficient matrix c_ab for single-qubit Pauli errors.\", \"- [ ] /math: Verify that the c_ab matrix computed by the code agent is proportional to the identity matrix as required by the theory.\", \"- [ ] /math: Complete the formal proof relating the conditions to the code distance.\", \"- [ ] /router: Perform final QA review of all artifacts, checking for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction\", \"tensor-calculus\"], \"acceptance_criteria\": [\"- The LaTeX document must contain a complete and correct proof of the Knill-Laflamme conditions.\", \"- The Python script must execute without errors and produce output that correctly verifies the conditions for the [[7,1,3]] Steane code against single-qubit Pauli errors.\", \"- The final report must cite at least two relevant arXiv preprints, including their full IDs (e.g., quant-ph/9609029).\", \"- The connection between the code's minimum distance d and its ability to correct floor((d-1)/2) errors must be formally proven.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3177, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a high-performance solver for large, sparse, symmetric positive-definite linear systems arising from finite element method (FEM) simulations. Specifically, I want to implement a preconditioned conjugate gradient (PCG) method in Python, but accelerate the computationally-dominant sparse matrix-vector product (SpMV) operation using a custom CUDA kernel. Please derive the PCG algorithm with a Jacobi preconditioner, analyze the expected impact on the system's eigenvalue distribution, and then implement and benchmark the solver against a CPU-based equivalent, verifying the convergence rate and performance gains.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org gpu sparse matrix vector product optimization cuda OR site:nvidia.com parallel iterative solvers jacobi preconditioner\\\", mode=web)\", \"/math(Derive the preconditioned conjugate gradient algorithm for Ax=b using a diagonal (Jacobi) preconditioner, and analyze how this transformation is expected to improve the condition number by clustering the system's eigenvalue spectrum.)\", \"/code(Implement the PCG solver in Python using Numba's CUDA JIT. Create a custom SpMV kernel for matrices in CSR format. Generate a test sparse matrix from a 2D Poisson equation discretization to validate CUDA kernel correctness and measure convergence.)\", \"/math(Verify that the empirical convergence rate from the code's output aligns with the theoretical bounds predicted by the Chebyshev polynomial analysis of the conjugate gradient method given the new eigenvalue distribution.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art techniques for CUDA SpMV optimization. This informs the theoretical derivation in /math. The core implementation and benchmarking occur in /code. A final /math step is crucial for formally verifying that the empirical results from the code align with the mathematical theory, closing the loop between derivation and experimentation.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Jacobi-preconditioned conjugate gradient algorithm.\", \"- Python source code for the GPU-accelerated solver, including the custom Numba/CUDA kernel.\", \"- A performance report including plots of convergence (residual norm vs. iteration) and speedup (GPU vs. CPU wall time).\", \"- A JSON file containing the final performance metrics (speedup, iteration count).\", \"- A bibliography with at least two cited papers on GPU-accelerated numerical methods.\"], \"thinking_outline\": [\"1. Research efficient data structures (e.g., CSR, ELL) for sparse matrices on GPUs and common preconditioners suitable for parallel architectures.\", \"2. Formulate the preconditioned conjugate gradient algorithm mathematically, showing how the preconditioner M transforms the system Ax=b into M\u207b\u00b9Ax = M\u207b\u00b9b and improves the condition number of M\u207b\u00b9A.\", \"3. Implement the solver in Python, focusing on an efficient Numba/CUDA kernel for the SpMV operation, which is the primary bottleneck.\", \"4. Validate the implementation's numerical correctness by comparing its solution for a small, known system against a trusted library like SciPy's `linalg.cg`.\", \"5. Benchmark the GPU solver's wall-clock time against a pure NumPy/SciPy CPU implementation across a range of matrix sizes, and calculate the speedup factor.\", \"6. Audit the observed number of iterations to convergence and check if it is consistent with the theoretical improvements predicted from the mathematical analysis of the system's eigenvalue properties.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results inform the mathematical derivation. The derivation provides the algorithm for the /code agent. The /code agent produces empirical data (convergence plots, timings) which are then handed to the final /math agent for formal verification against the theory. Fallback: If the code fails to converge or shows poor performance, the /math agent will be re-invoked to analyze alternative preconditioners before attempting a new implementation.\", \"todo_list\": [\"- [ ] /general-search: Find reference implementations or papers on CUDA SpMV kernels for CSR-formatted matrices.\", \"- [ ] /math: Derive the full PCG algorithm, clearly defining each step (residual, search direction, step size).\", \"- [ ] /code: Implement the CSR data structure and the corresponding SpMV CUDA kernel using Numba.\", \"- [ ] /code: Set up a test problem by creating the sparse Laplacian matrix for a 2D grid.\", \"- [ ] /math: Verify that the code's output on a 4x4 test grid matches a hand-calculated or SciPy result.\", \"- [ ] /code: Profile the execution time for large matrices (e.g., 4096x4096 grid) and generate speedup plots.\", \"- [ ] /math: Analyze the final convergence report to confirm the iteration count reduction is statistically significant.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"cuda\"], \"acceptance_criteria\": [\"- The implemented PCG solver must converge to a solution with a relative residual norm below 1e-8.\", \"- The custom CUDA-accelerated SpMV must demonstrate at least a 10x wall-clock time speedup over a pure NumPy implementation for a matrix of size >1,000,000x1,000,000.\", \"- The final report must correctly link the mathematical theory of preconditioning to the empirically observed reduction in iterations.\", \"- The submission includes all expected artifacts, including verifiable code and a report with clear plots and citations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3180, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Alternating Least Squares (ALS) update rules for Canonical Polyadic (CP) decomposition with a non-negativity constraint on factor matrices. Then, implement this algorithm in Python using a sparse tensor library like `tensorly` to analyze a high-order biomedical knowledge graph, and validate the model's convergence properties and representation quality through reconstruction error and link prediction AUPRC.\n", "completion": "{\"route_plan\": [\"/math(Derive the Alternating Least Squares (ALS) update rules for non-negative Canonical Polyadic (CP) decomposition, analyzing the convergence properties of the block coordinate descent and its relationship to Bayesian inference models for high-dimensional biomedical data.)\", \"/general-search(query=\\\"site:arxiv.org site:jmlr.org non-negative canonical polyadic decomposition sparse tensor factorization bioinformatics knowledge graph representation learning\\\", mode=web)\", \"/code(Implement the derived non-negative CP-ALS algorithm in Python using the 'tensorly' library for sparse tensors. Validate the implementation against a known baseline, track reconstruction error convergence, and compute link prediction AUPRC for a synthetic biomedical knowledge graph.)\"], \"route_rationale\": \"/math is required for the formal derivation of the constrained optimization update rules. The blue /general-search agent will find state-of-the-art literature on sparse tensor factorization in bioinformatics to ground the theory and application. /code is needed to translate the mathematical derivation into a performant Python implementation and to conduct the empirical validation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the non-negative CP-ALS update rules.\", \"A commented Python script implementing the algorithm for sparse tensors, including synthetic data generation and evaluation functions.\", \"A summary report including a literature review with at least two arXiv citations, plots of reconstruction error convergence, and a table of final AUPRC scores.\", \"A verification log confirming the consistency between the mathematical derivation and the code's update loop.\"], \"thinking_outline\": [\"1. Formulate the optimization problem for Canonical Polyadic (CP) decomposition with an L2 loss and non-negativity constraints on the factor matrices.\", \"2. Derive the Alternating Least Squares (ALS) update rule for each factor matrix by solving the resulting non-negative least squares subproblem using block coordinate descent.\", \"3. Implement the derived algorithm in Python using the `tensorly` library, ensuring correct handling of sparse tensor formats (e.g., COO) to maintain computational efficiency.\", \"4. Verify the correctness of the implementation by checking the Karush-Kuhn-Tucker (KKT) conditions for a single update step on a small, synthetic problem.\", \"5. Conduct experiments on a larger synthetic sparse tensor to analyze the algorithm's convergence behavior by plotting reconstruction error against iteration number.\", \"6. Validate the quality of the learned representations by using them for a downstream link prediction task and evaluating the performance with the AUPRC metric.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: the Python implementation of the update rules in /code must be a direct translation of the final equations from the /math derivation; fallback: if a mismatch is found, /math must re-check the derivation for errors while the /code developer debugs the implementation logic).\", \"todo_list\": [\"- [ ] /math: Derive the update rules for non-negative CP-ALS using block coordinate descent.\", \"- [ ] /general-search: Find >=2 recent arXiv papers on sparse tensor factorization for knowledge graph completion in bioinformatics.\", \"- [ ] /code: Implement the CP-ALS algorithm in Python with `tensorly`, focusing on sparse tensor representations.\", \"- [ ] /code: Create a synthetic sparse tensor dataset representing a biomedical knowledge graph for testing.\", \"- [ ] /math: Verify that the Python implementation's update logic directly corresponds to the derived mathematical formulas.\", \"- [ ] /code: Generate plots for reconstruction error vs. iteration and compute the final AUPRC metric for link prediction.\", \"- [ ] /router: Perform final QA check on all artifacts, ensuring consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"tensor-decomposition\", \"representation-learning\", \"bioinformatics\"], \"acceptance_criteria\": [\"- The mathematical derivation for the non-negative CP-ALS update rules is complete, correct, and clearly articulated.\", \"- The Python implementation correctly processes sparse tensors, demonstrates convergence on a synthetic dataset, and reports a final reconstruction fit > 0.9.\", \"- The final report includes well-labeled convergence plots, AUPRC scores for a link prediction task, and cites at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3188, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want a foundational understanding of the 'self-attention' mechanism in Transformers. Could you derive the scaled dot-product attention formula, implement it in Python to verify the matrix operations, and find the original paper discussing its computational complexity?\n", "completion": "{\"route_plan\": [\"/math(Derive the scaled dot-product attention formula, Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V, and explain the scaling factor's role in preventing vanishing gradients for Bayesian models.)\", \"/code(Implement the derived attention formula in Python using NumPy. Create example Q, K, V tensors and verify the output dimensions and values, tracking the convergence of the output matrix.)\", \"/general-search(query=\\\"site:arxiv.org 'Attention Is All You Need' computational complexity analysis\\\", mode=both)\"], \"route_rationale\": \"/math is used first to establish the theoretical foundation of the attention mechanism. /code then validates this theory with a practical implementation, providing a concrete example. Finally, the blue /general-search agent is tasked with finding authoritative sources to contextualize the work and explore its complexity.\", \"expected_artifacts\": [\"A markdown file with the mathematical derivation of scaled dot-product attention.\", \"A Python script (`attention.py`) implementing the mechanism with a runnable example.\", \"A list of at least two citations from arXiv, including their IDs, for the foundational Transformer papers.\", \"A verification report confirming the Python output matches the expected dimensions and values from a manual calculation.\"], \"thinking_outline\": [\"1. Formulate the mathematical expression for scaled dot-product attention, clearly defining the Query (Q), Key (K), and Value (V) matrices and the key dimension `d_k`.\", \"2. Derive the role of the scaling factor `1/sqrt(d_k)` in maintaining unit variance in the dot products, which prevents the softmax function from saturating and producing vanishing gradients during training.\", \"3. Implement the formula in Python using a standard numerical library like NumPy, paying close attention to matrix multiplication order and broadcasting rules.\", \"4. Validate the implementation by creating small, integer-valued Q, K, V matrices and manually calculating the expected output to check against the code's result.\", \"5. Use search to locate the original \\\"Attention Is All You Need\\\" paper and a secondary source discussing its computational complexity.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: code output must match manual calculation for a small test case; fallback: if mismatch, return to /math to re-check derivation and scaling factor logic).\", \"todo_list\": [\"- [ ] Use /math to derive the scaled dot-product attention formula and explain the scaling factor's purpose.\", \"- [ ] Use /code to implement the attention formula in Python with a clear example.\", \"- [ ] Verify that the Python code's output matches a manually calculated example for correctness.\", \"- [ ] Use /general-search to find the original 'Attention Is All You Need' paper and another on its complexity.\", \"- [ ] Submit all artifacts (derivation, code, citations, verification report) for router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"llm-optimization\", \"transformer-architecture\", \"self-attention\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly shows the matrix operations and justifies the `1/sqrt(d_k)` scaling factor.\", \"- The Python script runs without errors and produces an attention matrix with the correct dimensions for given Q, K, V inputs.\", \"- The output of the Python script for a provided test case exactly matches a pre-calculated ground truth value.\", \"- At least two relevant arXiv papers, including the original \\\"Attention Is All You Need\\\", are cited with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3197, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a safe reinforcement learning agent for a robotic manipulator task. The core requirement is to implement Constrained Policy Optimization (CPO) using Lagrangian relaxation to handle safety constraints defined by obstacle proximity. Please derive the update rules for the policy and the Lagrangian multipliers, verify the Karush-Kuhn-Tucker (KKT) conditions are met, and provide a PyTorch implementation demonstrating convergence while maintaining near-zero safety violations.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Constrained Policy Optimization Lagrangian relaxation Lyapunov stability robotics\\\", mode=rag)\", \"/math(Derive the Lagrangian dual for the constrained policy update, formulate the trust region optimization objective, and analyze the convergence criteria with respect to control theory principles.)\", \"/code(Implement the CPO agent in Python with PyTorch and a PyBullet simulation, ensuring CUDA acceleration and tracking Bayesian safety critic estimates alongside reward and constraint cost.)\", \"/math(Validate the empirical results by checking if the trained policy's gradient updates satisfy the derived KKT conditions within a statistical confidence interval, applying Chebyshev's inequality for robustness.)\"], \"route_rationale\": \"The plan starts with research via the blue /general-search agent to ground the approach in current literature. This informs the theoretical derivation in /math. The core logic is then implemented in /code. A final /math step provides crucial verification by connecting the empirical results back to the initial theoretical framework, ensuring correctness.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the CPO update rules and KKT conditions.\", \"- A documented Python implementation of the CPO agent and the robotics simulation environment.\", \"- A final report (report.md) summarizing the methodology, results, and including plots for reward and constraint violations.\", \"- A verification script output confirming that the implementation aligns with the mathematical derivation.\", \"- A bibliography with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of Constrained Policy Optimization (CPO) and Lagrangian methods for safe RL.\", \"2. Formulate the constrained Markov Decision Process (CMDP) and derive the policy and Lagrange multiplier update rules from first principles.\", \"3. Implement the CPO algorithm and a suitable robotics simulation environment, instrumenting it to collect all relevant metrics.\", \"4. Train the agent and analyze its performance in terms of cumulative reward and constraint satisfaction.\", \"5. Verify that the implementation's update steps empirically satisfy the derived KKT conditions throughout the training process.\", \"6. Audit the final results to confirm that the observed constraint violation rate is below the specified threshold and aligns with theoretical expectations.\"], \"handoff_plan\": \"/general-search (papers/theory) -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA (final review). The math derivation must be complete before coding begins. The verification step takes the trained model from /code and the derivation from the first /math step as inputs. Fallback: If verification fails, the task reverts to the initial /math step to check for flaws in the derivation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 seminal papers on CPO or Lagrangian-based safe RL.\", \"- [ ] /math: Produce a self-contained PDF document with the full derivation of the CPO objective and update rules.\", \"- [ ] /code: Implement the CPO agent and the PyBullet simulation environment.\", \"- [ ] /code: Add logging for cumulative reward, constraint cost, and Lagrange multiplier values.\", \"- [ ] /math: Verify that the gradients and updates in the trained model from /code align with the KKT conditions derived earlier.\", \"- [ ] /math: Generate plots showing reward convergence and constraint satisfaction over training epochs.\", \"- [ ] /router: Perform final QA on all artifacts, checking for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"safe-reinforcement-learning\", \"robotics\", \"constrained-optimization\"], \"acceptance_criteria\": [\"- The Python code must successfully run and train an agent that improves its policy over time.\", \"- The final report must show a safety constraint violation rate below 1% over the last 10% of training episodes.\", \"- The mathematical derivation must be complete, correct, and clearly referenced in the implementation's documentation.\", \"- The project must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3201, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the SE(3)-equivariant message passing update rule for a graph neural network potential, leveraging tensor field networks and irreducible representations. Implement this in PyTorch Geometric, ensuring the Hamiltonian dynamics are conserved. Validate the model's energy conservation on the MD17 dataset for ethanol, comparing against a baseline SchNet model, and analyze the computational complexity of the forward pass.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org SE(3)-equivariant graph neural network tensor field network molecular dynamics\\\", mode=both)\", \"/math(From the sourced arXiv papers, formally derive the SE(3)-equivariant message passing update rules using tensor calculus and irreducible representations. Analyze the convergence properties for the potential energy surface.)\", \"/code(Implement the derived equivariant GNN potential in PyTorch Geometric, setting up an NVE molecular dynamics simulation for ethanol from the MD17 chemistry dataset. Track total energy and bootstrap confidence intervals for drift., using Python)\", \"/math(Analyze the simulation output, verifying energy conservation by computing the drift rate. Formulate the computational complexity and compare it with the empirical wall-clock time, referencing the model's eigenvalue spectrum.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on SE(3)-equivariant GNNs. The /math agent then formalizes the theory, deriving the core equations. Next, the /code agent implements this mathematical model and runs the physics simulation. A final /math agent call is used to analyze the simulation results and verify the model's physical consistency and computational properties.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF containing the full mathematical derivation of the SE(3)-equivariant update rules.\", \"- Python source code for the GNN model and the molecular dynamics simulation script.\", \"- A final report in Markdown summarizing the results, including plots of energy drift over time and a complexity analysis.\", \"- A JSON file containing the final computed metrics (energy drift rate, wall-time).\", \"- A bibliography file with citations for at least two relevant research papers.\"], \"thinking_outline\": [\"1. Perform a literature search to identify key papers on SE(3)-equivariant networks and tensor field networks, such as those by Thomas et al. or Geiger et al.\", \"2. Formally derive the message, aggregation, and update functions for the GNN using the principles of group theory and tensor calculus, ensuring SE(3) equivariance is maintained at each step.\", \"3. Implement the derived network architecture in PyTorch Geometric, paying close attention to the correct implementation of tensor products and spherical harmonics.\", \"4. Set up and execute a molecular dynamics simulation in an NVE ensemble to generate trajectories and monitor the total system energy.\", \"5. Validate the model's correctness by verifying that the total energy is conserved throughout the simulation, calculating the energy drift rate and comparing it to a non-equivariant baseline.\", \"6. Audit the theoretical computational complexity derivation against empirical timing measurements from the implementation to ensure they align.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /math agent will verify the theoretical soundness of concepts from the literature found by /general-search. The /code agent's implementation will be cross-checked against the /math agent's derivation. The final /math agent validates the physical plausibility of the /code agent's simulation output. Fallback: If the derivation is intractable, the /math agent will request a new search from /general-search for alternative formulations. If energy is not conserved, the task reverts to the /code agent for debugging.\", \"todo_list\": [\"- [ ] /general-search: Find seminal arXiv papers on Tensor Field Networks and SE(3)-equivariant GNNs for molecular potentials.\", \"- [ ] /math: Derive the equivariant message passing, aggregation, and update functions based on the literature.\", \"- [ ] /code: Implement the GNN potential and the NVE simulation loop in PyTorch Geometric.\", \"- [ ] /math: Formulate the expressions for total energy drift and theoretical computational complexity.\", \"- [ ] /math: Verify that the implemented energy calculation in the code aligns with the Hamiltonian derivation.\", \"- [ ] /code: Audit the implementation of the tensor products to ensure it correctly reflects the mathematical derivation of equivariance.\", \"- [ ] /math: Analyze the final simulation data to compute the primary and secondary metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"equivariant-networks\"], \"acceptance_criteria\": [\"- The mathematical derivation of the SE(3)-equivariant update rule is complete, correct, and self-contained in the provided PDF.\", \"- The implementation correctly conserves energy in an NVE simulation, with a total energy drift rate at least an order of magnitude lower than a non-equivariant GNN baseline.\", \"- The final report includes plots of total energy vs. simulation time, a computational complexity analysis (Big-O notation), and cites at least 2 relevant arXiv papers with IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3203, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Fokker-Planck equation corresponding to the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for a Bayesian neural network. Analyze the convergence properties of the non-equilibrium stationary distribution, and establish a rigorous upper bound on the Kullback-Leibler divergence between this stationary distribution and the true Bayesian posterior, particularly under non-convex, multi-modal loss landscapes.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Stochastic Gradient Langevin Dynamics Fokker-Planck convergence non-convex Bayesian\\\", mode=both)\", \"/math(Derive the Fokker-Planck partial differential equation for the weight posterior under SGLD, treating the mini-batch noise as a Wiener process and ensuring the resulting drift-diffusion terms accurately reflect the Bayesian learning objective and its gradient.)\", \"/code(Outline a JAX implementation to simulate the SGLD process for a toy Bayesian regression model, including logic to estimate the empirical stationary distribution and compute its KL-divergence against a known true posterior, ensuring numerical stability., using Python)\", \"/math(Analyze the derived Fokker-Planck equation to find its stationary solution. Formally prove an upper bound on the KL-divergence from the true posterior, relating it to the learning rate and mini-batch size, and discuss the convergence rate.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on SGLD and Fokker-Planck analysis. The /math agent then performs the core theoretical derivation. A /code agent is tasked with outlining a numerical simulation to create a verifiable test case for the theory. Finally, the /math agent returns to use the initial derivation to prove the convergence bounds, informed by the practical considerations from the coding step.\", \"expected_artifacts\": [\"- A formal mathematical document detailing the derivation of the Fokker-Planck equation and the proof of the KL-divergence bound.\", \"- A Python script (`.py`) containing a commented outline of the JAX simulation for SGLD.\", \"- A summary report in Markdown format, synthesizing the theoretical findings and simulation design, including at least two citations to arXiv papers.\", \"- A verification log detailing the consistency check between the mathematical derivation and the simulation's logic.\"], \"thinking_outline\": [\"1. Formulate the stochastic differential equation (SDE) that approximates the discrete-time SGLD update rule.\", \"2. Apply Ito's lemma or the standard SDE-to-PDE conversion to derive the corresponding Fokker-Planck equation for the probability density of the network weights.\", \"3. Analyze the stationary solution of the derived Fokker-Planck equation and identify its deviation from the target Gibbs-Boltzmann distribution (the true posterior).\", \"4. Derive a formal upper bound for the KL-divergence between the stationary distribution and the true posterior, expressing the bound in terms of key hyperparameters like step size and temperature.\", \"5. Validate the self-consistency of the derived bound by checking its behavior in limiting cases (e.g., as the step size approaches zero).\", \"6. Audit the conceptual JAX implementation plan to ensure it correctly measures the quantities needed to numerically verify the theoretical convergence rate and KL-divergence bound.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (proof) -> router QA (verification: Cross-check the KL-divergence bound from the final math step against the initial Fokker-Planck equation and the quantities measured in the code outline; fallback: If inconsistent, return to the /math agent to re-derive the bound with corrected assumptions).\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize 2-3 key papers on Fokker-Planck analysis of SGLD.\", \"- [ ] /math: Complete the full derivation of the Fokker-Planck equation from the SGLD SDE.\", \"- [ ] /code: Write the JAX simulation outline, including functions for the SGLD update, potential energy, and KL-divergence estimation.\", \"- [ ] /math: Complete the proof for the KL-divergence upper bound and analyze its asymptotic behavior.\", \"- [ ] /math: Verify that the derived analytical bound is consistent with the structure of the Fokker-Planck equation.\", \"- [ ] /code: Add explicit comments in the code outline specifying where and how to log metrics for verifying the convergence rate.\", \"- [ ] /router: Perform final QA review, ensuring all artifacts are present and the final proof aligns with the simulation design.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-calculus\", \"bayesian-inference\", \"fokker-planck\"], \"acceptance_criteria\": [\"- The derived Fokker-Planck equation must be mathematically correct and fully justified.\", \"- The KL-divergence bound must be a valid, non-trivial upper bound, proven with rigorous mathematical steps.\", \"- The provided JAX code outline must be syntactically plausible and logically sound for simulating the described process.\", \"- The summary report must correctly cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"The final analytical expression for the upper bound on D_KL(p_stationary || p_posterior). This is a theoretical result derived via mathematical proof.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"The difference between the theoretical upper bound and the KL-divergence value estimated from the simulation outline at stationarity. This metric assesses the tightness of the bound.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3204, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement the K-SVD algorithm for dictionary learning and apply it to a compressed sensing problem, specifically image inpainting. Please derive the update steps for both the dictionary and the sparse coefficients using Orthogonal Matching Pursuit (OMP), then implement the full pipeline in Python to reconstruct an image from which 50% of pixels have been randomly removed. Analyze the algorithm's convergence and reconstruction quality using PSNR and SSIM metrics.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD dictionary learning convergence analysis image inpainting\\\", mode=rag)\", \"/math(Derive the K-SVD update rules, including the SVD step for dictionary atom updates, and analyze the theoretical convergence criteria, paying attention to potential issues with local minima and eigenvalue stability.)\", \"/code(Implement the K-SVD algorithm and OMP for sparse coding in Python. Apply it to an image inpainting task using a random mask, evaluate reconstruction PSNR, and use bootstrap resampling to estimate confidence intervals on the Fourier domain error.)\", \"/math(Verify the implemented algorithm's convergence behavior empirically against the theoretical bounds. Analyze the computational complexity of the sparse coding step and check if it aligns with the expected big-O complexity using Chebyshev-style bounds on iteration counts.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on K-SVD for a solid theoretical foundation. /math is then used to formally derive the algorithm's update equations. /code translates this mathematical specification into a working implementation for the image inpainting task. A final /math step is crucial for verifying that the empirical performance and complexity of the implementation match the theoretical guarantees identified during the research and derivation phases.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the K-SVD and OMP update rules.\", \"A commented Python script implementing the dictionary learning and image inpainting pipeline.\", \"A final report in PDF format showing the original, masked, and reconstructed images, along with plots of PSNR vs. iteration.\", \"A JSON file containing the final PSNR, SSIM, and computation time metrics.\", \"A bibliography with at least two cited arXiv papers on dictionary learning.\"], \"thinking_outline\": [\"1. Research the K-SVD algorithm and its application to compressed sensing, focusing on its optimization objective and convergence properties from authoritative sources.\", \"2. Formulate the dictionary learning problem as an optimization task and formally derive the iterative update rules for the dictionary atoms and the sparse representations.\", \"3. Implement the complete algorithm in Python, including patch extraction, dictionary initialization, the main K-SVD loop, and the OMP pursuit algorithm.\", \"4. Validate the implementation by checking for decreasing reconstruction error at each iteration and ensuring the sparsity constraint on the coefficients is met.\", \"5. Execute the full image inpainting experiment, generate visualizations of the learned dictionary atoms and the reconstructed image, and compute the final performance metrics.\", \"6. Audit the empirical results by comparing the final reconstruction quality and convergence speed against published benchmarks or theoretical expectations.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent for derivation. The math agent's derivation is the formal specification for the code agent. The code agent produces an implementation and empirical results, which the second math agent then verifies against the theory. Verification failure at the final math step will trigger a fallback to the code agent for debugging. Inconsistent theory will loop back to the general-search agent.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on K-SVD for image processing, noting convergence guarantees.\", \"- [ ] /math: Produce a step-by-step derivation of the dictionary update and sparse coding stages of K-SVD.\", \"- [ ] /code: Implement the K-SVD algorithm using NumPy/SciPy, including a function for OMP.\", \"- [ ] /code: Set up the image inpainting experiment, including data loading, masking, and metric calculation (PSNR, SSIM).\", \"- [ ] /math: Verify that the logic in the Python code correctly implements the derived mathematical update rules.\", \"- [ ] /math: Analyze the computational complexity of the implementation and compare it with the theoretical complexity.\", \"- [ ] /code: Run the experiment, generate all required artifacts (plots, metrics), and write the final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The Python code must execute without errors and successfully reconstruct the image.\", \"- The final reconstructed image must achieve a PSNR of at least 25 dB on a standard test image (e.g., 'lena', 'cameraman') with 50% missing pixels.\", \"- The mathematical derivation must be clear, correct, and properly link the optimization problem to the final iterative algorithm.\", \"- The report must cite at least two relevant academic papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3237, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a high-performance Python implementation of online dictionary learning for sparse signal representation, specifically using the Lagrangian-based update rule for the dictionary atoms. The sparse coding step must be handled by the Iterative Shrinkage-Thresholding Algorithm (ISTA). The system must be benchmarked on synthetic data with known ground-truth sparsity, evaluating both reconstruction error and dictionary recovery, focusing on the algorithm's convergence properties and computational complexity.\n", "completion": "{\"route_plan\": [\"/math(Formulate the online dictionary learning objective function with an L1-norm sparsity penalty. Derive the Lagrangian for the dictionary update step and the proximal gradient update for the ISTA sparse coding step, analyzing the theoretical convergence criteria.)\", \"/general-search(query=\\\"site:arxiv.org online dictionary learning ISTA convergence analysis benchmark\\\", mode=web)\", \"/code(Implement the online dictionary learning algorithm in Python using NumPy. The implementation must include the ISTA sparse solver and the Lagrangian-based dictionary update. Create a synthetic data generator and a testing harness to measure Mean Squared Error (MSE) and dictionary recovery metrics, comparing against ADMM baselines.)\"], \"route_rationale\": \"The plan starts with /math to establish the rigorous mathematical foundation and derive the core update equations. The blue /general-search agent then consults recent literature to find canonical benchmarks and best practices for implementation, ensuring the work is current. Finally, /code translates the verified mathematical blueprint and research context into a high-quality, benchmarked Python implementation.\", \"expected_artifacts\": [\"A formal document (LaTeX/Markdown) detailing the mathematical derivation of the ISTA and dictionary update rules.\", \"A Python script (`online_dl.py`) containing the full implementation of the algorithm and benchmarking harness.\", \"A list of at least two cited arXiv pre-prints that informed the implementation or benchmark design.\", \"A JSON file (`metrics.json`) reporting the final dictionary recovery rate and reconstruction MSE.\", \"A plot (`convergence.png`) showing the reconstruction error as a function of iterations.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem for sparse coding and online dictionary learning, defining the roles of the data fidelity term and the L1-regularization term.\", \"2. Derive the iterative update rules for the sparse coefficients (ISTA) and the dictionary atoms using a proximal gradient descent approach on the objective function's Lagrangian.\", \"3. Implement the derived algorithms in Python, including a synthetic data generation process with a known ground-truth dictionary and sparse codes.\", \"4. Design and execute experiments to measure reconstruction error (MSE) and the algorithm's ability to recover the ground-truth dictionary atoms (e.g., via cosine similarity).\", \"5. Verify that the implemented update steps in the code numerically correspond to the derived mathematical expressions by testing with simple, known inputs.\", \"6. Analyze and plot the convergence curves and validate that the empirical performance aligns with theoretical expectations or benchmarks found in the literature.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: code implementation must match mathematical derivations for ISTA and dictionary updates; fallback: if verification fails, re-route to /code with specific inconsistencies highlighted for debugging).\", \"todo_list\": [\"- [ ] /math: Derive the update equations for ISTA (sparse codes) and the dictionary atoms.\", \"- [ ] /math: Formulate the complete online dictionary learning objective function and its Lagrangian.\", \"- [ ] /general-search: Find 2+ papers on online dictionary learning benchmarks and implementation strategies.\", \"- [ ] /code: Implement the ISTA solver and the dictionary update logic in Python using NumPy.\", \"- [ ] /code: Create a synthetic data generator with a known sparse signal model.\", \"- [ ] /code: Run experiments, plot convergence, and log primary/secondary metrics to a JSON file.\", \"- [ ] /math: Verify that the Python implementation of the update rules is numerically consistent with the mathematical derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"convex-optimization\"], \"acceptance_criteria\": [\"The Python code must execute without errors and generate all specified artifacts (plot, metrics JSON).\", \"The final report must contain a complete mathematical derivation of the ISTA and dictionary update rules, which must be correctly reflected in the code's logic.\", \"The dictionary recovery rate must exceed 95% on synthetic data with a signal-to-noise ratio > 30 dB.\", \"The report must cite at least two relevant academic papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3240, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive a formal connection between the information bottleneck principle and the adversarial robustness of diffusion models. Specifically, can you formulate a proof using rate-distortion theory to establish a lower bound on the L_p norm of the minimal adversarial perturbation required to shift the model's output distribution by a given Kullback-Leibler divergence? The proof should leverage concepts from algebraic topology to characterize the decision boundaries in the latent space.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'information bottleneck' 'rate-distortion theory' adversarial robustness generative models algebraic topology\\\", mode=web)\", \"/math(Formulate the optimization problem and derive a proof for the lower bound on adversarial perturbation L2 norm using rate-distortion theory from arxiv sources. The proof must explicitly define the latent manifold's topology and state convergence conditions.)\", \"/code(Implement a variational autoencoder in JAX to empirically test the derived bound. The implementation must track FID and KL divergence metrics for adversarial attacks and include logic for enforcing convergence constraints on the encoder., using Python)\", \"/math(Verify the empirical VAE results against the derived theoretical bound. Analyze any deviation by performing a spectral analysis of the learned metric tensor, specifically focusing on its eigenvalue decay on the latent manifold.)\"], \"route_rationale\": \"The 'blue' /general-search agent is used first to gather state-of-the-art literature from arXiv. The first /math call is for formal theoretical derivation based on this literature. /code is then used to create a computational experiment to validate the theory empirically. A final /math call verifies the code's output against the formal proof, providing a rigorous check on the entire reasoning chain.\", \"expected_artifacts\": [\"- A formal, self-contained mathematical proof in a PDF document, detailing the derivation of the adversarial perturbation bound.\", \"- A commented JAX/Flax script implementing the variational autoencoder and the validation experiment.\", \"- A JSON file containing the primary and secondary metrics from the computational experiment.\", \"- A verification report comparing the theoretical bound with the empirical results, including an analysis of any discrepancies and citing at least two relevant arXiv papers.\", \"- A list of all cited works with full bibliographic information and arXiv IDs.\"], \"thinking_outline\": [\"1. Formalize the problem by defining adversarial robustness for generative models using the language of information theory, specifically rate-distortion and KL divergence.\", \"2. Derive the theoretical lower bound on the perturbation norm by applying the information bottleneck principle to a compressed latent representation, using principles from the sourced literature.\", \"3. Validate the logical soundness of the mathematical proof, ensuring all assumptions about the data manifold and model architecture are explicitly stated and justified.\", \"4. Construct a computational experiment using a VAE to generate empirical data on the relationship between perturbation norm and output distribution shift.\", \"5. Verify the computational results by comparing the measured minimum perturbation norm against the derived theoretical bound, quantifying the gap.\", \"6. Analyze any significant gap between theory and practice, hypothesizing causes such as optimization error, architectural limitations, or simplifying assumptions in the proof.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The initial /math agent uses papers from /general-search to construct the proof. The /code agent implements a model based on the math derivation. The final /math agent receives the empirical results from /code and the proof from the first /math agent to perform verification. Fallback: If verification reveals a >15% gap, the task is routed back to the first /math agent to refine the theoretical model's assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv connecting information bottleneck theory to adversarial machine learning.\", \"- [ ] /math: Produce the formal proof for the adversarial perturbation lower bound, clearly annotating where concepts from the literature are applied.\", \"- [ ] /code: Implement the VAE model and the adversarial attack procedure (e.g., PGD) for empirical testing.\", \"- [ ] /code: Instrument the code to log all specified metrics (gap, KL divergence, FID) to a structured JSON file.\", \"- [ ] /math: Verify that the assumptions made in the proof (e.g., Gaussian latent space) are correctly implemented in the VAE code.\", \"- [ ] /math: Execute the final verification step, comparing the theoretical bound from the proof with the empirical data from the code's output.\", \"- [ ] router: Perform a final quality assurance check on all artifacts, ensuring the proof, code, and verification report are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"algebraic-topology\"], \"acceptance_criteria\": [\"- The submitted mathematical proof must be rigorous, logically sound, and all steps must be fully justified.\", \"- The JAX code must be executable and reproduce the metric values reported in the verification artifact to within a 5% tolerance.\", \"- The final verification report must show a theoretical-empirical gap of less than 15% or provide a rigorous mathematical explanation for the discrepancy.\", \"- The report must correctly cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3246, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a scalable Gaussian Process model for a high-dimensional Bayesian optimization task. Please derive the Evidence Lower Bound (ELBO) for a Stochastic Variational Gaussian Process (SVGP) using an Ornstein-Uhlenbeck process as the prior and a Mat\u00e9rn 5/2 kernel. The implementation should leverage inducing points and be written in Python with GPyTorch. Additionally, provide a theoretical analysis of the posterior contraction rates and verify that the empirical convergence of the implementation matches this analysis.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org stochastic variational gaussian process Ornstein-Uhlenbeck posterior contraction rates\\\", mode=both)\", \"/math(Derive the Evidence Lower Bound for the SVGP model with an Ornstein-Uhlenbeck prior, analyzing the kernel's spectral properties and theoretical convergence guarantees for Bayesian posterior.)\", \"/code(Implement the SVGP model in Python using GPyTorch, focusing on the ELBO optimization via minibatching and ensuring numerical stability during Cholesky decomposition for the kernel eigenvalue problem.)\", \"/math(Verify the empirical posterior contraction rate from the code's output against the derived theoretical bounds, checking for statistical consistency and convergence diagnostics.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on SVGP theory. This informs the /math agent's formal derivation of the ELBO. The /code agent then translates this mathematical framework into a verifiable implementation. A final /math step is crucial for cross-validating the code's empirical behavior against the initial theoretical model, ensuring correctness.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full mathematical derivation of the ELBO.\", \"- A Python script (`svgp_bo.py`) containing the complete, commented implementation of the SVGP model and Bayesian optimization loop.\", \"- A performance report (`report.md`) including convergence plots of the ELBO, NLPD, and RMSE metrics on a benchmark function.\", \"- A bibliography file (`citations.bib`) with at least two cited arXiv papers on variational inference for GPs.\", \"- A verification summary (`verification.txt`) comparing empirical vs. theoretical posterior contraction rates.\"], \"thinking_outline\": [\"1. Research modern approaches to variational inference in Gaussian Processes, focusing on methods that offer both scalability and strong theoretical convergence guarantees.\", \"2. Formulate the specific SVGP model, defining the Ornstein-Uhlenbeck prior, the variational family (e.g., Gaussian with inducing points), and formally derive the Evidence Lower Bound (ELBO).\", \"3. Implement the model in Python, paying close attention to the optimization of variational parameters and kernel hyperparameters. Structure the code for a Bayesian optimization task.\", \"4. Design and execute an experiment on a synthetic high-dimensional function (e.g., Hartmann6) to evaluate the model's performance and convergence.\", \"5. Verify that the numerical implementation is stable, specifically auditing the matrix inversion and Cholesky decomposition steps for potential issues with ill-conditioned kernel matrices.\", \"6. Validate the empirical results by comparing the observed posterior contraction rate against the theoretical bounds derived in the mathematical analysis.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The derivation from /math must be complete before /code begins. The experimental results from /code are required for the final verification by /math. Fallback: If verification fails, the task is routed back to /code with specific error analysis from the /math agent.\", \"todo_list\": [\"- [ ] /general-search: Find SOTA papers on SVGP convergence and Ornstein-Uhlenbeck processes.\", \"- [ ] /math: Derive the ELBO for the specified SVGP model.\", \"- [ ] /code: Implement the SVGP model and Bayesian optimization loop in GPyTorch.\", \"- [ ] /code: Generate convergence plots for ELBO and other metrics from a benchmark run.\", \"- [ ] /math: Derive the theoretical posterior contraction rate for the model.\", \"- [ ] /math: Verify the code's empirical convergence against the theoretical derivation.\", \"- [ ] /code: Audit the implementation for numerical stability, especially matrix operations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-processes\", \"bayesian-inference\", \"gaussian-processes\"], \"acceptance_criteria\": [\"- The ELBO derivation must be mathematically sound and complete.\", \"- The Python implementation must successfully run and demonstrate ELBO convergence on a standard benchmark function (e.g., Hartmann6).\", \"- The final report must include a quantitative comparison between theoretical and empirical posterior contraction rates, with discrepancies of no more than 15%.\", \"- The project must cite at least two relevant arXiv pre-prints published within the last 5 years.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3278, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the mathematical formulation for a Variational Quantum Eigensolver (VQE) using a Unitary Coupled Cluster Singles and Doubles (UCCSD) ansatz for the LiH molecule. Detail the parameterization of the quantum circuit, formulate the classical optimization problem for the energy expectation value of the Hamiltonian, and analyze the convergence properties of a gradient-based optimizer. Provide a proof-of-concept simulation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational quantum eigensolver UCCSD ansatz LiH Hamiltonian\\\", mode=both)\", \"/math(Derive the energy expectation value <H> for the UCCSD ansatz applied to the LiH Hamiltonian, focusing on the eigenvalue problem and establishing the Euler-Lagrange equations for the variational parameters from quantum chemistry.)\", \"/code(Implement a Python simulation of the VQE algorithm using Qiskit. Define the LiH Hamiltonian, construct the UCCSD ansatz circuit, and use a gradient-based optimizer to minimize the energy expectation value, ensuring convergence tracking.)\", \"/math(Verify the convergence of the optimization by analyzing the Hessian of the energy landscape near the found minimum and compare the final eigenvalue against established chemistry benchmarks for LiH.)\"], \"route_rationale\": \"The blue /general-search agent will first gather foundational literature on VQE and UCCSD for molecular Hamiltonians. This context is then passed to /math for the rigorous theoretical derivation of the objective function and its gradients. /code implements the derived algorithm in a quantum simulation framework. A final /math step is used to formally verify the numerical results and analyze the convergence properties, ensuring theoretical and practical alignment.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the VQE energy functional and its gradient for the UCCSD ansatz.\", \"A Python script implementing the VQE simulation for the LiH molecule, including the quantum circuit construction and classical optimization loop.\", \"A results summary comparing the computed ground state energy to known values, including a plot of energy vs. optimization iterations.\", \"A list of cited arXiv papers on VQE and quantum chemistry.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation of VQE and the UCCSD ansatz by reviewing relevant literature for molecular systems like LiH.\", \"2. Formulate the molecular Hamiltonian for LiH in the second quantization formalism and map it to a qubit Hamiltonian using the Jordan-Wigner transformation.\", \"3. Derive the analytical expression for the energy expectation value as a function of the ansatz parameters and compute its gradient.\", \"4. Implement the quantum circuit and classical optimization loop, simulating the process to find the minimum energy eigenvalue.\", \"5. Validate the implementation by comparing the final computed energy against established computational chemistry benchmarks for LiH.\", \"6. Verify the correctness of the gradient computation by comparing the analytical gradient with a numerical finite-difference approximation at several points in the parameter space.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA (verification: code output matches theoretical energy bounds and literature values; fallback: if divergence, re-run /code with a different classical optimizer or simplified ansatz).\", \"todo_list\": [\"- [ ] /general-search: Find at least two key papers on VQE for molecular Hamiltonians, focusing on the UCCSD ansatz and LiH.\", \"- [ ] /math: Derive the expectation value of the qubit Hamiltonian for the parameterized UCCSD ansatz.\", \"- [ ] /math: Derive the analytical gradient of the energy expectation value with respect to the circuit parameters.\", \"- [ ] /code: Implement the VQE simulation in Python, including the ansatz circuit, Hamiltonian, and a classical optimizer loop.\", \"- [ ] /code: Add logging to track the energy value and parameter updates at each optimization step.\", \"- [ ] /math: Verify that the implemented gradient in the code matches the analytical derivation via numerical checks.\", \"- [ ] /math: Compare the final converged energy eigenvalue with published values for LiH to validate the overall result.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"variational-algorithms\", \"computational-chemistry\"], \"acceptance_criteria\": [\"- The mathematical derivation of the energy functional and its gradient must be complete and self-contained.\", \"- The Python code must execute without errors and converge to a ground state energy for LiH within 5% of the accepted Full CI value.\", \"- The final report must include a convergence plot and cite at least two relevant arXiv papers.\", \"- The numerical gradient check must show a relative error of less than 1e-5 compared to the analytical gradient.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3282, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the objective function for training a Bayesian Neural Network using amortized variational inference with a Gaussian variational posterior. Detail the reparameterization trick to obtain a low-variance Monte Carlo gradient estimator for the Evidence Lower Bound (ELBO) and implement this in JAX, showing convergence on a regression task and comparing the posterior predictive uncertainty against a bootstrap ensemble baseline.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) for a Bayesian neural network with a Gaussian variational posterior, focusing on the reparameterization trick for a low-variance gradient estimator and analyzing its convergence properties.)\", \"/general-search(query=\\\"site:arxiv.org amortized variational inference bayesian neural network reparameterization trick variance reduction\\\", mode=rag)\", \"/code(Implement the Bayesian neural network in JAX, using the derived ELBO gradient estimator for training. Validate convergence on a synthetic dataset and compare posterior uncertainty against a deep ensemble baseline via bootstrap methods., using Python)\"], \"route_rationale\": \"The plan starts with /math to establish the core theoretical foundation and derive the necessary equations. Then, the blue /general-search agent is invoked to gather context on state-of-the-art variance reduction techniques which can inform the implementation. Finally, /code implements the derived theory and performs empirical validation, a standard full-stack reasoning workflow.\", \"expected_artifacts\": [\"- A LaTeX document detailing the full derivation of the ELBO and its reparameterized gradient.\", \"- A Python script in JAX implementing the BNN, training loop, and evaluation.\", \"- A summary report including plots of ELBO convergence, posterior predictive distributions, and a table comparing BNN vs. ensemble performance metrics.\", \"- A list of at least two cited arXiv papers on variational inference or Bayesian deep learning.\", \"- A verification log confirming the alignment between the mathematical derivation and the code's gradient computation.\"], \"thinking_outline\": [\"1. Define the probabilistic graphical model for a Bayesian Neural Network (BNN), specifying the prior over weights and the likelihood function.\", \"2. Formulate the variational inference problem by introducing an amortized variational posterior (the inference network) and derive the Evidence Lower Bound (ELBO) objective.\", \"3. Derive the low-variance Monte Carlo gradient estimator for the ELBO using the reparameterization trick, clearly separating the stochastic sampling from the parameters requiring gradients.\", \"4. Implement the BNN and the VI training procedure in JAX. Validate the implementation by ensuring the ELBO on the training set is non-decreasing and converges.\", \"5. Verify the correctness of the uncertainty quantification by comparing the BNN's posterior predictive NLL and calibration against a non-Bayesian deep ensemble baseline trained with bootstrap aggregation.\", \"6. Audit the final implementation to confirm that the gradient computation numerically aligns with the analytical derivation from step 3.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: The /code agent's implementation of the ELBO gradient must be numerically consistent with the analytical formula provided by the /math agent. Check for convergence. fallback: If the ELBO diverges or gradients explode, the /math agent must re-examine the derivation for potential errors, and the /general-search agent will be tasked to find alternative stable estimators.)\", \"todo_list\": [\"- [ ] /math: Derive the reparameterized ELBO gradient for a BNN with Gaussian priors and posteriors.\", \"- [ ] /general-search: Find 2+ recent arXiv papers on improving ELBO gradient estimators or practical BNN applications.\", \"- [ ] /code: Implement the BNN model and the amortized VI training loop in JAX.\", \"- [ ] /code: Implement the evaluation harness, including a bootstrap deep ensemble baseline for comparison.\", \"- [ ] /math: Verify that the symbolic gradient derived matches the computational graph implemented in the JAX code.\", \"- [ ] /code: Validate the model's convergence and generate all required plots and metrics for the final report.\", \"- [ ] /router: Perform final QA check, ensuring all acceptance criteria are met and artifacts are generated.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"- The ELBO derivation must be mathematically sound and complete, including all steps for the reparameterization trick.\", \"- The JAX implementation must successfully train, with the ELBO showing clear convergence on a synthetic regression task.\", \"- The final report must show the BNN achieves a test set Negative Log-Likelihood (NLL) comparable to or better than the bootstrap ensemble baseline.\", \"- The solution must cite at least two relevant papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3290, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am working on an IOI-style problem that requires counting Hamiltonian paths in graphs of bounded treewidth, but with an added constraint involving their k-coloring properties. Standard dynamic programming on tree decompositions is insufficient. I hypothesize that a solution involves matrix exponentiation over a specific semiring or a novel combinatorial identity from recent arXiv literature on algebraic graph theory. Can you assist in finding the relevant theory, formalizing the recurrence, proving its complexity, and implementing a high-performance solver?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'counting hamiltonian paths' bounded treewidth 'chromatic number' algebraic complexity OR 'meet-in-the-middle'\\\", mode=web)\", \"/math(Formalize the DP recurrence on the tree decomposition based on literature findings. Analyze the convergence properties of any matrix exponentiation steps and bound the transition matrix eigenvalue spectrum.)\", \"/code(Implement the DP algorithm in C++ using a high-performance template. Profile its runtime against a large test case database and analyze performance using Bayesian bootstrap methods for confidence interval estimation., using Python)\", \"/math(Audit the C++ implementation's logic against the formal mathematical derivation. Verify that base cases and transitions correctly implement the recurrence relations, using Chebyshev's inequality for probabilistic checks on random graph families and re-deriving the computational complexity.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey recent academic literature for the required advanced algorithmic techniques, as requested by the user. The findings are passed to /math to create a formal model and complexity proof. This model is then handed to /code for a high-performance C++ implementation. A final /math step is crucial for formally verifying that the code's logic perfectly matches the derived mathematical theory, ensuring correctness.\", \"expected_artifacts\": [\"A formal mathematical document detailing the dynamic programming recurrence, proof of correctness, and a tight time/space complexity analysis.\", \"Well-commented C++ source code for the solver, including a build script and usage instructions.\", \"A performance report with benchmarks on various graph classes, including runtime and memory usage statistics.\", \"A bibliography with at least two citations to relevant arXiv preprints or peer-reviewed conference papers.\", \"A verification summary from the final /math audit, confirming the implementation's correctness.\"], \"thinking_outline\": [\"1. Conduct a literature search for state-of-the-art algorithms for counting Hamiltonian paths on graphs with bounded treewidth, specifically focusing on techniques that incorporate vertex coloring constraints.\", \"2. Based on the findings, formulate the precise dynamic programming states, transition functions, and base cases over a tree decomposition of the input graph.\", \"3. Formally derive the algorithm's time and space complexity as a function of the number of vertices, treewidth, and other relevant parameters. Prove the correctness of the recurrence relation.\", \"4. Validate the theoretical complexity by comparing it against known lower bounds for #P-complete problems to ensure the approach is likely optimal.\", \"5. Implement the algorithm in a performant language like C++ and develop a comprehensive test suite with diverse graph structures.\", \"6. Verify the implementation's numerical output against a brute-force solver on small graph instances to rigorously confirm correctness.\"], \"handoff_plan\": \"/general-search -> /math (verification: findings are relevant) -> /code (verification: math is implementable) -> /math (verification: code matches math spec) -> router QA (verification: all artifacts are consistent and meet acceptance criteria; fallback: re-run /math or /code with more specific constraints if verification fails).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent papers (post-2018) on algebraic methods for counting paths in graphs with low treewidth.\", \"- [ ] /math: Define the full DP state on tree decomposition bags and provide a rigorous proof for the transition logic.\", \"- [ ] /math: Derive the final time and space complexity, expressing it using Big-O notation in terms of |V|, |E|, and treewidth k.\", \"- [ ] /code: Implement the algorithm in C++17, ensuring it compiles with g++ -O3.\", \"- [ ] /code: Generate a set of 10 test graphs, including edge cases, to validate the solution.\", \"- [ ] /math: Formally verify that the implemented C++ code logic is equivalent to the mathematical recurrence relation.\", \"- [ ] /router: Perform final QA, checking all artifacts against acceptance criteria and ensuring citations are properly formatted.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"dynamic-programming\", \"graph-algorithms\", \"computational-complexity\"], \"acceptance_criteria\": [\"- The provided complexity analysis in the math document must be tight and formally proven.\", \"- The C++ implementation must pass all provided test cases within the specified time limits and produce the correct counts.\", \"- The final report must cite at least two relevant academic papers with accessible links (arXiv ID or DOI).\", \"- The verification report from the final math audit must confirm a 1:1 correspondence between the algorithm's theory and implementation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3356, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a physics-informed neural network (PINN) to solve the 1D nonlinear Schr\u00f6dinger equation (NLSE). The implementation must handle complex-valued wavefunctions and enforce the conservation of physical invariants, specifically the L2-norm (particle number) and Hamiltonian energy, directly within the loss function to improve long-term stability and physical realism. Can you generate the JAX code for this, including the mathematical derivation of the loss components?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org complex-valued PINN Hamiltonian conservation nonlinear schrodinger equation\\\", mode=web)\", \"/math(Derive the complex-valued PDE residual for the NLSE and formulate the Euler-Lagrange equations for the Hamiltonian energy and L2-norm conservation laws to be used as soft constraints in the Bayesian-inspired loss function.)\", \"/code(Implement a JAX-based PINN with a complex-valued neural network architecture. The loss function must incorporate the derived PDE residual, boundary conditions, and Hamiltonian energy conservation terms, with appropriate weighting. Use Fourier features for inputs., using Python)\", \"/math(Verify the numerical stability and accuracy of the trained model by computing the relative error of the conserved quantities across the spatiotemporal domain and comparing the solution's eigenvalue spectrum to theoretical predictions., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art methods for Hamiltonian PINNs. This informs the /math agent's formal derivation of the complex-valued loss terms. The /code agent then implements this system in JAX, which is ideal for complex numbers and automatic differentiation. A final /math step is crucial for numerical verification of the physical conservation laws, ensuring the implementation is correct.\", \"expected_artifacts\": [\"- A LaTeX document with the full mathematical derivation of the NLSE residuals and conservation law constraints.\", \"- A well-documented Python script implementing the complex-valued PINN in JAX.\", \"- A final report (report.md) summarizing the methodology, results, and including plots of the learned solution, error evolution, and conserved quantities over time.\", \"- A JSON file (metrics.json) containing the final PDE residual and conservation law errors.\", \"- A list of at least two cited arXiv papers on related PINN methodologies.\"], \"thinking_outline\": [\"1. Research contemporary approaches for encoding conservation laws and handling complex-valued outputs in Physics-Informed Neural Networks, focusing on Hamiltonian systems.\", \"2. Formally derive the loss function components: the complex-valued PDE residual for the NLSE, and the integral forms of the conserved L2-norm and Hamiltonian energy.\", \"3. Implement the neural network, training loop, and custom loss function in JAX, paying close attention to the handling of complex gradients and numerical integration.\", \"4. Train the model on a defined spatiotemporal grid and monitor the convergence of all loss components.\", \"5. Verify the correctness of the implementation by checking if the physical quantities (L2-norm, energy) remain constant for the trained solution over the entire time domain.\", \"6. Validate the model's accuracy by comparing its output to a known analytical soliton solution of the NLSE, computing the L-infinity error.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: final /math agent checks if code output conserves energy to a tolerance of 1e-5; fallback: if verification fails, re-assign to /code with revised mathematical constraints from /math).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 recent papers on Hamiltonian or conservative PINNs.\", \"- [ ] /math: Produce a clean derivation of the complex NLSE residual and the integral forms of the conserved quantities.\", \"- [ ] /code: Write the JAX implementation of the PINN, including the network, loss, and training loop.\", \"- [ ] /math: Verify that the symbolic derivation of the loss terms from the first /math step correctly maps to the JAX implementation from the /code step.\", \"- [ ] /code: Add logging to track the PDE residual and conservation law errors during training.\", \"- [ ] /math: After training, run a script to compute and plot the conserved quantities over time from the saved model output.\", \"- [ ] /router: Perform final QA check on all artifacts, ensuring acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-neural-networks\", \"partial-differential-equations\", \"scientific-computing\"], \"acceptance_criteria\": [\"- The final mean squared PDE residual must be below 1e-6.\", \"- The relative change in the computed Hamiltonian energy and L2-norm across the time domain must be less than 1e-4.\", \"- The implementation must successfully reproduce a known one-soliton solution with an L2 relative error below 1%.\", \"- The report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3383, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and analyze Black Box Variational Inference (BBVI) for a Latent Dirichlet Allocation (LDA) model. However, instead of the standard conjugate Dirichlet prior over topics, I want to use a non-conjugate Log-Normal prior to induce a different sparsity structure. Please provide the full mathematical derivation for the ELBO and its gradients using the reparameterization trick, a PyTorch implementation demonstrating convergence on a text corpus, and a formal verification of the gradient computation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org black box variational inference non-conjugate priors reparameterization trick lda\\\", mode=rag)\", \"/math(Derive the Evidence Lower Bound (ELBO) for the Bayesian Latent Dirichlet Allocation model with a non-conjugate Log-Normal prior, ensuring the derivation details the variational family and the complexity of the Monte Carlo estimator for the expectation term.)\", \"/code(Implement the stochastic variational inference loop in PyTorch for the specified Bayesian model. Validate ELBO convergence on a biomedical text corpus and track topic coherence metrics using the `gensim` library for evaluation., using Python)\", \"/math(Formally verify the correctness of the analytical gradient derivation by comparing it against numerical estimates from the PyTorch implementation, establishing a tight confidence interval for the Monte Carlo gradient error.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather state-of-the-art literature on non-conjugate variational inference, which is essential for this advanced task. This research informs the /math agent's derivation of the model-specific ELBO and its gradients. The /code agent then translates these mathematical formulas into a working PyTorch implementation. A final /math verification step is crucial to ensure the theoretical derivation and the practical implementation are consistent, a common failure point in complex probabilistic models.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF containing the complete mathematical derivation of the model, ELBO, and the reparameterization-based gradient estimator.\", \"- A Python script (`bbvi_lda.py`) implementing the model, variational distribution, and SVI optimization loop.\", \"- A final report in Markdown (`report.md`) presenting plots of the ELBO convergence, topic coherence scores, and a brief analysis of the results.\", \"- A list of at least two cited arXiv papers with IDs that informed the methodology.\", \"- A standalone Python script (`verify_gradients.py`) that numerically validates the gradient computation.\"], \"thinking_outline\": [\"1. Formulate the complete probabilistic graphical model for Latent Dirichlet Allocation (LDA) using a Log-Normal prior for the topics, clearly defining all random variables and parameters.\", \"2. Define a suitable mean-field variational family (e.g., fully-factorized Gaussian) for the latent variables and derive the Evidence Lower Bound (ELBO) by applying Jensen's inequality.\", \"3. Derive the gradient of the ELBO with respect to the variational parameters, using the reparameterization trick to handle the expectation over the non-conjugate parts of the model.\", \"4. Implement the model and the stochastic variational inference (SVI) training loop in PyTorch, ensuring efficient batching and Monte Carlo estimation of the gradients.\", \"5. Validate the analytical gradient implementation by comparing its output to numerical differentiation results (e.g., using finite differences or PyTorch's built-in `gradcheck` utility) on a small-scale problem.\", \"6. Verify the overall model's performance by training it on a standard text dataset, confirming that the ELBO converges and the learned topics are coherent and interpretable.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses these to produce the core derivations -> /code implements the derivations and runs experiments -> /math receives the code and derivation to perform a final verification of the gradient logic -> router QA (verification: all artifacts are consistent and acceptance criteria are met; fallback: if gradient verification fails, return to /code to debug the gradient estimator implementation).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key papers on Black Box Variational Inference and its application to non-conjugate models.\", \"- [ ] /math: Derive the ELBO for the LDA model with a Log-Normal prior.\", \"- [ ] /math: Derive the reparameterization gradient estimator for all variational parameters.\", \"- [ ] /code: Implement the PGM, the variational family, and the SVI training loop in PyTorch.\", \"- [ ] /code: Execute the training on a suitable dataset and generate plots for the ELBO and topic coherence metrics.\", \"- [ ] /math: Create a formal proof or a computational script to verify that the derived gradients match numerical estimates.\", \"- [ ] /code: Package all code, plots, and the final report into the specified file structure.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-inference\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO and its reparameterization gradient is complete, mathematically sound, and aligns with the provided literature.\", \"- The PyTorch implementation runs without error and demonstrates monotonic convergence of the ELBO on a test dataset (e.g., 20 Newsgroups).\", \"- The gradient verification script passes, confirming that the analytical gradient implementation is correct within a small tolerance (e.g., 1e-4).\", \"- The final report includes citations for at least two relevant arXiv papers with their corresponding IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3398, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a graph neural network for a molecular dynamics simulation that explicitly conserves energy. Can you formulate the model using Hamiltonian mechanics, derive the symplectic integration scheme required for long-term stability, and implement it using a recent equivariant GNN architecture like DimeNet++? The goal is to simulate a small organic molecule and demonstrate superior energy conservation over a baseline non-Hamiltonian GNN with a standard Runge-Kutta integrator.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hamiltonian graph neural networks molecular dynamics symplectic integrator equivariance\\\", mode=rag)\", \"/math(Derive the continuous-time Hamiltonian equations of motion for a molecular system. Discretize using a Velocity Verlet integrator, ensuring the derived force field is the negative gradient of the GNN's predicted potential energy, a key concept in chemistry.)\", \"/code(Implement an equivariant GNN in PyTorch to model the Hamiltonian potential energy. Integrate the derived symplectic update rule for a simulation of a small molecule, tracking total energy for subsequent bioinformatics analysis and ensuring CUDA compatibility., using Python)\", \"/math(Analyze the long-term energy drift from the simulation output. Compute the mean absolute deviation from the initial energy and validate that its growth rate is bounded, contrasting with a non-symplectic Euler integrator's linear drift., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with literature review via the blue /general-search agent to ground the approach in current SOTA. /math is then used to formally derive the physical model and integration scheme. /code implements this specification. A final /math call validates the simulation output, confirming that the implementation correctly exhibits the energy conservation properties derived earlier.\", \"expected_artifacts\": [\"- A LaTeX document with the full derivation of the Hamiltonian equations of motion and the symplectic integration scheme.\", \"- A Python script implementing the GNN and the molecular dynamics simulation.\", \"- A PNG plot comparing the total energy drift over time for the symplectic vs. a baseline integrator.\", \"- A JSON file containing the final metrics, including mean energy drift and computational throughput.\", \"- A summary report (.md) including a list of cited arXiv papers with their IDs.\"], \"thinking_outline\": [\"1. Review recent literature on Hamiltonian and equivariant Graph Neural Networks for molecular simulations to select an appropriate architecture and integration method.\", \"2. Formulate the system's Hamiltonian, separating kinetic and potential energy terms, and derive the corresponding equations of motion (Hamilton's equations).\", \"3. Implement the GNN to learn the potential energy function and integrate it with a symplectic algorithm like Velocity Verlet to propagate the system state.\", \"4. Verify that the forces implemented in the code are the exact negative gradient of the potential energy predicted by the GNN, as required by the derivation.\", \"5. Run a long-term simulation and collect trajectory and energy data.\", \"6. Validate the model's performance by quantifying the energy drift and comparing it against a non-conservative baseline integrator to confirm stability.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides recent papers to the math agent for derivation. The math agent's final equations serve as the formal specification for the code agent. The code agent produces simulation data logs, which are passed to the final math agent for analysis and plotting. (Verification: Final analysis must confirm energy drift is bounded as predicted by theory. Fallback: If drift is high, loop back to /code to check for implementation errors in the integrator or force calculation).\", \"todo_list\": [\"- [ ] /general-search: Find >=2 recent arXiv papers on Hamiltonian GNNs for MD.\", \"- [ ] /math: Derive the Hamiltonian equations and the discrete Velocity Verlet integration steps.\", \"- [ ] /code: Implement the equivariant GNN potential model and the MD simulation loop.\", \"- [ ] /code: Add instrumentation to log total system energy at each timestep.\", \"- [ ] /math: Verify the code's force calculation by comparing its output with a numerically differentiated potential from the GNN for a test case.\", \"- [ ] /math: Post-process the simulation logs to compute and plot the energy drift metrics.\", \"- [ ] router: Perform final QA review, checking consistency between derivation, code, and results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"graph-neural-networks\", \"molecular-dynamics\", \"hamiltonian-mechanics\"], \"acceptance_criteria\": [\"- The mathematical derivation of the symplectic integrator from the continuous Hamiltonian must be complete and correct.\", \"- The Python code must successfully run a simulation for at least 10,000 steps without crashing.\", \"- The primary metric, Mean Energy Drift, for the implemented model must be at least one order of magnitude lower than that of a baseline non-symplectic Euler integrator.\", \"- The final report must cite at least two relevant arXiv papers with IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate the absolute difference between the total energy at timestep `t` and the initial energy at `t=0`, then average this value over all timesteps in the simulation.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the wall-clock time required to run the simulation and divide the total number of steps by this time.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3432, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Karush-Kuhn-Tucker (KKT) conditions for a neural network weight regularization problem where the objective is to minimize cross-entropy loss subject to an L2 norm constraint on the weights and a non-negativity constraint on a specific layer's activations. Analyze the resulting Euler-Lagrange equations.\n", "completion": "{\"route_plan\": [\"/math(Formulate the Lagrangian for the specified cross-entropy loss minimization problem, incorporating both the L2 weight norm inequality constraint and the activation non-negativity constraint. Derive the complete Karush-Kuhn-Tucker (KKT) conditions, including primal feasibility, dual feasibility, and complementary slackness, ensuring rigorous treatment of the Euler-Lagrange equations.)\", \"/general-search(query=\\\"site:arxiv.org lagrange multipliers neural network regularization KKT conditions\\\", mode=web)\", \"/code(Implement a Python script using PyTorch to numerically verify the derived KKT conditions on a toy model. The script should use a gradient-based optimizer to find a solution and then check if the stationarity and complementary slackness conditions hold within a tolerance of 1e-4, reporting the convergence behavior.)\"], \"route_rationale\": \"/math is required to perform the core theoretical derivation of the KKT conditions. The blue /general-search agent will then find authoritative literature on arXiv to ground the theoretical work in established research. Finally, /code is necessary to create a numerical simulation in Python that validates the analytical results from /math.\", \"expected_artifacts\": [\"* A formal mathematical proof detailing the Lagrangian formulation and the derivation of all KKT conditions, delivered as a LaTeX-formatted PDF.\", \"* A Python script (Jupyter Notebook or .py) that implements the numerical verification on a toy problem.\", \"* A summary report in Markdown (.md) explaining the results, including a table showing the validation of KKT conditions and citing at least two relevant research papers with arXiv IDs.\", \"* A JSON file containing the final computed values of the Lagrange multipliers and the validation tolerance checks.\"], \"thinking_outline\": [\"1. Define the primal optimization problem: identify the objective function (cross-entropy loss) and the constraint functions (L2 norm and non-negativity).\", \"2. Construct the Lagrangian function by introducing Lagrange multipliers for each constraint.\", \"3. Derive the Karush-Kuhn-Tucker (KKT) conditions: stationarity (gradient of Lagrangian w.r.t. primal variables is zero), primal feasibility, dual feasibility, and complementary slackness.\", \"4. Implement a numerical solver to find an approximate solution for a small-scale version of the problem.\", \"5. Verify that the numerical solution satisfies the derived KKT conditions within a predefined numerical tolerance, checking for consistency between the analytical and computational results.\"], \"handoff_plan\": \"/math derives the KKT conditions and hands off the formal equations -> /general-search finds supporting literature and passes relevant papers and techniques -> /code implements a numerical test based on the equations from /math and insights from /general-search, producing validation metrics -> router QA (verification: check if the code's output satisfies the KKT conditions derived by /math within the specified tolerance; fallback: if verification fails, re-route to /math to check for errors in the derivation).\", \"todo_list\": [\"- [ ] /math: Derive the KKT conditions for the specified constrained optimization problem.\", \"- [ ] /general-search: Find two or more peer-reviewed articles or preprints on applying KKT conditions to neural network regularization.\", \"- [ ] /code: Develop a Python script to set up a toy problem and numerically solve it.\", \"- [ ] /code: Implement functions within the script to explicitly validate each of the KKT conditions on the obtained solution.\", \"- [ ] /router: Review all generated artifacts for mathematical correctness, code functionality, and adherence to the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"constrained-optimization\", \"lagrange-multipliers\", \"deep-learning-regularization\"], \"acceptance_criteria\": [\"* The mathematical derivation of the KKT conditions must be complete and formally correct.\", \"* The Python code must execute without errors and produce numerical results that satisfy the KKT stationarity and complementary slackness conditions to within a tolerance of 1e-4.\", \"* The final report must clearly connect the theoretical derivation to the numerical results and include at least two citations to relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the L2 norm of the gradient of the Lagrangian with respect to the primal variables at the solution. A value < 1e-4 is required for acceptance.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"For each inequality constraint, compute the product of the Lagrange multiplier and the constraint function value. The maximum absolute value across all constraints should be < 1e-4.\"}}", "categories": ["math_first", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3478, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive comparison of the K-SVD and Method of Optimal Directions (MOD) dictionary learning algorithms for sparse image representation. Your analysis should include a formal mathematical derivation of the update rules for both, an investigation of their convergence properties, and a Python implementation to empirically evaluate their performance on an image inpainting task under a compressed sensing paradigm. The evaluation must quantify reconstruction accuracy using PSNR and SSIM, and analyze the resulting dictionary coherence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org sparse coding 'K-SVD' vs 'Method of Optimal Directions' MOD convergence analysis for image inpainting\\\", mode=both)\", \"/math(Derive the Lagrangian for the dictionary learning objective and detail the alternating optimization steps for K-SVD and MOD, analyzing the convergence criteria and potential reliance on matrix eigenvalue properties.)\", \"/code(Implement K-SVD and MOD in Python with JAX for potential CUDA acceleration; include Orthogonal Matching Pursuit and an evaluation harness for image inpainting that uses a 2D Fourier basis for initial dictionary representation.)\", \"/math(Formally verify the implementation's output by checking the non-increasing nature of the objective function and establishing a confidence interval for the reconstruction error based on Chebyshev's inequality for a fixed number of iterations.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative literature on K-SVD and MOD. This theoretical grounding is passed to /math for a rigorous derivation of the algorithms. /code then translates these mathematical formulas into an efficient, verifiable implementation. A final /math step is used to cross-validate the empirical results from the code against theoretical expectations, ensuring correctness before final reporting.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF detailing the mathematical derivation of K-SVD and MOD update rules and their convergence properties.\", \"A well-documented Python script implementing both algorithms and the image inpainting evaluation framework.\", \"A JSON file containing final performance metrics: PSNR, SSIM, average sparsity, and computation time for each algorithm.\", \"A summary report including plots of reconstruction error vs. iteration, visualizations of learned dictionaries, and example image reconstructions, with citations for at least two academic papers.\"], \"thinking_outline\": [\"1. Formulate the dictionary learning problem as the minimization of a reconstruction error term subject to an L0-norm sparsity constraint on the coefficients.\", \"2. Derive the two-stage, alternating optimization procedure for both MOD (updating the entire dictionary at once via pseudo-inverse) and K-SVD (updating dictionary atoms sequentially via SVD).\", \"3. Implement the sparse coding stage using Orthogonal Matching Pursuit (OMP) and the distinct dictionary update rules for MOD and K-SVD.\", \"4. Validate the OMP implementation on a known dictionary and signal to ensure it correctly recovers sparse coefficients up to the target sparsity level.\", \"5. Design and run the image inpainting experiment by applying random masks to images, training dictionaries on the remaining patches, and reconstructing the full images.\", \"6. Verify the correctness of the overall implementation by plotting the objective function's value over iterations and confirming it is monotonically non-increasing for both algorithms.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: code's empirical convergence plot must align with the properties established in the math derivation; fallback: if the objective function increases, the /math agent must re-audit the derivation before the /code agent debugs the update-step implementation)\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on K-SVD and MOD algorithms, focusing on convergence proofs and image inpainting applications.\", \"- [ ] /math: Derive the complete update rules for K-SVD and MOD, starting from the penalized optimization objective.\", \"- [ ] /code: Implement the K-SVD and MOD algorithms, including an OMP subroutine for the sparse coding stage.\", \"- [ ] /code: Develop the experimental setup for image inpainting, including data loading, patch extraction, and masking.\", \"- [ ] /math: Verify that the implemented code's objective function is monotonically decreasing across iterations.\", \"- [ ] /code: Generate plots comparing PSNR vs. iterations, visualize learned dictionaries, and show side-by-side original vs. reconstructed images.\", \"- [ ] /router: Perform final QA, ensuring the empirical results in the report are consistent with the mathematical derivations and cited literature.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The submitted code must implement both K-SVD and MOD algorithms and run without errors.\", \"- The final report must contain a complete mathematical derivation of both algorithms' update rules, consistent with the code's logic.\", \"- The K-SVD implementation must achieve a PSNR of at least 28 dB on a standard test image (e.g., 'Barbara') with 50% missing pixels.\", \"- The report must cite at least two relevant academic papers, including their arXiv or DOI identifiers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate as `20 * log10(MAX_I) - 10 * log10(MSE)`, where MAX_I is the maximum pixel value (255) and MSE is the mean squared error between the original and reconstructed image pixels.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate as the average number of non-zero coefficients per image patch after the sparse coding step. This should be verified to be equal to or less than the target sparsity K.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3491, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive comparison of the K-SVD and Method of Optimal Directions (MOD) dictionary learning algorithms for sparse signal representation. Your analysis should include a formal mathematical derivation of the update rules for both algorithms, a Python implementation from scratch using NumPy, and an empirical evaluation of their convergence rates and reconstruction accuracy on synthetic Gaussian sparse signals. Please also discuss the theoretical convergence guarantees and how they relate to the observed performance.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:ieeexplore.ie site:arxiv.org K-SVD Method of Optimal Directions dictionary learning convergence analysis\\\", mode=rag)\", \"/math(Derive the update rules for the dictionary D and sparse codes X for both K-SVD (using SVD on error matrix columns) and MOD (using Moore-Penrose pseudoinverse). Frame this as an alternating minimization problem and analyze the conditions for convergence., includes cybersecurity)\", \"/code(Implement K-SVD and MOD algorithms in Python. The implementation must include an Orthogonal Matching Pursuit (OMP) function for the sparse coding step and should be instrumented to track reconstruction error and dictionary atom convergence per iteration.)\", \"/math(Define the experimental protocol: primary metric is reconstruction Signal-to-Noise Ratio (SNR) in dB, secondary is convergence rate (iterations to reach 99% of final SNR). Specify a Bayesian framework for comparing the performance distributions.)\", \"/code(Execute benchmarks on synthetic signals with varying sparsity levels (k). Generate and save plots comparing K-SVD and MOD performance on SNR vs. iteration, and analyze the computational complexity, respecting the defined CPU budget., using Python)\"], \"route_rationale\": \"The plan starts with literature review via the blue /general-search agent to ground the work in authoritative sources. /math is used first for theoretical derivation and later to formalize the experimental design, ensuring rigor. /code is used for implementation and then for executing the designed experiments, separating the core logic from the benchmarking harness. This flow ensures theory precedes practice and that evaluation is well-defined.\", \"expected_artifacts\": [\"A formal mathematical derivation of K-SVD and MOD update rules in a PDF document.\", \"A documented Python script (`dictionary_learning.py`) containing the implementations and benchmarking harness.\", \"A JSON file (`metrics.json`) reporting the final SNR, convergence rates, and compute times for each algorithm and sparsity level.\", \"A summary report (`report.md`) analyzing the results, comparing empirical findings with theoretical convergence guarantees, and including citations.\", \"Generated plots (`snr_vs_iter.png`, `final_snr.png`) comparing the algorithms.\"], \"thinking_outline\": [\"1. Research the foundational papers on K-SVD and the Method of Optimal Directions (MOD) to understand their formulation and theoretical properties.\", \"2. Formally derive the iterative update steps for both the dictionary and the sparse coefficients for each algorithm, clearly stating the objective function being minimized.\", \"3. Implement the derived algorithms in Python, paying close attention to numerical stability and creating a robust testing framework with synthetic data.\", \"4. Validate the implementation by checking if the objective function monotonically decreases over iterations for a simple test case.\", \"5. Execute the comparative benchmark according to the defined experimental protocol, collecting metrics on reconstruction accuracy and convergence speed.\", \"6. Synthesize the theoretical derivations and empirical results into a coherent report, verifying that the observed performance aligns with the convergence guarantees discussed in the literature.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (validation protocol) -> /code (benchmarking) -> router QA. Verification: The math derivation from the first /math step must be cross-referenced during the /code implementation. The metrics from the second /math step must be the exact ones computed in the second /code step. Fallback: If benchmarking results contradict theory, revisit the /math derivation and /code implementation for logical errors.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two seminal papers on K-SVD and MOD.\", \"- [ ] /math: Produce a clean derivation of the update rules for both algorithms.\", \"- [ ] /code: Implement the K-SVD, MOD, and OMP algorithms in a single Python script.\", \"- [ ] /math: Formalize the metrics (SNR, convergence rate) and the structure of the experiment.\", \"- [ ] /code: Execute the benchmark, generate plots, and save the results to a JSON file.\", \"- [ ] /math: Verify that the implementation logic in the Python script correctly reflects the derived update rules.\", \"- [ ] /code: Verify that the generated metrics in `metrics.json` match the definitions from the validation protocol.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"convex-optimization\"], \"acceptance_criteria\": [\"- The Python implementation must run without errors and reproduce the benchmark results given the specified random seed.\", \"- The final report must correctly link the mathematical derivations to the observed performance differences between K-SVD and MOD.\", \"- The measured SNR for both algorithms must improve monotonically and plateau, indicating convergence.\", \"- All generated artifacts must conform to the specified formats in the `io_schema`.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3503, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for the Method of Optimal Directions (MOD) algorithm for dictionary learning under an L1 sparsity constraint. Provide a rigorous proof of convergence for the alternating minimization scheme, analyzing its connection to the mutual coherence of the learned dictionary. Then, implement MOD in Python to reconstruct a sparse signal from incomplete measurements, validating the reconstruction error against theoretical bounds related to the dictionary's properties and its eigenvalue distribution.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org method of optimal directions dictionary learning convergence proof mutual coherence\\\", mode=rag)\", \"/math(Derive the Lagrangian for the Method of Optimal Directions (MOD) sparse coding objective, applying proximal gradient descent to find the update rules for both the dictionary atoms and sparse codes. Analyze the convergence guarantees and connect them to the mutual coherence metric.)\", \"/code(Implement the MOD algorithm in Python using NumPy, simulating compressed sensing recovery for a synthetic sparse signal. Track the reconstruction error and dictionary coherence over iterations, using a Bayesian information criterion for model selection.)\", \"/math(Verify the numerical convergence rate from the Python implementation against the theoretical bounds derived earlier. Analyze the phase transition diagram of reconstruction success versus signal sparsity and measurement rate, considering the impact of the dictionary's eigenvalue distribution.)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather authoritative literature on MOD convergence. This informs the /math agent's formal derivation and proof. The /code agent then implements this verified algorithm for empirical testing. A final /math call is used to perform a higher-level analysis, bridging the gap between the theoretical proof and the numerical simulation results.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full derivation of MOD update rules and the complete convergence proof.\", \"A well-commented Python script (`mod_implementation.py`) implementing the MOD algorithm and the compressed sensing simulation.\", \"A JSON file (`metrics.json`) reporting the final Mean Squared Error and Dictionary Mutual Coherence.\", \"A markdown report (`analysis_report.md`) summarizing the findings, comparing theoretical and empirical results, and including all required citations.\"], \"thinking_outline\": [\"1. Formulate the dictionary learning problem as a penalized optimization problem with an L1-norm sparsity regularizer.\", \"2. Derive the alternating minimization update steps for the dictionary (D) and the sparse codes (X) for the Method of Optimal Directions (MOD) by solving the respective subproblems.\", \"3. Prove the convergence of the MOD algorithm by showing that the objective function is non-increasing at each step and is bounded below, thus guaranteeing convergence to a stationary point.\", \"4. Implement the derived algorithm and a testbed for sparse signal recovery from random projections.\", \"5. Verify that the implementation's empirical convergence rate and reconstruction accuracy align with the theoretical guarantees, particularly in relation to the dictionary's mutual coherence.\", \"6. Audit the final report to ensure the mathematical derivations, code logic, and empirical conclusions are self-consistent and correctly cite the foundational literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides foundational papers (e.g., Elad & Aharon, 2006) to /math. The /math agent provides the formal algorithm derivation to /code for implementation. The /code agent produces numerical results (error plots, coherence values) and hands them back to the second /math agent for theoretical analysis. (verification: Code implementation is checked against the /math derivation; numerical results from /code are checked against theoretical bounds from /math. fallback: If convergence is not observed in the code, revisit the /math derivation for potential errors in the update rules or assumptions).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Method of Optimal Directions (MOD), focusing on convergence proofs and mutual coherence analysis.\", \"- [ ] /math: Derive the update rules for MOD using its Lagrangian formulation and prove its convergence to a stationary point.\", \"- [ ] /code: Implement the MOD algorithm and a compressed sensing recovery simulation in Python.\", \"- [ ] /code: Generate plots of reconstruction error and dictionary coherence across iterations.\", \"- [ ] /math: verify that the implemented update rules in the code exactly match the mathematical derivation.\", \"- [ ] /math: Analyze the simulation results, comparing empirical performance against the theoretical convergence guarantees.\", \"- [ ] /router: Perform final QA check on all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\", \"optimization\"], \"acceptance_criteria\": [\"The mathematical derivation of the MOD update rules is correct and the convergence proof is rigorous and complete.\", \"The Python implementation successfully runs and demonstrates sparse signal reconstruction, with reported metrics (MSE, coherence) being plausible for the task.\", \"The final analysis report correctly links the theoretical bounds with the empirical reconstruction error from the simulation.\", \"All claims are supported by at least 2 citations to peer-reviewed papers or arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate the average squared difference between the original sparse signal and the reconstructed signal after the final iteration.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate max_{i!=j} |<d_i, d_j>| for the learned dictionary D, where d_i are the L2-normalized columns. A lower value indicates a better dictionary for sparse recovery.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3511, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a Physics-Informed Neural Network (PINN) to solve the 2D incompressible Navier-Stokes equations for a lid-driven cavity problem, focusing on high Reynolds numbers where turbulent effects become significant. Please derive the PDE residual loss, implement a JAX model using automatic differentiation, and propose a regularization strategy based on recent literature to handle the stiffness and improve convergence. Compare the PINN's performance against a Fourier Neural Operator (FNO) baseline.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'physics informed neural network' 'Navier-Stokes' regularization turbulent OR 'Fourier Neural Operator' convergence analysis\\\", mode=rag)\", \"/math(Derive the residual form of the 2D incompressible Navier-Stokes equations for a PINN loss function, incorporating both energy conservation constraints and Fourier analysis insights from the literature search.)\", \"/code(Implement a PINN in JAX to solve the lid-driven cavity problem, using automatic differentiation for the PDE residual and CUDA for acceleration. Track convergence and potential spectral bias issues., using Python)\", \"/math(Verify that the trained PINN's velocity field solution is approximately divergence-free by computing the Jacobian's trace and analyzing the error distribution. Check for convergence to a physically plausible solution.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey state-of-the-art regularization techniques for PINNs in fluid dynamics. The /math agent then formalizes the PDE loss based on these findings. The /code agent implements the model and runs the numerical experiment. A final /math agent call is used for post-hoc verification of the physical constraints on the learned solution, ensuring the model output is physically meaningful.\", \"expected_artifacts\": [\"A summary of 2-3 relevant research papers on PINN regularization for fluid dynamics.\", \"A complete LaTeX derivation of the Navier-Stokes residual loss function, including boundary conditions and the regularization term.\", \"A Python script (JAX) implementing both the PINN and a baseline FNO model for the lid-driven cavity problem.\", \"A final report (.md) containing convergence plots, visualizations of the final velocity and pressure fields, and a quantitative comparison of the models based on L2 error and PDE residual.\", \"A metrics.json file containing the final primary and secondary metric values for both models.\"], \"thinking_outline\": [\"1. Research and synthesize current literature on regularization techniques for PINNs applied to the Navier-Stokes equations, particularly for high Reynolds numbers, and foundational papers on Fourier Neural Operators.\", \"2. Formulate the strong and residual forms of the 2D incompressible Navier-Stokes equations (momentum and continuity equations) and derive the composite loss function, including data, boundary, and physics-based residual terms.\", \"3. Implement the PINN architecture in JAX, leveraging automatic differentiation to compute the PDE residuals. Implement a comparative FNO model using established libraries.\", \"4. Train both models on the lid-driven cavity benchmark, carefully monitoring the convergence of the loss components and the relative L2 error against a high-fidelity numerical solution.\", \"5. Verify the numerical stability and convergence of the PINN, checking for issues like spectral bias or gradient pathologies during training.\", \"6. Validate that the final velocity field produced by the PINN is approximately divergence-free, a key physical constraint for incompressible flow, by analyzing the trace of the output's Jacobian.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: router will cross-reference the derived loss in the /math artifact with the implementation in the /code artifact; fallback: if the derivation and implementation mismatch, the task is returned to the /math agent for correction).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent arXiv papers on improving PINN stability or accuracy for high Reynolds number Navier-Stokes problems.\", \"- [ ] /math: Provide a step-by-step derivation of the PINN loss function, clearly defining all terms: momentum residuals, continuity (incompressibility) residual, and boundary condition losses.\", \"- [ ] /code: Implement the PINN model in JAX, ensuring the network architecture and optimizer are clearly defined.\", \"- [ ] /code: Implement the baseline Fourier Neural Operator model and the training loop for both models.\", \"- [ ] /math: Verify that the symbolic partial derivatives in the derivation match the automatic differentiation implementation in the code.\", \"- [ ] /math: Formulate the mathematical procedure for checking the divergence-free condition on the output of the trained neural network.\", \"- [ ] /code: Generate all plots and compute the final metrics specified in acceptance_criteria, saving them to the expected artifact paths.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"fourier-neural-operator\"], \"acceptance_criteria\": [\"The provided JAX code must be executable and reproduce the reported training curves and final metrics.\", \"The final PDE residual (MSE) for the PINN must be below 1e-4 on a grid of test points.\", \"The final report must include a side-by-side visualization of the velocity fields from the PINN, FNO, and a ground-truth solver.\", \"The literature summary must cite at least two papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3517, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and compare two variational inference schemes for a Bayesian neural network. Specifically, derive the coordinate ascent updates for a structured variational posterior (e.g., matrix-variate Gaussian) over a layer's weight matrix, and compare its convergence and resulting ELBO against the standard mean-field Bayesian approximation. The implementation should be in a modern probabilistic programming framework and validated on a standard regression benchmark.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org structured variational inference bayesian neural network matrix-variate posterior\\\", mode=web)\", \"/math(Derive the evidence lower bound (ELBO) and its gradients for a Bayesian neural network with a matrix-variate Gaussian posterior. The derivation must handle the KL-divergence term against a standard normal prior and establish convergence criteria.)\", \"/code(Implement the structured VI algorithm in Python/Pyro, training on a benchmark dataset to optimize the ELBO. The model must track predictive log-likelihood and AUPRC for uncertainty quantification and use a federated learning data loader for a potential extension.)\", \"/math(Verify the correctness of the coded ELBO gradient by implementing a finite-difference checker. The check must confirm that the analytical gradient from the math derivation matches the numerical approximation, analyzing potential convergence issues related to any Chebyshev polynomial approximation used in the sampler.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to find state-of-the-art formulations for structured VI. Then, /math is used for the formal derivation of the objective function and its gradients. /code implements the derived algorithm in a probabilistic programming framework. Finally, /math returns to perform a rigorous verification of the implementation against the derivation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the structured VI updates in a PDF document.\", \"A well-documented Python implementation of the Bayesian neural network with both mean-field and structured VI.\", \"A comparative analysis report in Markdown, including convergence plots (ELBO vs. epoch) and predictive uncertainty visualizations.\", \"A bibliography file with citations to at least two relevant research papers with their arXiv IDs.\", \"The output of the gradient verification script confirming the match between analytical and numerical gradients.\"], \"thinking_outline\": [\"1. Formulate the probabilistic model for a Bayesian Neural Network (BNN) and define the prior distributions over its weights.\", \"2. Derive the Evidence Lower Bound (ELBO) for both a mean-field and a structured (matrix-variate Gaussian) variational posterior over the BNN weights.\", \"3. Implement the BNN and both inference schemes using PyTorch with the Pyro library.\", \"4. Validate the correctness of the ELBO gradient implementation by comparing the analytical gradients derived in step 2 with numerical gradients computed via finite differences.\", \"5. Train both models on a benchmark dataset (e.g., UCI regression) and compare convergence speed, final predictive log-likelihood, and uncertainty estimates between the two VI methods.\", \"6. Audit the results to check that the structured VI approach provides a tighter ELBO and captures posterior correlations more effectively than the mean-field approximation.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (verification) -> router QA. The code agent receives the mathematical derivation from the math agent. The verification math agent receives the code and the original derivation to perform a consistency check. Fallback: If the numerical gradient check fails, the task reverts to the code agent to debug the gradient implementation, flagging the specific weight matrix causing the discrepancy.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize two key papers on structured variational inference for BNNs, focusing on matrix-variate posteriors.\", \"- [ ] /math: Produce a LaTeX document with the full derivation of the ELBO and its gradients for the structured posterior.\", \"- [ ] /code: Implement the BNN and both VI training loops in Pyro.\", \"- [ ] /code: Instrument the code to log ELBO, predictive log-likelihood, and training time per epoch.\", \"- [ ] /math: Implement a gradient checker using finite differences and verify the `d(ELBO)/d(params)` computation from the code agent.\", \"- [ ] /code: Generate comparison plots and a summary metrics table for the final report.\", \"- [ ] /router: Perform final QA, ensuring all acceptance criteria are met and artifacts are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"variational-inference\", \"probabilistic-programming\"], \"acceptance_criteria\": [\"- The provided Python code must run and reproduce the reported metrics and plots.\", \"- The mathematical derivation must be complete, self-contained, and directly correspond to the implemented algorithm.\", \"- The final report must clearly demonstrate that the structured VI method achieves a tighter (or equal) ELBO compared to the mean-field baseline.\", \"- The gradient verification script must pass, showing a difference between numerical and analytical gradients below a tolerance of 1e-4.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3535, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of the quantum error correction threshold for the 2D toric code under a depolarizing noise model. Please derive the theoretical threshold by mapping the problem to a statistical mechanics model, then validate this derivation by implementing a Monte Carlo simulation in Python to numerically estimate the threshold, including a minimum-weight perfect matching decoder.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org toric code error threshold 'depolarizing noise' 'percolation theory' mapping statistical mechanics\\\", mode=both, includes optimality)\", \"/math(Derive the error threshold for the toric code using stabilizer group theory and mapping to the random-bond Ising model, analyzing the complexity and convergence properties.)\", \"/code(Implement a CUDA-accelerated Monte Carlo simulation in Python to find the toric code threshold, using a minimum-weight perfect matching decoder and analyzing the logical error rate with bootstrap confidence intervals.)\", \"/math(Validate the simulation's convergence by comparing the logical error rate scaling against the derived theoretical bounds from arxiv, performing a statistical analysis of the finite-size effects.)\"], \"route_rationale\": \"/general-search (blue agent) is used first to gather foundational literature from arXiv. /math then formulates the theoretical model and derivation. /code implements a simulation to validate the theory numerically. A final /math step verifies the simulation's output against the derived bounds, ensuring a closed-loop reasoning process.\", \"expected_artifacts\": [\"A LaTeX-formatted mathematical derivation of the toric code error threshold via mapping to the random-bond Ising model.\", \"A documented Python script implementing the Monte Carlo simulation with a minimum-weight perfect matching decoder.\", \"A JSON file containing the raw simulation results: logical error rate vs. physical error rate for multiple code distances.\", \"A final report in Markdown summarizing the theory, simulation results, and a comparison, including at least 2 citations to arXiv papers.\", \"A verification log comparing the derived theoretical bounds to the simulation's estimated threshold and its confidence intervals.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation of the toric code using the stabilizer formalism and its connection to statistical mechanics models like the random-bond Ising model.\", \"2. Derive the analytical expression for the error correction threshold by mapping the problem to a phase transition in the corresponding statistical model.\", \"3. Design and implement a Monte Carlo simulation to model the toric code under depolarizing noise, including an efficient decoding algorithm (e.g., minimum-weight perfect matching).\", \"4. Execute the simulation across a range of physical error rates to identify the threshold where the logical error rate drops sharply.\", \"5. Validate the simulation results by comparing the numerically estimated threshold against the derived theoretical value and analyze any discrepancies due to finite-size effects.\", \"6. Verify the implementation's correctness by checking for the expected exponential suppression of logical error rates with increasing code distance below the threshold.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The blue agent provides foundational papers to the math agent for derivation. The math agent's theoretical bounds are passed to the code agent as a target for the simulation. The code agent's simulation output (logical error rates) is handed back to the math agent for verification against the theory. (Verification: Does the simulated threshold fall within the expected theoretical range? Fallback: If simulation diverges, the code agent must debug the decoder or simulation parameters and re-run).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on the toric code threshold, stabilizer formalism, and mapping to statistical mechanics models.\", \"- [ ] /math: Formulate the derivation of the error threshold for the toric code, clearly stating all assumptions about the noise model.\", \"- [ ] /code: Implement the toric code simulation, including syndrome measurement and a MWPM decoder (e.g., using `pymatching`).\", \"- [ ] /code: Run simulations to generate data for logical error rate vs. physical error rate for various code distances (e.g., L=3, 5, 7).\", \"- [ ] /math: Analyze the simulation data to extract the numerical threshold and its confidence interval by finding the crossing point of the logical error rate curves.\", \"- [ ] /math: Verify that the numerical threshold from the code agent aligns with the theoretical derivation from the first math step.\", \"- [ ] /code: Verify that the logical error rate scales correctly with the code distance below the identified threshold.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction\", \"statistical-physics\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly maps the quantum error correction problem to a classical statistical mechanics model and arrives at a theoretical threshold value consistent with literature.\", \"- The Python simulation successfully reproduces the expected phase transition behavior, with the logical error rate decreasing with code size below the threshold.\", \"- The numerically estimated threshold from the simulation agrees with the derived theoretical value within a statistically justified margin of error (e.g., within 5%).\", \"- The final report includes at least two citations to relevant arXiv preprints or peer-reviewed publications on the topic.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3548, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the stabilizer generators for the [[5, 1, 3]] quantum error correcting code, analyze its distance by examining logical operators, and then implement a Python simulation using Qiskit to calculate the logical error rate under a depolarizing noise model. I need to see the convergence behavior as the physical error rate approaches the fault-tolerance threshold and reference key papers on the topic.\n", "completion": "{\"route_plan\": [\"/math(Derive the stabilizer generators for the 5-qubit perfect code, explicitly showing the commutation relations and checking the +1 eigenvalue subspace. Analyze the code's complexity by determining its distance d and encoding rate k/n.)\", \"/code(Implement a Python simulation of the 5-qubit code using Qiskit under a depolarizing noise channel. Calculate the logical error rate versus the physical error rate and analyze the convergence towards the fault-tolerance threshold using a Bayesian-inspired fitting model.)\", \"/general-search(query=\\\"site:arxiv.org 5-qubit perfect code fault-tolerant threshold depolarizing noise stabilizer formalism\\\", mode=web, includes optimality)\"], \"route_rationale\": \"The plan follows a theory-to-practice workflow. /math is used first to establish the theoretical foundations of the 5-qubit code. /code then implements a simulation based on this theory to validate its performance characteristics. Finally, the blue /general-search agent is tasked with gathering authoritative academic references to contextualize and support the findings.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the 5-qubit code's stabilizers, logical operators, and distance.\", \"- Python source code (using Qiskit) for simulating the code's logical error rate under a depolarizing noise channel.\", \"- A generated plot showing the logical error rate versus the physical error rate, clearly indicating the pseudo-threshold.\", \"- A summary report in Markdown that integrates the derivation, simulation results, plot, and citations of at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Define the 5-qubit perfect code using the stabilizer formalism, specifying the four commuting stabilizer generators.\", \"2. Mathematically derive the properties of the stabilizer group, including checking that all generators square to the identity and have a +1 eigenvalue.\", \"3. Determine the code's distance by constructing the logical Pauli operators (X_L, Z_L) and finding the minimum weight of a non-trivial logical operator.\", \"4. Implement the encoding, error-syndrome measurement, and recovery operations in a Python simulation under a circuit-level depolarizing noise model.\", \"5. Validate the simulation results by checking if the logical error rate curve crosses the physical error rate (the y=x line) at a plausible threshold value consistent with literature.\", \"6. Audit the cited papers to confirm their relevance to the 5-qubit code and fault-tolerance thresholds, synthesizing findings into the final report.\"], \"handoff_plan\": \"/math (derivation) -> /code (simulation). The code agent must implement the specific stabilizer generators and logical operators derived by the math agent. /code (results) -> /general-search (literature review). The search agent uses keywords from the simulation to find relevant context. All artifacts -> router QA (verification: ensure logical error rate plot is consistent with derivation and cited literature; fallback: if simulation fails to show error correction, re-run /math to check logical operator derivation).\", \"todo_list\": [\"- [ ] /math: Derive the stabilizer generators for the [[5,1,3]] code and its logical Pauli operators.\", \"- [ ] /math: Prove the code distance is 3 by analyzing the minimum weight of non-trivial logical operators.\", \"- [ ] /code: Implement the Qiskit simulation for the 5-qubit code, including encoding, depolarizing noise, and syndrome-based recovery.\", \"- [ ] /code: Generate data and plot the logical error rate (P_L) vs. physical error rate (p) for a range of p values.\", \"- [ ] /math: Verify that the implemented logical operators in the code correspond directly to the derived mathematical formalism.\", \"- [ ] /code: Validate that the simulation shows P_L < p for p below the pseudo-threshold, confirming error correction behavior.\", \"- [ ] /general-search: Find and summarize at least two seminal papers on the 5-qubit code or fault-tolerance thresholds.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"Quantum Error Correction\", \"Fault-Tolerant Computation\", \"Stabilizer Codes\"], \"acceptance_criteria\": [\"- The mathematical derivation of the stabilizer group and code distance (d=3) must be correct and complete.\", \"- The Python simulation must correctly reproduce the characteristic threshold behavior, where the logical error rate is less than the physical error rate for small p.\", \"- The final report must contain a plot of logical vs. physical error rate and cite at least two relevant arXiv papers with their IDs.\", \"- The calculated Fault-Tolerance Pseudo-Threshold must be within a reasonable range reported in the literature for this code and noise model.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3552, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the computational complexity of octal games. Specifically, I need a formal proof demonstrating that determining the nim-value for a position in the octal game with code 0.156 (Dawson's Kay) is PSPACE-complete. Please also provide a Python implementation of an algorithm to compute these nim-values and analyze its practical time and space complexity against the theoretical bounds, referencing the Sprague-Grundy theorem throughout.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:cs.cmu.edu 'octal games' 'Sprague-Grundy' 'PSPACE-complete' Dawson's Kay complexity reduction\\\", mode=rag)\", \"/math(Formalize the PSPACE-completeness proof for octal game 0.156 by constructing a reduction from a known PSPACE-complete problem like QBF, analyzing the game's state-space complexity and adversarial nature.)\", \"/code(Implement a Python algorithm with memoization to compute nim-values for Dawson's Kay, including runtime validation checks and instrumentation for measuring time and space usage for varying input sizes.)\", \"/math(Verify the implemented algorithm's correctness by proving it correctly computes the g-function based on the Sprague-Grundy theorem, and formally validate its asymptotic time/space complexity against the empirical results., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on octal games and complexity proofs. This literature informs the /math agent's formal proof construction. The /code agent then implements the theory. A final /math step is crucial for formally verifying the algorithm's correctness and validating its complexity, linking the theoretical and practical components.\", \"expected_artifacts\": [\"A LaTeX document containing the formal proof of PSPACE-completeness, including the reduction from QBF.\", \"A Python script (`dawson_kay_solver.py`) implementing the nim-value computation algorithm with memoization.\", \"A verification report in Markdown (`verification_report.md`) that proves the algorithm's correctness and compares its theoretical vs. empirical complexity.\", \"A JSON file (`performance_metrics.json`) with benchmark results (input size, time, space).\", \"A bibliography file (`references.bib`) with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Research the state-of-the-art on the complexity of impartial games, focusing on octal games and the Sprague-Grundy theorem to identify common proof techniques for PSPACE-completeness.\", \"2. Formulate a reduction from a canonical PSPACE-complete problem (e.g., Quantified Boolean Formulas) to the problem of determining the winner in Dawson's Kay.\", \"3. Implement an efficient, memoized recursive algorithm to compute the Grundy numbers (nim-values) for positions in the game.\", \"4. Validate the implementation's correctness by cross-referencing computed values for small N with known sequences and checking against the game rules.\", \"5. Empirically measure the algorithm's performance and validate that its time and space usage scale as predicted by the theoretical complexity analysis.\", \"6. Synthesize all findings into a cohesive report, including the formal proof, commented code, and performance analysis.\"], \"handoff_plan\": \"/general-search -> /math (proof) -> /code -> /math (verification) -> router QA. The initial /math agent receives literature from search to build the proof. The /code agent receives the game rules and recurrence relation from the proof. The second /math agent receives the Python code and empirical data for formal verification. Verification: The final /math step must confirm the algorithm correctly implements the recurrence derived in the first /math step. Fallback: If verification fails, the task is routed back to /code with specific counterexamples for debugging.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on octal game complexity and PSPACE reductions.\", \"- [ ] /math: Draft the PSPACE-completeness proof, clearly defining the reduction and polynomial-time mapping.\", \"- [ ] /code: Implement the nim-value computation algorithm, ensuring robust memoization and performance tracking.\", \"- [ ] /math: Formally derive the algorithm's time and space complexity from its recursive structure.\", \"- [ ] /code: Generate empirical performance data for a range of input sizes.\", \"- [ ] /math: Verify that the implemented code's logic correctly mirrors the Sprague-Grundy recurrence relation from the proof.\", \"- [ ] /math: Compare the theoretical complexity bounds with the empirical data, explaining any discrepancies.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"combinatorial-game-theory\", \"algorithmic-complexity\", \"sprague-grundy-theorem\"], \"acceptance_criteria\": [\"- The PSPACE-completeness proof must be logically sound, with a clearly defined polynomial-time reduction from a known PSPACE-complete problem.\", \"- The Python code must correctly compute nim-values for Dawson's Kay for at least n=100 and include unit tests for base cases.\", \"- The complexity analysis must show a tight bound, and the empirical results in `performance_metrics.json` must align with the theoretical analysis.\", \"- The report must cite at least two relevant academic sources with their arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3561, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the augmented Lagrangian and develop a distributed consensus ADMM implementation for solving the group LASSO problem, then formally analyze its convergence rate against a centralized solver.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org consensus ADMM group lasso convergence analysis\\\", mode=both)\", \"/math(Derive the ADMM update equations for the group LASSO problem's augmented Lagrangian, ensuring the formulation accounts for distributed consensus constraints and analyzing the problem's convexity.)\", \"/code(Implement the distributed ADMM algorithm in Python/NumPy, along with a centralized baseline solver. Profile performance and track the convergence of the primal residual and objective value.)\", \"/math(Analyze the empirical convergence data, plotting objective value vs. iteration. Compare this against theoretical rates from the literature, possibly using Chebyshev inequality bounds as a reference.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to survey modern ADMM convergence literature for the specific problem class. This informs the formal derivation by /math. Then, /code implements the derived algorithm for empirical validation. Finally, /math returns to formally analyze the empirical results in the context of the initial theoretical findings.\", \"expected_artifacts\": [\"A PDF document with the complete mathematical derivation of the consensus ADMM updates for group LASSO.\", \"A Python script (`.py`) containing the implementation of the distributed ADMM solver and the centralized baseline.\", \"A PNG image plotting the convergence of both solvers (objective value vs. iteration number).\", \"A final summary report in Markdown format, including the plot, analysis, and citations.\", \"A JSON file containing the logged metrics, such as primal residual and objective value per iteration.\"], \"thinking_outline\": [\"1. Research recent literature on consensus ADMM, focusing on formulations and theoretical convergence guarantees for L1-regularized problems like group LASSO.\", \"2. Formulate the group LASSO problem for a distributed setting and derive the augmented Lagrangian. From this, derive the explicit iterative update rules for the primal variables, dual variables (Lagrange multipliers), and the consensus variable.\", \"3. Implement the derived ADMM algorithm in Python. Create a synthetic dataset and also implement a standard centralized solver (e.g., using scikit-learn or CVXPY) to serve as a ground truth baseline.\", \"4. Execute both algorithms on the synthetic dataset. Validate the ADMM implementation by checking that the primal and dual residuals approach zero and the objective function decreases monotonically.\", \"5. Plot the objective function value versus iteration count for both algorithms on a semi-log scale to visually compare their convergence rates. Summarize findings and relate them to the theoretical guarantees identified in step 1.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> router QA (verification: router will cross-reference the implemented algorithm in /code against the derivation from /math and the literature from /general-search; fallback: if convergence is not achieved, /code will be instructed to implement a simpler, standard ADMM variant without the consensus term).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize >=2 key arXiv papers on consensus ADMM for group LASSO, noting their convergence assumptions.\", \"- [ ] /math: Provide a step-by-step derivation of the primal-dual update rules for the ADMM algorithm based on the problem formulation.\", \"- [ ] /code: Implement the derived distributed ADMM algorithm and a baseline centralized solver in Python.\", \"- [ ] /code: Validate the implementation by confirming that primal/dual residuals converge towards zero on a small, well-posed problem instance.\", \"- [ ] /math: Analyze the empirical results from /code, generate the convergence plot, and compare the observed rate to theoretical expectations.\", \"- [ ] /router: Perform final QA, verifying that the derivation, code, and analysis are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"convex-optimization\", \"admm\", \"distributed-computing\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for the ADMM updates must be complete and correct.\", \"- The Python code must execute without errors, demonstrate convergence on a synthetic dataset, and generate the specified comparison plot.\", \"- The final report must clearly explain the methodology, present the results, and cite at least two relevant academic papers with arXiv IDs.\", \"- The empirical convergence rate must be plausibly analyzed and compared to the theoretical findings from the literature search.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3572, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nCharacterize the critical points of the non-convex adversarial objective f(x, y) = x^2 - y^2 + 0.1 * x^4 * sin(y). Derive the analytical conditions for its saddle points, analyze the Hessian eigenvalue structure, and implement a numerical method to locate one.\n", "completion": "{\"route_plan\": [\"/math(Derive the first-order optimality conditions (gradient is zero) for the function f(x, y) = x^2 - y^2 + 0.1 * x^4 * sin(y) to locate critical points. Then, compute the Hessian matrix and its associated eigenvalue properties to classify these points.)\", \"/general-search(query=\\\"site:arxiv.org gradient descent-ascent saddle point optimization convergence\\\", mode=rag)\", \"/code(Implement a gradient descent-ascent (GDA) algorithm in Python with NumPy to find a saddle point of the given function. Visualize the optimization trajectory and verify the convergence to a point where the Hessian has mixed eigenvalues.)\"], \"route_rationale\": \"The plan starts with /math for rigorous symbolic derivation of the problem's theoretical properties (critical points, Hessian). It then consults the blue /general-search agent to find modern numerical approaches for this class of problem. Finally, /code implements and validates a suitable algorithm, grounding the mathematical theory in a concrete, verifiable experiment.\", \"expected_artifacts\": [\"A markdown document with the complete mathematical derivation of the gradient, Hessian, and conditions for saddle points.\", \"A Python script (`saddle_point_finder.py`) implementing the gradient descent-ascent algorithm.\", \"A PNG image (`trajectory.png`) visualizing the function's contours and the algorithm's path to the saddle point.\", \"A summary of findings that includes at least two citations to relevant literature on saddle point optimization.\", \"A JSON file (`results.json`) containing the coordinates of the found saddle point and its corresponding Hessian eigenvalues.\"], \"thinking_outline\": [\"1. Analytically compute the gradient of f(x,y) and set it to zero to find the system of equations defining the critical points.\", \"2. Compute the Hessian matrix of f(x,y) to prepare for second-order analysis.\", \"3. Formulate the conditions for a saddle point based on the signs of the Hessian's eigenvalues (one positive, one negative).\", \"4. Implement the gradient descent-ascent (GDA) numerical optimization algorithm, which is suitable for finding saddle points.\", \"5. Verify that the point found by the GDA algorithm is indeed a saddle point by numerically computing the gradient's norm and the Hessian's eigenvalues at that location and checking them against the analytical criteria.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent provides the analytical target conditions (zero gradient, mixed eigenvalues) to the /code agent. The /general-search agent informs the algorithm choice for /code. The /code agent produces a numerical solution and artifacts. Verification: The router QA will check if the numerical eigenvalues in `results.json` from /code match the saddle point criteria derived by /math. Fallback: If the GDA algorithm in /code fails to converge, it will re-run with a smaller learning rate; if it still fails, the task is flagged for router review.\", \"todo_list\": [\"- [ ] /math: Derive the gradient and Hessian of the objective function and solve for the analytical form of critical points.\", \"- [ ] /general-search: Find and summarize two relevant papers from arXiv on numerical methods for finding saddle points, such as Gradient Descent-Ascent.\", \"- [ ] /code: Implement the GDA algorithm in Python to numerically find a non-trivial saddle point (x, y != 0).\", \"- [ ] /code: Instrument the code to log the gradient norm and Hessian eigenvalues at the final point.\", \"- [ ] /code: Generate a contour plot visualizing the function and the optimization trajectory.\", \"- [ ] /router: Validate that the numerically computed eigenvalues from the /code output confirm a saddle point, as defined by the /math derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-eigenvalues\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly identifies the analytical conditions for critical points.\", \"- The Python code converges to a point where the L2 norm of the gradient is less than 1e-6.\", \"- The Hessian matrix evaluated at the converged point has one strictly positive and one strictly negative eigenvalue.\", \"- The final report includes at least two correctly formatted citations with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"computation_guidance\": {\"gradient_norm_at_solution\": \"Compute the L2 norm of the gradient vector at the algorithm's final iterate.\", \"hessian_eigenvalues\": \"Compute the eigenvalues of the Hessian matrix at the final iterate; their signs determine the nature of the critical point.\"}}}", "categories": ["math_first", "metrics_computation", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3586, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the loss landscape geometry of neural networks under adversarial training. Specifically, I need to analyze the prevalence and nature of saddle points. Please derive the Hessian matrix for a simplified model under a PGD-based adversarial attack, analyze its eigenvalue spectrum to formally characterize the conditions for strict saddles, and then prove that a second-order optimization method like Newton-CG can achieve guaranteed convergence to a local minimum by escaping these saddles.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org strict saddle property non-convex optimization adversarial training hessian analysis\\\", mode=rag)\", \"/math(Derive the Hessian of the adversarial loss function, analyzing its eigenvalue spectrum to formally prove the conditions under which the strict saddle property holds, ensuring the derivation accounts for convergence criteria.)\", \"/code(Implement a numerical simulation in Python with JAX to compute the Hessian at critical points for a toy model, validate the theoretical eigenvalue analysis, and visualize the adversarial loss landscape to locate saddle points.)\", \"/math(Construct a formal proof of convergence for a trust-region or Newton-CG method, leveraging the previously verified strict saddle property and ensuring the proof addresses the negative curvature escape mechanism and eigenvalue bounds.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on the strict saddle property. This informs the theoretical work in /math, which will derive the core properties of the Hessian. Then, /code is used for empirical validation of the mathematical theory, a critical verification step. Finally, /math synthesizes the theoretical and empirical results into a formal convergence proof.\", \"expected_artifacts\": [\"A PDF report containing the full mathematical derivation of the Hessian and the formal proof of convergence for the second-order method.\", \"A Python script (.py) implementing the numerical validation, including Hessian computation and landscape visualization.\", \"A set of PNG images visualizing the loss landscape and the eigenvalue spectrum at identified saddle points.\", \"A summary of results with citations to at least two relevant arXiv papers.\", \"A metrics.json file reporting the mismatch between theoretical and numerical Hessian eigenvalues.\"], \"thinking_outline\": [\"1. Use literature search to establish the formal definition of the strict saddle property and its implications for non-convex optimization in deep learning, particularly within adversarial contexts.\", \"2. Derive the analytical form of the Hessian matrix for the specified adversarial loss function.\", \"3. Analyze the eigenvalues of the Hessian to find conditions (e.g., related to network architecture or perturbation size) that guarantee the strict saddle property, meaning all saddle points have a Hessian with at least one strictly negative eigenvalue.\", \"4. Validate the analytical derivation by numerically computing the Hessian spectrum for a concrete model in code and verifying that the signs of the eigenvalues match the theoretical predictions under the derived conditions.\", \"5. Construct a rigorous proof showing that a second-order optimization algorithm can escape these saddles and converge to a second-order stationary point (a local minimum).\", \"6. Audit the final proof to ensure all assumptions are clearly stated and justified by the preceding analysis and citations.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (validation) -> /math (proof) -> router QA. The initial search results from /general-search provide the theoretical foundation for /math's derivation. The math derivation is then passed to /code, which must implement the derived formulas; verification requires the numerical eigenvalue signs to match the theory. Fallback: If verification fails, /code will flag the discrepancies and hand back to /math for revision. The verified properties are then used by the final /math agent to build the convergence proof, which is reviewed by the router.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two seminal arXiv papers on the strict saddle property and optimization.\", \"- [ ] /math: Provide the step-by-step derivation of the Hessian matrix for the adversarial loss.\", \"- [ ] /math: Analyze the Hessian's eigenvalue spectrum and formally state the conditions for the strict saddle property.\", \"- [ ] /code: Implement a Python script to compute and visualize the Hessian eigenvalues at critical points of a sample function.\", \"- [ ] /math: Verify that the theoretical conditions on the eigenvalues align with the numerical results from the /code agent.\", \"- [ ] /code: Check for numerical stability issues (e.g., ill-conditioning) in the Hessian computation and report.\", \"- [ ] /math: Formulate the final convergence proof, explicitly referencing the verified strict saddle property.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"optimization-theory\"], \"acceptance_criteria\": [\"The mathematical proof of convergence is rigorous, complete, and logically sound.\", \"The Python code successfully runs, and the numerical Hessian eigenvalue analysis corroborates the theoretical findings within a tolerance of 1e-5.\", \"The final report is well-structured, clearly presents all derivations and proofs, and cites at least two relevant arXiv papers with their IDs.\", \"The visualized loss landscape correctly identifies saddle points and regions of negative curvature.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3594, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating methods to regularize the search space in differentiable AutoML. Can you derive the second-order gradient approximation for a DARTS-like objective function that is augmented with a topological complexity penalty based on the persistent homology of the operation graph's clique complex? Please provide the full derivation, a JAX implementation to validate it on a standard vision benchmark, and a proof sketch analyzing its convergence properties under non-convexity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org differentiable architecture search DARTS persistent homology regularization graph neural networks\\\", mode=both)\", \"/math(Derive the AutoML objective's gradient, incorporating the topological penalty from the arXiv literature, and analyze the theoretical convergence rate under specific Bayesian assumptions.)\", \"/code(Implement the differentiable AutoML search loop in JAX, tracking validation AUPRC and ensuring the persistent homology computation is efficient enough for the given compute budget., using Python)\", \"/math(Verify the convergence properties of the implemented algorithm by analyzing the Hessian eigenvalue distribution and comparing it to the theoretical model derived earlier.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on differentiable AutoML (DARTS) and topological data analysis. This informs the /math agent's formal derivation of the novel regularized gradient. Next, the /code agent implements this complex mathematical formulation for empirical validation. A final handoff to the /math agent is required to theoretically verify that the empirical results from the code align with the derived convergence properties.\", \"expected_artifacts\": [\"- A LaTeX document containing the full mathematical derivation of the topologically-regularized DARTS gradient.\", \"- A Python script using JAX to implement the search algorithm and evaluate a discovered architecture.\", \"- A plot visualizing the training loss and validation accuracy over search epochs, comparing the regularized method to a baseline.\", \"- A verification report comparing the numerical gradients from the implementation with the analytical derivation for a small test case.\"], \"thinking_outline\": [\"1. Formalize the DARTS bi-level optimization problem and introduce a topological regularization term using persistent homology on the computation graph's adjacency matrix.\", \"2. Derive the second-order Taylor approximation for the inner optimization loop's response to architecture parameters, incorporating the gradient of the topological penalty using the implicit function theorem.\", \"3. Implement the differentiable architecture search pipeline in JAX, using a library like `giotto-tda` for the persistent homology calculations, and ensuring differentiability through implicit differentiation.\", \"4. Conduct experiments on a benchmark dataset (e.g., CIFAR-10) to train the search process and extract a final discrete architecture.\", \"5. Verify that the empirical convergence of the training loss aligns with the theoretical derivation by checking the gradient norms and Hessian-vector products.\", \"6. Audit the final discovered architecture for non-trivial topological features (e.g., fewer redundant paths or cycles) compared to a baseline DARTS model without the regularization.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The initial math derivation from /math must be directly implementable by /code. The final verification by /math uses empirical results (plots, logs) from /code to validate the theory. Fallback: If the derivation from /math is numerically unstable in /code, /math must propose a relaxation or alternative formulation.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers for DARTS (arXiv:1806.09055) and applications of persistent homology to graphs or deep learning.\", \"- [ ] /math: Formulate the regularized bi-level objective and derive the gradient update rule for the architecture parameters.\", \"- [ ] /code: Implement the DARTS training loop with the differentiable topological regularizer using JAX.\", \"- [ ] /code: Run the search experiment on CIFAR-10, logging loss, accuracy, and the value of the topological penalty.\", \"- [ ] /math: Verify the analytical gradient derivation by comparing its output against a numerical finite-difference approximation on a small-scale computational graph.\", \"- [ ] /code: Validate that the discovered cell architecture is well-formed and can be trained to completion.\", \"- [ ] /math: Analyze the convergence plot from /code and formally check if the decay rate is consistent with the derived theoretical bounds.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-architecture-search\", \"differentiable-automl\", \"computational-topology\"], \"acceptance_criteria\": [\"- The LaTeX document provides a complete, step-by-step derivation of the gradient for the architecture parameters, including the contribution from the persistent homology term.\", \"- The JAX implementation runs without error and produces a valid, discrete cell architecture as output.\", \"- The final report includes a plot showing lower test error or improved sample efficiency for the topologically regularized search compared to a baseline.\", \"- The gradient verification report shows a relative error of less than 1e-4 between the analytical and numerical gradients.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Train the final discovered architecture from scratch on CIFAR-10 for a fixed number of epochs and report the error rate on the official test set.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate the average number of connected components (P_0) in the learned architecture's graph during the search phase, which should be encouraged towards 1 by the regularizer.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3610, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the update rules for a linearized ADMM variant applied to a global consensus problem with non-smooth L1 regularization for federated learning. Implement this in Python, ensuring convergence diagnostics like primal and dual residuals are tracked, and compare its convergence rate against the standard ADMM formulation on a synthetic LASSO dataset.\n", "completion": "{\"route_plan\": [\"/math(Derive the augmented Lagrangian and primal-dual update steps for linearized ADMM for federated consensus optimization, analyzing the convergence conditions under non-smooth L1 regularization and a given confidence interval.)\", \"/general-search(query=\\\"site:arxiv.org linearized ADMM federated learning consensus non-smooth\\\", mode=rag)\", \"/code(Implement the derived linearized ADMM in Python using NumPy, including convergence checks based on primal/dual residuals. Validate on a synthetic federated LASSO problem, tracking convergence rate and bootstrap error.)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation and derive the necessary equations. The blue /general-search agent then gathers supporting literature and state-of-the-art context from authoritative sources like arXiv. Finally, /code provides empirical validation of the derived theory, implementing the algorithm and comparing its performance.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the linearized ADMM updates and convergence analysis.\", \"A Python script implementing both standard and linearized ADMM for federated consensus LASSO, with visualizations of convergence.\", \"A summary report citing at least two relevant arXiv papers that discuss linearized or inexact ADMM variants.\", \"A JSON file containing the final primal/dual residual convergence metrics for both implemented algorithms.\"], \"thinking_outline\": [\"1. Formulate the federated consensus problem as a global optimization problem with separable objectives and a consensus constraint.\", \"2. Construct the augmented Lagrangian for the problem and derive the primal-dual update steps for the linearized ADMM variant, paying close attention to the proximal operator for the L1 norm.\", \"3. Implement the derived algorithm in Python, alongside a standard ADMM baseline, ensuring correct computation of primal and dual residuals for convergence checking.\", \"4. Validate the implementation's correctness by checking if the residuals approach zero on a solvable synthetic dataset. Verify that the solution matches that of a centralized solver.\", \"5. Analyze the convergence plots to compare the performance of linearized ADMM vs. standard ADMM, and check if the results align with theoretical expectations from the literature.\", \"6. Synthesize all findings\u2014derivation, code, experimental results, and literature references\u2014into a comprehensive report.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: code implementation of primal/dual residuals must match the derived /math formulas; fallback: if convergence fails, revisit /math to check derivation for errors in the proximal operator or linearization term).\", \"todo_list\": [\"- [ ] /math: Derive the linearized ADMM update equations for the specified federated consensus problem.\", \"- [ ] /general-search: Find and summarize two key papers on linearized ADMM or its application in federated settings.\", \"- [ ] /code: Implement the derived linearized ADMM and a standard ADMM baseline in Python.\", \"- [ ] /code: Add convergence diagnostics (primal/dual residuals) and plotting utilities.\", \"- [ ] /math: Verify that the proximal operator implementation in the code correctly corresponds to the L1-norm derivation.\", \"- [ ] /code: Validate the algorithm's convergence on a synthetic dataset and compare performance metrics.\", \"- [ ] /router: Final QA review of all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"convex-optimization\", \"admm\", \"federated-learning\"], \"acceptance_criteria\": [\"The mathematical derivation of the linearized ADMM updates must be complete and self-contained.\", \"The Python implementation must converge on a synthetic LASSO problem, with both primal and dual residuals decreasing to a tolerance below 1e-4.\", \"The final report must correctly cite at least two relevant arXiv pre-prints and compare the empirical results to theoretical expectations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3638, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to analyze the properties of the doubly robust estimator for the Average Treatment Effect (ATE) under the potential outcomes framework. Please provide a full mathematical derivation of its influence function to prove its semi-parametric efficiency. Then, construct a simulation study in Python to compare its finite-sample bias and variance against the Inverse Probability Weighting (IPW) estimator, especially under misspecification of either the propensity score model or the outcome model. The analysis must be grounded in foundational causal inference literature.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:stat.berkeley.edu 'doubly robust estimation' 'causal inference' 'influence function' 'semiparametric efficiency theory'\\\", mode=both)\", \"/math(Derive the efficient influence function for the Average Treatment Effect using potential outcomes, proving the doubly robust property where consistency holds if either the propensity score or outcome model is correct, referencing key causal inference concepts., includes cybersecurity)\", \"/code(Implement a Python simulation using statsmodels for logistic regression and OLS to compare the doubly robust estimator's bias and variance against IPW under scenarios of model misspecification for both causal components.)\", \"/math(Verify that the empirical variance reduction observed in the simulation for the doubly robust estimator aligns with the theoretical asymptotic efficiency gains predicted by its influence function compared to the IPW estimator., includes cybersecurity)\"], \"route_rationale\": \"The route starts with the blue /general-search agent to gather authoritative papers on doubly robust estimation. /math is then used for the formal theoretical derivation based on this literature. /code operationalizes the theory in a simulation to test the properties empirically. A final /math step provides theoretical validation for the empirical results, ensuring consistency between theory and practice.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the doubly robust estimator, its influence function, and the proof of its robustness property.\", \"A Python script (`.py`) implementing the simulation study, including data generation with confounding and implementations of both IPW and doubly robust estimators.\", \"A JSON file summarizing the simulation results, reporting the bias, variance, and MSE for each estimator under the four conditions of model misspecification.\", \"A summary report in Markdown (`.md`) that interprets the results and connects them to the theoretical derivations, including at least two citations.\", \"A verification note confirming the alignment between the empirical variance from the simulation and the predictions from semi-parametric efficiency theory.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of semi-parametric efficiency in causal inference, focusing on doubly robust estimators and the role of the efficient influence function.\", \"2. Formulate the estimation problem using the potential outcomes framework, clearly stating the assumptions of consistency, positivity, and unconfoundedness (ignorability).\", \"3. Derive the doubly robust estimator for the ATE and formally prove that its bias is a product of the errors in the propensity score and outcome models.\", \"4. Implement a simulation to empirically demonstrate this property, comparing the DR estimator's performance (bias, variance, MSE) against a standard IPW estimator.\", \"5. Validate that the simulation's code logic correctly reflects the derived mathematical formulas for both estimators.\", \"6. Check that the observed reduction in variance for the DR estimator in the simulation is consistent with its theoretical asymptotic efficiency properties.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math uses papers for formal derivation -> /code implements derived estimators in a simulation -> /math verifies empirical variance against theory -> router QA (verification: check that the MSE from the simulation confirms the DR estimator's superiority under misspecification; fallback: if simulation fails to show this, flag the /code implementation for an audit of the model specification logic).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on doubly robust estimation (e.g., from Robins, Rotnitzky, van der Laan) and their connection to semi-parametric theory.\", \"- [ ] /math: Formally derive the ATE estimator and its influence function, showing why it is 'doubly robust'.\", \"- [ ] /code: Write a Python script to generate simulated data with a known ATE and confounding variable.\", \"- [ ] /code: Implement the IPW and doubly robust estimators within the script.\", \"- [ ] /code: Execute simulation loops for correctly and incorrectly specified propensity and outcome models, storing performance metrics.\", \"- [ ] /math: Verify that the derivation for the influence function aligns with the estimator's implementation in the code.\", \"- [ ] /code: Generate a table and plots summarizing the bias, variance, and MSE results from the simulation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"doubly-robust-estimation\", \"econometrics\"], \"acceptance_criteria\": [\"- The mathematical derivation must correctly prove the double robustness property.\", \"- The Python simulation must demonstrate that the doubly robust estimator has low bias when either the propensity score or outcome model is correctly specified.\", \"- The final analysis must show that the doubly robust estimator achieves lower variance than the IPW estimator when both models are correctly specified.\", \"- The report must cite at least two foundational papers on causal inference or doubly robust methods with their arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3648, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive a formal bound on information leakage from a generative language model under adversarial extraction attacks. Use rate-distortion theory to model the trade-off between model utility and data privacy, defining the distortion measure as the success probability of an adversary performing a membership inference attack. The final proof must consider the model as a stochastic channel and apply concepts from Shannon's channel coding theorem.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org rate-distortion theory generative model privacy adversarial attacks differential privacy\\\", mode=web)\", \"/math(Derive the rate-distortion function R(D) for a Bayesian generative model, where distortion D is the adversarial success probability. Define the information source as the training data distribution and the channel as the stochastic model parameters, ensuring the proof accounts for entropy constraints.)\", \"/code(Implement a CUDA-accelerated Monte Carlo simulation in Python (JAX) to empirically estimate mutual information between a synthetic dataset and a small transformer's outputs under a simulated membership inference adversary, tracking convergence of the estimate.)\", \"/math(Analyze the simulation's empirical mutual information estimates against the theoretical rate-distortion bound. Verify the conditions under which the bound holds and quantify the theoretical-empirical divergence, checking for estimator convergence., includes cybersecurity)\"], \"route_rationale\": \"The blue general-search agent first gathers foundational academic literature. The first /math agent then constructs the core theoretical framework and proof based on this literature. The /code agent creates a computational experiment to empirically validate the theoretical model. The final /math agent synthesizes the theoretical and empirical results, providing a crucial verification step.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the rate-distortion bound.\", \"A Python script implementing the simulation of information leakage.\", \"A JSON file reporting the final theoretical bound, empirical mutual information, and theoretical-empirical divergence.\", \"A summary report (README.md) explaining the methodology, results, and citations.\", \"Verification logs confirming the alignment between the mathematical derivation and the simulation logic.\"], \"thinking_outline\": [\"1. Formalize the generative model as a communication channel and the adversarial attack as a distortion measure using information-theoretic primitives (entropy, mutual information).\", \"2. Derive the rate-distortion function R(D) that lower-bounds the rate of information leakage for a given level of adversarial success D.\", \"3. Develop a simulation to empirically measure the mutual information between training data and model outputs under a specific attack model.\", \"4. Validate the simulation's correctness by checking its output on a known toy problem with an analytical solution, such as a Binary Symmetric Channel.\", \"5. Compare the empirically measured information leakage from the simulation with the derived theoretical R(D) bound to check for consistency.\", \"6. Audit the assumptions made in the proof (e.g., data distribution, model class) against the conditions implemented in the simulation to ensure the comparison is valid.\"], \"handoff_plan\": \"/general-search -> /math (proof derivation) -> /code (simulation) -> /math (analysis & verification) -> router QA (final review). Verification: The final /math agent explicitly checks that the code's implementation of mutual information estimation aligns with the theoretical constructs from the first /math agent's proof. Fallback: If empirical results significantly violate the theoretical bound, the task is routed back to the first /math agent to revisit proof assumptions or to the /code agent to debug the estimation logic.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least 3 key papers on information-theoretic privacy for generative models.\", \"- [ ] /math: Produce the complete LaTeX derivation of the rate-distortion function for the specified scenario.\", \"- [ ] /code: Write and document the Python simulation code for estimating mutual information.\", \"- [ ] /math: Verify that the formulas used for mutual information estimation in the code are consistent with the theoretical derivation.\", \"- [ ] /code: Execute the simulation and generate the JSON artifact with empirical results.\", \"- [ ] /math: Perform the final analysis comparing the theoretical bound with the empirical data and document any divergence.\", \"- [ ] /router: Conduct final QA review of all artifacts, ensuring they meet the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"ai-safety\", \"rate-distortion-theory\"], \"acceptance_criteria\": [\"- The LaTeX document must contain a complete, mathematically rigorous proof of the rate-distortion bound.\", \"- The Python simulation must run without errors and produce a JSON output file matching the specified schema.\", \"- The theoretical-empirical divergence metric must be below a pre-specified threshold (e.g., 15%) or any larger gap must be rigorously justified in the final report.\", \"- The final report must cite at least three relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3652, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating dictionary learning for compressed sensing MRI. Can you derive the update rules for a dictionary learning algorithm, like K-SVD or one based on ADMM, that minimizes both a data fidelity term and a sparsity-inducing L1 norm? Please then implement this to learn an incoherent dictionary from synthetic sparse signals, analyze its convergence properties, and evaluate its reconstruction performance against theoretical guarantees based on the learned dictionary's mutual coherence and spark property.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org dictionary learning K-SVD ADMM compressed sensing convergence analysis\\\", mode=rag)\", \"/math(Derive the alternating minimization update steps for dictionary learning, formulating the sparse coding stage using Orthogonal Matching Pursuit and the dictionary update as a rank-1 approximation problem for each atom, ensuring the analysis considers convergence criteria and ADMM formulation.)\", \"/code(Implement the derived dictionary learning algorithm in Python using NumPy/SciPy. Generate synthetic sparse signals, apply a random Gaussian measurement matrix to create compressed samples, and use the algorithm to recover the original signals, tracking reconstruction SNR.)\", \"/math(Analyze the mutual coherence and estimate the spark of the dictionary learned by the code. Relate these empirical values to the theoretical RIP-based recovery guarantees, and check the convergence rate against the derived theory by analyzing the objective function's eigenvalue spectrum.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to ground the work in authoritative literature. This is followed by /math for the formal derivation of the optimization algorithm. /code is then used to implement the derived mathematics and run an empirical validation. The final /math step closes the loop by analyzing the empirical results from the code against the formal theory, providing a comprehensive verification of the entire process.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the dictionary and sparse code update rules.\", \"A Python script implementing the K-SVD or ADMM-based dictionary learning algorithm and a synthetic data experiment.\", \"A plot of reconstruction Signal-to-Noise Ratio (SNR) versus signal sparsity level for the learned dictionary.\", \"A final report summarizing the analysis of the learned dictionary's properties (coherence, spark) and comparing empirical results to theory, with at least two citations.\", \"A verification log confirming the alignment between the mathematical derivation and the code's objective function behavior.\"], \"thinking_outline\": [\"1. Research foundational dictionary learning algorithms (e.g., K-SVD, Method of Optimal Directions) and their application in compressed sensing, focusing on the underlying optimization problem.\", \"2. Formulate the dictionary learning problem as a constrained optimization task, clearly defining the objective function with its data fidelity and sparsity-promoting terms.\", \"3. Derive the iterative update rules for both the dictionary atoms and the sparse coefficients, paying close attention to the sub-problems and their closed-form or algorithmic solutions (e.g., SVD for dictionary update).\", \"4. Implement the derived algorithm and a synthetic experiment to test signal recovery from incomplete measurements. Validate the implementation by plotting the objective function value per iteration to ensure it is monotonically decreasing.\", \"5. Analyze the properties (e.g., mutual coherence) of the dictionary learned from the synthetic data. Verify that the empirical reconstruction performance aligns with the theoretical guarantees predicted by the dictionary's properties.\", \"6. Audit the final report to ensure the mathematical derivations, code implementation, and theoretical analysis are consistent and well-supported by the cited literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: the final analysis in /math must use the dictionary generated by /code, and the derivation in the first /math step must match the code's implementation; fallback: if the empirical convergence from /code does not match theory, the second /math agent must flag the discrepancy and suggest debugging steps for the /code agent).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers (e.g., Aharon et al., 2006) on K-SVD and ADMM for dictionary learning.\", \"- [ ] /math: Provide a full derivation of the alternating minimization procedure for the dictionary learning objective function.\", \"- [ ] /code: Implement the algorithm, the synthetic data pipeline, and the signal reconstruction testbed.\", \"- [ ] /math: Compute the mutual coherence of the dictionary produced by /code and derive the corresponding recovery guarantees.\", \"- [ ] /math: Verify that the objective function implemented in /code matches the formal derivation.\", \"- [ ] /code: Validate the implementation by ensuring the objective function value decreases monotonically over iterations.\", \"- [ ] /router: Perform final QA check, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\", \"convex-optimization\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly formulates the optimization problem and derives the update rules for both sparse coding and dictionary update stages.\", \"- The Python implementation correctly executes the derived algorithm, demonstrating convergence on a synthetic dataset.\", \"- The final analysis report includes a calculation of the learned dictionary's mutual coherence, cites at least 2 relevant arXiv papers with IDs, and compares the empirical reconstruction performance to theoretical bounds.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3669, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the theoretical underpinnings of MAML and its generalization capabilities. Can you derive a PAC-Bayesian generalization bound for the MAML algorithm, specifically by leveraging concepts from information geometry and tensor calculus to analyze the curvature of the task manifold? The proof should explicitly connect the Hessian of the meta-objective function to the task-averaged loss landscape and discuss how its spectral properties influence the final bound.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org PAC-Bayesian MAML generalization bound information geometry Hessian\\\", mode=rag)\", \"/math(Derive the PAC-Bayesian generalization bound for MAML. Formulate the meta-objective's Hessian using tensor calculus, analyzing its eigenvalue spectrum to ensure Bayesian posterior concentration.)\", \"/code(Implement a JAX model to numerically validate the derived bound on a synthetic sinusoidal regression task. Verify the convergence rate and check if the empirical risk aligns with the theoretical bound under a federated learning-style task sampling distribution., using Python)\", \"/math(Formally audit the JAX implementation against the derived proof, verifying that the computed Hessian's trace and the KL-divergence term match the theoretical expressions within a specified numerical tolerance., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on PAC-Bayesian bounds for meta-learning. This informs the core theoretical work in /math, where the proof is derived. Next, /code provides numerical validation for the theory in a controlled setting. A final /math step is crucial for verifying the consistency between the abstract proof and the concrete implementation, ensuring correctness.\", \"expected_artifacts\": [\"A formal mathematical proof of the PAC-Bayesian generalization bound for MAML, delivered as a LaTeX document.\", \"A Python script (JAX) implementing the numerical validation experiment.\", \"A plot comparing the theoretical bound against the empirical generalization gap from the simulation.\", \"A summary report in Markdown (.md) that synthesizes the findings and includes all citations.\", \"A JSON file containing the computed metrics, including the final generalization gap discrepancy.\"], \"thinking_outline\": [\"1. Formulate the meta-learning problem within the PAC-Bayesian framework, defining the prior and posterior distributions over meta-parameters.\", \"2. Utilize tensor calculus to express the Hessian of the meta-objective function, relating it to the geometry of the underlying task manifold.\", \"3. Derive the generalization bound by bounding the KL divergence between the prior and posterior, linking this divergence to the spectral properties (e.g., trace, top eigenvalue) of the task-averaged Hessian.\", \"4. Validate the key steps of the derivation against the foundational literature sourced from the initial search to ensure theoretical soundness.\", \"5. Implement a numerical simulation to compute both the empirical generalization gap and the derived theoretical bound for a set of synthetic tasks.\", \"6. Verify that the numerical results from the simulation are consistent with the theoretical predictions and that the code correctly implements the mathematical formalism.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /math agent will consume arXiv papers from /general-search to ground the proof. The /code agent will implement the final derivation from /math. The final /math agent will audit the code's output against the theory. Verification: The router will check for consistency between the proof, code, and final report. Fallback: If the numerical validation fails, the task is routed back to the first /math agent to re-check the derivation's assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find at least 3 seminal papers on PAC-Bayesian bounds for meta-learning, focusing on MAML or related gradient-based algorithms.\", \"- [ ] /math: Construct the complete mathematical derivation, starting from PAC-Bayes basics and ending with the final MAML-specific bound, citing the sourced literature.\", \"- [ ] /code: Develop a self-contained JAX script to run the simulation on a synthetic few-shot regression problem.\", \"- [ ] /code: Instrument the code to log all required metrics, including empirical risk, true risk, and terms needed for the theoretical bound calculation.\", \"- [ ] /math: Verify that the implementation of the KL-divergence and Hessian trace in the code aligns with the mathematical expressions in the proof.\", \"- [ ] /math: Produce the final LaTeX proof document and the summary report, including the comparison plot.\", \"- [ ] /router: Perform final QA, ensuring all acceptance criteria are met and artifacts are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"pac-bayesian\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The derived PAC-Bayesian bound must be mathematically sound and all steps in the proof must be rigorously justified.\", \"- The JAX implementation must run without errors and produce numerical results that are consistent with the claims made in the theoretical derivation.\", \"- The final report must clearly articulate the connection between the theory and the experiment, supported by a plot showing the theoretical bound consistently above the empirical generalization gap.\", \"- The solution must cite at least 3 relevant academic papers using their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3672, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive and implement a variational inference scheme for a latent stochastic differential equation (SDE) model. Specifically, derive the Evidence Lower Bound (ELBO) by applying Girsanov's theorem to handle the change of measure between the prior and approximate posterior path distributions. Then, formulate an unbiased, low-variance pathwise gradient estimator for this ELBO suitable for Bayesian neural networks and validate its correctness numerically.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference stochastic differential equations Girsanov theorem pathwise gradients\\\", mode=both)\", \"/math(Derive the ELBO for a latent SDE model, applying Girsanov's theorem for the KL divergence between path measures. Formulate the reparameterized gradient estimator for Bayesian posterior inference.)\", \"/code(Implement the variational SDE model in JAX using an Euler-Maruyama solver. Code the training loop to optimize the ELBO, ensuring CUDA compatibility and tracking convergence metrics., using Python)\", \"/math(Verify the analytical gradient from the code implementation by comparing its output against a central finite-difference approximation for a set of test parameters, auditing for numerical stability., includes cybersecurity)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature on variational inference for SDEs. The math agent then performs the core theoretical derivation based on these findings. The code agent translates the complex mathematical formulation into a practical, GPU-accelerated implementation. A final math agent call is crucial for numerical verification, ensuring the implementation correctly reflects the derivation by comparing analytical and numerical gradients.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the SDE ELBO and its pathwise gradient.\", \"A self-contained Python script implementing the Bayesian neural SDE and the variational inference training loop.\", \"A summary report with plots showing ELBO convergence and the learned posterior dynamics.\", \"A list of at least two cited arXiv papers with full IDs.\", \"A verification log showing the L2 error between the analytical and finite-difference gradients.\"], \"thinking_outline\": [\"1. Formulate the prior and variational posterior SDEs, where the drift and diffusion coefficients are parameterized by neural networks.\", \"2. Apply Girsanov's theorem to express the Radon-Nikodym derivative between the two path measures, which is essential for deriving the KL divergence term in the ELBO.\", \"3. Derive the final ELBO expression and the corresponding pathwise gradient estimator using the reparameterization trick adapted for SDEs.\", \"4. Implement the SDE solver, the neural network parameterization, and the stochastic optimization loop to maximize the ELBO.\", \"5. Validate the gradient implementation by writing a numerical check that compares the analytical gradient's output with a finite-difference approximation; the discrepancy should be close to machine precision.\", \"6. Audit the model's convergence behavior on a synthetic dataset and analyze the properties of the learned posterior process.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent. The math agent's derivation is the formal specification for the code agent. The code agent's implementation is then passed back to the math agent for numerical gradient verification. (Verification: L2 norm of gradient difference < 1e-4; Fallback: If verification fails, the code agent must debug the gradient implementation, flagging potential errors in the math agent's derivation for review). Final artifacts are submitted to the router for a comprehensive quality check.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on variational inference for neural SDEs, focusing on gradient estimation.\", \"- [ ] /math: Derive the complete ELBO expression using Girsanov's theorem and formulate the gradient estimator.\", \"- [ ] /code: Implement the neural SDE, the Euler-Maruyama integrator, and the training procedure in JAX.\", \"- [ ] /math: Create and execute a numerical gradient checker to verify the code agent's implementation.\", \"- [ ] /code: Train the model on a synthetic dataset and generate plots of the ELBO curve and posterior samples.\", \"- [ ] /code: Document the implementation and package all artifacts for final review.\", \"- [ ] /router: Perform final QA review, checking derivation correctness, code functionality, and metric reporting.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-calculus\", \"bayesian-inference\", \"variational-methods\"], \"acceptance_criteria\": [\"The mathematical derivation must correctly apply Girsanov's theorem to derive the KL divergence between path measures.\", \"The final implementation must demonstrate monotonic convergence of the ELBO on a suitable synthetic dataset.\", \"The numerical verification of the analytical gradient must show an L2 error below 1e-4 when compared to a finite-difference approximation.\", \"The submission must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3673, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm new to Transformers and want to understand the core performance bottleneck. Can you explain the quadratic computational complexity of the self-attention mechanism and summarize key optimization techniques like FlashAttention from recent arXiv papers?\n", "completion": "{\"route_plan\": [\"/math(Derive the O(n^2 * d) computational complexity for the self-attention mechanism, clearly defining sequence length 'n' and dimension 'd' while analyzing matrix multiplication convergence properties.)\", \"/general-search(query=\\\"site:arxiv.org transformer attention optimization 'FlashAttention' 'sparse attention' computational complexity\\\", mode=rag)\", \"/code(Implement a basic self-attention layer in Python using NumPy to empirically verify the quadratic complexity with respect to sequence length, measuring wall-clock time. Use CUDA for acceleration if available.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation of the attention mechanism's complexity. The 'blue' /general-search agent then curates authoritative research on modern optimizations. Finally, /code provides a concrete, empirical validation of the theoretical findings from /math.\", \"expected_artifacts\": [\"A formal mathematical derivation of the O(n^2 * d) complexity for self-attention.\", \"A Python script implementing a naive self-attention layer and a benchmark to measure its performance.\", \"A plot (e.g., PNG file) showing runtime versus sequence length, demonstrating the quadratic relationship.\", \"A summary of at least two optimization techniques, citing their respective arXiv papers with IDs.\", \"A verification report confirming the consistency between the theoretical derivation and empirical results.\"], \"thinking_outline\": [\"1. Formulate the self-attention mechanism as a sequence of matrix operations involving Query (Q), Key (K), and Value (V) matrices.\", \"2. Analyze the computational complexity (floating-point operations) of each matrix multiplication step, particularly the (Q * K^T) operation, in terms of sequence length 'n' and embedding dimension 'd'.\", \"3. Use the search tool to find and summarize key optimization techniques from authoritative sources like arXiv, focusing on methods that reduce the quadratic dependency on 'n'.\", \"4. Implement the naive attention mechanism in code and generate a plot of its runtime against increasing sequence lengths to empirically demonstrate the O(n^2) complexity.\", \"5. Verify that the empirical plot shows a clear quadratic trend, validating the theoretical derivation from step 2.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: The empirical complexity plot from /code must align with the theoretical O(n^2) derivation from /math; fallback: If there is a mismatch, re-run /code with a wider range of sequence lengths to reduce measurement noise and confirm the trend).\", \"todo_list\": [\"- [ ] /math: Derive the computational complexity of the self-attention mechanism.\", \"- [ ] /general-search: Find and summarize recent papers on attention optimization techniques.\", \"- [ ] /code: Implement and benchmark a naive self-attention layer to show its complexity empirically.\", \"- [ ] Verification: Check that the plot from the /code step exhibits a clear quadratic curve, matching the /math derivation.\", \"- [ ] Router QA: Review all generated artifacts for correctness, clarity, and adherence to acceptance criteria.\"], \"difficulty\": \"introductory\", \"tags\": [\"transformers\", \"llm-optimization\", \"computational-complexity\"], \"acceptance_criteria\": [\"The final report must contain a correct mathematical derivation showing O(n^2 * d) complexity.\", \"At least two relevant research papers (e.g., FlashAttention, sparse attention) must be cited with their arXiv IDs.\", \"The generated code must execute without errors and produce a plot that visually confirms a quadratic relationship between sequence length and runtime (R-squared > 0.95 for a quadratic fit).\", \"The summary of optimization techniques must correctly identify how they address the quadratic bottleneck.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3685, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDevelop a dynamic programming algorithm on a tree decomposition to compute the first Betti number for a given graph G. The algorithm's complexity must be analyzed in terms of treewidth and number of vertices, considering a Bayesian perspective on the input graph distribution. Can this approach be generalized to higher-order Betti numbers using simplicial complexes, and what are the computational complexity implications for such an extension?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org dynamic programming tree decomposition betti number parameterized complexity\\\", mode=rag)\", \"/math(Formulate the dynamic programming states and recurrence relations on the tree decomposition, incorporating Bayesian priors on graph structure for average-case complexity analysis. Reference control theory for state-space size.)\", \"/code(Implement the DP algorithm in Python using networkx, including routines for generating test graphs with known Betti numbers for runtime validation. Optimize for large treewidth using CUDA if possible.)\", \"/math(Generalize the proof of correctness and complexity analysis for higher Betti numbers, analyzing the convergence of the state complexity and its relation to the graph's eigenvalue spectrum.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to survey existing literature on parameterized algorithms for topological invariants. /math then formalizes the core DP logic. /code provides an empirical implementation for validation. A final /math step tackles the theoretical generalization, which is a key part of the user query.\", \"expected_artifacts\": [\"- A formal paper (PDF/LaTeX) detailing the DP state representation, recurrence relations, and a proof of correctness.\", \"- A documented Python implementation of the algorithm capable of running on graphs specified by an adjacency list.\", \"- A complexity analysis report discussing both worst-case and average-case performance.\", \"- A literature review summary with at least two cited arXiv papers.\", \"- Verification output showing the code passes all test cases.\"], \"thinking_outline\": [\"1. Research existing literature on computing topological invariants, specifically Betti numbers, on graphs of bounded treewidth to establish a theoretical baseline.\", \"2. Formulate the dynamic programming states on a tree decomposition's bags. The state must capture information about connected components and cycle structures within subgraphs.\", \"3. Define the DP transition functions for the three types of tree decomposition nodes: introduce, forget, and join. Prove that these transitions correctly update the state.\", \"4. Validate the DP formulation by implementing it and testing against graphs where the Betti number is easily computed via other means (e.g., `|E| - |V| + |C|`).\", \"5. Derive the algorithm's time and space complexity as a function of the number of vertices `n` and the treewidth `k`.\", \"6. Audit the theoretical complexity analysis by comparing it against the empirical performance of the implementation on synthetic graphs with varying `n` and `k`.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: code output on canonical graph families must match the closed-form Betti numbers derived by /math; fallback: if a mismatch occurs, /math must re-evaluate the DP state representation for information loss at join nodes).\", \"todo_list\": [\"- [ ] /general-search: Find foundational papers on Courcelle's theorem and algorithms for computing topological graph properties.\", \"- [ ] /math: Define the DP state tuple for a bag, likely involving partitions of boundary vertices and tracking edges within induced subgraphs.\", \"- [ ] /math: Derive the recurrence relations for join, introduce, and forget nodes in the tree decomposition.\", \"- [ ] /code: Implement the core DP algorithm in Python, assuming a tree decomposition is provided as input.\", \"- [ ] /code: Develop a test suite with graphs like cycles, trees, and complete graphs to validate correctness.\", \"- [ ] /math: Verify that the logic implemented in the /code step is a faithful representation of the mathematical recurrence.\", \"- [ ] /math: Analyze the complexity blow-up for generalizing to higher-order Betti numbers, connecting it to the size of simplicial complexes.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"dynamic-programming\", \"graph-theory\", \"algebraic-topology\"], \"acceptance_criteria\": [\"- The DP algorithm's correctness proof is sound and references the properties of tree decompositions.\", \"- The Python implementation correctly computes the first Betti number for all provided test graphs with up to 30 vertices and treewidth 5.\", \"- The final report accurately states the time complexity, which should be exponential in treewidth but polynomial in the number of vertices.\", \"- The discussion on generalization correctly identifies that tracking higher-dimensional cycles (voids) requires exponentially larger state representations.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3702, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive the Natural Policy Gradient (NPG) update rule using information geometry, specifically showing how the Fisher Information Matrix acts as a metric tensor for KL-divergence. Then, implement this NPG algorithm in PyTorch for a continuous control theory problem like Pendulum-v1, using the conjugate gradient method to solve for the search direction. Finally, provide an empirical comparison against the standard REINFORCE algorithm, analyzing convergence speed and asymptotic performance.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:jmlr.org site:arxiv.org natural policy gradient kakade fisher information matrix conjugate gradient reinforcement learning\\\", mode=both)\", \"/math(Derive the natural policy gradient update as a constrained optimization problem that bounds the KL-divergence between policy updates, establishing the Fisher Information Matrix's role in preconditioning the gradient to ensure stable convergence and analyzing its connection to trust regions.)\", \"/code(Implement the NPG algorithm in PyTorch for the 'Pendulum-v1' environment. Use the conjugate gradient method to efficiently compute the Fisher-vector product without explicit matrix inversion, ensuring proper handling of control theory dynamics and reward shaping., using Python)\", \"/math(Verify that the implemented conjugate gradient solver correctly approximates the inverse Fisher-gradient product. Analyze the empirical convergence plots from the code to confirm the theoretical advantages over vanilla policy gradients, checking for statistical significance.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on Natural Policy Gradients. This theoretical grounding is then passed to /math for a formal derivation. Next, /code implements the derived algorithm for empirical validation. Finally, a second /math call is used to cross-verify the implementation's mathematical correctness against the derivation and to formally analyze the empirical results, ensuring a closed loop of theory, practice, and verification.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF containing the full mathematical derivation of the Natural Policy Gradient update rule.\", \"- A Python script implementing both NPG and REINFORCE algorithms for the Pendulum-v1 environment.\", \"- A citation list with at least two seminal papers on Natural Policy Gradients (with arXiv IDs/DOIs).\", \"- A JSON file containing the final performance metrics (average return, policy entropy) for both algorithms.\", \"- Plots comparing the learning curves (average return vs. training episodes) of NPG and REINFORCE.\"], \"thinking_outline\": [\"1. Research the foundational theory of Natural Policy Gradients, focusing on the connection between the Fisher Information Matrix (FIM) and the KL-divergence metric in policy space.\", \"2. Formally derive the NPG update rule, starting from a trust-region optimization perspective and arriving at the preconditioned gradient `g' = F^-1 * g`.\", \"3. Implement the algorithm in code, paying special attention to the efficient calculation of the Fisher-vector product `Fv` and the use of a conjugate gradient solver to find the search direction without inverting the FIM.\", \"4. Validate the implementation by running experiments and comparing the sample efficiency and final performance of NPG against a vanilla REINFORCE baseline on a continuous control task.\", \"5. Audit the numerical stability and correctness of the conjugate gradient solver implementation and verify that the empirical results align with the theoretical promises of improved convergence.\", \"6. Synthesize all findings into a comprehensive report including the derivation, implementation details, and empirical analysis.\"], \"handoff_plan\": \"/general-search provides foundational papers -> /math produces a formal derivation -> /code implements the derivation and runs experiments -> /math verifies the implementation's mathematical logic against the derivation and analyzes the empirical data -> router QA (verification: ensure experimental results support theoretical claims; fallback: if convergence fails, re-run /code with adjusted hyperparameters).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Natural Policy Gradients (e.g., Kakade, 2001).\", \"- [ ] /math: Produce a step-by-step derivation of the NPG update rule from first principles.\", \"- [ ] /code: Implement NPG and a REINFORCE baseline in PyTorch for the 'Pendulum-v1' environment.\", \"- [ ] /code: Implement the conjugate gradient algorithm to solve for the NPG search direction.\", \"- [ ] /math: Verify the code's Fisher-vector product implementation matches the derived mathematical formula.\", \"- [ ] /math: Analyze and plot the learning curves, providing a statistical comparison of NPG vs. REINFORCE.\", \"- [ ] /router: Review final report for theoretical correctness, empirical validity, and alignment with acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"information-geometry\"], \"acceptance_criteria\": [\"- The final report must contain a correct, step-by-step derivation of the Natural Policy Gradient.\", \"- The NPG implementation must empirically demonstrate statistically significant faster convergence or higher final average return compared to the REINFORCE baseline on Pendulum-v1.\", \"- The code must correctly implement the conjugate gradient method to avoid explicit FIM inversion.\", \"- At least two relevant academic papers must be cited correctly using their arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3725, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to explore the fundamental limits of steganographic watermarking in LLMs for AI safety. Can you derive the rate-distortion function for embedding a binary message into generated text from a simplified autoregressive model, assuming a specific semantic distortion metric? Then, simulate this process to empirically validate the theoretical R(D) curve under a paraphrasing-based adversarial attack, measuring the trade-off between the watermark's bit-rate and its robustness.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org rate-distortion theory steganography large language models adversarial robustness\\\", mode=web)\", \"/math(Derive the rate-distortion function R(D) for a Bernoulli text source under a Hamming distortion metric, considering adversarial channel effects and Bayesian inference for decoding.)\", \"/code(Implement a Python simulation of the steganographic system using a pre-trained small model, measuring bit-error rate and semantic distortion against an adversarial paraphraser. Track convergence metrics for any iterative decoding.)\", \"/math(Validate the empirical data from the simulation against the theoretical R(D) curve. Analyze the gap using Chebyshev's inequality to bound the deviation and check for eigenvalue stability in the distortion matrix.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey state-of-the-art literature on information-theoretic AI safety. This informs the theoretical work in /math, which formally models the problem. /code is then used to create an empirical simulation for validation. A final /math step closes the loop by comparing theoretical predictions with simulation results, ensuring a rigorous analysis.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the rate-distortion function R(D) for the specified model.\", \"- A Python script implementing the steganographic embedding, adversarial attack, and data extraction simulation.\", \"- A visualization (e.g., a plot) overlaying the theoretical R(D) curve with the empirical data points from the simulation.\", \"- A summary report in Markdown format that explains the methodology, results, and includes citations to relevant literature.\", \"- A JSON file containing the computed metrics, including the rate-distortion gap and bit-error rates.\"], \"thinking_outline\": [\"1. Research current literature on information-theoretic approaches to generative model watermarking and steganography to ground the theoretical model.\", \"2. Formulate a tractable mathematical model: define the source (text model), a quantifiable distortion metric (e.g., semantic similarity drop), and the adversarial channel (e.g., a paraphrasing attack).\", \"3. Derive the rate-distortion function R(D) for this model, possibly using the Blahut-Arimoto algorithm or an analytical solution for the simplified case.\", \"4. Implement a simulation in Python to empirically measure the achievable rate for various distortion levels by embedding, attacking, and extracting messages.\", \"5. Validate that the simulation's empirical results align with the theoretical R(D) curve and analyze any discrepancies.\", \"6. Audit the Python code to verify the correctness of the distortion metric implementation and the adversarial attack simulation.\"], \"handoff_plan\": \"/general-search (literature survey) -> /math (theoretical derivation) -> /code (empirical simulation) -> /math (verification & analysis) -> router QA (final review). Verification for the math-to-code handoff involves checking if the code's distortion function matches the mathematical definition. Fallback: If simulation results diverge significantly from theory, the router will loop back to the /math agent to re-examine model assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent arXiv papers on rate-distortion theory applied to generative model watermarking.\", \"- [ ] /math: Provide a step-by-step derivation of the rate-distortion function R(D) for the specified text source and distortion metric.\", \"- [ ] /code: Implement the simulation, including modules for text generation, message embedding, adversarial paraphrasing, and message extraction.\", \"- [ ] /code: Generate empirical data points (Rate, Distortion) by running the simulation across a range of embedding strengths.\", \"- [ ] /math: Plot the theoretical curve and empirical data on the same axes and compute the mean squared error between them.\", \"- [ ] /math: Verify that the code's implementation of the distortion metric is a faithful translation of the mathematical formula.\", \"- [ ] /router: Perform a final quality assurance check on all artifacts, ensuring they meet the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"steganography\"], \"acceptance_criteria\": [\"- A complete, step-by-step mathematical derivation of the rate-distortion function is present and mathematically sound.\", \"- The Python simulation code runs without errors and produces a set of empirical (Rate, Distortion) data points.\", \"- A generated plot clearly shows both the theoretical R(D) curve and the empirical data, with the `Empirical_Rate_Distortion_Gap` metric reported and below a pre-defined threshold (e.g., 0.1).\", \"- The final report cites at least 2 relevant arXiv papers with valid identifiers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3733, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to optimize a GPU-accelerated simulation of a physical system governed by the Poisson equation. The core task involves solving a large, sparse, symmetric positive-definite linear system Ax=b. Standard Conjugate Gradient is too slow. I need a comprehensive analysis of the Preconditioned Conjugate Gradient (PCG) method. Please derive the PCG algorithm with an appropriate preconditioner like Incomplete Cholesky, implement it using Python with CuPy for CUDA execution, and rigorously verify that its convergence rate improvement over standard CG aligns with theoretical predictions based on the matrix's condition number.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org gpu incomplete cholesky preconditioner CUDA sparse linear system convergence\\\", mode=rag)\", \"/math(Derive the Preconditioned Conjugate Gradient algorithm, analyzing the impact of a chosen preconditioner on the system's eigenvalue distribution to prove accelerated convergence.)\", \"/code(Implement PCG and standard CG in Python with CUDA via CuPy to solve a 2D Poisson equation. Benchmark performance, tracking residual norm convergence and GPU timings.)\", \"/math(Analyze the empirical results from the CUDA implementation, verifying that the convergence rate improvement aligns with theoretical bounds derived from the preconditioned system's eigenvalue spectrum.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to research modern GPU-friendly preconditioners. This informs the /math agent's formal derivation of the PCG algorithm and its convergence theory. The /code agent then implements this theory for empirical validation on a GPU. A final /math step closes the loop by verifying that the experimental results match the theoretical predictions, fulfilling the math_plus_code variant requirement.\", \"expected_artifacts\": [\"A formal mathematical derivation of the PCG algorithm and its convergence analysis, including the role of the preconditioner on the matrix condition number.\", \"A well-documented Python script using CuPy that implements both CG and PCG for solving the 2D Poisson equation on a GPU.\", \"A generated plot comparing the convergence history (residual norm vs. iteration) of CG and PCG.\", \"A final report in Markdown format synthesizing the theory, implementation, benchmark results, and analysis, including citations.\", \"A JSON file containing the final metrics: convergence rate improvement and wall-clock times.\"], \"thinking_outline\": [\"1. Research effective preconditioners (e.g., Incomplete Cholesky, Jacobi) for sparse, symmetric positive-definite systems that are amenable to parallel implementation on GPUs.\", \"2. Formally derive the Preconditioned Conjugate Gradient (PCG) algorithm, showing how it transforms the original system to improve the condition number of the iteration matrix.\", \"3. Implement both the standard Conjugate Gradient and the PCG algorithms in Python using the CuPy library for CUDA-based GPU execution. The test case will be a large sparse matrix arising from the finite difference discretization of the 2D Poisson equation.\", \"4. Validate the correctness of the implementation by comparing its output on a small-scale problem with a known solution from a trusted library like SciPy.\", \"5. Conduct benchmarks to measure iterations to convergence and wall-clock time for both algorithms on a large-scale problem.\", \"6. Verify that the observed improvement in convergence rate from the benchmarks is consistent with the theoretical speedup predicted by the mathematical analysis of the preconditioned system's eigenvalue spectrum.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search results inform the mathematical derivation. The derivation provides the algorithm for the code implementation. The code's output (plots, timings) is then passed back to the math agent for the final verification step. Fallback: If empirical results contradict theory, the code agent must debug the implementation; if the bug persists, the math agent must re-evaluate the theoretical assumptions.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize two recent papers on efficient preconditioners for iterative solvers on GPU architectures.\", \"- [ ] /math: Provide a step-by-step derivation of the Preconditioned Conjugate Gradient algorithm.\", \"- [ ] /math: Analyze the effect of preconditioning on the condition number and the expected convergence rate based on eigenvalue distribution.\", \"- [ ] /code: Write a CuPy script to generate the sparse matrix for a 2D Poisson problem and implement both CG and PCG solvers.\", \"- [ ] /code: Generate a plot comparing the L2 norm of the residual vs. iteration count for both methods.\", \"- [ ] /math: Verify that the implementation's convergence behavior aligns with the theoretical derivation by analyzing the output plot and metrics.\", \"- [ ] /math: Write the final analysis comparing theoretical expectations with empirical results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"iterative-methods\"], \"acceptance_criteria\": [\"- The provided mathematical derivation of PCG must be complete and correct.\", \"- The Python/CuPy code must run without errors and implement both CG and PCG algorithms correctly.\", \"- The final report must clearly demonstrate, with a plot and metrics, that PCG requires significantly fewer iterations to converge than standard CG.\", \"- The analysis must connect the observed convergence improvement to the theoretical impact of the preconditioner on the system's condition number.\", \"- The solution must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as (Iterations_CG / Iterations_PCG) to reach a residual norm tolerance of 1e-8. A value > 1 indicates improvement.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the `time.time()` difference before and after the main solver loop for both CG and PCG on a fixed-size problem (e.g., 1024x1024 grid).\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3744, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive the evidence lower bound (ELBO) for a continuous-time generative model where the latent variable dynamics are governed by a Stochastic Differential Equation (SDE), specifically an Ornstein-Uhlenbeck process. The goal is to use Girsanov's theorem to correctly formulate the KL divergence between the approximate posterior SDE and the prior SDE, and then implement this in PyTorch to empirically investigate whether this formulation mitigates posterior collapse compared to standard VAEs.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference stochastic differential equation Girsanov theorem Bayesian diffusion\\\", mode=rag)\", \"/math(Derive the ELBO for a continuous-time VAE with an SDE latent space, focusing on the KL divergence term between the prior and posterior SDEs using Girsanov's theorem. Ensure the derivation handles the Radon-Nikodym derivative for Bayesian path measures and shows clear convergence criteria.)\", \"/code(Implement the SDE-VAE in PyTorch with a suitable numerical solver like Euler-Maruyama. Train the model on a standard dataset (e.g., MNIST), tracking the KL divergence and reconstruction loss metrics. The implementation must be CUDA-compatible for efficient Bayesian posterior sampling., using Python)\", \"/math(Analyze the empirical results from the code execution. Verify that the KL divergence term does not collapse to zero and maintains a meaningful confidence interval during training, and compare this behavior against the theoretical properties derived from the Bayesian framework.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on this advanced topic. The core theoretical work is then handled by /math. Next, /code provides an empirical implementation to test the derived theory. A final /math step is crucial for verification, connecting the empirical results back to the initial derivation, ensuring a rigorous math-plus-code validation loop.\", \"expected_artifacts\": [\"A formal mathematical derivation of the SDE-VAE ELBO in LaTeX format.\", \"A runnable Python script implementing the SDE-VAE model, trainer, and SDE solver.\", \"A summary report (.md) containing plots of training metrics (ELBO, KL divergence, reconstruction loss) and an analysis of the posterior collapse phenomenon.\", \"A BibTeX file or list of citations for at least two relevant academic papers.\", \"A JSON file containing the final logged training metrics.\"], \"thinking_outline\": [\"1. Research the theoretical foundations of continuous-time generative models, focusing on how SDEs are used to define priors and approximate posteriors in a variational Bayesian framework.\", \"2. Apply Girsanov's theorem to derive the change of measure required to compute the KL divergence between the two SDE-defined probability measures, which is the key component of the ELBO.\", \"3. Implement the derived mathematical model in Python, including a stable numerical solver for the SDEs and the training loop for the variational autoencoder.\", \"4. Validate the implementation's numerical stability and convergence properties by running small-scale experiments and checking for exploding or vanishing gradients.\", \"5. Train the full model and collect metrics, then critically analyze the results to verify that the latent SDE formulation successfully prevents the KL term from collapsing to zero.\", \"6. Synthesize the theoretical derivation and empirical results into a cohesive report that confirms the initial hypothesis.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers to the math agent. The math agent produces a formal derivation, which the code agent implements. The code agent hands off trained model logs and plots to the final math agent for theoretical verification. Verification: The final math step checks if empirical KL divergence from code matches the derived theory. Fallback: If KL collapses, the code agent will be asked to re-check the SDE solver implementation and hyperparameters before re-running.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on SDE-based VAEs or score-based generative models from arXiv.\", \"- [ ] /math: Produce a step-by-step derivation of the ELBO using Girsanov's theorem for the specified SDEs.\", \"- [ ] /code: Implement the SDE-VAE model, including the Euler-Maruyama solver and the full training and evaluation loop.\", \"- [ ] /code: Train the model on MNIST for a sufficient number of epochs and log the primary and secondary metrics.\", \"- [ ] /math: Verify that the KL divergence calculation in the Python code is a correct discretization of the continuous-time formula derived.\", \"- [ ] /code: Validate the numerical stability of the SDE solver by testing it with varying step sizes and checking for divergence.\", \"- [ ] /math: Write the final analysis comparing the empirical metric plots against the theoretical expectations regarding posterior collapse.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-calculus\", \"diffusion-models\"], \"acceptance_criteria\": [\"The mathematical derivation of the ELBO is complete, correct, and explicitly uses Girsanov's theorem.\", \"The provided Python code trains successfully without numerical instability and reproduces the reported metrics.\", \"The final report includes plots demonstrating that the KL divergence does not collapse to near-zero values during training.\", \"The submission includes at least two citations to relevant arXiv pre-prints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3800, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Karush-Kuhn-Tucker (KKT) conditions for a two-layer neural network's L2 regularization problem, framed as a constrained optimization task where the Frobenius norm of the weight matrices is bounded. Implement a simple solver in Python to find the optimal weights and verify the KKT conditions hold, analyzing its convergence behavior.\n", "completion": "{\"route_plan\": [\"/math(Formulate the L2-regularized loss minimization for a neural network as a constrained optimization problem. Derive the complete Karush-Kuhn-Tucker (KKT) conditions and analyze the Hessian's eigenvalue spectrum for convexity.)\", \"/general-search(query=\\\"site:arxiv.org site:jmlr.org constrained optimization neural network regularization KKT\\\", mode=rag)\", \"/code(Implement a Python solver using PyTorch for the constrained optimization problem. Validate that the numerical solution satisfies the derived KKT conditions and plot the convergence of the primal and dual variables.)\"], \"route_rationale\": \"The /math tool is used first to establish the rigorous theoretical foundation by deriving the KKT conditions. Next, the blue /general-search agent will find authoritative literature to contextualize and cross-reference this theoretical approach. Finally, the /code tool implements a numerical solver to practically apply the theory and validate that the mathematical conditions for optimality hold in a computational setting.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the Lagrangian and the KKT conditions.\", \"A Python script (`.py`) implementing the constrained optimization solver for a simple neural network.\", \"A PNG image plotting the convergence of the training loss, constraint violation, and the dual variable (Lagrange multiplier).\", \"A metrics file (`.json`) reporting the final loss, constraint violation, and KKT stationarity residual.\", \"A brief summary report in Markdown citing at least two relevant academic papers with their arXiv IDs.\"], \"thinking_outline\": [\"1. Define the objective function (e.g., Mean Squared Error loss on a dataset) and the inequality constraint (e.g., the squared Frobenius norm of the network's weights must be less than or equal to a constant C).\", \"2. Construct the Lagrangian function for this constrained optimization problem by introducing a Lagrange multiplier (dual variable) for the inequality constraint.\", \"3. Derive the four Karush-Kuhn-Tucker (KKT) conditions for optimality: stationarity (gradient of the Lagrangian is zero), primal feasibility, dual feasibility, and complementary slackness.\", \"4. Implement a numerical solver, such as projected gradient descent or an augmented Lagrangian method, in Python to find the optimal weights and Lagrange multipliers.\", \"5. Validate that the numerically obtained solution satisfies all derived KKT conditions within a small tolerance (e.g., 1e-5), thereby confirming the solution's optimality.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent produces the theoretical derivations. The /general-search agent provides supporting literature to confirm the formulation. The /code agent implements the solver and generates numerical results to validate the theory. Finally, the router performs a quality assurance check. Verification: The router will check if the numerical KKT residuals from /code are below the tolerance specified in the acceptance criteria. Fallback: If verification fails, the task is routed back to the /math agent to re-check the gradient derivations.\", \"todo_list\": [\"- [ ] /math: Define the L2-regularized loss as a constrained problem and derive the corresponding KKT conditions.\", \"- [ ] /general-search: Find at least two papers on arXiv or JMLR discussing KKT analysis for neural network regularization.\", \"- [ ] /code: Implement a PyTorch-based solver for the constrained problem on a synthetic dataset.\", \"- [ ] /code: Instrument the code to track primal loss, constraint violation, and dual variables over training iterations.\", \"- [ ] /code: Validate that the final solution from the solver satisfies the analytical KKT conditions (stationarity, feasibility, slackness) within a tolerance of 1e-5.\", \"- [ ] /router: Review final report for consistency between mathematical derivations, cited literature, and numerical verification results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"constrained-optimization\", \"lagrangian-multipliers\", \"kkt-conditions\"], \"acceptance_criteria\": [\"- The mathematical derivation of all four KKT conditions is complete and correct.\", \"- The Python implementation converges to a solution where the L2 norm constraint is satisfied (violation < 1e-6).\", \"- The final numerical solution satisfies the complementary slackness condition (i.e., multiplier * constraint_gap = 0) with a tolerance less than 1e-5.\", \"- The final report includes correct citations for at least two relevant academic papers with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3823, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a Bayesian neural network for a regression task. Please derive the continuous-time stochastic differential equation (SDE) representation for Stochastic Gradient Langevin Dynamics (SGLD). Specifically, show how the discrete SGLD update rule is an Euler-Maruyama discretization of an overdamped Langevin SDE. Then, implement this in JAX, using the SDE formulation to inform the choice of a preconditioner matrix analogous to the Ornstein-Uhlenbeck process for better convergence and sampling efficiency. Validate the model's uncertainty quantification capabilities by analyzing its posterior predictive distribution and calibration.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian deep learning stochastic differential equations langevin dynamics convergence\\\", mode=web)\", \"/math(Derive the continuous-time Langevin SDE whose Euler-Maruyama discretization yields the SGLD update rule. Analyze its stationary distribution via the Fokker-Planck equation to confirm it matches the target Bayesian posterior.)\", \"/code(Implement a Bayesian neural network using JAX/Flax. Implement the SGLD sampler with a diagonal preconditioner, ensuring numerical stability and tracking convergence diagnostics like the Gelman-Rubin statistic., using Python)\", \"/math(Verify that the empirical moments of the sampled weights from the code implementation align with the theoretical stationary distribution. Formulate the Expected Calibration Error metric for the regression task., includes numpy)\"], \"route_rationale\": \"The blue general-search agent first gathers state-of-the-art literature on SDEs in Bayesian inference. This informs the /math agent's formal derivation of the SDE and its properties. The /code agent then translates this theory into a practical JAX implementation. A final /math step is crucial for verifying that the code's output empirically matches the theoretical derivations, ensuring correctness.\", \"expected_artifacts\": [\"- A LaTeX/PDF document with the full mathematical derivation connecting the SGLD update rule to the continuous-time Langevin SDE via the Fokker-Planck equation.\", \"- A Python script (`sde_bnn.py`) containing the JAX implementation of the Bayesian neural network and the SGLD sampler.\", \"- A summary report in Markdown (`report.md`) presenting the model's performance, including plots of the predictive posterior and calibration curves.\", \"- A JSON file (`metrics.json`) containing the final Negative Log-Likelihood and Expected Calibration Error.\", \"- A bibliography file (`references.bib`) with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Use search to find foundational papers on SGLD as a discretized SDE, focusing on works by Welling & Teh and subsequent analyses of its non-asymptotic convergence.\", \"2. Formally derive the SDE corresponding to SGLD and prove that its stationary distribution is the correct Bayesian posterior P(w|D). This involves analyzing the drift and diffusion terms and relating them to the Fokker-Planck equation.\", \"3. Implement the BNN and SGLD sampler in JAX. Pay close attention to the learning rate schedule and the injection of properly scaled Gaussian noise, which are critical for correct sampling.\", \"4. Train the model on a suitable regression benchmark (e.g., a UCI dataset) and collect samples from the posterior over weights after a burn-in period.\", \"5. Validate the implementation by computing empirical statistics (mean, variance) of the sampled weights and checking their consistency with the theoretical properties of the stationary distribution derived in step 2.\", \"6. Audit the model's uncertainty quality by computing the Expected Calibration Error (ECE) and generating reliability diagrams to ensure the predicted confidence intervals are meaningful.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (formal derivation) -> /code (implementation) -> /math (verification & metric derivation) -> router QA (verification: code's empirical moments match math derivation's theory; fallback: if divergence, /code agent must debug sampler implementation, focusing on noise scaling and step size schedule).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on SGLD and its connection to SDEs.\", \"- [ ] /math: Derive the SDE from the SGLD update rule and analyze its stationary distribution.\", \"- [ ] /code: Implement the Bayesian Neural Network and the SGLD sampler in JAX.\", \"- [ ] /code: Train the model and generate posterior samples for weights.\", \"- [ ] /math: Verify the statistical properties of the weight samples against the theoretical derivation.\", \"- [ ] /code: Compute final metrics (NLL, ECE) and generate visualization plots.\", \"- [ ] /router: Perform final QA review, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-differential-equations\", \"bayesian-inference\", \"langevin-dynamics\"], \"acceptance_criteria\": [\"- The mathematical derivation must explicitly state the SDE and use the Fokker-Planck equation to show its stationary distribution is proportional to exp(-U(w)), where U(w) is the potential.\", \"- The JAX implementation must run without errors and produce posterior samples that show convergence (e.g., via trace plots).\", \"- The final report must show an Expected Calibration Error (ECE) below 0.05 on the test set.\", \"- The project must cite at least two relevant papers, with arXiv IDs included in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3835, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm building a reinforcement learning project and need to understand policy gradients. Can you derive the REINFORCE algorithm from the policy gradient theorem, explain the role of a stochastic policy, and then implement it to solve the CartPole-v1 environment to show its convergence?\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient theorem for an episodic reinforcement learning objective, showing how the log-derivative trick leads to the REINFORCE update rule and discussing its convergence properties.)\", \"/general-search(query=\\\"site:arxiv.org REINFORCE 'policy gradient theorem' Simple Statistical Gradient-Following Algorithms\\\", mode=rag, context=\\\"Find the original paper by Williams (1992) and a modern tutorial that discusses practical implementation details like Bayesian variance reduction.\\\")\", \"/code(Implement the REINFORCE algorithm in Python using PyTorch to solve the Gymnasium CartPole-v1 classic control theory problem. The implementation must track and plot the episodic reward for visualization.)\"], \"route_rationale\": \"The route begins with /math to establish the core theoretical foundation of the policy gradient theorem. This is followed by the blue /general-search agent to locate the seminal academic paper and a modern tutorial, providing both historical context and practical implementation advice. Finally, /code translates the derived theory into a working agent to empirically validate the algorithm's effectiveness on a standard benchmark.\", \"expected_artifacts\": [\"A markdown document detailing the mathematical derivation of the policy gradient theorem and the REINFORCE algorithm.\", \"A Python script (`reinforce_cartpole.py`) that implements the REINFORCE agent for CartPole-v1.\", \"A PNG image (`reward_convergence.png`) plotting the total reward per episode over the course of training.\", \"A JSON file (`training_metrics.json`) containing the final average episodic reward and episode length.\", \"A short bibliography citing at least two authoritative sources with arXiv IDs.\"], \"thinking_outline\": [\"1. Define the objective function J(\u03b8) for a stochastic policy \u03c0_\u03b8 in a reinforcement learning setting as the expected total reward.\", \"2. Apply the log-derivative trick to derive the policy gradient theorem, expressing \u2207J(\u03b8) as an expectation of \u2207log(\u03c0_\u03b8) * R(\u03c4), which can be estimated from samples.\", \"3. Formulate the REINFORCE algorithm's stochastic gradient ascent update rule based on the derived gradient estimator.\", \"4. Implement the policy network and the REINFORCE training loop in Python to interact with the CartPole-v1 environment.\", \"5. Validate the implementation by running the training process and checking if the agent's average reward over the final 100 episodes surpasses the defined success threshold.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent produces the core derivation. The /general-search agent finds authoritative papers to validate the derivation and gather implementation tips. The /code agent implements the validated algorithm. Router QA performs the final check on the code's performance metrics. Fallback: If the agent fails to converge, the /code agent will revisit the findings from /general-search to implement a simple reward baseline to reduce variance and retry.\", \"todo_list\": [\"- [ ] Use /math to derive the REINFORCE algorithm's update rule from first principles.\", \"- [ ] Use /general-search to find two seminal papers or tutorials on policy gradients.\", \"- [ ] Use /code to implement the REINFORCE agent for the CartPole-v1 environment and generate a convergence plot.\", \"- [ ] Verify that the agent's final performance meets the primary acceptance metric.\", \"- [ ] Package all artifacts (derivation, code, plot, metrics, citations) and submit for router QA review.\"], \"difficulty\": \"introductory\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies the log-derivative trick to arrive at the policy gradient theorem.\", \"- The Python implementation successfully trains an agent to solve CartPole-v1, achieving an average reward greater than 195 over 100 consecutive episodes.\", \"- The final report includes a convergence plot clearly showing episodic rewards increasing over time.\", \"- At least two relevant arXiv papers are cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate the arithmetic mean of the total rewards accumulated in each of the final 100 episodes of training. A value > 195 is considered a success for CartPole-v1.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate the arithmetic mean of the number of timesteps the agent survived in each of the final 100 episodes.\"}}", "categories": ["math_first", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3872, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive and implement a Natural Policy Gradient (NPG) algorithm using the Fisher Information Matrix (FIM) as the metric tensor for a continuous control task. Investigate the convergence properties by analyzing the KL-divergence between policy updates, ensuring the trust region constraint is satisfied. Compare its sample efficiency and stability against a standard REINFORCE implementation, specifically for a challenging high-dimensional control theory problem.\n", "completion": "{\"route_plan\": [\"/math(Derive the Natural Policy Gradient update rule, expressing the Fisher Information Matrix as an expectation over the policy's score function. Analyze the preconditioning effect on the optimization landscape for a complex control theory problem, considering convergence criteria.)\", \"/general-search(query=\\\"site:arxiv.org natural policy gradient convergence proof Fisher Information Matrix KL-divergence trust region\\\", mode=rag)\", \"/code(Implement the NPG algorithm in PyTorch, using automatic differentiation for the score function and a conjugate gradient solver for the FIM inverse-vector product. Validate on a MuJoCo environment, tracking policy entropy and eigenvalue distribution of the empirical FIM., using Python)\"], \"route_rationale\": \"The plan starts with /math to establish the core theoretical foundation of NPG. This is followed by the blue /general-search agent to survey authoritative literature from arXiv for canonical proofs and implementation details. Finally, /code translates the theory and research into a working, validated implementation for empirical analysis.\", \"expected_artifacts\": [\"- A PDF with the mathematical derivation of the NPG update rule and the Fisher Information Matrix.\", \"- A Python script implementing the NPG agent and the continuous control environment.\", \"- A `metrics.json` file containing convergence data, including average return and KL-divergence per epoch.\", \"- A `README.md` file summarizing findings and citing at least two relevant arXiv papers.\", \"- A verification report comparing the theoretical KL-divergence constraint with empirical measurements from the code execution.\"], \"thinking_outline\": [\"1. Start with the mathematical formulation of the standard policy gradient theorem and identify the issues with vanilla gradient ascent in parameter space due to non-uniform sensitivity.\", \"2. Derive the Natural Policy Gradient by introducing the Fisher Information Matrix as a metric tensor on the manifold of policy distributions, leading to the update rule involving the inverse FIM.\", \"3. Develop a Python implementation using PyTorch and a continuous control environment. This must include an efficient method, like conjugate gradient, to compute the FIM-inverse-vector product without explicit matrix inversion.\", \"4. Verify the implementation's correctness by instrumenting the code to check that the empirical KL-divergence between policy updates remains below the trust region threshold.\", \"5. Run comparative experiments to benchmark NPG's performance (sample efficiency, final reward, stability) against a baseline REINFORCE algorithm, generating learning curves for analysis.\", \"6. Validate the final results by auditing the convergence plots and ensuring the reported metrics align with the expected behavior of trust region methods, particularly the constrained policy updates.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (literature review) -> /code (implementation & experimentation) -> router QA (verification: empirical KL-divergence from code must align with theoretical bounds from math; fallback: if mismatch, return to /code to debug FIM estimation or conjugate gradient solver).\", \"todo_list\": [\"- [ ] /math: Derive the NPG update `\u03b8_new = \u03b8_old + \u03b1 * F\u207b\u00b9 * \u2207J(\u03b8)`.\", \"- [ ] /math: Formulate the Fisher Information Matrix `F` as the expectation `E_\u03c0[\u2207log\u03c0(a|s)\u2207log\u03c0(a|s)\u1d40]`.\", \"- [ ] /general-search: Find canonical papers on Trust Region Policy Optimization (TRPO) and Natural Policy Gradients on arXiv.\", \"- [ ] /code: Implement the policy and value networks in PyTorch for a continuous action space environment like Pendulum-v1.\", \"- [ ] /code: Implement the conjugate gradient algorithm to efficiently solve the linear system `Fx = g` for the natural gradient direction.\", \"- [ ] /math: Verify that the code's KL-divergence calculation between two Gaussian policies matches the closed-form mathematical expression.\", \"- [ ] /code: Execute the training loop, generate convergence plots for average return vs. episodes, and serialize the final metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"optimization-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the NPG update and the FIM is present and correct.\", \"- The Python implementation runs successfully and demonstrates statistically significant improvement in sample efficiency or final performance over a REINFORCE baseline on the chosen benchmark.\", \"- The final report includes convergence plots showing average return over training episodes and correctly cites at least two relevant academic papers with arXiv IDs.\", \"- The empirical KL-divergence per policy update, as logged in the metrics file, remains bounded within the specified trust region delta (e.g., < 0.01).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3892, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the full ADMM update steps for training a feed-forward neural network with a nuclear norm constraint on each weight matrix to encourage low-rank representations. Formulate the augmented Lagrangian and show the primal and dual variable updates. Then, implement this in PyTorch and empirically validate the convergence properties and constraint satisfaction against theoretical guarantees, particularly focusing on the impact of the penalty parameter rho on the eigenvalue distribution of the learned weights.\n", "completion": "{\"route_plan\": [\"/math(Derive the augmented Lagrangian and full ADMM update rules for minimizing a loss function subject to a nuclear norm constraint. Analyze convergence conditions and the role of the penalty parameter, referencing the Bayesian interpretation.)\", \"/general-search(query=\\\"site:arxiv.org ADMM convergence guarantees deep learning nuclear norm regularization\\\", mode=web)\", \"/code(Implement the derived ADMM updates in PyTorch for a simple model. Design an experiment to track convergence of the objective function and primal/dual residuals, and analyze the resulting weight matrix eigenvalue decay., using Python)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation by deriving the ADMM algorithm. The 'blue' /general-search agent then gathers literature to ground the derivation in existing research and find convergence proofs. Finally, /code provides an empirical validation of the theory, checking if the implementation's behavior matches the mathematical predictions.\", \"expected_artifacts\": [\"A LaTeX document detailing the full derivation of the ADMM update rules and the augmented Lagrangian.\", \"A Python script implementing the ADMM-based training loop for a neural network.\", \"A summary report citing at least two arXiv papers on ADMM for machine learning.\", \"A verification plot showing objective value and constraint violation vs. iteration, confirming convergence.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem for neural network training with a nuclear norm penalty.\", \"2. Construct the augmented Lagrangian and derive the iterative update rules for the primal variables (weights) and the dual variable using the ADMM framework.\", \"3. Implement the derived algorithm in a Python deep learning framework, including the proximal operator for the nuclear norm (Singular Value Thresholding).\", \"4. Design an experiment to run the algorithm on a synthetic dataset, carefully monitoring objective value, primal and dual residuals to validate convergence.\", \"5. Verify that the final weight matrices exhibit a low-rank structure by inspecting their singular value spectrum, and check that this aligns with the theoretical expectations.\", \"6. Synthesize the mathematical derivation, code implementation, and empirical results into a cohesive report, referencing supporting literature.\"], \"handoff_plan\": \"/math derivation -> /general-search literature review -> /code implementation. The math agent's derivation of the update rules and proximal operator serves as the primary specification for the code agent. The search agent provides theoretical convergence guarantees that the code agent must empirically verify. Verification: Code agent must plot convergence metrics (objective, residuals) and confirm they decrease monotonically as predicted by theory. Fallback: If convergence is not observed, the code agent flags the issue, and the math agent re-audits the derivation for potential errors in the proximal operator or update steps.\", \"todo_list\": [\"- [ ] /math: Formulate the augmented Lagrangian for the nuclear norm constrained problem.\", \"- [ ] /math: Derive the closed-form solutions for the primal and dual variable updates in the ADMM scheme.\", \"- [ ] /general-search: Find and summarize at least two key arXiv papers on ADMM for deep learning, focusing on convergence theory.\", \"- [ ] /code: Implement the ADMM training loop in PyTorch, including the Singular Value Thresholding operator.\", \"- [ ] /code: Design and run an experiment to generate convergence plots for objective value and constraint residuals.\", \"- [ ] /math: Verify that the empirical convergence rate from the /code agent's plots is consistent with the theoretical guarantees found by the /general-search agent.\", \"- [ ] /code: Validate that the learned weight matrices show the expected low-rank structure by plotting their singular value decay.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"admm\", \"deep-learning-regularization\"], \"acceptance_criteria\": [\"The mathematical derivation of the ADMM updates must be complete and self-contained.\", \"The Python implementation must successfully train a model and generate plots showing monotonic convergence of the objective function and residuals.\", \"The final report must correctly cite at least two relevant arXiv papers and connect their theory to the empirical results.\", \"The singular value plots of the trained weight matrices must show a clear 'elbow,' indicating a successful low-rank approximation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3971, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the evidence lower bound (ELBO) for a variational Bayesian neural network where the posterior over the weights is modeled by a stochastic differential equation (SDE). Use Girsanov's theorem to correctly handle the change of measure from the prior SDE process to the approximate posterior process. Subsequently, implement a solver in JAX using the Euler-Maruyama method and validate the convergence properties of the ELBO on a synthetic dataset, ensuring the variational posterior does not suffer from posterior collapse.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference neural stochastic differential equations Girsanov theorem bayesian\\\", mode=rag)\", \"/math(Derive the continuous-time ELBO for a Bayesian neural SDE using Girsanov's theorem to define the Radon-Nikodym derivative between the prior and variational posterior measures, ensuring all convergence criteria are explicitly stated.)\", \"/code(Implement a JAX-based solver for the derived variational SDE using the Euler-Maruyama scheme. The implementation must track the ELBO and include a routine for Bayesian model averaging over multiple posterior samples., using Python)\", \"/math(Verify the discrete-time implementation of the ELBO gradient estimator aligns with the continuous-time derivation, specifically checking the discretization of the It\u00f4 integral and the log-likelihood term under the Euler-Maruyama approximation.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational theory from arXiv. This informs the /math agent's formal derivation of the ELBO. The derived equations are then handed to the /code agent for a JAX implementation. A final call to the /math agent is crucial for verifying that the discretized code correctly represents the continuous-time theory, a common source of error in such models.\", \"expected_artifacts\": [\"A formal proof in LaTeX deriving the ELBO for the specified neural SDE model using Girsanov's theorem.\", \"A documented JAX/Flax Python script implementing the variational inference training loop and the SDE solver.\", \"A verification document comparing the discretized implementation against the continuous-time mathematical derivation, highlighting any approximation errors.\", \"A results summary including plots of ELBO convergence and posterior predictive distributions, with citations to at least two arXiv papers.\"], \"thinking_outline\": [\"1. Review state-of-the-art literature on variational inference for neural SDEs, focusing on the application of Girsanov's theorem to formulate the KL-divergence term of the ELBO.\", \"2. Formally derive the ELBO objective function, clearly defining the prior process, the variational posterior process, and the resulting Radon-Nikodym derivative.\", \"3. Implement the numerical solver (Euler-Maruyama) for the SDEs and the stochastic variational inference optimization loop in JAX, parameterizing the drift and diffusion terms with neural networks.\", \"4. Validate the numerical SDE solver implementation by testing it on a simple Ornstein-Uhlenbeck process, which has a known analytical solution, to check for correctness.\", \"5. Train the full model on the target task and analyze the ELBO convergence, the variance of the gradient estimator, and the quality of the learned posterior uncertainty.\", \"6. Verify that the gradient estimators implemented in the code are unbiased or have characterized bias with respect to the true gradient of the derived continuous-time ELBO.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (verification) -> router QA. The final mathematical verification step audits the code's logic. Fallback: If verification fails, the task is routed back to /code with specific correction guidance from the /math agent's report.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers from arXiv on Neural SDEs and the use of Girsanov's theorem in variational Bayesian methods.\", \"- [ ] /math: Derive the ELBO for the variational posterior SDE, clearly stating all assumptions and the application of Girsanov's theorem.\", \"- [ ] /code: Implement the SDE solver and the variational training loop in JAX, parameterizing the drift and diffusion terms with Flax Linen modules.\", \"- [ ] /code: Add logging to track the ELBO, the KL-divergence term, and the expected log-likelihood during training, and save metrics to a JSON file.\", \"- [ ] /math: Verify that the discretized gradient calculation in the code is a valid Monte Carlo estimator for the gradient of the derived continuous-time ELBO.\", \"- [ ] /code: Write unit tests for the SDE solver component against a known analytical solution (e.g., Geometric Brownian Motion) to check for correctness.\", \"- [ ] /router: Perform final QA check, ensuring the mathematical proof and code implementation are consistent and all acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-differential-equations\", \"bayesian-inference\", \"variational-inference\"], \"acceptance_criteria\": [\"The mathematical derivation of the ELBO is correct and rigorously applies Girsanov's theorem to handle the change of measure.\", \"The JAX implementation successfully runs, and the ELBO metric shows a statistically significant upward trend of convergence during training.\", \"The verification report confirms the consistency between the discretized code and the continuous-time derivation, with approximation errors analyzed.\", \"The final report cites at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Higher is better. Computed as the average ELBO over the final 10% of training iterations on the test set.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Higher is better. Computed on a held-out test set by averaging the log-likelihood over multiple forward passes (samples) from the learned posterior.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 3994, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nImplement and analyze a first-order Model-Agnostic Meta-Learning (MAML) algorithm. Your task is to derive the meta-update rule for a sinusoidal regression task and empirically demonstrate its convergence properties on a few-shot learning benchmark, comparing it against a pre-trained baseline.\n", "completion": "{\"route_plan\": [\"/math(Derive the first-order MAML meta-objective gradient using a Bayesian interpretation for the task-specific parameters, ensuring the derivation for the inner loop update is explicit and rigorous.)\", \"/general-search(query=\\\"site:arxiv.org model-agnostic meta-learning convergence analysis sinusoidal regression benchmark\\\", mode=web)\", \"/code(Implement the MAML algorithm in PyTorch with CUDA support for a 5-shot sinusoidal regression task, targeting a final meta-loss below 0.05 after 10000 meta-training iterations., using Python)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation by deriving the core MAML gradient. Next, the blue /general-search agent gathers context on convergence analysis and standard benchmarks to inform the implementation. Finally, /code is used to build and validate the algorithm based on the mathematical derivation and research findings.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full derivation of the first-order MAML gradient.\", \"A Python script (`maml.py`) implementing the MAML algorithm and the sinusoidal regression benchmark.\", \"A PNG plot comparing the few-shot adaptation performance of MAML against a fine-tuned baseline on unseen tasks.\", \"A `metrics.json` file reporting the final post-adaptation MSE for both MAML and the baseline.\", \"A `report.md` file summarizing the methodology, results, and including citations to at least two relevant papers.\"], \"thinking_outline\": [\"1. Formalize the MAML objective function as an expectation over a distribution of tasks, defining the inner and outer loop optimization steps.\", \"2. Use the chain rule to derive the gradient of the meta-objective with respect to the meta-parameters, carefully handling the dependency created by the inner-loop gradient descent update.\", \"3. Implement the data generation pipeline for sampling sinusoidal tasks and the main MAML training loop in PyTorch, including the baseline model for comparison.\", \"4. Validate the correctness of the implemented meta-gradient by comparing its output to a numerical approximation or PyTorch's higher-order automatic differentiation capabilities.\", \"5. Execute the training and evaluation, producing plots that visualize the model's ability to quickly adapt to new sinusoidal functions from only a few examples.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The formal derivation from /math provides the exact gradient formula for the /code agent to implement. /general-search provides implementation details and performance expectations. /code produces the final artifacts (code, plots, metrics). Router QA performs the final review. Verification: The reported post-adaptation MSE in `metrics.json` must be lower for MAML than the baseline. Fallback: If MAML does not outperform the baseline, the /code agent will be instructed to debug the implementation, focusing on the meta-gradient calculation and learning rates, and re-run the experiment.\", \"todo_list\": [\"- [ ] /math: Provide a step-by-step derivation of the first-order MAML meta-gradient.\", \"- [ ] /general-search: Find and summarize two seminal papers on MAML, noting any common implementation details for the sinusoidal regression problem.\", \"- [ ] /code: Implement the sinusoidal task generator according to the specification in the original MAML paper.\", \"- [ ] /code: Write the PyTorch code for training MAML and the fine-tuning baseline model.\", \"- [ ] /code: Validate that the MAML model adapts faster (achieves lower loss with fewer steps) on test tasks than the baseline and generate comparison plots.\", \"- [ ] /router: Review all artifacts, ensuring the derivation is correct, the code is runnable, and the results support the conclusions.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"meta-learning\", \"few-shot-learning\", \"gradient-based-optimization\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for the MAML meta-gradient must be complete and correct.\", \"- The MAML implementation must achieve at least a 20% lower average post-adaptation Mean Squared Error on test tasks compared to the fine-tuning baseline.\", \"- The final report must contain a plot clearly showing the pre-adaptation and post-adaptation predictions of the MAML model on a sample test task against the ground truth.\", \"- The submitted code must be executable and include a `requirements.txt` file to ensure reproducibility.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4071, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need an optimal solution for the 'Maximum Weight k-Path Cover on a Tree with Colored Vertices' problem. Given a tree with N vertices, each with a weight and a color, find a set of vertex-disjoint paths, at most K of them, such that the total weight of vertices in these paths is maximized, with the constraint that no path can contain more than two vertices of the same color. This requires a complex tree DP state, possibly involving bitmasking. I need a formal proof of optimality for the DP recurrence, a well-commented Python implementation optimized for N up to 10^5, and a complexity analysis demonstrating it can pass a typical 1-second time limit.\n", "completion": "{\"route_plan\": [\"/math(Derive the dynamic programming recurrence for the k-path cover on a tree, proving its optimality and convergence to the global maximum. The state must handle path counts and color constraints, with a full analysis of the time and space complexity.)\", \"/general-search(query=\\\"site:arxiv.org OR site:cs.stackexchange.com 'tree dp' 'k-path cover' complexity OR 'meet in the middle' on trees 'causal structure'\\\", mode=web)\", \"/code(Implement the tree DP algorithm in Python, ensuring the runtime complexity matches the theoretical analysis. Store a large suite of generated test cases in a simple file-based database, like JSONL, for regression testing.)\"], \"route_rationale\": \"The route starts with /math to establish a formal, provably correct model for the complex DP state and transitions. The blue /general-search agent is then used to survey existing literature on authoritative sites for similar advanced algorithmic patterns, validating our approach. Finally, /code translates the verified mathematical model into an optimized, well-tested Python implementation.\", \"expected_artifacts\": [\"A formal mathematical document detailing the DP state, recurrence relations, and proof of optimality.\", \"Optimized Python source code for the tree DP solution, including comments and usage examples.\", \"A complexity analysis report (time and space) with verification against test cases.\", \"A list of citations from authoritative sources (e.g., cp-algorithms, IOI problem editorials) for related techniques.\", \"A verification log detailing the alignment between the mathematical model and the final code.\"], \"thinking_outline\": [\"1. Deconstruct the problem: Identify the core components as a tree traversal problem, a resource constraint (k paths), and a state constraint (colors). The objective is maximization.\", \"2. Formulate the DP state and recurrence: Define a multi-dimensional DP state on the tree nodes (e.g., `dp[u][state]`) that captures all necessary information for merging subproblems, including the number of paths used, the connection status of the current node `u`, and the color constraints within paths passing through `u`'s subtree.\", \"3. Prove correctness and analyze complexity: Use structural induction on the tree to prove that the DP recurrence correctly computes the optimal solution. Analyze the time and space complexity based on the state size and transitions.\", \"4. Validate the DP formulation: Cross-reference the proposed DP state and transitions against known solutions for similar problems found via search. Check for off-by-one errors in path counting or state merging logic.\", \"5. Implement and test the algorithm: Translate the mathematical recurrence into an efficient implementation using post-order traversal (DFS). Develop a comprehensive test suite including small handcrafted cases, edge cases (star graphs, linear graphs), and larger randomly generated trees to validate correctness and performance.\", \"6. Verify implementation against the formal model: Audit the code to ensure the state transitions, base cases, and merging logic perfectly match the mathematical derivation from step 2. Check that constraints on K and colors are correctly enforced.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent produces the formal DP recurrence. This is handed to /general-search for validation against known advanced techniques. The validated recurrence is passed to /code for implementation. (Verification: If /general-search finds a superior or fundamentally different standard approach, it must flag for review, potentially requiring a new plan starting from /math. Fallback: If the initial DP state proves to have intractable complexity, fallback to /general-search to investigate approximation algorithms). The final code and proofs are reviewed by the router QA.\", \"todo_list\": [\"- [ ] /math: Define the DP state for the tree path cover problem, considering path count, node connection state, and color constraints.\", \"- [ ] /math: Derive the full recurrence relations for merging DP states of children nodes and prove optimality.\", \"- [ ] /general-search: Find reference implementations or editorials for similar IOI-level tree DP problems to benchmark the proposed state complexity.\", \"- [ ] /code: Implement the tree DP using a post-order traversal (DFS) in Python.\", \"- [ ] /code: Create a test generator for trees and write unit tests for correctness and performance profiling.\", \"- [ ] /math: Verify that the implemented code logic is a one-to-one mapping of the derived mathematical recurrence.\", \"- [ ] /code: Validate the final implementation against the problem constraints (N <= 10^5, K, time limit) on generated test data.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"dynamic-programming\", \"graph-theory\", \"competitive-programming\"], \"acceptance_criteria\": [\"- The submitted Python code must solve the problem correctly for all provided test cases, including edge cases, within the specified time limits.\", \"- The mathematical proof of the DP recurrence's optimality must be sound and formally presented.\", \"- The complexity analysis must correctly state the time and space requirements and be consistent with empirical performance measurements.\", \"- At least two relevant citations from authoritative competitive programming sources must be provided.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4112, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the implicit regularization effects in deep learning and want to explore the connection between the Hessian eigenspectrum and the generalization gap. Can you derive the theoretical link between the sharpness of the loss minimum, as measured by the Hessian's top eigenvalue, and established generalization bounds? Furthermore, please provide a numerical experiment to empirically validate this relationship by tracking the spectral norm of the Hessian during training for a ResNet on CIFAR-10 and correlating it with the test-train accuracy gap.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Hessian spectral analysis generalization bounds deep learning sharpness\\\", mode=web)\", \"/math(Derive the formal relationship between the Hessian's top eigenvalue and generalization bounds for deep networks, referencing at least one proof structure from an arXiv paper found previously.)\", \"/code(Implement power iteration in PyTorch with CUDA support to track the Hessian's max eigenvalue during training of a ResNet, ensuring numerical stability and plotting its convergence against the generalization gap., using Python)\", \"/math(Verify the empirical correlation between the final Hessian eigenvalue and the test error against the derived theoretical bounds, auditing any significant deviations from the expected convergence behavior.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to survey recent literature on this advanced topic. The findings are passed to /math for a rigorous theoretical derivation. Then, /code is used for empirical validation of the theory. A final /math step is crucial for cross-validating the experimental results against the derived mathematical framework, ensuring consistency.\", \"expected_artifacts\": [\"- A formal mathematical derivation in LaTeX connecting the Hessian's spectral norm to generalization bounds.\", \"- Python source code implementing the eigenvalue tracking experiment for a ResNet model.\", \"- A plot visualizing the correlation between the largest Hessian eigenvalue and the generalization gap over training epochs.\", \"- A summary report including the derivation, experimental results, and a bibliography citing at least two relevant arXiv papers.\", \"- A JSON file containing the final computed metrics.\"], \"thinking_outline\": [\"1. Conduct a literature review to find state-of-the-art theoretical results and proofs connecting the Hessian matrix's spectral properties to generalization in deep learning.\", \"2. Formulate and write a rigorous mathematical derivation linking the maximum eigenvalue (spectral norm) of the Hessian to a known generalization bound, citing the reviewed literature.\", \"3. Implement a numerically stable method, such as the power iteration algorithm, to efficiently compute the top eigenvalue of the Hessian for a deep neural network during the training loop.\", \"4. Validate the power iteration implementation by checking its convergence on a smaller, known matrix to ensure correctness before applying it to the large neural network Hessian.\", \"5. Run the experiment, collecting data on the top eigenvalue, training loss, and test loss at regular intervals.\", \"6. Analyze the results and verify that the observed empirical relationship between the measured eigenvalue and the generalization gap aligns with the predictions from the theoretical derivation.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides foundational papers to /math. /math produces a formal derivation that /code must implement. /code generates empirical data and plots. The final /math agent verifies that the empirical data from /code is consistent with the theory from the first /math step. Fallback: If the empirical results from /code strongly contradict the theory, it triggers a review loop back to /math to check for flawed assumptions in the derivation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two recent, highly-cited arXiv papers on Hessian spectrum and generalization.\", \"- [ ] /math: Produce a detailed, step-by-step derivation based on the literature review.\", \"- [ ] /code: Write a Python script using PyTorch to train a ResNet on CIFAR-10.\", \"- [ ] /code: Integrate a power iteration module to compute and log the Hessian's top eigenvalue per epoch.\", \"- [ ] /math: Verify that the logic in the power iteration code correctly reflects the mathematical algorithm.\", \"- [ ] /code: Generate plots showing the top eigenvalue and generalization gap vs. training epochs.\", \"- [ ] /math: Compare the final plot from /code with the derived theory and write a concluding analysis.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"generalization-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation is rigorous, self-contained, and correctly references source material.\", \"- The Python code runs without errors and successfully reproduces the experiment, generating the specified plots.\", \"- The final report clearly demonstrates a correlation (or lack thereof) between the Hessian eigenvalue and generalization gap, supported by both the derivation and the plots.\", \"- At least two relevant arXiv papers are cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the Pearson correlation coefficient between the time series of the Hessian's max eigenvalue and the time series of (test_loss - train_loss).\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the average number of power iterations required for the top eigenvalue estimate to converge within a tolerance of 1e-4 at each measurement step.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4138, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to solve the 2D incompressible Navier-Stokes equations for lid-driven cavity flow using a Physics-Informed Neural Network (PINN). Please derive the full residual loss formulation, implement it in JAX utilizing automatic differentiation for the differential operators, and validate the convergence properties and physical plausibility of the resulting velocity and pressure fields. I am particularly interested in how Fourier feature embeddings affect the spectral bias and convergence rate.\n", "completion": "{\"route_plan\": [\"/math(Derive the residual loss function for the 2D incompressible Navier-Stokes equations, expressing continuity and momentum equations. Formulate the total loss for a neural network ansatz, considering the Euler-Lagrange framework and its expected convergence properties.)\", \"/general-search(query=\\\"site:arxiv.org \\\\\\\"physics informed neural networks\\\\\\\" \\\\\\\"navier stokes\\\\\\\" fourier features spectral bias\\\", mode=both)\", \"/code(Implement a PINN in JAX to solve the lid-driven cavity flow problem using the derived Navier-Stokes residual. Use Fourier feature embeddings for the neural network, implement an Adam optimizer, and validate the model's convergence and physics-based loss reduction., using Python)\"], \"route_rationale\": \"The /math tool is used first to establish the theoretical foundation by deriving the specific PDE loss function. The blue /general-search agent is then used to gather state-of-the-art context and implementation details from authoritative sources like arXiv. Finally, the /code tool synthesizes the mathematical theory and research insights into a verifiable Python implementation.\", \"expected_artifacts\": [\"A formal mathematical derivation of the PINN loss function for the 2D Navier-Stokes equations.\", \"A complete Python script using JAX/Flax to train the PINN and solve the lid-driven cavity problem.\", \"A summary report containing convergence plots (loss vs. epoch) and visualizations of the final velocity/pressure fields.\", \"A verification log comparing the symbolic derivatives from the derivation with the JAX automatic differentiation outputs on a test function.\", \"A bibliography with at least two cited arXiv papers on PINNs for fluid dynamics.\"], \"thinking_outline\": [\"1. Formulate the 2D steady-state, incompressible Navier-Stokes equations (momentum and continuity) as a system of partial differential equations.\", \"2. Derive the physics-informed residual loss function by substituting a neural network surrogate model for velocity and pressure, u(x, y; \u03b8) and p(x, y; \u03b8), into the PDEs.\", \"3. Verify the mathematical derivation of the partial derivatives (e.g., Laplacian of velocity, pressure gradient) required for the residual loss by hand on a simple polynomial.\", \"4. Implement the PINN architecture with Fourier feature embeddings and the complete loss function in Python using JAX, leveraging its automatic differentiation capabilities (`jax.grad`, `jax.vmap`) to compute the PDE residuals efficiently.\", \"5. Train the model on the lid-driven cavity flow problem, monitoring the convergence of the total loss and its individual components (boundary condition loss and physics residual loss).\", \"6. Validate the trained model by comparing the predicted velocity field's primary vortex structure against established benchmark results and checking that the divergence of the velocity field is close to zero across the domain.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (literature review) -> /code (implementation). The mathematical derivation provides the specific equations for the code agent. The literature search informs best practices for the network architecture and training. The code agent implements both and produces artifacts for final review. Router QA will verify the code's loss function implementation against the initial math derivation. Fallback: If the code fails to converge, re-engage the /math agent to check for potential sign errors or missing terms in the loss formulation.\", \"todo_list\": [\"- [ ] /math: Derive the complete residual loss for the 2D steady, incompressible Navier-Stokes equations.\", \"- [ ] /general-search: Find 2+ seminal papers on PINNs for fluid dynamics, focusing on Navier-Stokes solutions and spectral bias.\", \"- [ ] /code: Set up the JAX environment and define the neural network architecture using Flax with Fourier feature embeddings.\", \"- [ ] /code: Implement the physics-informed loss function using `jax.grad` to compute all required partial derivatives from the PDE.\", \"- [ ] /math: Verify that the symbolic derivatives from the derivation match the computational graph implemented in the JAX code for a simple test case.\", \"- [ ] /code: Train the PINN on the lid-driven cavity problem, log metrics (MSRE), and generate visualizations of the velocity field.\", \"- [ ] /code: Validate the final velocity field against known qualitative benchmarks for the lid-driven cavity problem.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde\", \"fluid-dynamics\", \"automatic-differentiation\"], \"acceptance_criteria\": [\"- The JAX implementation must converge, with the final physics-based mean squared residual error (MSRE) falling below 1e-4.\", \"- The final report must contain a plot of the velocity field showing the characteristic primary vortex of the lid-driven cavity flow.\", \"- The mathematical derivation of the PDE residual must be complete and correctly implemented in the code's loss function.\", \"- The bibliography must cite at least two relevant papers from arXiv with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4152, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project in quantum computing and need to grasp the fundamentals of error correction. Can you explain the theory behind the three-qubit bit-flip code, provide a Python simulation showing how it corrects a single X-gate error, and find some foundational papers on fault-tolerant computation?\n", "completion": "{\"route_plan\": [\"/math(Derive the quantum circuit and matrix representation for the three-qubit bit-flip code, including the encoding, a single X-gate error, and the decoding process. Explain the syndrome measurement and its relation to the Pauli operator eigenvalue.)\", \"/code(Write a Python simulation using Qiskit to implement the three-qubit bit-flip code. Inject a single X-gate error with Bernoulli probability on one qubit and verify that the syndrome measurement and correction circuit successfully restore the original state.)\", \"/general-search(query=\\\"site:arxiv.org \\\\\\\"quantum error correction\\\\\\\" \\\\\\\"stabilizer codes\\\\\\\" introductory tutorial\\\", mode=both)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation of the bit-flip code. This is followed by /code to provide a practical, verifiable simulation of the theory. Finally, the blue /general-search agent is used to gather authoritative literature for deeper context and citation.\", \"expected_artifacts\": [\"A markdown document with the mathematical derivation of the bit-flip code logic, including state vectors and matrix operators.\", \"A commented Python script (`simulation.py`) implementing the bit-flip code in Qiskit, demonstrating successful error correction.\", \"A JSON file (`citations.json`) containing at least two relevant arXiv paper IDs with summaries.\", \"A verification report in the main markdown file confirming that the Python simulation's fidelity matches theoretical predictions for single-qubit errors.\"], \"thinking_outline\": [\"1. Formulate the mathematical model of the three-qubit bit-flip code, defining the encoding, error, and decoding operators using Dirac notation and matrix representations.\", \"2. Detail the syndrome extraction process, explaining how measuring ancilla qubits can identify the error's location without collapsing the logical state.\", \"3. Implement this entire process in a Python quantum simulation framework (e.g., Qiskit), demonstrating the encoding of a logical qubit, the application of a random single-qubit bit-flip error, and the full correction procedure.\", \"4. Verify that for any single-qubit bit-flip error, the simulation successfully restores the initial logical state with a fidelity approaching 1.0, and document this in a metrics report.\", \"5. Research and summarize the historical context and key concepts from seminal papers on quantum error correction, providing proper citations.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math provides the specification for the /code simulation. The code's output must validate the math. /general-search provides context. The final package is sent to router QA for review. Verification: The final state fidelity in the simulation must be > 0.999 for any single bit-flip error. Fallback: If verification fails, return to the /code agent with logs to debug the quantum circuit implementation.\", \"todo_list\": [\"- [ ] Use /math to derive the theory and operators for the 3-qubit bit-flip code.\", \"- [ ] Use /code to build a Python Qiskit simulation demonstrating the code's function against single X-gate errors.\", \"- [ ] Use /general-search to find foundational literature on quantum error correction and stabilizer codes.\", \"- [ ] Verify that the simulation's final state fidelity aligns with the mathematical derivation's prediction of perfect correction for single bit-flips.\", \"- [ ] Submit all artifacts (derivation, code, citations, metrics) for final router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"quantum-computing\", \"error-correction\", \"linear-algebra\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly represents the encoding, error, and decoding matrices/circuits for the bit-flip code.\", \"- The Python simulation successfully corrects a single X-gate error on any of the three data qubits and reports a final state fidelity > 0.999.\", \"- At least two relevant arXiv preprints on quantum error correction are cited with IDs and brief summaries.\", \"- The measured syndrome bits in the simulation correctly identify the qubit where the error was injected.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4228, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for a Bayesian logistic regression model with a mean-field variational family. Then, implement coordinate ascent variational inference (CAVI) to approximate the posterior, ensuring convergence checks are in place and the implementation is benchmarked.\n", "completion": "{\"route_plan\": [\"/math(Derive the complete Evidence Lower Bound (ELBO) and the coordinate-ascent variational inference (CAVI) update equations for a Bayesian logistic regression model assuming a mean-field variational family over the regression coefficients. Ensure all steps of the Bayesian derivation are explicit.)\", \"/general-search(query=\\\"site:scikit-learn.org site:jmlr.org bayesian logistic regression benchmark dataset CAVI\\\", mode=rag)\", \"/code(Implement the derived CAVI algorithm in Python using NumPy/SciPy for a Bayesian logistic regression model. Validate convergence by monitoring the ELBO and compare the final model's AUPRC against established benchmarks on the selected dataset.)\"], \"route_rationale\": \"The /math tool is required for the initial theoretical derivation of the ELBO and update equations. The blue /general-search agent is then used to find an appropriate public dataset and established performance benchmarks for validation. Finally, the /code tool implements the derived algorithm and performs the empirical evaluation against the benchmark.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the ELBO and CAVI update equations.\", \"A standalone Python script implementing the CAVI algorithm.\", \"A summary report in markdown format detailing the benchmark results, including ELBO convergence plots and final AUPRC scores.\", \"A verification log confirming that the ELBO is non-decreasing at every iteration of the algorithm.\", \"A list of citations for the algorithm and benchmark dataset used.\"], \"thinking_outline\": [\"1. Define the probabilistic graphical model for Bayesian logistic regression, specifying priors for the coefficients and the Bernoulli likelihood function.\", \"2. Formulate the mean-field variational inference objective by defining the variational distribution and deriving the Evidence Lower Bound (ELBO) specific to this model.\", \"3. Derive the closed-form coordinate-ascent update equations for each variational parameter by taking functional derivatives of the ELBO.\", \"4. Implement the CAVI algorithm, ensuring the ELBO is calculated and monitored at each step to verify that it is non-decreasing, which is a key property of the algorithm.\", \"5. Validate the implementation by running it on a standard dataset and comparing the posterior's predictive performance (AUPRC) against a known benchmark.\"], \"handoff_plan\": \"/math (equations) -> /general-search (dataset/benchmark) -> /code (implementation & evaluation) -> router QA (verification: Check that the implemented ELBO is non-decreasing and AUPRC score is close to benchmark; fallback: Request debugging from /code if ELBO decreases or AUPRC is anomalous).\", \"todo_list\": [\"- [ ] /math: Derive the ELBO and CAVI update equations for the specified Bayesian logistic regression model.\", \"- [ ] /general-search: Identify a suitable binary classification dataset and any published CAVI benchmark AUPRC scores for it.\", \"- [ ] /code: Implement the CAVI algorithm based on the equations provided by the math agent.\", \"- [ ] /code: Instrument the implementation to track and plot the ELBO value at each iteration to monitor convergence.\", \"- [ ] /code: Validate the implementation by ensuring the ELBO is non-decreasing and comparing the final AUPRC to the discovered benchmark.\", \"- [ ] /router: Review all artifacts for correctness, consistency, and fulfillment of the acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"variational-inference\", \"bayesian-methods\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"The mathematical derivation of the ELBO and CAVI updates is complete, self-contained, and mathematically sound.\", \"The Python code correctly implements the CAVI algorithm, and the ELBO is empirically shown to be non-decreasing across all iterations in the convergence plot.\", \"The final report includes a convergence plot and an AUPRC score on a standard dataset that is within 5% of an established public benchmark.\", \"The provided code is runnable and includes clear comments explaining the implementation of the core CAVI update steps.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Computed on a held-out test set to evaluate the model's predictive performance, particularly for imbalanced datasets.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"The number of iterations required for the relative change in ELBO to fall below a tolerance threshold (e.g., 1e-6), measuring the speed of convergence.\"}}", "categories": ["math_first", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4270, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the coordinate ascent variational inference (CAVI) update rules for a Bayesian logistic regression model with Gaussian priors. Implement this algorithm in Python and validate its convergence on a synthetic dataset, comparing the learned posterior approximation's properties.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) and the coordinate ascent update equations for a fully-factorized variational approximation in a Bayesian logistic regression model, analyzing the conditions for convergence.)\", \"/code(Implement the derived CAVI updates in Python using NumPy. Validate the implementation on a synthetic dataset generated from a Bernoulli likelihood, and plot the ELBO to verify monotonic convergence.)\", \"/general-search(query=\\\"site:arxiv.org 'variational inference' 'Bayesian logistic regression' convergence proof\\\", mode=both)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation by deriving the variational update equations. This is followed by /code to translate the theory into a working implementation and validate it empirically. Finally, the blue /general-search agent is used to gather authoritative academic references to contextualize and justify the chosen variational Bayesian methodology.\", \"expected_artifacts\": [\"A formal mathematical derivation of the ELBO and CAVI update rules.\", \"A Python script implementing the CAVI algorithm and a synthetic data generator.\", \"A PNG plot showing the ELBO's monotonic convergence over iterations.\", \"A brief markdown report summarizing the results and comparing the learned posterior means to the ground truth parameters, including at least two citations.\"], \"thinking_outline\": [\"1. Define the complete probabilistic model for Bayesian logistic regression, specifying the likelihood (Bernoulli) and priors (Gaussian).\", \"2. Formulate the Evidence Lower Bound (ELBO) using a mean-field variational family, where the posterior approximation factorizes over the model parameters.\", \"3. Derive the closed-form coordinate ascent update equations for each variational parameter by taking functional derivatives of the ELBO.\", \"4. Implement the CAVI algorithm in Python, including a function to generate synthetic data from a known logistic regression model.\", \"5. Validate the implementation by confirming that the ELBO is non-decreasing at each iteration and by measuring the error between the learned variational posterior means and the true parameters of the data-generating process.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent produces derivations that the /code agent uses as a blueprint for implementation; the /code agent's empirical convergence plot serves as a preliminary verification of the math. The /general-search agent provides literature to support the report. The final package is audited by the router. Fallback: If the ELBO fails to converge in the /code step, the task is routed back to /math for a derivation audit.\", \"todo_list\": [\"- [ ] /math: Derive the complete ELBO and CAVI update rules for the specified model.\", \"- [ ] /code: Implement the CAVI algorithm based on the mathematical derivation provided by the /math agent.\", \"- [ ] /code: Generate synthetic data and create a plot visualizing the ELBO's convergence.\", \"- [ ] /code: Validate the learned posterior parameters against the known ground truth of the synthetic data and log the metrics.\", \"- [ ] /general-search: Find and summarize two key papers that formalize variational inference for generalized linear models.\", \"- [ ] /router: Perform final QA check on all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"variational-inference\", \"bayesian-methods\", \"graphical-models\"], \"acceptance_criteria\": [\"The mathematical derivation of the CAVI update rules must be correct and self-contained.\", \"The Python implementation must execute without errors, and the generated ELBO plot must demonstrate monotonic, non-decreasing convergence.\", \"The 'Posterior Mean Absolute Error' metric must be less than 0.15 for the final learned parameters.\", \"The final report must include at least two relevant citations from arXiv or peer-reviewed journals, properly formatted.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4286, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on a large-scale computational fluid dynamics simulation involving the Navier-Stokes equations, which discretize into a massive, sparse, symmetric positive-definite linear system Ax=b. I need to implement a preconditioned conjugate gradient (PCG) solver in CUDA, but I'm struggling with selecting and implementing an efficient GPU-native preconditioner, like an incomplete Cholesky factorization (IC0). Can you derive the PCG algorithm, analyze its convergence properties with respect to the matrix condition number, and then implement a CUDA version using cuSPARSE to solve for the pressure field, validating its performance against a direct solver for a small test case?\n", "completion": "{\"route_plan\": [\"/math(Derive the preconditioned conjugate gradient algorithm for a symmetric positive-definite system, analyzing its convergence rate in terms of the preconditioned matrix's eigenvalue distribution and its impact on computational complexity.)\", \"/general-search(query=\\\"site:arxiv.org site:nvidia.com gpu incomplete cholesky factorization cusparse preconditioner conjugate gradient convergence\\\", mode=rag)\", \"/code(Implement the PCG solver with an incomplete Cholesky preconditioner in Python using CUDA via CuPy, referencing the cuSPARSE library for sparse matrix operations. Validate correctness and measure convergence against a direct solver.)\"], \"route_rationale\": \"/math is first to establish the theoretical foundation of the PCG algorithm and its convergence criteria. The blue /general-search agent then investigates practical, high-performance GPU implementations of preconditioners, which are crucial for efficiency. Finally, /code synthesizes the theory and research into a concrete, verifiable CUDA implementation, profiling its performance as requested.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Preconditioned Conjugate Gradient algorithm.\", \"A performance analysis report comparing the custom CUDA solver's convergence and runtime against a baseline.\", \"A well-documented Python script implementing the solver using CuPy and cuSPARSE.\", \"A summary of findings from literature on GPU-native preconditioners with at least two cited arXiv papers.\", \"A verification log confirming the numerical equivalence between the PCG and direct solver results.\"], \"thinking_outline\": [\"1. Formally derive the Preconditioned Conjugate Gradient (PCG) algorithm from the standard Conjugate Gradient method, clearly defining the role of the preconditioner matrix M.\", \"2. Analyze the convergence rate of PCG, relating it to the condition number \u03ba(M\u207b\u00b9A). Explain why a good preconditioner makes this value close to 1.\", \"3. Research efficient GPU implementations of preconditioners for sparse matrices, specifically focusing on Incomplete Cholesky (IC0) factorization and its suitability for the cuSPARSE library.\", \"4. Implement the PCG solver in Python using CuPy to interface with CUDA. Use cuSPARSE for sparse matrix-vector products (SpMV) and construct the IC0 preconditioner.\", \"5. Validate the implementation's numerical accuracy by comparing its solution `x` for a sample sparse system `Ax=b` against the result from a high-precision direct solver (e.g., `scipy.sparse.linalg.spsolve`).\", \"6. Profile the GPU solver's performance (wall-clock time vs. iteration count) and verify that the observed convergence behavior aligns with the theoretical analysis from step 2.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: the code implementation must correctly translate the derived PCG algorithm, and the choice of preconditioner must be justified by the literature search; fallback: if IC0 implementation proves too complex, use a simpler diagonal/Jacobi preconditioner and analyze the performance trade-off).\", \"todo_list\": [\"- [ ] /math: Derive the step-by-step Preconditioned Conjugate Gradient (PCG) algorithm.\", \"- [ ] /math: Analyze the algorithm's convergence rate as a function of the eigenvalue spectrum of the preconditioned system M\u207b\u00b9A.\", \"- [ ] /general-search: Find at least two recent arXiv papers on efficient sparse preconditioners (e.g., Incomplete Cholesky) for GPU architectures.\", \"- [ ] /code: Implement the PCG solver in Python using CuPy, leveraging cuSPARSE for sparse operations.\", \"- [ ] /code: Implement the chosen preconditioner (e.g., IC0) based on literature findings.\", \"- [ ] /code: Create a validation testbed comparing the solver's output and performance against `scipy.sparse.linalg.spsolve`.\", \"- [ ] /math: Verify that the termination conditions used in the code implementation (e.g., residual norm tolerance) are consistent with the convergence theory.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"scientific-computing\", \"cuda\"], \"acceptance_criteria\": [\"The mathematical derivation of the PCG algorithm is correct and self-contained.\", \"The CUDA implementation successfully solves a test sparse linear system with a residual norm below a specified tolerance (e.g., 1e-9).\", \"The performance report demonstrates a significant speedup (e.g., >5x) for a sufficiently large matrix compared to a CPU-based iterative solver.\", \"The final report includes citations for at least two relevant academic papers with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4389, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for a Bayesian logistic regression model and implement a mean-field variational inference algorithm in Python to approximate the posterior. Validate the convergence of your implementation against a known dataset and compare its predictive performance to a baseline MCMC sampler.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) for a Bayesian logistic regression model using Gaussian priors. Detail the coordinate ascent updates for a mean-field variational approximation, focusing on the convergence properties of the Bayesian update rules.)\", \"/general-search(query=\\\"site:archive.ics.uci.edu OR site:scikit-learn.org binary classification dataset AND site:arxiv.org bayesian logistic regression MCMC baseline\\\", mode=both)\", \"/code(Implement the mean-field CAVI algorithm for Bayesian logistic regression in Python using NumPy/SciPy. Validate the algorithm's convergence by tracking the ELBO and compare the approximated posterior distributions against a Hamiltonian Monte Carlo (HMC) baseline. Measure the AUROC as a performance metric.)\"], \"route_rationale\": \"The /math tool is required for the initial theoretical derivation of the ELBO and update equations. The blue /general-search agent is then used to find a suitable public dataset and an authoritative baseline MCMC implementation for comparison. Finally, the /code tool is used to implement the derived algorithm and perform the empirical validation against the baseline.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the ELBO and the coordinate ascent variational inference (CAVI) update equations.\", \"A standalone Python script (`cavi_implementation.py`) implementing the mean-field VI algorithm.\", \"A Jupyter Notebook (`analysis.ipynb`) demonstrating the code, showing ELBO convergence plots, posterior visualizations, and a quantitative comparison against the MCMC baseline.\", \"A `metrics.json` file reporting the final converged ELBO value and the test set AUROC.\", \"A `references.md` file with citations for the dataset and relevant theoretical papers on variational inference.\"], \"thinking_outline\": [\"1. Formulate the Bayesian logistic regression model, explicitly defining the likelihood function (Bernoulli) and prior distributions (Gaussian) for the model parameters.\", \"2. Derive the Evidence Lower Bound (ELBO) objective function for this model under a mean-field assumption for the variational distribution q(\u03b2).\", \"3. Derive the closed-form coordinate ascent update equations for the parameters (mean and variance) of each factor in the variational distribution.\", \"4. Implement the Coordinate Ascent Variational Inference (CAVI) algorithm and validate its correctness by monitoring the ELBO to ensure it is monotonically non-decreasing at each iteration.\", \"5. Compare the resulting approximate posterior distributions (means and variances) to those obtained from a gold-standard MCMC sampler to assess the quality of the variational approximation.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: code outputs must show monotonic ELBO convergence and posterior moments that are reasonably close to the MCMC baseline; fallback: if ELBO decreases, flag the /math derivation for review; if posterior mismatch is large, flag the /code implementation for bugs).\", \"todo_list\": [\"- [ ] /math: Derive the full expression for the ELBO in the context of a Bayesian logistic regression model with Gaussian priors.\", \"- [ ] /math: Derive the specific coordinate ascent update equations for the mean and variance of the variational posterior.\", \"- [ ] /general-search: Identify a suitable real-world binary classification dataset (e.g., from UCI) and a canonical paper or tutorial on MCMC for this model to serve as a baseline.\", \"- [ ] /code: Implement the CAVI algorithm from scratch using NumPy, based on the mathematical derivations.\", \"- [ ] /code: Validate the implementation by plotting the ELBO at each iteration to confirm it is non-decreasing.\", \"- [ ] /code: Compare the learned variational posterior distributions to a baseline MCMC implementation by visualizing posteriors and comparing AUROC on a test set.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"variational-inference\", \"bayesian-statistics\", \"graphical-models\"], \"acceptance_criteria\": [\"The provided mathematical derivation of the ELBO and CAVI updates is complete and mathematically sound.\", \"The Python implementation runs without errors, and the generated plot empirically demonstrates that the ELBO is non-decreasing over iterations.\", \"The final report includes a clear visual comparison (e.g., overlaid density plots) of the variational posterior and the MCMC baseline posterior.\", \"The model achieves an AUROC score on the held-out test set that is within 5% of the baseline MCMC model's score.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4397, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to derive and implement a stochastic variational inference (SVI) algorithm for a hierarchical Bayesian topic model. Please focus on deriving the natural gradient updates for the Evidence Lower Bound (ELBO) under a mean-field assumption. Critically, incorporate Rao-Blackwellization and control variates to reduce the variance of the stochastic gradient estimator. How does the convergence behavior change, and can you formalize the variance reduction?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org stochastic variational inference natural gradients control variates Rao-Blackwellization\\\", mode=both)\", \"/math(Derive the mean-field ELBO and its natural gradient update equations for a hierarchical Bayesian model, incorporating control variates as sourced from the arXiv literature to ensure theoretical soundness.)\", \"/code(Implement the derived SVI updates in Python with JAX/NumPyro, including the control variate logic and a baseline without it, targeting verifiable convergence metrics on a synthetic dataset.)\", \"/math(Analyze the output logs from the code to empirically verify the gradient variance reduction and formally connect the results back to the theoretical convergence guarantees from the initial research.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to build a strong theoretical foundation from authoritative arXiv papers. This research informs the /math agent's formal derivation. The /code agent then translates this theory into a practical, verifiable implementation. A final /math step closes the loop by analyzing the empirical results from the code, ensuring the implementation's behavior aligns with the derived theory.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the ELBO and the SVI update equations with control variates.\", \"A Python script implementing both the baseline and the enhanced SVI algorithm.\", \"A set of plots comparing the ELBO convergence and gradient variance between the two implementations.\", \"A summary report (README.md) explaining the results and citing the foundational papers with their arXiv IDs.\", \"A JSON file containing the final computed metrics for convergence rate and gradient variance.\"], \"thinking_outline\": [\"1. Use search to identify seminal papers on Stochastic Variational Inference (SVI), specifically focusing on natural gradients and advanced variance reduction techniques like control variates and Rao-Blackwellization.\", \"2. Formulate a specific hierarchical Bayesian model (e.g., Latent Dirichlet Allocation) and derive the Evidence Lower Bound (ELBO) under the mean-field assumption.\", \"3. Derive the stochastic natural gradient update rules for the variational parameters, explicitly incorporating a control variate to minimize gradient variance.\", \"4. Implement both the naive SVI and the control variate-enhanced SVI algorithms in a probabilistic programming framework like NumPyro.\", \"5. Validate the implementation by instrumenting the code to log gradient statistics; check that the mean of the stochastic gradient is unbiased and that its variance is lower in the enhanced version.\", \"6. Execute a comparative experiment, plotting the ELBO trace over iterations for both algorithms to audit the improvement in convergence speed and stability.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides foundational papers (PDFs/links) to the math agent. The math agent produces a formal derivation (LaTeX/PDF) which serves as a blueprint for the code agent. The code agent generates execution logs and metric data (CSV/JSON) that are passed back to the math agent for final analysis and verification. The complete set of artifacts is then passed to the router for a final quality check. Fallback: If the math agent cannot reconcile the theory from multiple papers, it will flag the conflict and request the search agent to find a more definitive survey or tutorial paper.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize 2-3 key arXiv papers on SVI with natural gradients and control variates.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the ELBO and the stochastic natural gradient updates.\", \"- [ ] /code: Implement the SVI algorithm with the control variate mechanism and a baseline version.\", \"- [ ] /code: Create a benchmarking suite to run both algorithms and log ELBO and gradient variance per iteration.\", \"- [ ] /math: Verify that the implementation's gradient computation aligns with the formal derivation.\", \"- [ ] /math: Analyze the logged data to produce plots and calculate final metrics for the report.\", \"- [ ] /router: Perform final QA review on all artifacts, checking for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-inference\", \"stochastic-variational-inference\", \"probabilistic-programming\"], \"acceptance_criteria\": [\"- The provided Python code must be executable and reproduce the reported plots and metrics.\", \"- The final report must show a statistically significant reduction in gradient variance (p < 0.05) for the control variate implementation compared to the baseline.\", \"- The mathematical derivation must be complete, clearly defining all terms and justifying each major step by referencing the sourced literature.\", \"- The solution must cite at least two relevant arXiv pre-prints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate the average increase in the held-out data ELBO over sliding windows of 100 training iterations. A higher, more stable rate indicates better performance.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"For a key global parameter, compute the variance of its stochastic gradient estimate across 100 independent mini-batches of data. This should be significantly lower for the control variate method.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4437, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm exploring Generative Adversarial Networks and keep hearing about training instability. Can you explain the concept of a saddle point in non-convex optimization, derive its conditions using the Hessian matrix, and then visualize it for a simple function like z = x^2 - y^2?\n", "completion": "{\"route_plan\": [\"/math(Derive the first and second-order necessary conditions for a critical point to be a saddle point, focusing on the indefiniteness of the Hessian matrix and its associated eigenvalue properties.)\", \"/code(Implement a Python script using Matplotlib and NumPy to create a 3D surface plot for f(x, y) = x^2 - y^2, clearly highlighting the saddle point at the origin and annotating the plot with the computed Hessian eigenvalue signs.)\", \"/general-search(query=\\\"site:arxiv.org GAN training saddle point optimization convergence\\\", mode=both)\"], \"route_rationale\": \"The route starts with /math to establish the formal theory of saddle points and Hessian analysis. Next, /code provides a concrete visualization to build intuition, directly implementing the theoretical case. Finally, the blue /general-search agent is used to find authoritative literature that connects this foundational concept to the user's motivating application (GANs), justifying the importance of the topic.\", \"expected_artifacts\": [\"A markdown file with the mathematical derivation of saddle point conditions via the Hessian matrix.\", \"A Python script `saddle_point_visualization.py` that generates the 3D plot.\", \"An image file `saddle_point.png` showing the function surface and the saddle point.\", \"A list of at least two citations from arXiv related to saddle points in machine learning optimization.\", \"A verification log confirming the consistency between the math derivation and the code visualization.\"], \"thinking_outline\": [\"1. Define a critical point in multivariable calculus where the gradient is the zero vector.\", \"2. Introduce the second partial derivative test and the role of the Hessian matrix in classifying critical points.\", \"3. Formally derive the condition for a saddle point: an indefinite Hessian matrix, which implies the existence of both positive and negative eigenvalues.\", \"4. Apply this theory to f(x, y) = x^2 - y^2 by computing its gradient and Hessian at the origin (0,0).\", \"5. Validate the theoretical findings by creating a 3D plot that visually confirms the saddle shape and curvatures predicted by the Hessian's eigenvalues.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent provides the theoretical conditions for a saddle point. The /code agent receives these conditions, implements a visualization, and verifies that the plot's curvature matches the derived eigenvalue signs. The /general-search agent finds contextual papers. Finally, router QA audits all artifacts for consistency. Fallback: If the visualization contradicts the math, the Hessian calculation in the /math step is re-audited.\", \"todo_list\": [\"- [ ] /math: Derive saddle point conditions using the Hessian matrix and its eigenvalues.\", \"- [ ] /code: Implement Python visualization for the function f(x, y) = x^2 - y^2.\", \"- [ ] /general-search: Find 2+ arXiv papers on saddle points in GANs or other deep learning models.\", \"- [ ] /verification: Check that the signs of the Hessian eigenvalues from the math step correctly predict the curvatures shown in the Python plot.\", \"- [ ] /router-qa: Review all generated artifacts for correctness, clarity, and fulfillment of acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-matrix\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly identifies the saddle point condition as an indefinite Hessian matrix (mixed-sign eigenvalues).\", \"- The Python script runs without errors and produces a 3D plot that accurately represents the function f(x, y) = x^2 - y^2, clearly marking the origin.\", \"- The final report includes at least two relevant citations from arXiv with their IDs.\", \"- The primary metric, Eigenvalue Sign Mismatch, must be 0.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4549, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm new to causal inference. Can you walk me through the mathematics of Judea Pearl's 'do-operator' on a simple three-variable Structural Causal Model, and then write a Python simulation to verify how an intervention differs from simple observation?\n", "completion": "{\"route_plan\": [\"/math(Define a simple three-variable chain Structural Causal Model (X->Y->Z) and formally derive the post-intervention distribution P(Y, Z | do(X=x)). Contrast this causal quantity with the observational Bayesian conditional probability P(Y, Z | X=x).)\", \"/code(Implement the SCM in Python using Bernoulli random variables. Simulate two datasets: one observational, one interventional via the 'do-operator'. Empirically validate that the simulated interventional probabilities match the mathematical derivations.)\"], \"route_rationale\": \"The route starts with /math to establish the formal, theoretical foundation of the do-operator and its distinction from standard conditioning. It then proceeds to /code to provide an empirical, computational verification of the mathematical theory, which is crucial for building intuition in causal discovery.\", \"expected_artifacts\": [\"A formal mathematical derivation of the observational and interventional distributions.\", \"A Python script (.py) that implements the Structural Causal Model and performs the simulation.\", \"A summary table or plot comparing the theoretically derived probabilities with the empirically computed frequencies from the simulation.\", \"A brief report in Markdown explaining the results and the significance of the difference between seeing and doing.\"], \"thinking_outline\": [\"1. Define a simple three-variable Structural Causal Model (SCM), for example a chain graph Z -> X -> Y with linear or Bernoulli relationships.\", \"2. Use the SCM to derive the observational conditional distribution P(Y|X=x).\", \"3. Apply the 'graph surgery' interpretation of the do-operator to derive the interventional distribution P(Y|do(X=x)).\", \"4. Explicitly highlight the mathematical and conceptual differences between the two resulting expressions.\", \"5. Write a Python simulation to generate data from both the original and the 'mutilated' graph to validate that the empirical frequencies converge to the derived probabilities.\"], \"handoff_plan\": \"/math -> /code -> router QA. The /math agent will produce the exact analytical forms for the observational and interventional distributions. The /code agent will implement these and simulate data to check if the empirical results match the analytical formulas. Verification: The simulation's output probabilities must match the math agent's derivations within a small margin of error. Fallback: If they do not match, the code agent must flag the discrepancy and ask the math agent to re-check its derivations.\", \"todo_list\": [\"- [ ] /math: Derive the analytical expressions for P(Y|X=x) and P(Y|do(X=x)) for a simple chain SCM.\", \"- [ ] /code: Implement the SCM and write a simulation to generate both observational and interventional datasets.\", \"- [ ] /code: Compute and compare the empirical probabilities from the simulation against the mathematical derivations.\", \"- [ ] /router: Final QA review of mathematical correctness, code logic, and the verification results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"causal-inference\", \"structural-causal-model\", \"do-calculus\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly applies the rules of SCMs and the do-operator.\", \"- The Python simulation correctly implements both the observational model and the graph modification required for the intervention.\", \"- The final report shows that the empirical probabilities from the simulation match the theoretical probabilities with less than 5% relative error.\", \"- The explanation clearly articulates why P(Y|X=x) is not equal to P(Y|do(X=x)) in the chosen model.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4634, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the coordinate ascent variational inference (CAVI) updates for the evidence lower bound (ELBO) in a Bayesian Latent Dirichlet Allocation model. Implement this in Python to analyze convergence on a synthetic dataset and compare its performance against the ground truth parameterization.\n", "completion": "{\"route_plan\": [\"/math(Derive the full-data evidence lower bound (ELBO) for a Bayesian Latent Dirichlet Allocation model and then derive the coordinate ascent variational inference (CAVI) update equations for the variational parameters, ensuring all steps are justified by Jensen's inequality.)\", \"/general-search(query=\\\"site:arxiv.org site:jmlr.org Latent Dirichlet Allocation Blei 2003 coordinate ascent variational inference\\\", mode=rag)\", \"/code(Implement the derived CAVI algorithm in Python using NumPy. Validate the implementation on a synthetic dataset by plotting the ELBO at each iteration to confirm monotonic convergence and comparing the inferred topic-word distributions to the ground truth using the Jensen-Shannon divergence.)\"], \"route_rationale\": \"The route begins with /math to establish the theoretical foundation by deriving the ELBO and the CAVI update equations. This is followed by the blue /general-search agent to find canonical literature, which serves to verify the mathematical derivations and inform implementation details. Finally, /code is used to translate the verified theory into a working implementation and empirically validate its correctness and convergence properties.\", \"expected_artifacts\": [\"- A PDF report containing the complete mathematical derivation of the ELBO and CAVI updates.\", \"- A well-commented Python script (`cavi_lda.py`) that implements the model and validation.\", \"- A plot (`elbo_convergence.png`) showing the ELBO value versus iteration number.\", \"- A JSON file (`metrics.json`) reporting the final ELBO, and the mean Jensen-Shannon divergence between inferred and true topic distributions.\", \"- A bibliography with at least two cited papers in BibTeX format.\"], \"thinking_outline\": [\"1. Define the complete probabilistic graphical model for Latent Dirichlet Allocation (LDA), specifying all random variables, parameters, and their conditional dependencies.\", \"2. Formulate the evidence lower bound (ELBO) for this model using Jensen's inequality and a fully-factorized (mean-field) variational family.\", \"3. Derive the closed-form Coordinate Ascent Variational Inference (CAVI) update equations for each local variational parameter by taking the relevant functional derivatives of the ELBO.\", \"4. Implement the CAVI algorithm in Python, including a synthetic data generation process and a function to compute the ELBO at each step.\", \"5. Validate the implementation by running it on synthetic data, plotting the ELBO to check for monotonic increase, and quantifying the error between the inferred variational posteriors and the known ground truth parameters.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent produces the core equations. The /general-search agent finds reference papers to cross-check the derivations from /math. The /code agent implements the verified equations. The final code and report are handed to the router for QA. Verification: The router will check that the ELBO plot from the code is monotonically non-decreasing, as predicted by the theory. Fallback: If the ELBO decreases, the router will flag the implementation for a bug-fix cycle, comparing the code logic against the mathematical derivation.\", \"todo_list\": [\"- [ ] /math: Produce a complete, step-by-step derivation of the CAVI update equations for LDA.\", \"- [ ] /general-search: Find and summarize at least two canonical papers on LDA and its variational inference algorithm.\", \"- [ ] /code: Write a Python script to generate synthetic data from a known LDA model.\", \"- [ ] /code: Implement the CAVI algorithm using the update equations from the /math agent.\", \"- [ ] /code: Plot the ELBO at each iteration to validate convergence and compute final performance metrics.\", \"- [ ] /router: Perform final QA by verifying that the code's ELBO plot is monotonic and the derived equations match the cited literature.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"bayesian-networks\"], \"acceptance_criteria\": [\"- The mathematical derivation of the CAVI updates from the ELBO must be complete and correct.\", \"- The Python implementation must demonstrate a monotonically non-decreasing ELBO on the synthetic dataset over at least 50 iterations.\", \"- The final report must cite at least two canonical papers on Variational Inference for LDA, such as Blei et al., 2003.\", \"- The mean Jensen-Shannon divergence between the inferred and true topic-word distributions must be less than 0.1.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4648, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the policy gradient theorem with a generalized advantage estimator (GAE) and an entropy bonus for maximum entropy RL. Then, implement the REINFORCE algorithm with this baseline in JAX, demonstrating its variance reduction properties on a classic control theory benchmark like CartPole. Analyze the convergence rate and bootstrap confidence intervals for the expected return.\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient objective function J(\u03b8) with an entropy regularization term and a state-dependent baseline, showing how the generalized advantage estimator (GAE) provides an unbiased estimate of the advantage function for improved convergence in control theory problems.)\", \"/general-search(query=\\\"site:arxiv.org policy gradient theorem generalized advantage estimator entropy regularization convergence analysis\\\", mode=rag)\", \"/code(Implement REINFORCE with the derived GAE and entropy bonus in JAX for the CartPole-v1 environment. Track policy entropy and bootstrap confidence intervals for the cumulative reward, verifying reduced variance compared to a simple reward-to-go baseline., using Python)\"], \"route_rationale\": \"The plan starts with /math to establish the core theoretical foundation of the GAE-based policy gradient. The blue /general-search agent is then used to find seminal papers to corroborate the derivation and inform implementation details. Finally, /code implements the derived theory and empirically validates the performance claims about variance reduction and convergence.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the full mathematical derivation of the GAE-based, entropy-regularized policy gradient.\", \"A Python script implementing the REINFORCE algorithm with GAE in JAX.\", \"A results.json file containing final cumulative rewards, policy entropy, and bootstrap confidence intervals for both the GAE agent and a baseline.\", \"A brief report summarizing the empirical results and comparing them against the theoretical claims, including citations to at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Start with the standard policy gradient theorem and introduce a general state-dependent baseline b(s_t) to show it yields an unbiased estimator of the gradient.\", \"2. Formulate the Generalized Advantage Estimator (GAE) as a specific, advanced form of this baseline, deriving its recursive structure involving the trade-off parameter \u03bb.\", \"3. Augment the objective function with an entropy regularization term and derive the final gradient update rule for the maximum entropy RL agent.\", \"4. Implement the derived algorithm in Python using JAX, targeting a suitable environment like CartPole-v1 to test the control theory application.\", \"5. Validate the implementation by empirically comparing the learning curve's variance against a simpler baseline (e.g., vanilla reward-to-go without GAE).\", \"6. Check that the computed bootstrap confidence intervals for the cumulative reward are demonstrably tighter for the GAE-based agent, providing statistical evidence for the theoretical variance reduction.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent provides the formal derivation. The /general-search agent verifies this derivation against authoritative literature. The /code agent implements the verified derivation. The router performs a final QA check. Verification: The code implementation of the gradient must exactly match the formula derived in /math, and the empirical variance reduction must be statistically significant. Fallback: If variance reduction is not observed, the /code agent must re-audit its GAE implementation against both the derivation and the reference papers.\", \"todo_list\": [\"- [ ] /math: Derive the policy gradient objective with GAE and entropy regularization.\", \"- [ ] /general-search: Find authoritative papers on GAE and maximum entropy RL to support the derivation.\", \"- [ ] /math: Verify that the derived gradient expression is consistent with the sources found by /general-search.\", \"- [ ] /code: Implement the REINFORCE agent using the verified GAE gradient in JAX.\", \"- [ ] /code: Implement a baseline agent (e.g., simple reward-to-go) for comparison.\", \"- [ ] /code: Run experiments, collect metrics (cumulative reward, variance, entropy), and generate plots.\", \"- [ ] /math: Audit the code's implementation of the GAE calculation to ensure it matches the mathematical formula precisely.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"control-theory\"], \"acceptance_criteria\": [\"The mathematical derivation must be complete and self-contained, correctly showing that GAE is an unbiased estimator of the advantage function.\", \"The JAX implementation must run without errors and demonstrate a statistically significant reduction in reward variance compared to the baseline, as measured by a Levene test with p < 0.05.\", \"The final report must correctly interpret the results, citing at least two relevant arXiv preprints on policy gradients or GAE, including their full arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4683, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am investigating the training dynamics of generative adversarial networks. I need a formal analysis of saddle points in a representative non-convex loss landscape. Specifically, can you derive the update rule for a trust-region or Newton-CG method that leverages the Hessian's negative curvature to escape these saddles, validate its convergence properties empirically on a test function, and contextualize this with recent arXiv research on second-order optimization?\n", "completion": "{\"route_plan\": [\"/math(Characterize saddle points using Hessian eigenvalue analysis and derive the update rule for a Newton-CG method that exploits negative curvature to guarantee convergence to a local minimum, referencing control theory stability criteria.)\", \"/code(Implement the derived Newton-CG algorithm in Python/JAX for a canonical non-convex function like the monkey saddle. Track the optimization trajectory and the minimum Hessian eigenvalue at each step to verify adversarial escape dynamics.)\", \"/general-search(query=\\\"site:arxiv.org second-order optimization saddle point escape generative adversarial networks convergence\\\", mode=rag)\"], \"route_rationale\": \"The route follows a theory-to-practice-to-justification pattern. /math is used first to establish the rigorous mathematical foundation for the saddle-point escape algorithm. /code then provides an empirical, verifiable implementation of the derived theory. Finally, the blue /general-search agent is tasked with finding authoritative sources to connect this specific solution to the broader research landscape in non-convex optimization for deep learning.\", \"expected_artifacts\": [\"A PDF report containing the mathematical derivation of the saddle point conditions and the Newton-CG update rule.\", \"A Python script (`saddle_escape.py`) implementing the algorithm and generating a contour plot of the optimization trajectory.\", \"A JSON file (`metrics.json`) reporting the escape iteration count and final loss value.\", \"A summary of at least two relevant arXiv papers, explaining how they relate to the implemented method.\"], \"thinking_outline\": [\"1. Formally define a saddle point in terms of the gradient and the Hessian matrix's eigenvalues. Contrast this with local minima.\", \"2. Derive the update step for a second-order optimization method (e.g., Newton-CG) that explicitly computes a direction of negative curvature when at a saddle point.\", \"3. Implement the algorithm and a suitable non-convex test function (e.g., f(x,y) = x^3 - 3xy^2).\", \"4. Verify that the implementation's optimization path successfully moves away from the saddle point at (0,0) towards a local minimum.\", \"5. Use search to find foundational papers on second-order methods and saddle-point-free Newton methods.\", \"6. Audit the theoretical derivation and empirical results against the claims and theorems presented in the cited literature.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent produces a formal derivation (LaTeX) that serves as a specification for the /code agent. The /code agent implements this specification and generates empirical results (plots, metrics), which are used to validate the theory. The /general-search agent finds papers that provide context for the findings. The final handoff to the router involves a QA check. Verification: The logic in the Python script must directly correspond to the update rule derived in the math document. Fallback: If the code fails to converge, the router will re-engage the /math agent to check for potential flaws in the convergence proof or algorithm derivation.\", \"todo_list\": [\"- [ ] /math: Derive the first and second-order optimality conditions for a saddle point.\", \"- [ ] /math: Formulate the update rule for a trust-region or Newton-CG algorithm that uses Hessian-vector products.\", \"- [ ] /code: Implement a canonical non-convex function with a known saddle point (e.g., monkey saddle).\", \"- [ ] /code: Implement the saddle-point escape algorithm derived by the /math agent.\", \"- [ ] /math: Verify that the algorithm implemented in the code accurately reflects the mathematical derivation.\", \"- [ ] /code: Validate the algorithm by running it, plotting the escape trajectory, and confirming it moves to a lower loss region.\", \"- [ ] /general-search: Find and summarize two key arXiv papers on escaping saddle points in deep learning.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"second-order-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly uses the Hessian's spectral properties to define an escape direction.\", \"- The Python implementation demonstrates successful escape from a saddle point at (0,0) on a standard test function, visualized in a contour plot.\", \"- The final report cites at least two relevant arXiv papers (with IDs) and integrates their findings.\", \"- The `escape_iteration_count` in `metrics.json` is a positive integer, indicating the saddle was found and escaped.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4825, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the rate-distortion function for a generative adversarial network (GAN) under an adversarial robustness constraint, where distortion is measured by the Wasserstein distance to a target distribution and the rate quantifies the mutual information between latent variables and generated adversarial examples. Implement a Python simulation to empirically validate the theoretical bounds for a simple Bernoulli data distribution and analyze the convergence properties.\n", "completion": "{\"route_plan\": [\"/math(Derive the rate-distortion function R(D) for a generative model under adversarial attack, defining distortion D as the expected Wasserstein distance and rate R as the mutual information I(Z; X_adv) given a Bernoulli latent variable Z.)\", \"/code(Implement a PyTorch simulation to estimate the empirical rate-distortion curve for a toy GAN on a Bernoulli dataset, tracking convergence and the AUPRC of an adversarial classifier., using Python)\", \"/general-search(query=\\\"site:arxiv.org rate-distortion theory generative adversarial network adversarial robustness privacy mutual information\\\", mode=rag, context=\\\"Query authoritative sources to find theoretical justifications for using mutual information as a measure of adversarial information leakage in generative models, focusing on recent cybersecurity and federated learning applications.\\\")\"], \"route_rationale\": \"The route starts with /math to establish the core theoretical framework of the rate-distortion function. This theory is then handed to /code for empirical validation via simulation, which is a standard practice in computational sciences. Finally, the blue /general-search agent is tasked with contextualizing this specific application by finding relevant literature, ensuring the approach is grounded in state-of-the-art research.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the rate-distortion function R(D).\", \"A Python script (`rate_distortion_sim.py`) implementing the GAN simulation and plotting the empirical curve.\", \"A metrics report (`metrics.json`) containing the final Wasserstein distances, mutual information estimates, and AUPRC scores across the curve.\", \"A literature summary (`references.md`) citing at least two relevant arXiv pre-prints.\"], \"thinking_outline\": [\"1. Formalize the problem using information-theoretic principles, defining rate as mutual information I(Z; X_adv) and distortion as the Wasserstein distance E[W(P_gen, P_adv)].\", \"2. Derive the closed-form expression or tight bounds for the rate-distortion function R(D) for a specified simple distribution, such as a multivariate Bernoulli.\", \"3. Implement a generative model (e.g., a small GAN) and an adversary in Python to simulate the generation of adversarial examples.\", \"4. Empirically compute the rate-distortion points by varying a trade-off parameter and plot the resulting curve, tracking convergence diagnostics.\", \"5. Validate the empirical curve against the theoretical bounds derived in step 2, quantifying the gap and analyzing potential sources of estimation error.\", \"6. Audit the problem formulation and results against state-of-the-art research found via /general-search to confirm alignment with information-theoretic privacy and AI safety literature.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: code implementation must match the derived mathematical equations and the empirical results must align with the theoretical bounds; fallback: if the Wasserstein gap is large, /math re-audits the derivation for simplifying assumptions and /code re-verifies the estimation methods).\", \"todo_list\": [\"- [ ] /math: Derive the rate-distortion function R(D) for the specified adversarial scenario with a Bernoulli distribution.\", \"- [ ] /code: Implement the GAN, adversary, and the mutual information estimator in PyTorch.\", \"- [ ] /code: Generate the empirical R(D) plot and compute final performance metrics (AUPRC, Wasserstein distance).\", \"- [ ] /general-search: Find and summarize at least two recent arXiv papers on information-theoretic security for generative models.\", \"- [ ] /math: Verify that the Python code's distortion and rate calculations correctly implement the formal mathematical definitions from the derivation.\", \"- [ ] /code: Validate the convergence of the model training and the stability of the metric estimations across multiple runs.\", \"- [ ] /router: Perform final QA review, checking consistency between the theoretical derivation, empirical results, and cited literature.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"information-theory\", \"generative-ai-safety\", \"coding-theory\"], \"acceptance_criteria\": [\"The mathematical derivation of R(D) must be complete, self-contained, and mathematically sound.\", \"The Python simulation must successfully execute and produce a plottable rate-distortion curve that is qualitatively consistent with the theory (i.e., monotonically non-increasing).\", \"The final report must cite at least two relevant arXiv pre-prints with their corresponding IDs.\", \"The primary metric (Wasserstein Gap) between the theoretical bound and the empirical curve at the median rate must be below 0.1.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4897, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the convergence proof for a linearized Alternating Direction Method of Multipliers (ADMM) variant applied to a composite objective function with a non-smooth L1 penalty, specifically focusing on the role of the augmented Lagrangian parameter rho. Implement and validate this algorithm for a sparse logistic regression problem, comparing its convergence rate against standard proximal gradient descent under varying rho values and establishing theoretical conditions for convergence.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org site:stanford.edu/~boyd linearized ADMM convergence proof composite optimization lasso\\\", mode=rag)\", \"/math(Derive the primal-dual update steps for linearized ADMM on sparse logistic regression. Construct the convergence proof by analyzing the augmented Lagrangian's decrease, establishing sufficient conditions on the step size and rho parameter for guaranteed convergence.)\", \"/code(Implement the derived linearized ADMM algorithm and a proximal gradient descent baseline in Python/JAX for a sparse logistic regression problem. Track the objective function value and primal residual per iteration, leveraging CUDA for linear algebra operations.)\", \"/math(Analyze the convergence plots generated by the code. Verify that the empirical convergence rate aligns with the derived theoretical bounds and investigate the sensitivity of the ADMM algorithm to the augmented Lagrangian parameter rho, providing a formal interpretation of its impact on performance.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather authoritative literature on linearized ADMM convergence. This informs the /math agent's formal derivation and proof construction. The /code agent then implements this theory for empirical validation. A final /math step is crucial for synthesizing the theoretical and empirical results, verifying the proof against observed behavior.\", \"expected_artifacts\": [\"A LaTeX document containing the full derivation of the linearized ADMM updates and the complete convergence proof.\", \"A self-contained Python script implementing the linearized ADMM and the proximal gradient baseline, along with data simulation.\", \"A final report in PDF format summarizing the theoretical and empirical findings, including convergence plots and analysis of parameter sensitivity.\", \"A verification log file confirming the alignment between the theoretical proof's conditions and the empirical convergence rates observed.\", \"A bibliography file with citations for at least 2 relevant arXiv papers on ADMM.\"], \"thinking_outline\": [\"1. Review foundational literature on ADMM, its variants (like linearized ADMM), and convergence proofs for composite optimization problems, particularly those involving non-smooth regularizers.\", \"2. Formulate the sparse logistic regression problem within the ADMM framework, defining the objective function, constraints, and the augmented Lagrangian.\", \"3. Derive the specific primal-dual update rules for this problem using the linearized ADMM formulation. Construct a formal proof of convergence by analyzing the potential function decrease, identifying key conditions on hyperparameters like the step size and penalty parameter rho.\", \"4. Implement the algorithm and a baseline method (proximal gradient descent). Validate the implementation by checking intermediate variable shapes and ensuring the augmented Lagrangian value is non-increasing under the theoretical conditions.\", \"5. Execute experiments to generate convergence plots for both algorithms under various settings of the penalty parameter rho.\", \"6. Critically audit the empirical results. Verify that the observed convergence behavior is consistent with the theoretical proof and explain any discrepancies or performance trade-offs observed.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. Handoff from /math to /code requires strict verification that the code's update rules match the derived equations. Handoff from /code to /math requires the generated plots and metrics JSON to be correctly formatted for analysis. Fallback: If the empirical results from /code contradict the proof from /math, the task reverts to the /math agent to re-check the proof's assumptions and boundary conditions.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers and lecture notes on linearized ADMM for composite optimization.\", \"- [ ] /math: Derive the linearized ADMM update rules for the sparse logistic regression objective.\", \"- [ ] /math: Construct the formal convergence proof based on the properties of the augmented Lagrangian.\", \"- [ ] /code: Implement the derived algorithm and a proximal gradient baseline in JAX or NumPy.\", \"- [ ] /code: Instrument the implementation to log objective value and primal residual metrics per iteration.\", \"- [ ] /math: Verify that the implemented update rules in the code exactly match the mathematical derivation from the previous step.\", \"- [ ] /math: Analyze the empirical convergence plots and validate them against the derived theoretical bounds.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"admm\", \"convex-optimization\", \"primal-dual\"], \"acceptance_criteria\": [\"The convergence proof must be mathematically sound and complete, referencing standard results from convex analysis and operator theory.\", \"The Python implementation must correctly execute, be well-documented, and reproduce the convergence plots presented in the final report.\", \"The final report must clearly connect the theoretical proof with the empirical results, providing a cogent analysis of the `rho` parameter's impact.\", \"The provided citations must be from authoritative sources (e.g., peer-reviewed journals, major conference proceedings, or highly-cited arXiv preprints).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"primary_guidance\": \"Compute as the slope of the log-log plot of `f(x_k) - f(x*)` vs. iteration count `k`, where `f(x*)` is the optimal objective value.\", \"secondary_guidance\": \"Compute as the L2 norm of the primal residual `r_k = Ax_k + Bz_k - c` at each iteration `k` to measure feasibility.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4898, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a high-performance, GPU-accelerated implementation of the Preconditioned Conjugate Gradient (PCG) method to solve the sparse linear system Ax=b resulting from a finite difference discretization of the 2D Poisson equation on a large grid. Please focus on using an Incomplete Cholesky (IC0) preconditioner, implement it in Python using CuPy for CUDA acceleration, and provide a thorough verification of both numerical correctness against an analytical solution and the convergence rate's dependency on the preconditioned matrix's eigenvalue distribution.\n", "completion": "{\"route_plan\": [\"/math(Derive the Preconditioned Conjugate Gradient algorithm and the Incomplete Cholesky (IC0) factorization as a preconditioner for a sparse, symmetric positive-definite system. Analyze the theoretical convergence rate improvements.)\", \"/general-search(query=\\\"site:arxiv.org incomplete cholesky conjugate gradient cuda performance analysis sparse\\\", mode=rag)\", \"/code(Implement the PCG solver with IC0 preconditioning in Python using CuPy. The implementation must handle sparse matrices in CSR format, validate CUDA kernel execution, and include robust checks for convergence criteria based on the residual norm.)\"], \"route_rationale\": \"/math is used first to establish the rigorous mathematical foundation and convergence properties. The 'blue' /general-search agent then finds state-of-the-art implementation techniques and performance benchmarks from authoritative sources to guide the coding phase. Finally, /code implements the derived algorithm, leveraging the insights from the search to create an optimized and verifiable GPU solution.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the PCG algorithm and the IC0 preconditioner.\", \"- A Python script (`pcg_solver.py`) containing the full CuPy implementation.\", \"- A PDF report (`report.pdf`) containing plots of performance benchmarks (wall-time vs. grid size) and convergence analysis (residual norm vs. iteration).\", \"- A JSON file (`metrics.json`) with final performance metrics.\", \"- A list of cited arXiv papers with IDs.\"], \"thinking_outline\": [\"1. Formulate the discrete 2D Poisson equation as a sparse linear system Ax=b, and derive the structure of the matrix A.\", \"2. Derive the step-by-step algorithm for the Preconditioned Conjugate Gradient method and the specific formulation for the Incomplete Cholesky (IC0) preconditioner.\", \"3. Implement the solver using Python and the CuPy library for GPU acceleration, paying special attention to efficient sparse matrix-vector products (SpMV) on the GPU.\", \"4. Verify the implementation's correctness by testing it on a small grid with a known analytical solution and checking the solution error.\", \"5. Validate the solver's performance and convergence properties by running it on large grids, plotting the residual norm against iteration count, and comparing the results with and without preconditioning.\", \"6. Document the mathematical derivations, implementation details, and experimental results in a comprehensive report.\"], \"handoff_plan\": \"/math provides the algorithmic specification -> /general-search finds optimization patterns for this class of algorithm on GPUs -> /code implements the specification using optimizations found -> router QA (verification: final code's convergence rate and solution correctness must match the mathematical derivation and pass all acceptance criteria; fallback: if convergence fails, loop back to /math to investigate preconditioner stability for the given matrix structure).\", \"todo_list\": [\"- [ ] /math: Derive the PCG algorithm and IC0 preconditioner.\", \"- [ ] /math: Analyze the effect of preconditioning on the matrix's condition number.\", \"- [ ] /general-search: Find reference implementations or performance studies of PCG on GPUs.\", \"- [ ] /code: Implement the sparse matrix setup and the PCG solver in CuPy.\", \"- [ ] /code: Implement the IC0 preconditioner and integrate it into the solver.\", \"- [ ] /math: Verify that the implementation's residual decay aligns with the theoretical convergence rate.\", \"- [ ] /code: Benchmark the solver's performance and generate convergence plots.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"gpu-computing\", \"numerical-methods\", \"linear-algebra\"], \"acceptance_criteria\": [\"- The implemented solver must converge to a solution with a residual norm below 1e-8 for a 1024x1024 grid.\", \"- The final report must contain a plot showing that the preconditioned solver requires at least 5x fewer iterations than the non-preconditioned version for a given problem size.\", \"- The provided code must be runnable and reproduce the metrics and plots submitted in the report.\", \"- The mathematical derivation must be clear, correct, and self-contained.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 4909, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to understand how a Gaussian Process works for Bayesian regression. Can you derive the posterior predictive distribution and then code a simple 1D example using Python to model a noisy sine wave, explaining the role of the covariance kernel?\n", "completion": "{\"route_plan\": [\"/math(Derive the posterior predictive distribution for a Bayesian Gaussian Process regression model, clearly defining the mean and covariance functions.)\", \"/general-search(query=\\\"Gaussian Process periodic kernel function for sinusoidal data site:distill.pub OR site:gaussianprocess.org\\\", mode=web)\", \"/code(Implement the Gaussian Process regression in Python with NumPy/SciPy, using the derived Bayesian formulas and a periodic kernel to fit a noisy sine wave. Plot the mean prediction and the 95% confidence interval.)\"], \"route_rationale\": \"The /math tool is required for the formal derivation of the GP equations. The blue /general-search agent is then used to find a suitable, specialized kernel function for the periodic data. Finally, the /code tool implements the derived theory and visualizes the results to provide a practical demonstration.\", \"expected_artifacts\": [\"A LaTeX-formatted derivation of the Gaussian Process posterior predictive mean and covariance.\", \"A Python script implementing 1D Gaussian Process regression.\", \"A plot file (.png) showing the noisy data, the true function, the GP mean prediction, and the 95% confidence interval.\", \"A brief markdown report summarizing the chosen kernel function with citations.\", \"A verification log confirming the dimensions of the covariance matrices in the code match the theoretical derivation.\"], \"thinking_outline\": [\"1. Define the prior distribution over functions using a Gaussian Process (GP), specifying the mean and covariance (kernel) functions.\", \"2. Apply Bayes' rule to the joint distribution of training and test data to derive the conditional distribution (posterior predictive) for the test points.\", \"3. Verify the matrix algebra for the posterior mean and covariance, ensuring dimensional consistency at each step of the derivation.\", \"4. Select an appropriate kernel function, such as the periodic kernel, for modeling the sinusoidal data pattern based on search results.\", \"5. Implement the derived equations in Python, calculate the posterior predictive mean and variance, and validate the visualization to check if the confidence interval properly captures the data's uncertainty.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: code output and plot match the derived formulas and theoretical expectations for uncertainty; fallback: if implementation fails, revisit the /math derivation for potential algebraic errors).\", \"todo_list\": [\"- [ ] Use /math to derive the Gaussian Process posterior predictive distribution.\", \"- [ ] Use /general-search to research and select a suitable periodic kernel for the coding task.\", \"- [ ] Use /code to implement the derived GP model and generate the required plot.\", \"- [ ] Verify that the dimensions of the covariance matrices in the code are correct and that the plot correctly displays the uncertainty.\", \"- [ ] Submit all artifacts for router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"stochastic-processes\", \"bayesian-methods\", \"gaussian-processes\"], \"acceptance_criteria\": [\"The mathematical derivation of the posterior predictive distribution is complete and correct.\", \"The Python code runs without errors and produces a plot visually representing a reasonable fit to the noisy sine wave.\", \"The generated plot correctly displays the mean prediction and a shaded 95% confidence interval.\", \"The summary report cites at least 2 authoritative sources for the selected kernel function.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5006, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project in natural language processing and need to understand the fundamentals. Can you derive the dot-product attention formula from first principles and then implement it in Python to verify the matrix operations? I'm particularly interested in the role of the softmax and scaling factor.\n", "completion": "{\"route_plan\": [\"/math(Derive the scaled dot-product attention formula, A(Q, K, V) = softmax(QK^T / sqrt(d_k))V, explaining the role of the scaling factor in preventing vanishing gradients and its relationship to the variance of the dot product, referencing the expected eigenvalue of the QQ^T matrix.)\", \"/code(Write a Python function using NumPy to implement the scaled dot-product attention mechanism derived previously. Include a simple test case with sample Query, Key, and Value matrices to validate the output shape and softmax distribution. Consider this a precursor to more complex Bayesian attention models.)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation and formal derivation of the attention mechanism. It then proceeds to /code to provide a practical, verifiable implementation of the derived mathematics, ensuring the user understands both theory and application.\", \"expected_artifacts\": [\"A LaTeX document detailing the step-by-step derivation of the scaled dot-product attention formula.\", \"A Python script (`attention.py`) containing the implementation and a simple test case demonstrating its execution.\", \"A verification report (`verification.md`) showing the output of the Python script and confirming that the matrix dimensions and softmax outputs match the theoretical derivation.\", \"A brief summary connecting this mechanism to foundational concepts in self-supervised representation learning.\"], \"thinking_outline\": [\"1. Begin by defining the Query (Q), Key (K), and Value (V) matrices and their roles in the attention mechanism.\", \"2. Derive the dot-product similarity score between Q and K, leading to the QK^T term.\", \"3. Explain the necessity of the scaling factor 1/sqrt(d_k) by analyzing the variance of the dot products, which prevents the softmax function from saturating.\", \"4. Formulate the complete scaled dot-product attention equation, including the softmax application and the final weighted sum with V.\", \"5. Validate the mathematical derivation by implementing it in Python and checking the output against a small, manually calculated example.\"], \"handoff_plan\": \"/math -> /code -> router QA (verification: code output dimensions and values must match the derived mathematical properties; fallback: if implementation fails, revisit the derivation for potential misunderstandings of matrix operations).\", \"todo_list\": [\"- [ ] /math: Derive the scaled dot-product attention formula, including the justification for the scaling factor.\", \"- [ ] /code: Implement the derived formula in a Python function using NumPy and create a simple test case.\", \"- [ ] Verification: Run the Python code and document in a markdown file that its output aligns with the mathematical theory regarding shape and value distribution.\", \"- [ ] Router QA: Submit the derivation, code, and verification report for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"attention-mechanism\", \"self-supervised-learning\", \"representation-learning\"], \"acceptance_criteria\": [\"The mathematical derivation correctly explains the role of the scaling factor 1/sqrt(d_k) in stabilizing softmax gradients.\", \"The Python implementation runs without errors and produces an output tensor of the correct shape based on the input matrices.\", \"The softmax output in the Python test case is a valid probability distribution (i.e., rows sum to 1).\", \"The final report clearly connects the mathematical formula to the Python code with a working example.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5047, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the analytical gradient for the cost function in a Variational Quantum Eigensolver (VQE) that uses a Unitary Coupled-Cluster Singles and Doubles (UCCSD) ansatz. Specifically, I need to see this applied to finding the ground state energy of a Lithium Hydride (LiH) molecule. Please provide a full Python implementation using Pennylane to compare the convergence of an optimizer using this analytical gradient versus one using the standard parameter-shift rule, analyzing the effects of shot noise on the optimization landscape and final energy precision.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org VQE analytical gradient UCCSD quantum chemistry\\\", mode=web)\", \"/math(Derive the analytical gradient of the VQE cost function C(\u03b8) = <\u03c8(\u03b8)|H|\u03c8(\u03b8)> for a UCCSD ansatz, focusing on the LiH Hamiltonian eigenvalue problem and its expected energy landscape.)\", \"/code(Implement in Python the VQE for LiH's ground state energy using both the derived analytical gradient and the parameter-shift rule, tracking convergence behavior under simulated shot noise.)\", \"/math(Perform a comparative analysis of the convergence data, verifying if the analytical gradient method provides a statistically significant improvement in iteration count and final eigenvalue accuracy over the parameter-shift rule.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on VQE analytical gradients. This theory is then passed to /math for a formal, rigorous derivation. The resulting mathematical formula serves as the specification for the /code agent, which implements and simulates the VQE optimization to generate empirical data. Finally, the simulation results are handed back to /math for statistical analysis and verification against theoretical expectations and known benchmarks.\", \"expected_artifacts\": [\"A formal mathematical derivation of the analytical gradient for the specified VQE problem, delivered as a LaTeX document.\", \"A commented Python script (using Pennylane) that implements the VQE simulation for LiH, comparing the two gradient calculation methods.\", \"A set of plots visualizing the energy convergence versus optimization iterations for both methods, with and without shot noise.\", \"A summary report (in Markdown) that analyzes the results, discusses the trade-offs, and includes citations for at least two relevant arXiv papers.\", \"A verification log confirming the numerical equivalence (within a tolerance) of the analytical gradient code and a finite-difference check for a test case.\"], \"thinking_outline\": [\"1. Use search to establish the theoretical framework for VQE, the UCCSD ansatz, and the specific form of the LiH molecular Hamiltonian.\", \"2. Formally derive the expression for the analytical gradient of the VQE cost function, \u2202\u27e8H\u27e9/\u2202\u03b8_k, by differentiating the expectation value with respect to the ansatz parameters.\", \"3. Implement the VQE algorithm in Python, including constructing the Hamiltonian, defining the UCCSD circuit, and creating two distinct gradient functions: one analytical, one using the parameter-shift rule.\", \"4. Validate the analytical gradient implementation by comparing its output against a numerical finite-difference approximation for several random parameter vectors to ensure correctness.\", \"5. Run optimization simulations for both gradient methods, systematically collecting data on the energy at each step, and repeat under different levels of simulated measurement shot noise.\", \"6. Verify the final converged ground state energy from the simulation against established computational chemistry benchmarks for LiH to confirm the physical accuracy of the result.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (analysis) -> router QA. The search results provide the theoretical basis for the /math agent's derivation. This derivation becomes the formal specification for the /code agent. The code agent's output (logs, metrics) is then passed to the second /math agent for statistical analysis and verification. Verification: The code's gradient implementation is checked against the math derivation. Fallback: If the derivation is incorrect or the code fails verification, the task reverts to the /math agent to correct the derivation, possibly informed by another /general-search query.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers and tutorials on VQE analytical gradients and the UCCSD ansatz for quantum chemistry simulations.\", \"- [ ] /math: Produce the step-by-step derivation of the analytical gradient for the VQE cost function with a UCCSD ansatz.\", \"- [ ] /code: Implement the full VQE simulation for LiH in Pennylane, including both gradient computation methods.\", \"- [ ] /code: Instrument the simulation to log energy, iteration count, and gradient norms for each optimization step.\", \"- [ ] /math: Verify that the analytical gradient code's output matches a finite-difference approximation for a small set of test parameters.\", \"- [ ] /math: Analyze the generated convergence plots and metrics to compare the two methods' performance and robustness to noise.\", \"- [ ] /router: Perform a final QA review of all artifacts, ensuring the derivation, code, and analysis are consistent and meet all acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"variational-algorithms\", \"quantum-chemistry\"], \"acceptance_criteria\": [\"- The provided derivation for the analytical gradient is mathematically sound and complete for the UCCSD ansatz.\", \"- The Python script successfully executes and generates convergence plots comparing the analytical and parameter-shift gradient methods.\", \"- The final computed ground state energy for LiH is within 1.5 mHa (millihartree) of the accepted FCI value (~ -7.882 Ha).\", \"- The final report clearly articulates the performance trade-offs between the two gradient strategies based on the simulation data.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as 1000 * |E_converged - E_FCI_benchmark|, where E_FCI_benchmark is the full configuration interaction energy for LiH at the given bond length.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"The number of optimizer steps required to reach an energy value E such that |E_k - E_{k-1}| < 1e-6 for 5 consecutive steps.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5066, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the properties of a Gaussian Process with a Mat\u00e9rn kernel. Derive its spectral density using the Wiener-Khinchin theorem and implement a Bayesian optimization loop to find the minimum of a noisy black-box function, validating convergence.\n", "completion": "{\"route_plan\": [\"/math(Derive the spectral density of the Mat\u00e9rn kernel via the Fourier transform, applying the Wiener-Khinchin theorem. Analyze the impact of the smoothness parameter on the resulting power spectrum for this Bayesian model.)\", \"/general-search(query=\\\"site:arxiv.org site:distill.pub Bayesian optimization Gaussian Process Mat\u00e9rn kernel acquisition functions\\\", mode=both)\", \"/code(Implement a Bayesian optimization loop in Python using a Gaussian Process with a Mat\u00e9rn kernel. Validate the convergence rate on the Branin-Hoo function and report the mean squared error of the found minimum.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation of the Mat\u00e9rn kernel's spectral properties. It then uses the blue /general-search agent to gather practical implementation patterns and review canonical acquisition functions from authoritative sources. Finally, /code synthesizes the theory and research into a working Python implementation and validates its performance empirically.\", \"expected_artifacts\": [\"A PDF with the mathematical derivation of the Mat\u00e9rn kernel's spectral density.\", \"A Python script implementing Bayesian optimization with a GP surrogate model.\", \"A convergence plot showing the best-found objective value vs. iteration number.\", \"A summary report in Markdown format, including performance metrics and at least two citations from authoritative sources.\", \"A JSON file containing the final metrics.\"], \"thinking_outline\": [\"1. Theoretically derive the spectral density of the Mat\u00e9rn kernel by taking the Fourier transform of its covariance function, as dictated by the Wiener-Khinchin theorem.\", \"2. Research canonical acquisition functions (e.g., Expected Improvement, Upper Confidence Bound) used in Bayesian optimization with Gaussian Process surrogates.\", \"3. Implement the Gaussian Process regression model and the chosen acquisition function in Python using libraries like scikit-learn or GPy.\", \"4. Construct the full Bayesian optimization loop and apply it to the two-dimensional Branin-Hoo benchmark function.\", \"5. Validate the implementation by plotting the convergence of the algorithm and checking that the found minimum is within a small tolerance of the known global minimum.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent produces the core derivation. The /general-search agent finds supporting literature and implementation guides. The /code agent implements the algorithm using the math and research as a guide. The final code and report are passed to the router for QA. Verification: The code's output must match the expected convergence behavior on the benchmark. Fallback: If convergence is not achieved, the /code agent will debug the acquisition function or GP hyperparameter settings and re-run.\", \"todo_list\": [\"- [ ] /math: Derive the spectral density for the Mat\u00e9rn kernel as a function of its smoothness and length-scale parameters.\", \"- [ ] /general-search: Find key papers and tutorials on implementing Bayesian optimization with Gaussian Processes.\", \"- [ ] /code: Implement the GP surrogate model and the Expected Improvement acquisition function.\", \"- [ ] /code: Construct the main Bayesian optimization loop and apply it to the Branin-Hoo function.\", \"- [ ] /code: Generate a convergence plot and calculate final performance metrics.\", \"- [ ] /router: QA review of the mathematical derivation, code correctness, and alignment between artifacts and acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"bayesian-inference\", \"stochastic-processes\", \"gaussian-processes\"], \"acceptance_criteria\": [\"The mathematical derivation for the spectral density must be correct and self-contained.\", \"The Python implementation must correctly find a minimum of the Branin-Hoo function with a value less than 0.4 (true minimum is ~0.397) within 100 function evaluations.\", \"The final report includes a convergence plot demonstrating a decreasing trend in the best-observed function value.\", \"The report cites at least two relevant academic papers with arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5210, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am working on Bayesian inference for neural networks with continuous-time dynamics. Could you derive the evidence lower bound (ELBO) for a variational posterior defined by a stochastic differential equation (SDE)? Please explicitly use Girsanov's theorem to handle the change of measure for the KL divergence term between the posterior and a standard Wiener process prior. Then, implement this in JAX, showing how to train the drift function of the variational SDE on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Bayesian deep learning 'stochastic differential equations' 'variational inference' 'Girsanov theorem' convergence analysis\\\", mode=both)\", \"/math(Derive the full ELBO for a Bayesian neural net modeled as an SDE. Apply Girsanov's theorem to compute the KL divergence between the variational posterior and a prior process, detailing all It\u00f4 calculus steps required for this Bayesian framework.)\", \"/code(Implement the derived SDE-based ELBO in JAX using an Euler-Maruyama solver. Train the neural network controlling the variational drift on a synthetic dataset, tracking ELBO convergence and posterior variance. Utilize CUDA for acceleration., using Python)\", \"/math(Verify the coded ELBO gradient's correctness by comparing the JAX autodiff result against a manually derived gradient expression for a simplified one-dimensional case. Analyze the variance of the resulting stochastic gradient estimator.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on Neural SDEs and variational inference. This context is passed to /math for the core theoretical derivation. The resulting mathematical formula is then handed to /code for implementation and experimentation. A final /math step is crucial for verifying the correctness of the gradient implementation, a common failure point in complex models.\", \"expected_artifacts\": [\"- A LaTeX-formatted PDF containing the complete mathematical derivation of the SDE ELBO.\", \"- A Python script (`main.py`) implementing the training and evaluation of the Bayesian Neural SDE.\", \"- A results summary (`results.md`) including plots of ELBO convergence and final model performance.\", \"- A bibliography file (`references.bib`) with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Conduct a literature survey to understand the state-of-the-art in variational inference for continuous-time Bayesian deep learning models, specifically those leveraging SDEs.\", \"2. Formally derive the evidence lower bound (ELBO) objective. The key step is applying Girsanov's theorem and It\u00f4 calculus to express the KL divergence between the prior and the variational posterior path measures.\", \"3. Implement the derived objective function and a corresponding SDE solver (e.g., Euler-Maruyama) in JAX. This includes parameterizing the drift term of the variational SDE with a neural network.\", \"4. Validate the implementation by deriving the gradient of the ELBO for a trivial, one-dimensional case analytically and comparing it to the gradient computed by JAX's automatic differentiation engine.\", \"5. Train the model on a synthetic dataset, analyzing the convergence of the ELBO and the behavior of the learned posterior distribution.\", \"6. Audit the final report to check for consistency between the mathematical derivation, the code implementation, and the experimental results.\"], \"handoff_plan\": \"/general-search (papers) -> /math (derivation) -> /code (implementation). The final derivation from /math and the code from /code are then passed to a final /math agent for gradient verification. All artifacts are then passed to the router for a final QA review. Fallback: If the KL divergence term proves intractable during derivation, the /math agent will flag this, and the router will initiate a new /general-search query for alternative divergence measures.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on variational inference for Neural SDEs using Girsanov's theorem.\", \"- [ ] /math: Derive the ELBO expression, showing all It\u00f4 calculus and change of measure steps clearly.\", \"- [ ] /code: Implement the SDE solver and the variational objective function in JAX.\", \"- [ ] /code: Set up the training loop, synthetic data generation, and logging for ELBO and posterior variance.\", \"- [ ] /math: Analytically derive the gradient for a simplified 1D case of the objective function for verification.\", \"- [ ] /code: Numerically verify the autodiff gradient from the JAX implementation against the analytical derivation from /math.\", \"- [ ] /math: Verify the final written proof in the report aligns perfectly with the code's logic and architecture.\", \"- [ ] Router QA: Review all artifacts for theoretical consistency, correctness, and adherence to acceptance criteria.\"], \"difficulty\": \"advanced\", \"tags\": [\"bayesian-neural-networks\", \"stochastic-calculus\", \"neural-sde\"], \"acceptance_criteria\": [\"- Pass: A complete, step-by-step mathematical derivation of the ELBO using Girsanov's theorem is provided.\", \"- Pass: The JAX code successfully trains the Neural SDE, and the ELBO convergence is demonstrated in a plot.\", \"- Pass: The numerical gradient verification check passes with a tolerance less than 1e-6.\", \"- Pass: The final report cites at least 2 relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Report the maximum converged value of the ELBO on the test set. Higher is better.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Estimate the variance of the stochastic gradients of the ELBO over a batch of 100 samples. Lower variance indicates more stable training.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5327, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nAnalyze the saddle point structure of the 2D Rastrigin function. Derive the conditions for escaping these points using perturbed gradient descent and verify the theoretical convergence rate by implementing the algorithm and plotting the escape trajectory. Focus on the role of the Hessian eigenvalue distribution.\n", "completion": "{\"route_plan\": [\"/math(Derive the gradient and Hessian for the 2D Rastrigin function, then determine the specific eigenvalue properties of its saddle points to establish the theoretical convergence guarantees for a perturbed descent method.)\", \"/code(Implement perturbed gradient descent in Python to escape saddle points of the Rastrigin function, ensuring the implementation tracks the trajectory and measures the number of iterations required for convergence, validating against the derived theory.)\", \"/general-search(query=\\\"site:arxiv.org perturbed gradient descent saddle point escape non-convex optimization\\\", mode=rag. Find foundational papers on the convergence rates of algorithms designed to handle saddle points, particularly those that leverage Hessian information.)\"], \"route_rationale\": \"The plan starts with /math to build the theoretical foundation by analyzing the function's geometry and deriving escape conditions. Next, /code is used to empirically validate these theoretical findings through simulation. Finally, the blue /general-search agent is invoked to gather supporting academic literature, providing context and justification for the methods used.\", \"expected_artifacts\": [\"A PDF or LaTeX document with the mathematical derivations of the gradient, Hessian, and saddle point analysis.\", \"A Python script (`.py`) implementing the perturbed gradient descent algorithm and generating visualizations.\", \"A summary report in Markdown (`.md`) that explains the findings, integrates the plots, and includes citations.\", \"A plot (`.png`) showing the optimization trajectory successfully escaping a saddle point.\", \"A JSON file containing the final metrics, including iterations to escape and final loss.\"], \"thinking_outline\": [\"1. Define the 2D Rastrigin function and analytically derive its gradient and Hessian matrix.\", \"2. Identify the locations of critical points and use the Hessian eigenvalue spectrum to classify them as local minima, maxima, or saddle points.\", \"3. Formulate the perturbed gradient descent algorithm, specifying the conditions on noise injection required to escape saddle points based on the Hessian's structure.\", \"4. Implement the algorithm in Python, initializing the optimizer near a known saddle point to observe the escape dynamics.\", \"5. Validate the implementation by plotting the optimization path and comparing the empirical number of iterations to escape with the theoretical bounds derived from the mathematical analysis.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent produces the analytical formulas and theoretical bounds, which are handed to the /code agent for implementation and verification. The /general-search agent finds literature to contextualize the results in the final report. The router performs a final QA check, comparing the code's output against the mathematical derivations. Fallback: If the code's empirical results contradict the theory, the task is routed back to /math for a re-audit of the Hessian analysis.\", \"todo_list\": [\"- [ ] /math: Derive the gradient and Hessian of the 2D Rastrigin function.\", \"- [ ] /math: Analyze the eigenvalues of the Hessian at various critical points to identify saddles.\", \"- [ ] /code: Implement the Rastrigin function and the perturbed gradient descent algorithm.\", \"- [ ] /code: Initialize the algorithm near a saddle point and generate a plot of the escape trajectory.\", \"- [ ] /code: Validate that the number of iterations to escape the saddle region is consistent with the theoretical bounds derived by the /math agent.\", \"- [ ] /general-search: Find and summarize two key papers on algorithms for escaping saddle points.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"gradient-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Hessian and its eigenvalues must be correct.\", \"- The Python code must execute without errors and generate a visualization clearly showing an optimizer trajectory escaping a saddle point and converging towards a local minimum.\", \"- The final report must cite at least two relevant arXiv papers with correct formatting.\", \"- The empirical `iterations_to_escape` metric must fall within the order of magnitude predicted by the theoretical analysis.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5347, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and analyze the reparameterization-invariant evidence lower bound (ELBO) using pathwise derivatives for a Bayesian Neural Network with a hierarchical prior structure. Compare its convergence properties and variance reduction against the standard score function estimator, specifically focusing on the heavy-tailed implicit posterior distributions common in biomedical signal processing.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org reparameterization-invariant variational inference pathwise derivatives bayesian hierarchical models\\\", mode=rag)\", \"/math(Derive the gradient estimators for the reparameterization-invariant ELBO and the score function estimator for a Bayesian neural network with hierarchical priors, establishing the theoretical conditions for convergence and variance reduction.)\", \"/code(Implement both VI gradient estimators in Python using JAX, construct the specified Bayesian neural network, and run experiments on simulated biomedical data to measure ELBO convergence and gradient variance.)\", \"/math(Analyze the empirical convergence results from the code implementation, verifying if they align with the derived theoretical variance reduction properties and checking for statistical significance in the Bayesian estimator performance.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find authoritative literature on advanced reparameterization-invariant VI. This research informs the /math agent's formal derivation of the required gradient estimators. The /code agent then implements these derivations for empirical testing. A final /math step is crucial for verification, comparing the empirical results from the code back to the theoretical guarantees derived earlier.\", \"expected_artifacts\": [\"- A PDF document containing the full mathematical derivations of the ELBO gradient estimators.\", \"- A runnable Python script implementing the Bayesian Neural Network and both variational inference algorithms.\", \"- A summary report in Markdown comparing the estimators, including plots of ELBO convergence and tables of gradient variance statistics.\", \"- A bibliography file in BibTeX format citing the sources discovered during the research phase.\", \"- A JSON file containing the final computed primary and secondary metrics.\"], \"thinking_outline\": [\"1. Review literature on pathwise derivatives and reparameterization-invariant VI to understand the state-of-the-art for non-centered parameterizations.\", \"2. Formally derive the gradient estimators for both the reparameterization-invariant and score function methods for the specified hierarchical Bayesian model.\", \"3. Implement the Bayesian neural network and both inference algorithms in JAX, ensuring the data simulation process is reproducible.\", \"4. Validate the implementation by checking gradient norms and initial ELBO values against expected theoretical ranges for sanity.\", \"5. Execute the comparative experiments, collecting data on ELBO convergence speed and the empirical variance of the gradient estimates across multiple runs.\", \"6. Verify that the experimental results on variance reduction are consistent with the theoretical guarantees derived in step 2, including a statistical significance test.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: empirical gradient variance from /code aligns with theoretical predictions from /math; fallback: if significant mismatch, return to /math to re-check derivation assumptions or to /code to debug implementation).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on reparameterization-invariant VI, focusing on pathwise derivatives.\", \"- [ ] /math: Derive the gradient expressions for both estimators based on the literature, clearly stating all assumptions.\", \"- [ ] /code: Implement the Bayesian neural network model with hierarchical priors and a data simulation pipeline.\", \"- [ ] /code: Implement both the reparameterization-invariant and score function gradient estimators.\", \"- [ ] /code: Run experiments to collect ELBO curves and gradient variance statistics over at least 5 independent seeds.\", \"- [ ] /math: Verify that the mathematical derivation for the gradient variance reduction is correctly reflected in the empirical results from /code.\", \"- [ ] /code: Audit the implementation's numerical stability by logging gradient norms and checking for NaNs.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-methods\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"- The implementation of both estimators must be provided in a runnable Python script with dependencies specified.\", \"- The final report must contain a plot clearly showing the mean and standard deviation of the ELBO convergence for both methods over training iterations.\", \"- The report must include a table comparing the empirical gradient variance, and this comparison must be justified with a p-value from a statistical test (e.g., a paired t-test).\", \"- The mathematical derivation must be complete, self-contained, and submitted as a PDF.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Computed as the number of iterations to reach 95% of the final converged ELBO value, averaged over 5 runs. Lower is better.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Computed as the trace of the empirical covariance matrix of the gradient estimates over a fixed batch of data, averaged over the last 10% of training. Lower is better.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5361, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the preconditioned conjugate gradient algorithm for solving sparse linear systems Ax=b. Analyze its convergence rate in terms of the condition number and eigenvalue distribution, then implement a CUDA-accelerated version in Python and benchmark its performance against a direct solver for a Poisson equation problem.\n", "completion": "{\"route_plan\": [\"/math(Derive the Preconditioned Conjugate Gradient (PCG) algorithm for a symmetric positive-definite system. Analyze its convergence rate based on the eigenvalue distribution of the preconditioned matrix, ensuring the derivation is rigorous.)\", \"/general-search(query=\\\"site:arxiv.org incomplete cholesky preconditioner conjugate gradient CUDA performance\\\", mode=web)\", \"/code(Implement the PCG algorithm in Python using CuPy for CUDA acceleration. Validate the implementation's correctness on a 2D Poisson problem and benchmark its performance against a direct solver, reporting speedup and residual convergence.)\"], \"route_rationale\": \"The /math tool is used first to establish the theoretical foundation and derivation of the PCG algorithm. The 'blue' /general-search agent then gathers state-of-the-art context on practical preconditioners and GPU implementation strategies from academic sources. Finally, /code is used to implement the derived theory and empirically validate the performance claims through benchmarking.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the Preconditioned Conjugate Gradient method and its convergence analysis.\", \"- A Python script implementing the PCG solver using CuPy for GPU acceleration.\", \"- A performance comparison plot (wall time vs. problem size) against a direct solver.\", \"- A summary report in Markdown format including the derivation, benchmark results, and citations.\", \"- A JSON file containing the final metrics (speedup, residual norm).\"], \"thinking_outline\": [\"1. Derive the standard Conjugate Gradient (CG) algorithm from the method of conjugate directions for solving Ax=b where A is symmetric and positive-definite.\", \"2. Introduce a preconditioner matrix M and derive the Preconditioned Conjugate Gradient (PCG) algorithm, explaining how it transforms the original system to improve conditioning.\", \"3. Analyze the convergence rate of PCG, formally relating it to the condition number and eigenvalue clustering of the preconditioned matrix M\u207b\u00b9A.\", \"4. Implement the PCG algorithm using a GPU-accelerated library (CuPy) and apply a suitable preconditioner (e.g., Jacobi) for a sparse system generated by discretizing the 2D Poisson equation.\", \"5. Verify the implementation's correctness by monitoring the L2 norm of the residual at each iteration and validate the final solution against the one obtained from a high-precision direct solver.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the algorithm specification. /general-search finds suitable preconditioners to inform the implementation choices. /code implements the algorithm from /math using insights from /general-search. The final code and report are passed to the router for QA. Verification: The router will check if the code's residual decay matches the theoretical convergence rate discussed in the derivation. Fallback: If the convergence is poor, /code will be tasked to implement a different preconditioner suggested by /general-search.\", \"todo_list\": [\"- [ ] /math: Provide a step-by-step derivation of the PCG algorithm and its theoretical convergence bound.\", \"- [ ] /general-search: Find and summarize two relevant papers on GPU-accelerated preconditioners for the conjugate gradient method.\", \"- [ ] /code: Implement the PCG algorithm in Python with CuPy, including a Jacobi preconditioner.\", \"- [ ] /code: Develop a benchmark to solve a 2D Poisson problem on an NxN grid, comparing PCG's wall time to `cupy.linalg.solve` for N in [256, 512, 1024].\", \"- [ ] /code: Validate the solver's output by logging the residual norm's decay per iteration and ensuring it reaches a tolerance of 1e-8.\", \"- [ ] /router: Review all generated artifacts for correctness, consistency, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"iterative-solvers\"], \"acceptance_criteria\": [\"- The mathematical derivation of the PCG algorithm and its convergence rate must be complete and free of errors.\", \"- The CUDA-accelerated code must successfully solve a 1024x1024 Poisson problem, achieving a final residual norm below 1e-8.\", \"- The final report must include a plot demonstrating a wall-time speedup of at least 2x for the PCG solver over the direct solver on the largest problem size.\", \"- The report must correctly cite at least two relevant academic papers with their arXiv IDs or DOIs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5380, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to understand how to implement the Jacobi method for solving large, diagonally dominant systems of linear equations using CUDA in Python for a basic physics simulation.\n", "completion": "{\"route_plan\": [\"/math(Derive the iterative update rule for the Jacobi method, Ax=b, by decomposing matrix A into its diagonal and off-diagonal components. Establish the convergence condition related to the matrix's eigenvalue spectrum.)\", \"/general-search(query=\\\"site:developer.nvidia.com introductory CUDA python Jacobi method OR Numba tutorial\\\", mode=both)\", \"/code(Implement the Jacobi method in Python using both NumPy for a CPU baseline and Numba for a CUDA-accelerated version. Profile the convergence and execution time for a sample diagonally dominant system.)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation of the Jacobi method. Then, the blue /general-search agent is used to find practical, high-authority implementation patterns for CUDA in Python. Finally, /code is used to implement, verify, and benchmark the algorithm based on the preceding steps.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Jacobi method's update rule and convergence condition.\", \"A self-contained Python script (`solver.py`) with both CPU (NumPy) and GPU (Numba) implementations.\", \"A plot (`performance.png`) comparing the wall-clock time of the CPU vs. GPU implementation against matrix size.\", \"A summary report (`report.md`) explaining the results and citing at least two sources.\"], \"thinking_outline\": [\"1. Formulate the Jacobi iteration by splitting the matrix A into D + L + U, and derive the iterative update formula.\", \"2. State the condition for guaranteed convergence (strict diagonal dominance) and explain its connection to the spectral radius of the iteration matrix.\", \"3. Research common patterns for writing a parallel kernel for this iterative method using Python's CUDA libraries like Numba or CuPy.\", \"4. Implement the solver for both CPU and GPU, ensuring the GPU kernel correctly handles memory transfers and parallel execution.\", \"5. Validate the correctness of the GPU implementation by comparing its output solution vector against the CPU version's result for a known problem, checking for numerical equivalence.\", \"6. Benchmark the performance of both implementations on a larger system, measuring wall-clock time until the solution's residual norm falls below a tolerance.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the precise algorithm for the /code agent. The /general-search results provide CUDA-specific implementation patterns to guide the coding. The final code and artifacts are passed to the router for quality assurance. Verification: Code output from the GPU implementation must match the CPU baseline for a test case. Fallback: If verification fails, the /code agent must debug the CUDA kernel, potentially referencing the /general-search results again for common pitfalls.\", \"todo_list\": [\"- [ ] Use /math to derive the Jacobi method's iterative formula and its convergence criteria.\", \"- [ ] Use /general-search to find introductory tutorials on implementing iterative solvers in Python with CUDA via Numba.\", \"- [ ] Use /code to write and profile both a NumPy (CPU) and a Numba (GPU) version of the solver.\", \"- [ ] Verify that the GPU and CPU solutions are numerically equivalent within a small tolerance for a test matrix.\", \"- [ ] Submit all artifacts (derivation, code, plot, report) for final router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"cuda\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly presents the Jacobi update rule and its convergence condition.\", \"- The submitted Python script runs without errors and produces a numerically correct solution for a 100x100 test matrix.\", \"- The performance plot clearly demonstrates a significant speedup for the GPU implementation over the CPU baseline on matrices larger than 500x500.\", \"- The final report cites at least 2 authoritative sources from the web search or external knowledge.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5395, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nImplement a Bayesian neural network using a Langevin dynamics MCMC sampler. Derive the continuous-time limit and analyze the posterior convergence guarantees for weights governed by an Ornstein-Uhlenbeck process prior, ensuring the implementation achieves a target AUPRC on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/math(Derive the Fokker-Planck equation for the posterior distribution of a Bayesian neural network's weights under a Langevin dynamics sampler with an Ornstein-Uhlenbeck prior, analyzing the criteria for convergence to the stationary distribution.)\", \"/general-search(query=\\\"site:arxiv.org stochastic gradient langevin dynamics ornstein-uhlenbeck prior bayesian neural network convergence\\\", mode=rag)\", \"/code(Implement the Stochastic Gradient Langevin Dynamics sampler in JAX for a Bayesian neural net with an OU prior. Validate on a synthetic dataset, tracking the AUPRC and ensuring posterior sample convergence via Gelman-Rubin diagnostics., using Python)\"], \"route_rationale\": \"/math is first to formally derive the underlying stochastic differential equations and convergence properties. The blue /general-search agent then gathers relevant literature from arXiv to inform the implementation. Finally, /code implements the derived SGLD algorithm and validates its performance empirically.\", \"expected_artifacts\": [\"A LaTeX document detailing the derivation of the Fokker-Planck equation for the system.\", \"A Python script implementing the SGLD sampler for the Bayesian Neural Network.\", \"A JSON file containing the final AUPRC metric and Gelman-Rubin statistics.\", \"A list of at least 2 relevant arXiv paper citations with IDs.\", \"A plot visualizing the posterior distribution of a sample weight against the prior.\"], \"thinking_outline\": [\"1. Formally define the Ornstein-Uhlenbeck (OU) process as a prior for the weights of a Bayesian Neural Network and write down the full posterior distribution.\", \"2. Derive the update rule for Stochastic Gradient Langevin Dynamics (SGLD) given this OU prior and a data likelihood function.\", \"3. Analyze the continuous-time limit of the SGLD updates, showing it corresponds to a specific stochastic differential equation (SDE) and deriving its stationary distribution.\", \"4. Implement the SGLD sampler and the BNN in JAX, including a function to generate a suitable synthetic classification dataset.\", \"5. Verify the implementation's correctness by checking that the empirical moments of the sampled weights match the known stationary distribution of the OU process in a simplified scenario (e.g., zero likelihood gradient).\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: code outputs must match theoretical convergence properties from math derivations; fallback: if convergence diagnostics fail, /code agent must flag the discrepancy and suggest adjusting SGLD hyperparameters like step size based on literature from /general-search).\", \"todo_list\": [\"- [ ] /math: Derive the SDE corresponding to the SGLD updates with an Ornstein-Uhlenbeck prior.\", \"- [ ] /general-search: Find reference papers on SGLD for BNNs and their convergence analysis.\", \"- [ ] /code: Implement the SGLD sampler in Python for a simple BNN architecture.\", \"- [ ] /code: Generate a synthetic dataset and train the BNN to obtain posterior samples across multiple chains.\", \"- [ ] /code: Validate the sampler's convergence by computing the Gelman-Rubin statistic (R-hat < 1.1).\", \"- [ ] /router: Perform QA by cross-referencing the implemented update rule in the code against the mathematical derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"bayesian-inference\", \"stochastic-calculus\", \"mcmc\"], \"acceptance_criteria\": [\"The mathematical derivation of the continuous-time limit and its stationary distribution is correct and complete.\", \"The Python implementation correctly samples from the posterior and achieves an AUPRC of at least 0.85 on the specified synthetic task.\", \"The Gelman-Rubin diagnostic (R-hat) for key model parameters is below 1.1, indicating successful chain convergence.\", \"The final report includes citations for at least two relevant arXiv papers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5423, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the stabilizer group generators for the rotated surface code on a d x d lattice. Then, implement a minimum-weight perfect matching (MWPM) decoder in Python to simulate its performance under a depolarizing noise model, analyzing the logical error rate's scaling with physical error rate `p` to find the error threshold. I'm particularly interested in the computational complexity of the Blossom algorithm for this graph-theoretic problem and its convergence properties.\n", "completion": "{\"route_plan\": [\"/math(Derive the X-type and Z-type stabilizer generators for the rotated surface code, formalizing the syndrome-to-graph mapping for minimum-weight perfect matching. Analyze the computational complexity and potential eigenvalue bounds of the graph Laplacian.)\", \"/code(Implement a Monte Carlo simulation in Python to find the error threshold. Use CUDA for parallel trial execution if possible and a bootstrap method to estimate the confidence interval of the threshold crossing point for various code distances.)\", \"/general-search(query=\\\"site:arxiv.org rotated surface code threshold depolarizing noise blossom algorithm\\\", mode=both, includes cybersecurity)\"], \"route_rationale\": \"The route follows a theory-to-practice-to-validation flow. /math is used first to establish the theoretical foundations of the surface code and the decoding problem. /code then implements this theory in a simulation to generate empirical evidence. Finally, the blue /general-search agent is used to find authoritative literature to contextualize and verify the simulation's results against established benchmarks.\", \"expected_artifacts\": [\"A LaTeX document detailing the derivation of the surface code stabilizer group and the formulation of the MWPM decoding problem.\", \"A Python script implementing the surface code simulation, noise model, and MWPM decoder.\", \"A plot showing logical error rate vs. physical error rate for multiple code distances, clearly indicating the error threshold.\", \"A summary report comparing the simulated threshold to values from at least two cited academic papers.\", \"A JSON file containing the raw metrics from the simulation runs.\"], \"thinking_outline\": [\"1. Define the geometry of the rotated surface code on a d x d lattice and formally derive the expressions for the star (X-type) and plaquette (Z-type) stabilizer operators.\", \"2. Formulate the quantum error correction problem as a classical graph theory problem: map error syndromes to a graph where minimum-weight perfect matching identifies the most likely error chain.\", \"3. Implement the simulation framework, including the depolarizing noise model, syndrome measurement, and construction of the syndrome graph.\", \"4. Validate the implementation by running simulations across a range of physical error rates for several code distances (e.g., d=3, 5, 7).\", \"5. Process the simulation output to plot the logical error rate against the physical error rate, identifying the threshold where the curves for different distances cross.\", \"6. Check the derived threshold value against canonical results from quantum computing literature to verify the simulation's accuracy.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivations from /math define the data structures and algorithms for /code. The simulation results (plots, metrics) from /code are then passed to /general-search to find comparable literature values. Finally, all artifacts are reviewed by the router. Verification: The logical error threshold from /code must be within a reasonable margin of error of the values found by /general-search. Fallback: If there is a major discrepancy, re-examine the graph construction logic in /code and the noise model assumptions from /math.\", \"todo_list\": [\"- [ ] /math: Derive the stabilizer operators for the rotated surface code.\", \"- [ ] /math: Define the mapping from a syndrome to a graph for MWPM decoding.\", \"- [ ] /code: Implement the surface code lattice, depolarizing noise model, and syndrome extraction.\", \"- [ ] /code: Integrate a MWPM solver (e.g., PyMatching) and run Monte Carlo simulations for d=3,5,7.\", \"- [ ] /code: Generate plots of logical vs. physical error rate to identify the threshold.\", \"- [ ] /math: Verify that the graph construction in the Python code correctly implements the mathematical formulation.\", \"- [ ] /general-search: Find at least two peer-reviewed papers or arXiv preprints with established values for the surface code error threshold under depolarizing noise.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"graph-algorithms\"], \"acceptance_criteria\": [\"- The mathematical derivation of the stabilizer group must be correct and complete for the rotated lattice.\", \"- The Python simulation must produce a logical error rate vs. physical error rate plot showing a clear threshold crossing for different code distances.\", \"- The calculated error threshold must be approximately 10.3% (\u00b10.5%), consistent with established literature for this code and decoder.\", \"- The final report must cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"guidance\": \"The primary metric, `error_threshold`, is the physical error rate `p` at which the logical error rate curves for different code distances intersect. The secondary metric, `decoder_runtime_ms`, is the average wall-clock time in milliseconds for the MWPM algorithm to complete for a single non-trivial syndrome, averaged over 1000 runs.\"}}", "categories": ["math_first", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5488, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nInvestigate the theoretical underpinnings of post-training quantization (PTQ) for Transformer attention layers, specifically focusing on 8-bit integer (INT8) quantization. Derive the error bounds introduced by quantizing the query-key dot product, assuming inputs have a near-Gaussian distribution. Then, implement a Python prototype to validate these bounds on a pre-trained BERT-base model, analyzing the trade-off between perplexity degradation and computational savings. Also, explore how techniques like spectral normalization of the weight matrices could mitigate quantization error propagation and improve model stability.\n", "completion": "{\"route_plan\": [\"/math(Derive quantization error propagation bounds for the self-attention mechanism, analyzing the impact on the softmax output's Lipschitz constant and its effect on training convergence properties., includes numpy)\", \"/general-search(query=\\\"site:arxiv.org transformer quantization error analysis INT8 Lipschitz stability spectral norm\\\", mode=web)\", \"/code(Implement INT8 post-training quantization for BERT's attention in PyTorch, validating derived error bounds against empirical perplexity metrics and analyzing the eigenvalue distribution of quantized matrices., using Python)\"], \"route_rationale\": \"The plan starts with /math to establish a theoretical foundation by deriving the error bounds. This is followed by the blue /general-search agent to gather existing research and contextualize the derivation. Finally, /code is used to empirically validate the theoretical findings on a real-world model, providing a concrete test of the theory.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the INT8 quantization error bounds for the attention mechanism.\", \"A Python script implementing the PTQ procedure and the validation experiment on a pre-trained BERT model.\", \"A JSON file reporting perplexity scores and eigenvalue statistics for both the baseline FP32 and quantized INT8 models.\", \"A summary report in Markdown synthesizing the theoretical findings, empirical results, and literature review, complete with citations.\"], \"thinking_outline\": [\"1. Formulate the quantization process for the query, key, and value matrices in the attention mechanism as a mathematical operation, introducing explicit quantization noise terms.\", \"2. Derive an upper bound on the propagated error of the attention output, considering the statistical properties of the activations and the Lipschitz continuity of the softmax function.\", \"3. Use the search tool to find recent literature on quantization-aware training and post-training quantization in Transformers, specifically focusing on error analysis and stability improvements.\", \"4. Implement the INT8 quantization scheme and apply it to a pre-trained BERT model. Validate the implementation by comparing quantized tensor outputs against FP32 outputs on a sample batch.\", \"5. Run experiments to measure the perplexity degradation on the WikiText-2 dataset and check if the empirical error falls within the derived theoretical bounds.\", \"6. Audit the final report to ensure the mathematical derivations, code implementation, empirical results, and literature citations are consistent and correctly presented.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math informs the search queries for /general-search. Both outputs inform the implementation and validation metrics for /code. The final results are cross-verified against the initial derivation. Fallback: If empirical results from /code significantly deviate from the theoretical bounds, revisit the assumptions in the /math derivation and refine the search query.\", \"todo_list\": [\"- [ ] /math: Derive error propagation bounds for INT8 quantization in self-attention.\", \"- [ ] /general-search: Find and summarize \u22652 key papers on Transformer quantization stability and error analysis.\", \"- [ ] /code: Implement symmetric per-tensor PTQ for a BERT-base model's attention layers using PyTorch.\", \"- [ ] /code: Measure perplexity on the WikiText-2 validation set for both FP32 and INT8 models.\", \"- [ ] /math: Verify that the assumptions made in the derivation (e.g., input distribution) are plausible for a pre-trained model's activations.\", \"- [ ] /code: Validate the Python implementation by checking intermediate tensor statistics and comparing against a known reference if available.\", \"- [ ] /router: Perform final QA review, ensuring theoretical bounds and empirical results are reconciled in the final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"transformer-optimization\", \"quantization\", \"numerical-stability\"], \"acceptance_criteria\": [\"- The mathematical derivation of the error bound must be complete, rigorous, and self-contained.\", \"- The Python code must successfully execute and reproduce the reported perplexity scores within a 5% tolerance.\", \"- The final report must clearly connect the theoretical analysis to the empirical results and cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5533, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nFor the non-convex minimax objective `f(x, y) = (x^2 - 1)^2 - (y - x^2)^2`, find all critical points, compute the Hessian matrix, and use its eigenvalue spectrum to classify each. Subsequently, implement gradient descent-ascent (GDA) to visualize the vector field and validate the theoretical convergence behavior.\n", "completion": "{\"route_plan\": [\"/math(Analytically derive the gradient and Hessian of f(x, y) = (x^2 - 1)^2 - (y - x^2)^2. Solve for all critical points where the gradient is zero and calculate the Hessian eigenvalue at each point to classify them.)\", \"/general-search(query=\\\"site:distill.pub site:arxiv.org 'gradient descent ascent' 'limit cycles' 'saddle point dynamics' visualization\\\", mode=rag)\", \"/code(Implement gradient descent-ascent in Python using NumPy/JAX. Visualize the function's contour plot, overlay the gradient vector field, and plot trajectories from various initializations to demonstrate convergence or divergence near the identified saddle points.)\"], \"route_rationale\": \"/math is first to establish the ground-truth analytical properties (critical points, stability). The blue /general-search agent is then used to find state-of-the-art visualization techniques and known dynamical behaviors for GDA near saddles. Finally, /code implements the algorithm and creates visualizations to empirically verify the mathematical derivations.\", \"expected_artifacts\": [\"A formal mathematical derivation of critical points and their classification via Hessian eigenvalue analysis.\", \"Python code implementing the GDA algorithm and generating visualizations (contour plot, vector field, trajectories).\", \"A summary report including the generated plots and a brief discussion connecting the analytical findings to the numerical results.\", \"A list of at least two cited papers or articles on saddle point optimization dynamics.\", \"A verification log confirming that the numerically computed gradient field matches the analytical derivation at sample points.\"], \"thinking_outline\": [\"1. Derive the partial derivatives of `f(x,y)` to find the gradient `\u2207f`.\", \"2. Set `\u2207f = [0, 0]` and solve the resulting system of nonlinear equations to find all critical points `(x*, y*)`.\", \"3. Compute the Hessian matrix `H(x,y)` of second-order partial derivatives.\", \"4. For each critical point, evaluate `H(x*,y*)` and compute its eigenvalues to classify the point (positive definite -> local min, negative definite -> local max, indefinite -> saddle).\", \"5. Validate the numerical implementation by checking if the GDA dynamics (attraction/repulsion) near each critical point align with the Hessian eigenvalue classification.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivations from /math provide the ground truth for what /code should visualize. /general-search provides context on best practices for this visualization. The final code and plots from /code are sent for QA, where they are checked against the mathematical proofs. Verification: Do the visualized trajectories match the stability predicted by the Hessian eigenvalues? Fallback: If there's a mismatch, re-run /math to check for algebraic errors.\", \"todo_list\": [\"- [ ] /math: Derive the gradient and Hessian, then solve for all critical points and their classifications.\", \"- [ ] /general-search: Find relevant literature on visualizing GDA dynamics and potential limit cycles.\", \"- [ ] /code: Implement the GDA algorithm in Python.\", \"- [ ] /code: Generate contour plots, vector fields, and trajectory visualizations.\", \"- [ ] /code: Validate the numerical gradient implementation against the analytical formula from /math at 5 random points.\", \"- [ ] /router: Perform final QA by comparing the mathematical classification of critical points with the visualized GDA behavior.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"optimization-theory\", \"saddle-point-analysis\", \"gradient-dynamics\"], \"acceptance_criteria\": [\"- All critical points of the function must be correctly identified and classified (local min/max, saddle) with supporting Hessian eigenvalue calculations.\", \"- The generated visualizations must clearly show the function's contour, the gradient vector field, and example GDA trajectories.\", \"- The observed behavior of GDA trajectories near critical points in the plots must be consistent with their mathematical classification.\", \"- The final report must cite at least two relevant papers or authoritative blog posts.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5585, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for a sparse Gaussian Process regression model using stochastic variational inference with inducing points. Then, implement this in Python using JAX, demonstrating its computational advantage over a full GP on a synthetic dataset with a non-stationary kernel. Finally, find and cite key literature from arXiv on the convergence guarantees of such Bayesian variational approximations, particularly within the PAC-Bayesian framework.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) for sparse Gaussian Process regression using variational inference with inducing points, clearly defining the prior, variational posterior, and likelihood terms to ensure theoretical Bayesian convergence.)\", \"/code(Implement the stochastic variational inference for sparse Bayesian Gaussian Processes in JAX, comparing its runtime complexity and predictive log-likelihood convergence against a full GP baseline on a synthetic dataset., using Python)\", \"/general-search(query=\\\"site:arxiv.org PAC-Bayesian convergence guarantees sparse Gaussian Process variational inference\\\", mode=web)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation by deriving the ELBO. This is followed by /code to empirically validate the theory, implement the algorithm, and quantify its performance benefits. Finally, the blue /general-search agent is tasked with gathering peer-reviewed literature to situate the theoretical and empirical results within the broader academic context of Bayesian convergence guarantees.\", \"expected_artifacts\": [\"A LaTeX document with the full ELBO derivation for the sparse GP.\", \"A Python script implementing the Stochastic Variational GP and the full GP baseline.\", \"A summary report of the literature review with at least two cited arXiv papers.\", \"A plot comparing the predictive log-likelihood and training time of the sparse vs. full GP.\", \"A verification log confirming the gradient implementation matches the analytical derivation.\"], \"thinking_outline\": [\"1. Formulate the full Gaussian Process regression model, defining the kernel, prior, and likelihood.\", \"2. Introduce the concept of inducing points and derive the variational lower bound (ELBO) for the sparse approximation.\", \"3. Implement both the full GP and the sparse variational GP in Python, focusing on computational efficiency for the latter.\", \"4. Validate the implementation by comparing the sparse GP's predictive performance and runtime against the full GP on a controlled synthetic dataset.\", \"5. Conduct a literature search to find theoretical papers discussing the convergence and error bounds of this variational approximation.\", \"6. Verify that the derived ELBO in the mathematical proof aligns precisely with the loss function implemented in the code using numerical gradient checks.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: /code agent verifies gradients against /math derivation; router checks if literature from /general-search supports the theoretical claims; fallback: if derivation is incorrect, return to /math with corrections from /code agent's numerical checks).\", \"todo_list\": [\"- [ ] /math: Derive the ELBO for the sparse GP with inducing points.\", \"- [ ] /code: Implement the SVI-GP algorithm in JAX.\", \"- [ ] /code: Implement the baseline full GP for comparison.\", \"- [ ] /code: Generate synthetic data and create plots comparing model performance and runtime.\", \"- [ ] /math: Verify that the gradients of the implemented ELBO in `/code` match the analytical gradients derived from the `/math` step using numerical checks.\", \"- [ ] /general-search: Find and summarize at least two key papers on PAC-Bayesian bounds for sparse GPs.\", \"- [ ] /router: Perform final QA review of all artifacts for correctness and consistency.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"gaussian-processes\", \"variational-inference\", \"bayesian-deep-learning\"], \"acceptance_criteria\": [\"The mathematical derivation of the ELBO is complete and mathematically sound.\", \"The Python implementation correctly runs and demonstrates a clear computational advantage (e.g., >5x speedup on a large dataset) for the sparse GP over the full GP while maintaining a comparable predictive log-likelihood.\", \"The literature review cites at least two relevant arXiv papers with IDs, summarizing their findings on convergence guarantees.\", \"The verification step confirming the alignment between the mathematical derivation and the code implementation passes.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5595, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nImplement a Bayesian Gaussian Process regressor to model non-linear data, deriving its posterior predictive distribution and analyzing the convergence properties of its kernel hyperparameters using Markov Chain Monte Carlo.\n", "completion": "{\"route_plan\": [\"/math(Derive the posterior predictive mean and covariance for a Gaussian Process regression model, assuming a squared exponential kernel. State the log marginal likelihood function required for Bayesian hyperparameter optimization.)\", \"/general-search(query=\\\"site:arxiv.org gaussian process regression MCMC hyperparameter tuning convergence diagnostics\\\", mode=both)\", \"/code(Implement the Gaussian Process regressor in Python using JAX. Include a Metropolis-Hastings sampler for kernel hyperparameter inference and a Bayesian linear regression baseline. Evaluate MCMC convergence using the Gelman-Rubin diagnostic.)\"], \"route_rationale\": \"/math is required first to establish the core equations for the Gaussian Process. The blue /general-search agent is then tasked with finding authoritative literature on MCMC sampling techniques and convergence diagnostics specific to GPs, providing robust methods for the implementation phase. Finally, /code synthesizes the mathematical theory and researched best practices into a working, verifiable JAX implementation.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the GP posterior predictive distribution.\", \"A Python script (`gp_mcmc.py`) with the complete implementation of the GP regressor, MCMC sampler, and baseline model.\", \"A summary report in Markdown (`report.md`) presenting the model comparison, final metrics, and plots of the predictive distribution and MCMC traces.\", \"A JSON file (`metrics.json`) containing the final NLL, RMSE, and R-hat values.\", \"A list of citations including at least two arXiv paper IDs.\"], \"thinking_outline\": [\"1. Formulate the Gaussian Process regression model mathematically, defining the prior over functions and the Gaussian likelihood.\", \"2. Derive the closed-form expressions for the posterior predictive distribution's mean and covariance, conditioned on the training data.\", \"3. Construct the log marginal likelihood function, which will be the target for hyperparameter optimization via a Bayesian MCMC approach.\", \"4. Implement the GP model and a Metropolis-Hastings sampler to generate posterior samples for the kernel's length-scale and variance hyperparameters.\", \"5. Validate the MCMC sampler's convergence by running multiple chains and checking that the Gelman-Rubin statistic (R-hat) is close to 1.0 for all parameters.\"], \"handoff_plan\": \"The /math agent will produce the core equations, which are handed directly to the /code agent. Concurrently, the /general-search agent provides research papers and diagnostic techniques to the /code agent. The /code agent then implements the model and produces all artifacts. /math -> /code <- /general-search -> router QA. The final router QA step verifies that the code's output (e.g., predictive mean) is consistent with the derived equations and that convergence metrics are met. Fallback: If MCMC convergence fails (R-hat > 1.1), the /code agent is instructed to re-run with more samples or switch to a simpler Maximum a Posteriori (MAP) estimation using gradient descent.\", \"todo_list\": [\"- [ ] /math: Derive the posterior predictive equations and log marginal likelihood for the GP.\", \"- [ ] /general-search: Find at least two papers on MCMC for GP hyperparameter tuning and convergence diagnostics.\", \"- [ ] /code: Implement the Gaussian Process model, MCMC sampler, and Bayesian linear regression baseline on a synthetic dataset.\", \"- [ ] /code: Generate plots of the predictive posterior mean/variance and MCMC trace plots for each hyperparameter.\", \"- [ ] /code: Validate MCMC convergence by calculating the Gelman-Rubin statistic for multiple chains.\", \"- [ ] router: Perform final QA, checking that the metrics in `metrics.json` align with the `acceptance_criteria` and the plots in the report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"stochastic-processes\", \"bayesian-inference\", \"gaussian-processes\"], \"acceptance_criteria\": [\"- The mathematical derivations for the GP posterior predictive mean and covariance must be correct and complete.\", \"- The final GP model must achieve a lower Negative Log-Likelihood (NLL) on the test set than the Bayesian Linear Regression baseline.\", \"- The Gelman-Rubin statistic (R-hat) for all sampled hyperparameters must be less than 1.1, indicating successful MCMC convergence.\", \"- The final report must include visualizations of both the model's predictive uncertainty and the MCMC trace plots.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5615, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the analytical form for the post-interventional distribution P(Y|do(X=x)) in a non-parametric Bayesian Structural Equation Model with additive noise. Assume the causal graph is a Directed Acyclic Graph (DAG) but its structure is unknown. Provide a proof of identifiability under the faithfulness assumption and implement a PC-algorithm variant using conditional independence tests based on bootstrapped mutual information to recover the causal graph from observational data.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org non-parametric structural causal model identifiability do-calculus PC algorithm\\\", mode=web)\", \"/math(Derive the interventional distribution P(Y|do(X)) using do-calculus and prove identifiability for the specified bayesian causal model, ensuring convergence criteria are discussed.)\", \"/code(Implement a Python variant of the PC algorithm for causal graph discovery using a bootstrap mutual information test, structuring the implementation for a typical bioinformatics dataset.)\", \"/math(Formulate a verification test case with a known causal DAG and generate synthetic data; analytically compute the true interventional distribution to validate the code's output based on econometrics principles.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on causal discovery and identifiability. This informs the /math agent's formal derivation and proof. The /code agent then implements the derived theory. A final /math step is crucial for verification, creating a ground-truth scenario to validate the code's correctness against analytical results.\", \"expected_artifacts\": [\"A LaTeX document containing the full derivation of the post-interventional distribution and the proof of identifiability.\", \"A Python script implementing the PC algorithm with bootstrapped mutual information tests.\", \"A verification report in Markdown comparing the algorithm's output on synthetic data against the analytical ground truth, including performance metrics.\", \"A bibliography file with citations to at least 3 seminal papers on causal inference.\"], \"thinking_outline\": [\"1. Review seminal literature on non-parametric Structural Causal Models (SCMs), do-calculus, and conditions for causal identifiability (e.g., backdoor criterion, faithfulness).\", \"2. Formally derive the adjustment formula for the post-interventional distribution P(Y|do(X=x)) based on the model assumptions and prove that the causal effect is identifiable from observational data.\", \"3. Implement the three phases of the PC algorithm: skeleton discovery using conditional independence tests, orientation of v-structures, and propagation of edge orientations.\", \"4. Validate the core conditional independence test implementation using statistical tests on data with known dependence structures.\", \"5. Construct a complete end-to-end verification pipeline: generate data from a known SCM, apply the implemented algorithm to recover the graph and estimate an interventional effect, then audit the result against the known ground truth.\", \"6. Consolidate the mathematical proofs, code implementation, and verification results into the final artifacts.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: The final math agent verifies the code's output against a known analytical solution; fallback: if the code fails verification, the model assumptions in the initial math derivation may be too strong and require revision).\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on non-parametric causal discovery, identifiability, and the PC algorithm.\", \"- [ ] /math: Derive the post-interventional distribution P(Y|do(X)) using the rules of do-calculus.\", \"- [ ] /math: Formally prove the conditions for identifiability in this model class under the faithfulness assumption.\", \"- [ ] /code: Implement the PC algorithm skeleton search and the bootstrapped mutual information test.\", \"- [ ] /code: Implement the edge orientation rules for the discovered graph skeleton.\", \"- [ ] /math: Design a synthetic data generator from a known SCM and derive the ground-truth interventional distribution.\", \"- [ ] /math: Verify that the code's output on the synthetic data aligns with the analytical ground truth derivation.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-inference\", \"structural-causal-model\", \"do-calculus\"], \"acceptance_criteria\": [\"- The proof of identifiability is mathematically sound, complete, and correctly applies the required assumptions (e.g., faithfulness, causal sufficiency).\", \"- The Python implementation correctly recovers the true DAG from synthetic data, achieving a Structural Hamming Distance (SHD) of 0.\", \"- The estimated post-interventional distribution from the code matches the analytically derived ground truth, achieving a Kullback-Leibler (KL) divergence of less than 0.01.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the Kullback-Leibler divergence between the probability mass function of the estimated post-interventional distribution and the true one derived analytically from the synthetic SCM.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate the SHD (number of edge additions, deletions, or reversals needed to transform the estimated graph into the true DAG) between the recovered causal graph and the ground truth graph.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5716, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to understand the foundations of regularization in deep learning. Can you please explain how Lagrange Multipliers are used for constrained optimization to derive the solution for L2-regularized linear regression (Ridge Regression)?\n", "completion": "{\"route_plan\": [\"/math(Formulate the L2-regularized linear regression problem as an equality-constrained optimization task. Use the method of Lagrange Multipliers to derive the objective function's gradient, find the optimal weights, and discuss the Hessian eigenvalue properties for convexity.)\", \"/code(Implement the derived closed-form solution for Ridge regression in Python using NumPy. Validate the implementation by checking its output against scikit-learn's `Ridge` model on synthetic data to ensure numerical convergence.)\", \"/general-search(query=\\\"site:arxiv.org OR site:*.edu 'Lagrange multipliers in machine learning' 'Bayesian interpretation of ridge regression'\\\", mode=rag)\"], \"route_rationale\": \"The route starts with /math to establish the core theoretical derivation using multivariable calculus. This is followed by /code to provide an empirical validation of the theory, a crucial step in building confidence in the result. Finally, the blue /general-search agent is used to find authoritative sources that contextualize the derivation within the broader fields of machine learning and Bayesian statistics.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Ridge Regression solution using the method of Lagrange Multipliers.\", \"A Python script (`.py`) implementing the derived solution and generating a verification plot comparing its predictions to scikit-learn.\", \"A summary markdown report (`.md`) explaining the connection between the Lagrange multiplier (\u03bb) and the regularization hyperparameter (\u03b1).\", \"A metrics file (`.json`) reporting the numerical difference between the custom and library implementations.\", \"A bibliography file (`.bib`) with at least two cited academic sources.\"], \"thinking_outline\": [\"1. Define the L2-regularized linear regression objective function and the corresponding equality constraint on the L2 norm of the weights.\", \"2. Construct the Lagrangian function by incorporating the constraint with a Lagrange multiplier, \u03bb.\", \"3. Calculate the partial derivatives of the Lagrangian with respect to the model weights and set them to zero to find the critical points.\", \"4. Solve the resulting system of linear equations to derive the closed-form solution for the optimal weights in terms of the data and \u03bb.\", \"5. Validate the derived mathematical solution by implementing it in Python and verifying its output against a standard machine learning library's implementation on synthetic data.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: code output must match the library result within a tolerance of 1e-6; fallback: if derivation is incorrect or code fails verification, re-run /math with revised constraints). The math agent's derivation provides the direct formula for the code agent to implement. The code agent's successful validation confirms the derivation. The search agent provides context for the final report.\", \"todo_list\": [\"- [ ] Use /math to derive the analytical solution for Ridge regression using Lagrange Multipliers.\", \"- [ ] Use /code to implement the derived formula in Python and generate a verification plot comparing it to scikit-learn.\", \"- [ ] Use /general-search to find two authoritative sources explaining the role of constrained optimization in machine learning.\", \"- [ ] [VERIFY] Check that the MSE Delta and Weight Vector L2 Norm Delta metrics are below the 1e-6 threshold.\", \"- [ ] Synthesize all artifacts into a coherent markdown report, including citations.\", \"- [ ] Submit final artifacts for router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"multivariable-calculus\", \"constrained-optimization\", \"regularization\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly uses Lagrange Multipliers to arrive at the standard closed-form solution for Ridge Regression.\", \"- The Python implementation's weight vector is numerically equivalent (within a tolerance of 1e-6) to the one produced by `sklearn.linear_model.Ridge`.\", \"- The final report includes at least two citations from authoritative sources (e.g., university course notes, peer-reviewed articles, or established textbooks).\", \"- The explanation correctly links the Lagrange multiplier to the regularization hyperparameter alpha.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5741, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the policy gradient theorem for a stochastic policy, then implement the REINFORCE algorithm with a value function baseline to reduce variance and demonstrate its superior convergence on a control theory task. Explain the causal justification for using a state-dependent baseline.\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient theorem and formally prove that introducing a state-dependent baseline maintains an unbiased gradient estimate, analyzing the conditions for convergence.)\", \"/general-search(query=\\\"site:arxiv.org policy gradient variance reduction baseline REINFORCE\\\", mode=web. Find foundational papers on actor-critic methods and the causal interpretation of baselines in reinforcement learning.)\", \"/code(Implement REINFORCE with and without a learned value function baseline in PyTorch for a classic control theory environment, plotting reward curves and gradient variance to validate the theory., using Python)\"], \"route_rationale\": \"The route starts with /math for the foundational theoretical derivation. The blue /general-search agent is then tasked with gathering supporting literature and advanced context. Finally, /code is used for empirical validation of the derived theory, implementing the algorithms and producing quantitative results for comparison.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the policy gradient theorem and the unbiased baseline property.\", \"- A Python script implementing both vanilla REINFORCE and REINFORCE with a baseline.\", \"- A plot comparing the learning curves (total reward vs. episodes) and gradient variance for both algorithms.\", \"- A brief report summarizing the findings and citing at least two relevant research papers.\"], \"thinking_outline\": [\"1. From first principles, derive the policy gradient theorem, expressing the gradient of the expected total reward with respect to the policy parameters.\", \"2. Introduce a generic state-dependent baseline into the formulation and prove mathematically that it does not bias the gradient estimate.\", \"3. Implement the vanilla REINFORCE (Monte-Carlo policy gradient) algorithm and a second version that co-learns a value function to serve as the baseline.\", \"4. Train both agents on a standard environment like CartPole-v1, collecting data on episode rewards and the empirical variance of the gradient estimates.\", \"5. Validate that the baseline-enabled agent exhibits lower gradient variance and achieves higher average rewards more quickly than the vanilla agent, confirming the theory.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the theoretical foundation. /general-search enriches this with peer-reviewed sources. /code implements the derived concepts. The final results are handed to the router for quality assurance. Verification: Code outputs must match theoretical predictions (i.e., faster convergence with baseline). Fallback: If code performance is poor, re-audit the /math derivation for potential errors and consult /general-search for alternative baseline implementations.\", \"todo_list\": [\"- [ ] /math: Derive the policy gradient theorem and prove the unbiased nature of a state-dependent baseline.\", \"- [ ] /general-search: Find 2+ papers on arXiv detailing variance reduction techniques for policy gradients, focusing on baselines.\", \"- [ ] /code: Implement vanilla REINFORCE and REINFORCE with a learned value function baseline for the CartPole-v1 environment using PyTorch.\", \"- [ ] /code: Instrument the code to log total reward, loss, and the empirical variance of the policy gradient estimates per training batch.\", \"- [ ] /code: Validate that the average reward trend for the baseline agent is statistically superior to the vanilla agent after a fixed number of episodes.\", \"- [ ] /router: Perform final QA check on the generated report, code, and plots to ensure all acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"variance-reduction\"], \"acceptance_criteria\": [\"- The mathematical derivation of the policy gradient theorem and the unbiased baseline property is correct and clearly presented.\", \"- The Python implementation correctly runs, and the agent with the baseline demonstrably achieves a higher average return faster than the vanilla agent on CartPole-v1.\", \"- The final report includes a plot comparing the learning curves (total reward vs. episodes) and the gradient variances, along with citations for at least two relevant papers.\", \"- The code is well-commented and includes instructions for reproducing the results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5763, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to understand the foundational theory of denoising diffusion models. Can you derive the forward and reverse process equations and explain the connection to score matching used in modern generative AI?\n", "completion": "{\"route_plan\": [\"/math(Derive the key equations for a variance-preserving forward diffusion process and the corresponding reverse-time SDE, highlighting the role of the score function.)\", \"/code(Implement a simple 1D diffusion model in Python for a Gaussian mixture, tracking the convergence of the learned score network and visualizing the reverse sampling trajectory.)\", \"/general-search(query=\\\"site:arxiv.org denoising diffusion probabilistic models OR score-based generative modeling tutorial\\\", mode=rag)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation by deriving the core equations. This theory is then validated practically using /code to build and visualize a working 1D model. Finally, the blue /general-search agent is used to ground the work in established academic literature and provide authoritative citations.\", \"expected_artifacts\": [\"A self-contained mathematical derivation of the forward and reverse diffusion processes.\", \"A Python script implementing a 1D diffusion model for a Gaussian target distribution.\", \"A plot visualizing the reverse process, transforming a noise distribution into the target data distribution.\", \"A list of at least two seminal papers on diffusion models with their arXiv IDs.\", \"A JSON file containing the computed metrics (KL Divergence, Sample Mean/Variance Error).\"], \"thinking_outline\": [\"1. Formulate the forward noising process as a discrete-time Markov chain that incrementally adds Gaussian noise to data.\", \"2. Derive the analytical form of the posterior q(x_{t-1}|x_t, x_0) which is crucial for training the reverse process.\", \"3. Show that the reverse denoising process p_theta(x_{t-1}|x_t) can be approximated by a neural network trained to predict the noise, which is equivalent to learning the score function.\", \"4. Implement this process in Python for a simple 1D Gaussian distribution, training a small neural network on the denoising objective.\", \"5. Validate the implementation by sampling from the learned reverse process and comparing the resulting distribution's statistics to the ground truth distribution.\", \"6. Use search to find and cite the foundational papers that introduced these core concepts, such as DDPMs.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent produces the core equations. The /code agent ingests these equations to build a simulation, verifying that the implementation aligns with the theory. The /general-search agent finds references that corroborate the derivation from /math. Final artifacts are passed to router QA. Verification: The mean and variance of samples generated by the /code agent must match the target distribution within a 10% tolerance. Fallback: If verification fails, the /code agent will debug the implementation against the /math agent's derivation.\", \"todo_list\": [\"- [ ] /math: Derive the forward process q(x_t|x_0) and the reverse process posterior q(x_{t-1}|x_t, x_0).\", \"- [ ] /code: Implement the 1D diffusion model, including the training loop and sampling function.\", \"- [ ] /code: Generate a plot visualizing the full reverse sampling process from noise to data.\", \"- [ ] /general-search: Find and list at least two foundational papers on diffusion models (e.g., Sohl-Dickstein et al. 2015, Ho et al. 2020).\", \"- [ ] /router-qa: Submit all artifacts (derivation, code, plot, citations, metrics) for final approval.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"diffusion-models\", \"generative-ai\", \"score-matching\"], \"acceptance_criteria\": [\"- The mathematical derivation for the forward and reverse processes must be correct and clearly explained.\", \"- The Python code must execute without errors and produce a plot that visually confirms the transformation from noise to a recognizable target distribution.\", \"- The final report must cite at least 2 relevant arXiv papers with valid IDs.\", \"- The mean and variance of the generated samples must be within a 10% error margin of the target distribution's true parameters.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"primary_computation\": \"Discretize the domain and compute the KL divergence between the probability density function of the generated samples and the target Gaussian distribution.\", \"secondary_computation\": \"Calculate the absolute percentage error between the empirical mean/variance of N=1000 generated samples and the true mean/variance of the target distribution.\"}}", "categories": ["math_first", "metrics_computation", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5833, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the evidence lower bound (ELBO) for a Bayesian neural network using a structured mean-field approximation where the variational posterior q(W) over the weights W is modeled by a matrix variate Gaussian distribution. Implement this in Python using PyTorch and demonstrate its convergence properties on a regression task, comparing its performance against a standard diagonal Gaussian (unstructured mean-field) assumption. Finally, find and cite literature that discusses the trade-offs in computational complexity and posterior approximation accuracy for such structured variational Bayesian methods.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) for a Bayesian neural network with a matrix variate Gaussian posterior, ensuring the final expression for the KL divergence term is tractable for eventual convergence analysis.)\", \"/code(Implement the derived Bayesian objective in PyTorch, using the reparameterization trick for the matrix variate posterior to train on a bioinformatics regression task, and track the test set AUPRC., using Python)\", \"/general-search(query=\\\"site:arxiv.org structured variational inference matrix variate gaussian bayesian neural network complexity\\\", mode=both)\"], \"route_rationale\": \"/math is used first to perform the symbolic derivation of the ELBO, which is the core theoretical task. /code then takes this mathematical object and implements it to provide an empirical validation of the theory. Finally, the blue /general-search agent is tasked with contextualizing the theoretical and empirical results by finding authoritative literature on the trade-offs of this advanced technique.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the ELBO in LaTeX format.\", \"- A runnable Python script implementing the specified BNN and training loop.\", \"- A plot showing ELBO convergence and predictive uncertainty on a test set.\", \"- A summary report including a metrics table and citations for at least two relevant papers.\", \"- A verification document confirming the alignment between the mathematical derivation and the implemented loss function.\"], \"thinking_outline\": [\"1. Formulate the Bayesian neural network model and define the probabilistic graphical model, specifying the prior p(W) and the matrix variate Gaussian variational posterior q(W).\", \"2. Derive the ELBO, L(q), paying special attention to analytically solving the KL-divergence term, KL(q(W) || p(W)), between the matrix variate posterior and the standard normal prior.\", \"3. Implement the BNN, the structured variational posterior using the reparameterization trick, and the derived ELBO as the loss function in Python.\", \"4. Validate the implementation by checking for ELBO convergence on a synthetic regression dataset and comparing gradient magnitudes to a baseline.\", \"5. Evaluate the trained model's predictive log-likelihood and uncertainty estimates on a hold-out test set, comparing it to a standard mean-field BNN.\", \"6. Audit the theoretical claims and empirical results against findings from relevant academic literature on structured variational inference.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent produces a LaTeX derivation of the ELBO. This is handed to /code for implementation. /code must verify that its loss function numerically matches the symbolic form (verification: gradient checking; fallback: request review from /math). The empirical results from /code are then passed to /general-search to find supporting literature. All artifacts are finally passed to the router for a consistency and quality check.\", \"todo_list\": [\"- [ ] /math: Derive the full ELBO expression for the BNN with a matrix variate Gaussian posterior.\", \"- [ ] /math: Analytically solve the KL-divergence term between the matrix variate Gaussian posterior and a standard matrix normal prior.\", \"- [ ] /code: Implement the BNN model, data loader, and the matrix variate Gaussian variational family.\", \"- [ ] /code: Implement the reparameterization trick for sampling from the variational posterior and the ELBO loss function.\", \"- [ ] /code: Train the model, log metrics, and generate plots for convergence and predictive uncertainty.\", \"- [ ] /math: Verify that the implemented loss function in the Python code is a correct representation of the derived mathematical expression.\", \"- [ ] /general-search: Find and summarize at least two arXiv papers discussing the computational complexity and approximation accuracy of structured VI methods.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-neural-networks\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"- The provided mathematical derivation of the ELBO for the matrix variate Gaussian posterior is complete and correct.\", \"- The Python code successfully trains the model, demonstrating non-divergent ELBO convergence over at least 100 epochs.\", \"- The final report cites at least two relevant arXiv papers with IDs, directly addressing structured variational inference.\", \"- A verification check confirms that the symbolic KL divergence term from the derivation matches the value computed by the code's loss function for a given input.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5873, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project on physics-informed neural networks (PINNs) and need a foundational example. Can you walk me through how to formulate the residual loss for the 1D heat equation, derive its form, and then implement a simple PINN in Python to solve it?\n", "completion": "{\"route_plan\": [\"/math(Formulate the 1D heat partial differential equation with initial and boundary conditions. Derive the physics-informed neural network loss function as the sum of squared errors from the PDE residual, boundary conditions, and initial conditions, referencing the role of Fourier analysis in classical solutions.)\", \"/code(Implement a simple feed-forward neural network in PyTorch to solve the 1D heat equation. Define the custom loss function based on the PDE residual and boundary conditions, and train the network using automatic differentiation to monitor convergence of the solution., using Python)\", \"/general-search(query=\\\"site:arxiv.org physics-informed neural networks 1D heat equation introductory tutorial\\\", mode=web)\"], \"route_rationale\": \"/math is used for the formal derivation of the PDE and the associated PINN loss function. /code translates this mathematical formulation into a working PyTorch implementation for validation. The blue /general-search agent is used to find foundational papers and tutorials to ground the work in existing literature.\", \"expected_artifacts\": [\"A markdown document with the mathematical derivation of the 1D heat equation and the complete PINN loss function.\", \"A Python script (`pinn_heat_equation.py`) implementing the PINN solver using PyTorch.\", \"A plot comparing the PINN's predicted solution to the analytical solution, and a second plot showing the training loss convergence.\", \"A summary of findings with citations to at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Define the 1D heat equation, including its initial and boundary conditions.\", \"2. Formulate the total loss function for a PINN, breaking it down into components: PDE residual loss, initial condition loss, and boundary condition loss.\", \"3. Implement a neural network in Python (PyTorch) that takes position `x` and time `t` as inputs and outputs the temperature `u(x, t)`.\", \"4. Use automatic differentiation to compute the partial derivatives required for the PDE residual and implement the custom loss function.\", \"5. Train the network and validate the final solution by comparing it to a known analytical solution, checking for numerical accuracy.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The math agent provides the exact loss function formulation. The code agent implements this derived loss in PyTorch. The general-search agent finds references to contextualize the method. Router QA verifies that the implemented loss in the code correctly matches the mathematical derivation and that the results align with principles from the cited literature. Fallback: If the code fails to converge, revisit the network architecture or training parameters based on best practices found in the literature search.\", \"todo_list\": [\"- [ ] /math: Derive the total loss function for a PINN solving the 1D heat equation.\", \"- [ ] /code: Implement the PINN solver in PyTorch, including the custom loss function and training loop.\", \"- [ ] /general-search: Find and summarize two foundational papers on PINNs relevant to this problem.\", \"- [ ] /verification: Plot the PINN solution against the analytical solution and verify the mean squared error is below the 1e-3 threshold.\", \"- [ ] /router-qa: Submit all artifacts for final review and approval.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"physics-informed-ml\", \"partial-differential-equations\", \"neural-networks\"], \"acceptance_criteria\": [\"- The mathematical derivation of the PINN loss function for the 1D heat equation is present and correct.\", \"- The Python code runs without errors and produces a solution that visually approximates the analytical solution.\", \"- The final report includes a loss convergence plot and cites at least two relevant arXiv papers with their IDs.\", \"- The mean squared error between the PINN prediction and the analytical solution on a test grid is less than 1e-3.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5913, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the posterior predictive distribution for a Gaussian Process regression model with a squared exponential kernel. Then, implement this model in Python to perform Bayesian optimization on a noisy, non-convex function, and analyze its convergence properties.\n", "completion": "{\"route_plan\": [\"/math(Derive the posterior predictive distribution for Gaussian Process regression, focusing on the Bayesian inference steps for a multivariate normal likelihood and prior, ensuring the final covariance matrix is positive semi-definite.)\", \"/general-search(query=\\\"site:arxiv.org gaussian process bayesian optimization convergence analysis acquisition functions\\\", mode=both)\", \"/code(Implement the Gaussian Process regression model and a Bayesian optimization loop in Python using NumPy/SciPy, validating the model's convergence on the Branin-Hoo function by tracking the simple regret.)\"], \"route_rationale\": \"The plan starts with /math to establish the core theoretical equations for the Gaussian Process posterior. Next, the blue /general-search agent is used to gather context on standard benchmarks and recent literature for Bayesian optimization. Finally, /code implements the derived theory and validates its performance on a benchmark identified during the search phase.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Gaussian Process posterior predictive mean and covariance.\", \"A Python script (`gp_optimizer.py`) implementing the Bayesian optimization loop from scratch.\", \"A plot (`convergence.png`) showing the simple regret versus the number of iterations.\", \"A summary report (`report.md`) explaining the methodology, results, and citing at least two relevant academic papers.\", \"A verification log comparing the implemented posterior against a trusted library like scikit-learn.\"], \"thinking_outline\": [\"1. Formulate the Gaussian Process regression model, defining the prior distribution over functions (using a squared exponential kernel) and the Gaussian likelihood for the observed data.\", \"2. Use the properties of multivariate Gaussian distributions and the Schur complement to derive the analytical form of the posterior predictive mean and covariance for new, unseen points.\", \"3. Implement the derived equations in Python, including the kernel function and the logic for updating the posterior Cholesky decomposition for numerical stability.\", \"4. Construct a Bayesian optimization loop using an acquisition function (e.g., Expected Improvement) to intelligently select the next point to sample from a standard benchmark function.\", \"5. Validate the correctness of the GP implementation by comparing its calculated posterior mean and variance for a simple 1D case against the results from scikit-learn's `GaussianProcessRegressor`.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent produces the core equations. The /general-search agent provides benchmarks and literature context for the implementation. The /code agent receives the equations and context to build and test the model. The final artifacts are passed to the router for QA. Verification: The code output is checked against a reference library; Fallback: If the verification fails, the task is returned to /code with the error analysis, potentially triggering a re-derivation by /math if the bug is traced to the equations.\", \"todo_list\": [\"- [ ] /math: Derive the GP posterior predictive mean and covariance equations, showing all steps.\", \"- [ ] /general-search: Find and summarize two key papers on Bayesian optimization with Gaussian Processes from arXiv.\", \"- [ ] /general-search: Identify the formulation of the Branin-Hoo function and its known global minimum.\", \"- [ ] /code: Implement the GP model and the Bayesian optimization loop with an Expected Improvement acquisition function.\", \"- [ ] /code: Validate the implemented GP posterior against scikit-learn's implementation on a simple 1D dataset.\", \"- [ ] /code: Generate a plot of the simple regret vs. iteration number for the Branin-Hoo benchmark.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"bayesian-optimization\", \"gaussian-processes\", \"stochastic-processes\"], \"acceptance_criteria\": [\"The mathematical derivation of the posterior predictive distribution must be complete, correct, and clearly follow from Bayesian principles for Gaussian variables.\", \"The Python implementation must find a function value within 5% of the known global minimum of the Branin-Hoo test function within 100 iterations.\", \"The verification check must show a Mean Squared Error of less than 1e-9 between the custom GP implementation's posterior mean/variance and the scikit-learn baseline on a predefined test case.\", \"The final report must include the convergence plot and correctly cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"computation_guidance\": {\"Simple Regret\": \"Calculate as `min(f(x_i))` for `i=1..N` minus `f(x*)`, where `x*` is the true global minimum of the test function.\", \"Posterior Mean/Variance MSE\": \"On a fixed 1D grid of 100 points, compute the mean squared error between the posterior mean/variance vectors of the custom implementation and the scikit-learn reference model.\"}}}", "categories": ["math_first", "metrics_computation", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5958, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the preconditioned conjugate gradient (PCG) algorithm for solving large, sparse, symmetric positive-definite linear systems. Analyze its convergence properties in terms of the condition number of the preconditioned matrix, and then implement a CUDA-accelerated version in Python using CuPy, benchmarking its performance against a CPU-based SciPy implementation on a test matrix with a known eigenvalue distribution.\n", "completion": "{\"route_plan\": [\"/math(Derive the Preconditioned Conjugate Gradient (PCG) algorithm and formally prove its convergence rate is bounded by the condition number of the preconditioned matrix, focusing on the resulting eigenvalue distribution.)\", \"/general-search(query=\\\"site:arxiv.org cuda sparse matrix-vector product incomplete cholesky preconditioner performance\\\", mode=web)\", \"/code(Implement the PCG algorithm in Python using CuPy for CUDA acceleration, including an Incomplete Cholesky preconditioner. Benchmark convergence against SciPy's sparse solver, tracking residual norm and wall-clock time.)\"], \"route_rationale\": \"The /math tool is required for the formal derivation and proof of convergence, which is foundational. The 'blue' /general-search agent will then research state-of-the-art literature on efficient CUDA kernels for the key operations (SpMV, preconditioning) to ensure a high-performance implementation. Finally, /code will synthesize the mathematical theory and implementation research into a working, benchmarked CUDA program.\", \"expected_artifacts\": [\"- A LaTeX document (`derivation.tex`) with the full derivation of the PCG algorithm and its convergence proof.\", \"- A Python script (`pcg_benchmark.py`) implementing the CUDA-accelerated PCG solver and the benchmarking comparison.\", \"- A summary report (`report.pdf`) containing benchmark results, performance plots (time vs. matrix size), and a discussion of findings, including citations.\", \"- A JSON file (`metrics.json`) with the raw performance data: wall-clock times, iteration counts, and final residual norms for both CPU and GPU solvers.\"], \"thinking_outline\": [\"1. Formally derive the Preconditioned Conjugate Gradient (PCG) algorithm from the base Conjugate Gradient method, clearly defining the role of the preconditioner matrix M.\", \"2. Analyze the convergence rate of PCG, relating it to the spectral properties (eigenvalue distribution) of the preconditioned matrix M\u207b\u00b9A.\", \"3. Validate the theoretical convergence proof by constructing a small-scale numerical example and manually stepping through the first few iterations to check residual orthogonality.\", \"4. Research efficient CUDA implementations for sparse matrix-vector products (SpMV) and incomplete Cholesky factorization, the chosen preconditioner.\", \"5. Implement the PCG solver using CuPy for GPU acceleration and benchmark its performance against `scipy.sparse.linalg.cg` on a well-conditioned sparse matrix.\", \"6. Verify the CUDA implementation's correctness by comparing its final solution vector against the CPU baseline for numerical accuracy up to a specified tolerance.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math defines the algorithm's structure. /general-search uses these details to find optimal CUDA kernel strategies. /code implements the algorithm using these strategies. The final handoff to router QA involves a full audit. Verification: The code's logic must match the derivation's steps exactly. Fallback: If performance is poor, /general-search is re-engaged to find alternative preconditioning strategies or SpMV kernels.\", \"todo_list\": [\"- [ ] /math: Derive the PCG algorithm and its convergence properties based on the eigenvalue distribution.\", \"- [ ] /math: Formulate the proof relating the convergence rate to the condition number \u03ba(M\u207b\u00b9A).\", \"- [ ] /general-search: Find reference literature on efficient CUDA kernels for sparse matrix-vector products (SpMV) and incomplete Cholesky factorization.\", \"- [ ] /code: Implement the PCG solver in Python with CuPy, using the researched SpMV techniques.\", \"- [ ] /code: Create a benchmarking harness to compare the CUDA implementation against `scipy.sparse.linalg.cg`, measuring wall-clock time and iterations.\", \"- [ ] /math: Verify that the algorithm implemented in `/code` correctly reflects the derived mathematical formulation.\", \"- [ ] /code: Validate the numerical output of the CUDA solver against the SciPy reference implementation to ensure correctness within machine precision.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"krylov-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation of PCG and its convergence proof must be complete, formally correct, and submitted as a LaTeX file.\", \"- The CUDA implementation must execute without errors and produce a solution vector that matches the SciPy baseline's output within a relative L2 norm difference of 1e-6.\", \"- The final report must demonstrate a significant performance speedup (\u22655x wall-clock time) for the GPU implementation on a sparse matrix with at least 1 million non-zero elements.\", \"- The report must cite at least two relevant academic papers on GPU-accelerated linear algebra with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 5982, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the optimization objective for Model-Agnostic Meta-Learning (MAML) and contrast its gradient-based adaptation with a prototypical network's metric learning approach. Implement both for a few-shot image classification task on a subset of Omniglot and analyze their performance under a Bayesian framework for uncertainty estimation.\n", "completion": "{\"route_plan\": [\"/math(Derive the MAML update rule as a second-order Taylor expansion and contrast it with the prototypical network's Euclidean distance-based loss function, analyzing convergence properties of the inner loop.)\", \"/general-search(query=\\\"site:arxiv.org bayesian MAML vs prototypical networks uncertainty estimation few-shot learning\\\", mode=rag)\", \"/code(Implement MAML and Prototypical Networks in PyTorch for few-shot classification on Omniglot, using CUDA for acceleration, and report few-shot accuracy and adaptation time., using Python)\"], \"route_rationale\": \"/math is chosen to establish the theoretical foundations and formal objectives. The blue /general-search agent is then used to survey recent literature and gather contextual information on Bayesian extensions and SOTA comparisons. Finally, /code is used for empirical validation of the derived theories on a standard benchmark dataset.\", \"expected_artifacts\": [\"- A PDF document with the complete mathematical derivations for both MAML and Prototypical Networks.\", \"- A standalone Python script implementing both algorithms and the evaluation protocol.\", \"- A summary report in Markdown comparing the two methods, including performance plots and citations.\", \"- A `metrics.json` file containing the final evaluation results, including accuracy and adaptation time.\"], \"thinking_outline\": [\"1. Formally derive the MAML learning objective, detailing the inner and outer loop gradient updates.\", \"2. Formally derive the Prototypical Network loss function based on class prototypes in an embedding space.\", \"3. Implement both algorithms in PyTorch, including the data loading pipeline for few-shot episodes using the Omniglot dataset.\", \"4. Design and execute a set of experiments to compare the N-way, K-shot performance of both models.\", \"5. Validate the correctness of the MAML implementation by checking the computation of second-order gradients on a toy problem with a known solution.\", \"6. Synthesize the theoretical and empirical results into a comparative analysis, discussing the trade-offs between the two approaches.\"], \"handoff_plan\": \"/math (derivations) -> /general-search (literature context) -> /code (implementation & evaluation). The math agent's derivations must be verified for correctness before being handed to the code agent. The code agent will use the formalisms from /math and insights from /general-search to build the implementation. If the code agent's results deviate significantly from published benchmarks found by /general-search, a fallback is triggered to re-audit the implementation against the derivations. Final artifacts are passed to the router for QA.\", \"todo_list\": [\"- [ ] /math: Derive the complete loss functions and gradient update rules for both MAML and Prototypical Networks.\", \"- [ ] /general-search: Find and summarize two key papers that compare gradient-based and metric-based meta-learning.\", \"- [ ] /code: Implement the MAML and Prototypical Network algorithms for the Omniglot dataset in PyTorch.\", \"- [ ] /code: Design and run experiments to compare 5-way, 1-shot and 5-way, 5-shot classification accuracy.\", \"- [ ] /code: Validate the MAML gradient computation on a simple quadratic function to ensure the higher-order derivative logic is correct.\", \"- [ ] /router: Review all artifacts for mathematical correctness, code reproducibility, and consistency between the report and empirical results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"meta-learning\", \"few-shot-learning\", \"metric-learning\"], \"acceptance_criteria\": [\"- The provided mathematical derivations for both algorithms are complete and correct.\", \"- The submitted code is well-documented, runs without error, and reproduces the reported metrics within a 1% tolerance.\", \"- The final report accurately compares the two methods, referencing at least two relevant academic papers with their arXiv IDs.\", \"- The 5-way 1-shot accuracy on the Omniglot test set must exceed 90% for at least one of the implemented models.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6025, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a new project in reinforcement learning and need to understand the policy gradient theorem from first principles. Can you derive the core REINFORCE algorithm update rule and then implement it to solve the classic CartPole control theory problem?\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient theorem for the expected cumulative reward objective, clearly showing the 'log-derivative trick' that is central to this class of control theory problems.)\", \"/general-search(query=\\\"site:neurips.cc OR site:mit.edu 'simple statistical gradient-following algorithms for connectionist reinforcement learning' williams\\\", mode=both, context=\\\"Find foundational literature explaining the REINFORCE algorithm, focusing on materials that discuss its high variance and conditions for convergence.\\\")\", \"/code(Implement the REINFORCE algorithm in Python using PyTorch and Gymnasium to solve CartPole-v1, ensuring the agent model is simple and avoids complex Bayesian network structures.)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation by deriving the policy gradient theorem. Next, the blue /general-search agent is tasked with finding authoritative sources to ground the derivation in established literature. Finally, /code is used to translate the theory into a practical, working implementation and validate the approach on a classic benchmark.\", \"expected_artifacts\": [\"A Markdown file with a step-by-step LaTeX derivation of the REINFORCE update rule.\", \"A Python script (`reproduce.py`) that implements the REINFORCE algorithm for the CartPole-v1 environment.\", \"A plot (`learning_curve.png`) showing the total reward per episode over the course of training.\", \"A final JSON report summarizing key metrics, hyperparameters, and citations.\", \"A log file capturing the output of the training script, including final performance metrics.\"], \"thinking_outline\": [\"1. Define the objective function J(\u03b8) as the expected total reward for a stochastic policy \u03c0_\u03b8.\", \"2. Use the log-derivative trick to rewrite the gradient \u2207_\u03b8 J(\u03b8) into an expectation that can be estimated from sampled trajectories.\", \"3. Formulate the REINFORCE algorithm, which performs stochastic gradient ascent using this estimated gradient.\", \"4. Implement this algorithm in Python, including the policy network, sampling loop, and parameter update step.\", \"5. Validate the implementation by training an agent on the CartPole-v1 environment and verifying that its performance improves to meet the success criterion.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the core equation. /general-search corroborates this with foundational papers. Both artifacts are handed to /code for implementation. The output from /code (learning curve plot and metrics) is then passed to router QA for final review. Verification: The code's learning curve must show a clear positive trend. Fallback: If the agent fails to learn, the implementation logic in /code will be audited against the derivation from /math.\", \"todo_list\": [\"- [ ] /math: Derive the REINFORCE update rule from the policy gradient theorem.\", \"- [ ] /general-search: Find and summarize at least two foundational papers on policy gradients, such as Williams (1992).\", \"- [ ] /code: Write and execute a Python script to train a REINFORCE agent on CartPole-v1.\", \"- [ ] Verification: Check that the agent's average reward on CartPole consistently exceeds 195 over 100 consecutive episodes.\", \"- [ ] Router QA: Submit all artifacts (derivation, code, plot, citations, logs) for final approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"control-theory\"], \"acceptance_criteria\": [\"The mathematical derivation correctly applies the log-derivative trick to arrive at the REINFORCE gradient estimator.\", \"The Python implementation solves CartPole-v1, defined as achieving an average reward of at least 195.0 over 100 consecutive episodes.\", \"The final report includes a learning curve plot that clearly shows reward improvement over time.\", \"The solution cites at least two authoritative sources, including at least one arXiv ID or peer-reviewed paper DOI.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6089, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDevise an algorithm to solve the 'Ordered Checkpoint Path' problem on a weighted Directed Acyclic Graph (DAG). Given a set of k checkpoint vertices in a specific order, find a maximum weight path that starts anywhere, visits these checkpoints sequentially, and ends anywhere. Your solution must include a formal proof of optimality for the dynamic programming recurrence and analyze its time complexity.\n", "completion": "{\"route_plan\": [\"/math(Formulate the dynamic programming recurrence relation for the Ordered Checkpoint Path problem on a DAG, defining the state representation and transition logic. Analyze the theoretical time complexity with respect to vertices, edges, and checkpoints.)\", \"/general-search(query=\\\"site:arxiv.org site:cs.stackexchange.com 'dynamic programming on DAG' 'ordered constraints' pathfinding complexity\\\", mode=rag)\", \"/code(Implement the derived DP algorithm in Python using an adjacency list for the graph. Include comprehensive test cases with known optimal solutions for verification and ensure the implementation adheres to the proven time complexity.)\"], \"route_rationale\": \"The /math tool is essential for deriving the core dynamic programming recurrence and proving its optimality. The blue /general-search agent will then be used to survey existing literature on similar constrained pathfinding problems to ensure our approach is sound and well-contextualized. Finally, /code will translate the mathematical model into a functional, tested program for empirical validation.\", \"expected_artifacts\": [\"A formal mathematical proof of the DP recurrence's optimality and correctness.\", \"A well-documented Python script implementing the algorithm.\", \"A summary of findings from the literature search, including at least 2 relevant citations.\", \"A verification report comparing code output against hand-calculated small examples.\"], \"thinking_outline\": [\"1. Define the subproblem and state representation for the dynamic programming solution, likely in the form DP[i][v], representing the max weight path ending at vertex v after visiting the first i checkpoints.\", \"2. Formulate the base cases and the recurrence relation that computes DP[i][v] based on previously computed states for checkpoints i and i-1.\", \"3. Prove the optimality of the recurrence relation, likely using an inductive argument on the number of checkpoints visited.\", \"4. Analyze the time and space complexity of the overall algorithm in terms of the number of vertices (V), edges (E), and checkpoints (k).\", \"5. Validate the implementation by creating several small, non-trivial DAG examples, manually computing the optimal path, and verifying that the code produces the identical result.\"], \"handoff_plan\": \"/math (formulation) -> /general-search (validation of approach) -> /code (implementation) -> router QA (verification: check code output against analytical results for test cases; fallback: re-engage /math if logic is flawed).\", \"todo_list\": [\"- [ ] /math: Define the DP state, recurrence, and base cases for the Ordered Checkpoint Path problem.\", \"- [ ] /math: Provide a formal proof of correctness for the recurrence and analyze the complexity.\", \"- [ ] /general-search: Find literature on similar pathfinding problems with ordered constraints to contextualize the solution.\", \"- [ ] /code: Implement the DP algorithm in Python, including graph representation and memoization table.\", \"- [ ] /code: Develop a test suite with at least 3 distinct DAGs to validate the implementation's correctness.\", \"- [ ] /router: QA review of the final proof, code, and test results for consistency and correctness.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"dynamic-programming\", \"graph-theory\", \"algorithmic-competitions\"], \"acceptance_criteria\": [\"The DP recurrence relation must be mathematically sound and accompanied by a formal proof of optimality.\", \"The Python implementation must pass all provided test cases and run within the expected asymptotic time complexity.\", \"The final report must correctly cite at least two relevant academic papers or highly-regarded algorithmic resources.\", \"The time complexity analysis must be correct and rigorously derived.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6133, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive a formal proof using structural induction for the correctness of a tail-recursive list reversal function specified in higher-order logic. Then, synthesize a verified Python implementation using this proof as a guide, ensuring the synthesis process respects termination constraints derived from the inductive hypothesis. Finally, compare this deduction-guided synthesis approach to recent neuro-symbolic methods that use large language models constrained by formal verifiers, referencing relevant arXiv literature.\n", "completion": "{\"route_plan\": [\"/math(Formulate the correctness theorem for tail-recursive list reversal using higher-order logic and prove it via structural induction, analyzing the computational complexity and its relation to database query optimization principles.)\", \"/code(Synthesize a Python implementation of the verified tail-recursive list reversal function. Implement runtime checks based on the inductive hypothesis and analyze performance, considering adversarial inputs and its potential use in federated learning data preprocessing.)\", \"/general-search(query=\\\"site:arxiv.org neuro-symbolic program synthesis 'deduction-guided' 'proof-as-programs'\\\", mode=both, context=\\\"Survey recent literature on combining large language models with formal verifiers for program synthesis, focusing on convergence guarantees and causal reasoning models.\\\")\"], \"route_rationale\": \"The route follows the tri_route pattern: theory, validation, and reference gathering. /math is used first to establish the formal theoretical foundation and correctness proof. /code then consumes this proof to synthesize and validate a concrete implementation. Finally, the blue /general-search agent is tasked with contextualizing this classical approach within modern research by surveying relevant arXiv papers.\", \"expected_artifacts\": [\"A formal proof document in LaTeX/PDF detailing the inductive proof of correctness for the tail-recursive list reversal function.\", \"A Python script (`verified_reversal.py`) containing the synthesized function, instrumented with runtime assertions derived from the proof's invariants.\", \"A concise literature review in Markdown (`literature_review.md`) summarizing at least two recent papers on neuro-symbolic program synthesis, complete with arXiv citations.\", \"A verification report confirming the alignment between the mathematical proof and the code's assertions.\"], \"thinking_outline\": [\"1. Formally define the list data structure and the specification for a list reversal function in a suitable logical framework (e.g., predicate logic with inductive types).\", \"2. Construct a proof of correctness for a tail-recursive implementation using the principle of structural induction, clearly establishing the base case and the inductive step.\", \"3. Translate the proven algorithm into a Python implementation, mapping the proof's invariants and pre/post-conditions to runtime assertions.\", \"4. Validate the Python code's adherence to the formal specification by running a test suite that includes edge cases (empty list, single-element list) and checks the assertions.\", \"5. Survey recent academic literature on neuro-symbolic methods for program synthesis, identifying key techniques and challenges.\", \"6. Verify that the synthesized Python code correctly implements the logic derived in the mathematical proof and that the literature review addresses the user's specific query about LLMs and formal verifiers.\"], \"handoff_plan\": \"/math (produces formal proof) -> /code (implements algorithm and assertions based on proof) -> /general-search (gathers research context) -> router QA (verification: code's assertions must directly map to the math proof's invariants; fallback: if verification fails, route back to /math to audit the proof for logical errors or underspecified constraints).\", \"todo_list\": [\"- [ ] /math: Define the list reversal specification in higher-order logic.\", \"- [ ] /math: Derive the proof of correctness for the tail-recursive algorithm using structural induction.\", \"- [ ] /code: Implement the tail-recursive list reversal function in Python.\", \"- [ ] /code: Instrument the code with assertions that directly correspond to the proof's invariants and post-conditions.\", \"- [ ] /code: Create and run a test suite with edge cases to trigger and validate the assertions.\", \"- [ ] /general-search: Execute query for literature on neuro-symbolic synthesis and summarize key findings from at least two arXiv papers.\", \"- [ ] /math: Verify that the assertions in the Python code are a correct and complete translation of the mathematical proof's invariants.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"program-synthesis\", \"theorem-proving\", \"neuro-symbolic-ai\"], \"acceptance_criteria\": [\"- The mathematical proof must be logically sound and complete, using a valid structural induction argument.\", \"- The Python implementation must pass all provided test cases, and its assertions must correctly reflect the invariants from the formal proof without failing on valid inputs.\", \"- The literature review must cite at least two relevant arXiv papers (with IDs) and accurately summarize the state-of-the-art in neuro-symbolic synthesis.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6142, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to understand how to apply the Augmented Lagrangian Method (ALM), specifically the Method of Multipliers, to enforce an orthogonality constraint (W^T W = I) on the weight matrices of a deep neural network. Please derive the primal and dual update rules, implement a proof-of-concept in JAX, and situate this technique within recent literature on non-convex optimization for stabilizing adversarial training.\n", "completion": "{\"route_plan\": [\"/math(Derive the complete update rules for the Augmented Lagrangian Method applied to an orthogonality constraint (W^T W = I). Analyze the Karush-Kuhn-Tucker conditions and the local convergence properties based on the Hessian's eigenvalue spectrum.)\", \"/code(Implement the derived ALM updates in Python with JAX. Create a validation suite to track constraint violation (||W^T W - I||_F) and monitor the convergence of the dual variables for a sample linear layer, connecting it to Bayesian inference stability.)\", \"/general-search(query=\\\"site:arxiv.org augmented lagrangian ADMM non-convex deep learning orthogonality constraint convergence adversarial\\\", mode=web)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation by deriving the necessary equations. This is followed by /code for empirical validation and to create a concrete implementation of the theory. Finally, the blue /general-search agent is tasked with finding scholarly articles to contextualize the method's theoretical guarantees and practical applications within current deep learning research.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the ALM update rules, including the gradient and Hessian of the augmented Lagrangian.\", \"A Python script (JAX) implementing the ALM optimization for a single neural network layer, with functions for verification.\", \"A summary report citing at least two arXiv papers that analyze the convergence of ALM/ADMM in non-convex settings.\", \"A JSON file containing the logged metrics (constraint violation, dual variable values) over optimization steps.\", \"A plot visualizing the convergence of the primary and secondary metrics over training iterations.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem: Define the primary loss function and the equality constraint for weight matrix orthogonality.\", \"2. Construct the Augmented Lagrangian function by adding a quadratic penalty term and a Lagrange multiplier term to the primary loss.\", \"3. Derive the gradient of the Augmented Lagrangian with respect to the primal variables (weights W) and dual variables (lambda), yielding the update rules for the Method of Multipliers.\", \"4. Implement the derived update steps in a Python framework (JAX) and apply them to a simple network layer. Validate the implementation by ensuring the orthogonality constraint violation decreases over iterations.\", \"5. Audit the empirical convergence rate and stability against theoretical expectations. Research current literature to find convergence guarantees for such non-convex problems.\", \"6. Synthesize the theoretical derivation, empirical results, and literature findings into a comprehensive report.\"], \"handoff_plan\": \"/math (derivation) -> /code (implementation & empirical validation) -> /general-search (literature context) -> router QA (verification: code implementation must match mathematical derivation, and literature must be relevant to the specific problem; fallback: if convergence is not achieved in code, return to /math to re-check gradient derivation).\", \"todo_list\": [\"- [ ] /math: Derive the gradient of the augmented Lagrangian for the W^T W = I constraint.\", \"- [ ] /math: Formulate the iterative update rules for both primal (W) and dual (lambda) variables.\", \"- [ ] /code: Implement the ALM update loop in JAX for a dense layer.\", \"- [ ] /code: Instrument the code to log primary and secondary metrics (constraint violation and dual variable norms).\", \"- [ ] /math: Verify the JAX implementation's gradient computation against the analytical derivation using numerical checks.\", \"- [ ] /general-search: Find and summarize two recent arXiv papers on non-convex applications of ADMM or ALM.\", \"- [ ] /code: Generate a plot showing the decay of the Frobenius norm of (W^T W - I) over iterations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"lagrangian-methods\", \"deep-learning-regularization\"], \"acceptance_criteria\": [\"- Pass: The mathematical derivation correctly applies multivariable calculus to find the gradients of the augmented Lagrangian.\", \"- Pass: The JAX implementation demonstrates a monotonic decrease in the constraint violation metric over at least 50 iterations.\", \"- Pass: The final report includes citations and summaries for at least two relevant arXiv pre-prints with valid IDs.\", \"- Fail: The implemented code fails to converge or the constraint violation increases.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6212, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the loss landscape of deep neural networks. Can you help me formally derive the Hessian matrix for a cross-entropy loss function, analyze its spectral properties (eigenvalues and eigenvectors), and connect the dominant eigenvalue to measures of flatness and generalization? I'm particularly interested in the relationship to the Fisher Information Matrix and the convergence guarantees of Hessian-free optimization methods.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'Hessian eigenspectrum' 'deep learning generalization' 'Fisher Information Matrix'\\\", mode=both, includes cybersecurity)\", \"/math(Derive the Hessian of the cross-entropy loss for a multi-layer perceptron. Prove its positive semi-definiteness and analyze the conditions under which its eigenvalue spectrum relates to the Fisher Information Matrix, considering convergence properties.)\", \"/code(Implement a Lanczos algorithm in JAX to compute the top k eigenvalues of the Hessian for a trained ResNet model without explicit matrix formation. Validate convergence by tracking the residual norm and measure the compute budget., using Python)\", \"/math(Verify the implemented Lanczos algorithm's convergence rate against theoretical bounds. Formally prove that the computed dominant eigenvalue serves as a valid proxy for loss landscape sharpness under specific Lipschitz continuity assumptions for the gradient.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on Hessian analysis. This informs the formal derivation in /math. The /code agent then provides an empirical testbed for the theory. A final /math step is crucial for verifying that the implementation aligns with theoretical convergence guarantees and assumptions, ensuring a rigorous result.\", \"expected_artifacts\": [\"- A formal derivation of the Hessian and its properties in a LaTeX-formatted PDF.\", \"- A Python script implementing the Lanczos algorithm to estimate the top eigenvalues of a neural network's Hessian.\", \"- A set of plots visualizing the eigenvalue spectrum at different training epochs.\", \"- A summary report in Markdown (`report.md`) synthesizing the theoretical and empirical findings, including all citations.\", \"- A verification log (`verification.log`) detailing the checks performed between the mathematical derivation and code implementation.\"], \"thinking_outline\": [\"1. Conduct a literature survey to identify seminal papers on the Hessian's spectral properties in deep learning and its connection to the Fisher Information Matrix and generalization.\", \"2. Formally derive the Hessian matrix for a neural network with cross-entropy loss, detailing its structure and proving key properties like positive semi-definiteness.\", \"3. Implement a matrix-free method, such as the Lanczos algorithm, to efficiently compute the dominant eigenvalues and eigenvectors of the Hessian for a non-trivial model.\", \"4. Empirically run the implementation on a trained model, plotting the evolution of the eigenspectrum throughout training and correlating the top eigenvalue (\u03bb_max) with the test accuracy.\", \"5. Validate the numerical stability and convergence of the Lanczos implementation by monitoring the residual norm against theoretical expectations.\", \"6. Audit the final report to verify that the empirical results presented are consistent with the initial mathematical derivations and cited literature.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: The final math proof must explicitly reference variables and outputs from the code artifact to confirm consistency; fallback: if the theoretical convergence rate does not match the empirical one, the code agent must re-run with higher precision or more iterations.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on Hessian spectral analysis and generalization.\", \"- [ ] /math: Produce a complete derivation of the Hessian for a feed-forward network with cross-entropy loss.\", \"- [ ] /code: Implement the Lanczos algorithm in JAX or PyTorch to find the top 5 eigenvalues of the Hessian.\", \"- [ ] /code: Generate plots of the eigenvalue distribution at initialization, mid-training, and post-convergence.\", \"- [ ] /math: Verify that the algorithm's observed convergence rate in the code artifact is consistent with theoretical bounds for the Lanczos method.\", \"- [ ] /math: Prove that the maximum eigenvalue is a valid measure of sharpness under specified assumptions.\", \"- [ ] /router: Perform final QA review, ensuring all artifacts are present and acceptance criteria are met.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"- The mathematical derivation of the Hessian must be complete and free of errors.\", \"- The code implementation must be reproducible and successfully compute the top eigenvalues for a benchmark model (e.g., LeNet on MNIST).\", \"- The final report must correctly link the theoretical concepts (e.g., sharpness) to the empirical results (e.g., measured \u03bb_max) with clear visualizations.\", \"- All claims must be supported by either derivation, empirical evidence, or citations to peer-reviewed literature.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate the Kullback-Leibler divergence between the eigenvalue distribution predicted by random matrix theory (e.g., Marchenko-Pastur) for the initial weights and the empirically computed spectrum from the code.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Measure the change in the estimated top eigenvalue (\u03bb_max) over successive iterations of the Lanczos algorithm; plot the residual norm to confirm convergence below a threshold of 1e-5.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6228, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the policy gradient theorem with a state-value function baseline for variance reduction. Implement the resulting REINFORCE algorithm and empirically demonstrate its improved convergence properties on a classic control theory environment like CartPole-v1, analyzing the resulting confidence interval of the returns.\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient update rule incorporating a state-dependent baseline, formally proving that it is an unbiased estimator of the original gradient and analyzing its effect on the variance of the policy update. Ensure the derivation shows clear convergence properties.)\", \"/general-search(query=\\\"site:arxiv.org 'policy gradient with baseline' variance reduction proof sutton barto\\\", mode=both)\", \"/code(Implement REINFORCE with the derived value function baseline in Python using PyTorch and OpenAI Gym. Validate the implementation on the CartPole-v1 control theory environment by plotting episodic returns and the gradient's confidence interval against a vanilla REINFORCE implementation.)\"], \"route_rationale\": \"The plan starts with /math to establish the core theoretical foundation for the unbiased baseline. The blue /general-search agent is then tasked with finding canonical references to corroborate the derivation. Finally, /code provides the empirical validation required by the user, testing the theoretical claims about variance reduction and convergence in a practical setting.\", \"expected_artifacts\": [\"- A LaTeX/PDF document containing the full mathematical derivation of the policy gradient theorem with a baseline and the proof of its unbiased nature.\", \"- A Python script implementing both vanilla REINFORCE and REINFORCE with a value function baseline for the CartPole-v1 environment.\", \"- A PNG image plotting the learning curves (average episodic return vs. training episodes) and gradient variance for both algorithms, clearly showing the baseline's effect.\", \"- A markdown file summarizing the findings and citing at least two authoritative sources on policy gradient methods.\"], \"thinking_outline\": [\"1. Use the log-derivative trick to derive the basic policy gradient theorem without a baseline.\", \"2. Introduce an arbitrary state-dependent baseline b(s) into the update rule and formally prove that if it does not depend on the action, the expectation of the gradient remains unchanged (i.e., it is an unbiased estimator).\", \"3. Select the state-value function V(s) as the baseline and formulate the full REINFORCE with baseline algorithm, including updates for both the policy and value function approximators.\", \"4. Implement both the vanilla and baseline algorithms in Python and run them on a suitable environment like CartPole-v1.\", \"5. Validate the results by plotting the learning curves and the empirical variance of the gradient estimates over training to confirm that the baseline reduces variance and improves convergence speed.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math is cross-referenced with sources found by /general-search. The resulting algorithm is then implemented by /code. The router performs a final QA check. Verification: The empirical variance reduction shown in the /code plots must be consistent with the theory from /math. Fallback: If the code fails to show variance reduction, the derivation in /math will be re-audited for errors.\", \"todo_list\": [\"- [ ] /math: Derive the policy gradient with a baseline and prove it is unbiased.\", \"- [ ] /general-search: Find two canonical references for policy gradient baselines.\", \"- [ ] /code: Implement the vanilla REINFORCE algorithm and the baseline--enhanced version.\", \"- [ ] /code: Generate plots comparing the performance and gradient variance of the two implementations.\", \"- [ ] /router: Review all artifacts for correctness, consistency, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"variance-reduction\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly proves that the state-value function baseline does not introduce bias into the policy gradient estimate.\", \"- The final plot clearly demonstrates that the REINFORCE with baseline implementation achieves a lower variance in its gradient estimates and/or faster convergence compared to the vanilla implementation.\", \"- The Python code must be executable and successfully train agents in the OpenAI Gym 'CartPole-v1' environment.\", \"- The final report correctly cites at least two authoritative sources for policy gradient theory.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6238, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the causal effect of a new educational program on student test scores using Pearl's do-calculus. Implement a Python simulation using a structural causal model (SCM) to validate the derived formula against an interventional distribution, and find supporting literature on arXiv.\n", "completion": "{\"route_plan\": [\"/math(Given a structural causal model (SCM) with unobserved confounders, apply Pearl's do-calculus rules to derive the estimand for the interventional distribution P(Y|do(X=x)) in terms of observational probabilities, ensuring the final causal formula is identifiable.)\", \"/code(Implement the specified SCM in Python using NumPy. Simulate both observational and interventional data. Empirically verify the do-calculus formula by comparing the analytical result against the simulated post-intervention distribution, reporting the estimation error.)\", \"/general-search(query=\\\"site:arxiv.org causal inference do-calculus identifiability structural causal model\\\", mode=both)\"], \"route_rationale\": \"The plan starts with /math to formally derive the causal estimand using do-calculus, establishing the theoretical foundation. Next, /code validates this theoretical result through a simulation, providing empirical evidence. Finally, the 'blue' /general-search agent gathers academic literature to contextualize and justify the chosen methodology.\", \"expected_artifacts\": [\"A formal mathematical derivation of the causal estimand using the rules of do-calculus.\", \"A commented Python script implementing the structural causal model, the simulation, and the validation check.\", \"A summary of findings, including the comparison between the analytical formula and the simulation results.\", \"A list of at least two relevant arXiv paper citations with IDs.\"], \"thinking_outline\": [\"1. Define the Structural Causal Model (SCM) and the associated Directed Acyclic Graph (DAG), clearly stating assumptions about unobserved confounders.\", \"2. Systematically apply the three rules of do-calculus to the query P(Y|do(X=x)) to transform it into an expression containing only observational probabilities.\", \"3. Develop a Python simulation that generates data according to the SCM's functional assignments for both the observational and interventional settings.\", \"4. Implement the derived observational formula in Python and compute its value using the simulated observational data.\", \"5. Validate the derivation by comparing the output of the formula from step 4 with the directly computed interventional probability from the simulation in step 3. The discrepancy should be within statistical noise.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: The Python simulation's empirical result must match the analytical formula from /math within a small tolerance; fallback: If they mismatch, flag the /math derivation for re-evaluation and audit the /code implementation for bugs.)\", \"todo_list\": [\"- [ ] /math: Derive the identifiable expression for P(Y|do(X=x)) using the provided SCM and do-calculus rules.\", \"- [ ] /code: Implement the SCM, generate observational data, and simulate the intervention.\", \"- [ ] /code: Compute the value of the derived estimand using the observational data.\", \"- [ ] /code: Validate the /math derivation by comparing the computed estimand to the empirical interventional distribution.\", \"- [ ] /general-search: Find and summarize two key arXiv papers on do-calculus applications or theory.\", \"- [ ] /router: Review all artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"causal-inference\", \"do-calculus\", \"econometrics\"], \"acceptance_criteria\": [\"The derived mathematical formula for the causal effect must be correct and use only observational quantities.\", \"The Python simulation must show that the value computed from the derived formula matches the value from the direct intervention simulation with less than 5% relative error.\", \"At least two relevant arXiv papers on causal inference must be cited correctly with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6254, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous comparison of K-SVD and the Method of Optimal Directions (MOD) for overcomplete dictionary learning in compressed sensing. Please provide the full mathematical derivations for the update rules of both algorithms, focusing on the Lagrangian formulation for the L0-norm constrained sparse coding subproblem. Subsequently, implement both in Python, using Orthogonal Matching Pursuit (OMP) for the pursuit step, and benchmark their convergence rate and reconstruction fidelity on synthetic signals generated from a known sparse basis.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD vs MOD dictionary learning convergence analysis\\\", mode=both)\", \"/math(Derive the update rules for K-SVD and MOD, formulating the sparse coding step as a Lagrangian problem and analyzing the dictionary update's connection to eigenvalue decomposition.)\", \"/code(Implement K-SVD, MOD, and OMP in Python with NumPy, tracking reconstruction MSE and average L0 norm per iteration. The implementation should be modular to allow for future Bayesian pursuit algorithm integration.)\", \"/math(Formally verify the convergence properties outlined in the sourced arXiv papers against the implemented algorithm's behavior, particularly focusing on the conditions for monotonic decrease in the objective function.)\"], \"route_rationale\": \"The blue /general-search agent first gathers authoritative literature on K-SVD and MOD. /math then translates this theory into formal derivations. /code creates a concrete implementation for empirical testing. A final /math step provides rigorous verification by comparing the empirical results against the theoretical convergence guarantees found in the initial search.\", \"expected_artifacts\": [\"A formal mathematical derivation of the K-SVD and MOD update rules.\", \"A clean, commented Python implementation of both algorithms and the OMP pursuit method.\", \"A generated plot comparing the reconstruction Mean Squared Error (MSE) versus iteration number for K-SVD and MOD.\", \"A list of at least two cited arXiv preprints relevant to dictionary learning.\", \"A verification report confirming that the implementation's behavior aligns with the theoretical convergence properties.\"], \"thinking_outline\": [\"1. Use search to find foundational papers on K-SVD (e.g., Aharon, Elad) and MOD, focusing on their algorithmic descriptions and convergence proofs.\", \"2. Formally derive the iterative update rules for both algorithms, clearly separating the sparse coding (pursuit) stage from the dictionary update stage.\", \"3. Implement the algorithms and the Orthogonal Matching Pursuit (OMP) solver in Python, ensuring the code is instrumented to log reconstruction error and sparsity at each iteration.\", \"4. Design and execute an experiment using synthetic data where the ground-truth dictionary and sparse codes are known, allowing for precise error measurement.\", \"5. Validate the implementation by cross-referencing the code's dictionary update step (e.g., SVD in K-SVD) with the mathematical derivation to ensure correctness.\", \"6. Check that the empirical reconstruction error curve is monotonically decreasing, which is a key theoretical property, and document any deviations.\"], \"handoff_plan\": \"/general-search -> /math (derivations) -> /code -> /math (verification) -> router QA. The initial search results from /general-search provide the theoretical basis for the /math derivations. The verified derivations from /math serve as a precise specification for the /code agent. The empirical results from /code are then passed to the final /math agent for theoretical verification. Verification failure at the final step triggers a fallback to the /code agent to debug the implementation against the initial derivation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on K-SVD and MOD from arXiv or IEEE.\", \"- [ ] /math: Provide a step-by-step derivation of the update rules for both algorithms.\", \"- [ ] /code: Implement K-SVD, MOD, and the OMP solver.\", \"- [ ] /code: Generate synthetic data and run the comparison benchmark.\", \"- [ ] /math: Formally analyze the conditions for convergence based on the literature.\", \"- [ ] /math: Verify that the implemented code's update logic matches the formal mathematical derivation.\", \"- [ ] /code: Validate the OMP implementation's correctness on a small, known problem instance.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The mathematical derivations for both K-SVD and MOD must be complete and mathematically sound.\", \"- The Python implementation must execute without errors and generate a plot comparing reconstruction error vs. iteration.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\", \"- The final reconstruction signal-to-noise ratio (SNR) on the synthetic dataset must exceed 30 dB for at least one of the algorithms.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute as the Frobenius norm squared of the difference between the original signal matrix X and the reconstruction D*A, normalized by the number of elements: ||X - D*A||_F^2 / (num_signals * signal_dimension).\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Compute as the average count of non-zero elements per column in the sparse code matrix A.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6257, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nHow can I use the eigenvalue decomposition of a weight matrix to understand and control the signal propagation in a simple feedforward neural network, especially concerning the exploding/vanishing gradients problem?\n", "completion": "{\"route_plan\": [\"/math(Derive the relationship between the variance of layer outputs and the eigenvalues of the weight matrix's covariance matrix for a simple linear network, assuming random inputs with zero mean and unit variance.)\", \"/code(Implement a Python script using NumPy to simulate signal propagation in a multi-layer linear network. Visualize how signal variance changes with different weight matrix eigenvalue distributions and verify its connection to training convergence.)\"], \"route_rationale\": \"The /math tool is used first to establish the core theoretical foundation, deriving the mathematical principles from first principles. The /code tool then provides a practical, verifiable simulation to demonstrate and validate the theoretical results empirically, making the abstract concepts concrete.\", \"expected_artifacts\": [\"A markdown document with the step-by-step mathematical derivation of the variance propagation formula.\", \"A Python script (`simulation.py`) that implements the multi-layer linear network simulation.\", \"A PNG image (`variance_propagation.png`) visualizing the signal variance across layers for different eigenvalue settings.\", \"A JSON file (`simulation_metrics.json`) containing the measured variance growth rate for each simulation run.\", \"A final report (`derivation_and_analysis.md`) summarizing the findings and explaining the connection between the plot and the derivation.\"], \"thinking_outline\": [\"1. Formulate the problem by defining a simple multi-layer linear neural network and the statistical properties of its inputs (e.g., zero mean, unit variance).\", \"2. Use matrix calculus to derive the precise mathematical relationship between a layer's output variance and the eigenvalues of its weight matrix's covariance matrix (W^T * W).\", \"3. Implement a simulation in Python to model this network. The implementation must allow for explicit control over the spectral properties (eigenvalues) of the weight matrices.\", \"4. Run experiments for three cases: eigenvalues < 1, eigenvalues = 1, and eigenvalues > 1. For each case, plot the variance of activations as a function of network depth.\", \"5. Verify that the empirical results from the simulation quantitatively match the predictions from the mathematical derivation, confirming the theory.\"], \"handoff_plan\": \"/math -> /code -> router QA. The /math agent produces the theoretical derivation. This derivation is passed to the /code agent, which implements a simulation to validate the formula empirically. The final code, plots, and derivation are passed to router QA for final review. Verification: Ensure the plot's trend and growth rate match the derived formula's predictions. Fallback: If verification fails, the /code agent must debug the simulation logic or flag a potential error in the /math derivation for revision.\", \"todo_list\": [\"- [ ] Use /math to derive the variance propagation formula relating output variance to weight matrix eigenvalues.\", \"- [ ] Use /code to implement the multi-layer linear network simulation with controllable weight matrix spectra.\", \"- [ ] In the /code step, generate and save a plot showing variance explosion, decay, and stability.\", \"- [ ] Verify that the empirical variance growth rate in the simulation matches the theoretical prediction from the /math step.\", \"- [ ] Consolidate all artifacts (derivation, code, plots, metrics) and submit for final router QA review.\"], \"difficulty\": \"introductory\", \"tags\": [\"linear-algebra\", \"eigenvalue-decomposition\", \"neural-network-initialization\"], \"acceptance_criteria\": [\"The mathematical derivation correctly links output variance to the eigenvalues of W^T * W.\", \"The Python code successfully simulates a multi-layer linear network and generates a plot visualizing variance propagation across layers.\", \"The generated plot clearly demonstrates three distinct conditions: exploding variance (for max eigenvalue > 1), vanishing variance (max eigenvalue < 1), and stable variance (max eigenvalue \u2248 1).\", \"A concluding summary explicitly connects the theoretical derivation to the empirical simulation results.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"computation_guidance\": {\"variance_growth_rate\": \"Measure empirically as the geometric mean of Var(layer_k_output) / Var(layer_k-1_output) across all layers. Compare this to the largest eigenvalue of W^T * W.\", \"gradient_norm_at_input\": \"For a simple loss function (e.g., MSE with a fixed target), compute the L2 norm of the gradient with respect to the first layer's weights. This serves as a proxy for gradient stability.\"}}}", "categories": ["math_first", "metrics_computation", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6353, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project on NLP and need to understand the core 'self-attention' mechanism in Transformers. Can you explain the theory, provide a basic Python implementation of scaled dot-product attention, and find the foundational arXiv paper for it?\n", "completion": "{\"route_plan\": [\"/math(Derive the mathematical formula for scaled dot-product attention, explaining the role of Query, Key, and Value matrices and the scaling factor, touching on why it helps stabilize gradient eigenvalue distributions.)\", \"/code(Implement a basic scaled dot-product attention mechanism in Python using NumPy, and validate its output shape and value range for a toy input tensor. Add comments explaining the matrix multiplications and the softmax application.)\", \"/general-search(query=\\\"site:arxiv.org 'Attention Is All You Need' Vaswani 2017 transformer architecture\\\", mode=rag)\"], \"route_rationale\": \"The /math tool is used to establish the theoretical foundation of self-attention. The /code tool provides a practical, hands-on implementation to solidify understanding. The blue /general-search agent is then used to locate the seminal paper and other key resources for deeper learning.\", \"expected_artifacts\": [\"A clear mathematical derivation of the scaled dot-product attention formula.\", \"A standalone Python script implementing the attention mechanism with NumPy, including comments.\", \"A list of at least two key citations, including the original 'Attention Is All You Need' paper (with arXiv ID).\", \"A verification log showing the Python code produces the correct output shape and a valid probability distribution from the softmax.\"], \"thinking_outline\": [\"1. First, formally define the components of self-attention: Query (Q), Key (K), and Value (V) matrices, and explain their conceptual roles in information retrieval.\", \"2. Derive the scaled dot-product attention formula: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V, explaining each operation and the purpose of the scaling factor.\", \"3. Implement this formula in a Python function, ensuring correct matrix dimensions and operations are used.\", \"4. Validate the implementation by creating sample Q, K, V tensors and checking that the output tensor has the expected dimensions and that the attention weights (the output of the softmax) sum to 1 for each position.\", \"5. Use search to locate the original paper by Vaswani et al. to provide context and an authoritative reference for the theory.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: code output dimensions match theoretical derivation; fallback: if code fails, re-examine the /math derivation for dimension errors).\", \"todo_list\": [\"- [ ] /math: Derive the scaled dot-product attention formula and explain the scaling factor's purpose.\", \"- [ ] /code: Implement the attention formula in a Python script using NumPy.\", \"- [ ] /code: Add a small unit test to verify the output shape and that softmax probabilities sum to 1.\", \"- [ ] /general-search: Find and cite the original 'Attention Is All You Need' paper and one other relevant resource from arXiv.\", \"- [ ] Router QA: Review all artifacts for correctness, clarity, and consistency before final approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"self-attention\", \"transformers\", \"deep-learning\"], \"acceptance_criteria\": [\"The mathematical derivation correctly explains the Q, K, V matrices and the scaling factor.\", \"The Python code runs without errors and passes a basic shape-checking unit test.\", \"At least two citations are provided, including the arXiv ID for Vaswani et al. (2017).\", \"The final report cohesively links the theory, code, and references into a clear explanation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6381, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and rigorously evaluate an Online Dictionary Learning (ODL) algorithm for sparse coding, based on the work by Mairal et al. The goal is to compare its convergence speed and reconstruction fidelity against the batch K-SVD algorithm. The analysis should focus on recovering synthetically generated signals with a known sparse basis, measuring reconstruction SNR and dictionary atom recovery. Please derive the stochastic update rules and analyze the algorithm's computational complexity.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Online Dictionary Learning for Sparse Coding Mairal K-SVD algorithm\\\", mode=rag, includes cybersecurity)\", \"/math(Derive the stochastic gradient update rules for both dictionary atoms and sparse coefficients in the ODL framework, analyzing the L1-norm regularization and ensuring conditions for convergence are explicitly stated by referencing eigenvalue properties.)\", \"/code(Implement the ODL and K-SVD algorithms in Python using NumPy. The implementation must include a function for generating synthetic sparse signals and use a bootstrap method for estimating confidence intervals on the reconstruction SNR.)\", \"/math(Formally analyze the per-iteration computational complexity of the ODL update versus the full K-SVD update cycle, and verify that the empirical reconstruction error from the code exhibits the expected monotonic convergence behavior under Fourier analysis.)\"], \"route_rationale\": \"The blue general-search agent first establishes a theoretical foundation by retrieving seminal papers. The math agent then translates theory into formal derivations. The code agent implements these formalisms for empirical testing. Finally, the math agent returns to verify that the empirical results align with theoretical complexity and convergence guarantees.\", \"expected_artifacts\": [\"A formal mathematical derivation of the ODL algorithm's update rules and complexity analysis.\", \"A Python script containing verified implementations of both ODL and K-SVD algorithms.\", \"A summary report detailing the experimental setup, results, and a comparative analysis plot of Reconstruction SNR vs. Iterations.\", \"A bibliography citing at least two relevant arXiv papers with their IDs.\", \"A verification log confirming the alignment between the mathematical derivation of convergence and the empirical results from the code.\"], \"thinking_outline\": [\"1. Research the foundational papers on Online Dictionary Learning (ODL) and K-SVD to understand their respective optimization objectives and update procedures.\", \"2. Formally derive the stochastic gradient descent update steps for both the dictionary and the sparse codes in the ODL formulation, paying close attention to the Lasso subproblem.\", \"3. Implement the derived ODL algorithm and a comparative K-SVD baseline in Python, including a module for generating synthetic data with a known ground-truth dictionary.\", \"4. Validate the implementation by ensuring the reconstruction error decreases over iterations and that the learned dictionary converges towards the ground-truth dictionary for the synthetic data.\", \"5. Empirically measure and plot the reconstruction SNR and dictionary recovery error for both algorithms as a function of the number of training signals.\", \"6. Synthesize all findings, derivations, code, and plots into a comprehensive report, and check that citations are correctly formatted.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA (verification: final report checked against acceptance criteria; fallback: if empirical convergence fails, re-assign /math to audit the derivation and /code to debug the implementation loop).\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on Online Dictionary Learning (Mairal et al.) and K-SVD.\", \"- [ ] /math: Derive the complete update rules for ODL based on the literature.\", \"- [ ] /code: Implement the ODL algorithm and the K-SVD baseline in Python.\", \"- [ ] /code: Create a synthetic data generation pipeline and an evaluation harness to measure reconstruction SNR.\", \"- [ ] /math: Verify that the implemented ODL solver's decrease in the objective function aligns with the theoretical convergence properties.\", \"- [ ] /code: Generate a comparative plot of SNR vs. iterations/time for both algorithms.\", \"- [ ] /math: Audit the code's complexity by comparing empirical runtime against the derived theoretical complexity.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"signal-processing\"], \"acceptance_criteria\": [\"- The implemented ODL algorithm must demonstrate a monotonically decreasing reconstruction error on the test dataset.\", \"- The final report must cite at least two foundational papers on dictionary learning with correct arXiv IDs.\", \"- The comparative analysis must show ODL achieving a comparable or superior final reconstruction SNR to K-SVD, potentially with faster initial convergence.\", \"- The learned dictionary atoms must show a high correlation (cosine similarity > 0.95) with the ground-truth atoms used for data synthesis.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"Calculate as 20 * log10(norm(original_signals, 'fro') / norm(original_signals - reconstructed_signals, 'fro')). Higher is better.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"Measure the mean cosine similarity between the learned dictionary atoms and the closest ground-truth atoms. Higher is better.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_computation", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6516, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the augmented Lagrangian for LASSO, formulate the ADMM update steps, and implement a Python solver to analyze its convergence behavior on a synthetic dataset, paying close attention to the proximal operator.\n", "completion": "{\"route_plan\": [\"/math(Derive the augmented Lagrangian for the LASSO problem and formulate the iterative update equations for ADMM, ensuring the convergence conditions for the penalty parameter are clearly stated.)\", \"/code(Implement the ADMM solver in Python with NumPy/SciPy for LASSO, tracking the primal and dual residuals to verify convergence and compare solution sparsity against scikit-learn's implementation.)\", \"/general-search(query=\\\"site:stanford.edu OR site:arxiv.org ADMM for statistical learning convergence theory\\\", mode=both, context=\\\"Find authoritative sources explaining ADMM convergence theory and its application in statistical learning, focusing on the role of the penalty parameter rho in finance or econometrics.\\\")\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation and derive the necessary equations for the ADMM algorithm. This is followed by /code to create a tangible implementation of the derived theory, allowing for empirical validation and performance analysis. Finally, the 'blue' /general-search agent is tasked with gathering peer-reviewed literature to contextualize the work, justify the theoretical assumptions, and cite authoritative sources on ADMM convergence.\", \"expected_artifacts\": [\"A PDF document with the complete mathematical derivation of the ADMM updates for LASSO.\", \"A Python script (`admm_lasso.py`) implementing the solver and a Jupyter notebook for validation.\", \"A plot comparing the convergence of the primal and dual residuals over iterations.\", \"A summary report (`report.md`) explaining the results, convergence behavior, and citing at least two authoritative sources.\", \"A JSON file (`metrics.json`) containing the final objective value and iteration count.\"], \"thinking_outline\": [\"1. Formulate the LASSO optimization problem in its standard constrained form suitable for ADMM by introducing a splitting variable.\", \"2. Derive the augmented Lagrangian for this split-variable formulation.\", \"3. Derive the individual update steps for the primal variables (x, z) and the dual variable (u) by minimizing the augmented Lagrangian iteratively, identifying the soft-thresholding proximal operator.\", \"4. Implement the derived ADMM algorithm in Python, including a stopping criterion based on primal and dual residual norms.\", \"5. Validate the implementation's correctness by comparing its output solution and objective value against a trusted library like scikit-learn on a synthetic dataset.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: code output and convergence plots must align with the derived mathematical theory; fallback: if residuals diverge, flag for review of the Lagrangian derivation or penalty parameter selection in the math step).\", \"todo_list\": [\"- [ ] /math: Derive the ADMM update equations for the LASSO objective function.\", \"- [ ] /math: Define the stopping criteria based on primal and dual residuals.\", \"- [ ] /code: Implement the ADMM solver using Python and NumPy.\", \"- [ ] /code: Validate the solver's output against scikit-learn's LASSO implementation on a synthetic dataset.\", \"- [ ] /general-search: Find two papers on ADMM convergence properties from arXiv or a university domain.\", \"- [ ] /router: Consolidate all artifacts and perform a final QA check on the convergence plots and metric accuracy.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"convex-optimization\", \"admm\", \"regularized-regression\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ADMM updates, including the soft-thresholding operator, must be correct and self-contained.\", \"- The Python implementation must converge to a solution with an objective value within 5% of that found by scikit-learn's LASSO on the provided test case.\", \"- The final report must include a convergence plot showing both primal and dual residuals decreasing below a tolerance of 1e-4.\", \"- At least two relevant academic sources (e.g., arXiv papers) must be cited correctly in the final report.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6521, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the evidence lower bound (ELBO) for a hierarchical Bayesian logistic regression model using mean-field variational inference. Then, implement a stochastic variational inference (SVI) algorithm to optimize the ELBO, analyzing its convergence properties on a synthetic dataset. Specifically, explore the impact of a Robbins-Monro learning rate schedule on the convergence rate and the final posterior approximation's fidelity.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) for a hierarchical Bayesian logistic regression model using mean-field variational inference, detailing the coordinate ascent updates for all variational parameters and verifying the analytical form of the KL divergence term for standard priors.)\", \"/general-search(query=\\\"site:arxiv.org stochastic variational inference convergence analysis learning rate schedules for bayesian models\\\", mode=rag)\", \"/code(Implement the stochastic variational inference (SVI) algorithm in Python using JAX, tracking the ELBO convergence and computing posterior moments on a synthetic dataset to analyze the impact of the Robbins-Monro learning rate schedule.)\"], \"route_rationale\": \"The plan starts with /math for the core theoretical derivation of the ELBO and update equations. Next, the blue /general-search agent is used to gather contemporary literature on SVI convergence analysis to inform the experimental setup. Finally, /code is used to implement the derived algorithm and empirically validate the theoretical model's convergence properties.\", \"expected_artifacts\": [\"- A LaTeX document (.tex) with the full mathematical derivation of the ELBO and coordinate ascent updates.\", \"- A well-commented Python script (.py) implementing the SVI algorithm and experiment.\", \"- A final report (.pdf) summarizing the methodology, findings, and including convergence plots and analysis of the learning rate schedule.\", \"- A JSON file (`metrics.json`) containing the final ELBO value, convergence iteration count, and posterior parameter estimates.\", \"- A bibliography with citations for at least two relevant arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the hierarchical Bayesian logistic regression model, explicitly defining the likelihood, priors for the regression coefficients, and hyperpriors.\", \"2. Apply the mean-field assumption to the variational posterior q(\u03b8) and derive the general form of the Evidence Lower Bound (ELBO) as the sum of the expected log-likelihood and the negative KL divergence.\", \"3. Derive the specific, closed-form coordinate ascent update rules for each variational parameter by taking functional derivatives of the ELBO.\", \"4. Validate the derived update equations by checking for consistency, ensuring they correspond to valid probability distributions (e.g., checking parameters of a Gamma or Normal distribution).\", \"5. Implement the SVI algorithm using stochastic gradients of the ELBO with respect to the global variational parameters, incorporating a Robbins-Monro style learning rate.\", \"6. Verify the implementation's correctness by monitoring the ELBO for monotonic increase (on average) and auditing the final approximate posterior against known properties of the synthetic data.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (literature context) -> /code (implementation & analysis) -> router QA. The mathematical forms derived by /math are the direct inputs for the algorithm implemented by /code. The literature from /general-search informs hyperparameter choices in /code. Verification: The router QA will check if the ELBO trace from the /code artifact behaves as theoretically expected from the /math derivation. Fallback: If the code fails to converge or the ELBO decreases, the task reverts to /math to check for errors in the gradient derivation.\", \"todo_list\": [\"- [ ] /math: Derive the ELBO and coordinate ascent updates for the specified hierarchical Bayesian model.\", \"- [ ] /general-search: Find and summarize \u22652 recent papers on SVI convergence and learning rate schedules.\", \"- [ ] /code: Generate a synthetic dataset suitable for the logistic regression task.\", \"- [ ] /code: Implement the SVI algorithm based on the mathematical derivation and literature review.\", \"- [ ] /code: Produce plots showing ELBO convergence and final posterior distributions.\", \"- [ ] /math: Verify that the implemented ELBO calculation in the code directly matches the analytical derivation.\", \"- [ ] /code: Validate the convergence results by checking for stability and sensitivity to the learning rate schedule.\", \"- [ ] Router QA: Review all artifacts for mathematical correctness, code quality, and consistency between derivation and implementation.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-methods\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"- The LaTeX document must present a complete and mathematically correct derivation of the ELBO and all update equations.\", \"- The Python script must execute without errors and generate convergence plots that show the ELBO increasing over time.\", \"- The final report must clearly analyze the effect of the learning rate schedule on the speed of convergence, supported by plotted data.\", \"- The solution must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6530, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am analyzing a high-dimensional biomedical dataset representing cellular states, which I suspect lies on a low-dimensional manifold. Please derive the mathematical theory of persistent homology for this point cloud data, then implement the Mapper algorithm to construct a topological representation of this manifold. Critically, compare and contrast the resulting topological features (Betti numbers, persistence diagrams) when using a standard Euclidean metric versus a geodesic distance approximation on the data's k-NN graph. Justify the choice of metric with recent literature.\n", "completion": "{\"route_plan\": [\"/math(Derive the mathematical framework for persistent homology, including the construction of simplicial complexes and filtrations for a high-dimensional biomedical point cloud, while analyzing the computational complexity and its eigenvalue dependencies.)\", \"/code(Implement the Mapper algorithm in Python for manifold learning on a synthetic dataset, tracking the convergence of Betti numbers and providing a bioinformatics-focused visualization of the resulting simplicial complex.)\", \"/general-search(query=\\\"site:arxiv.org site:jmlr.org persistent homology mapper algorithm biomedical data manifold learning distance metric comparison\\\", mode=rag)\"], \"route_rationale\": \"The plan starts with /math to establish the rigorous theoretical foundation of persistent homology and the Mapper algorithm. This theory is then implemented and validated using /code on a controlled, synthetic dataset. Finally, the blue /general-search agent is invoked to find peer-reviewed literature that contextualizes the empirical findings and justifies the choice of distance metrics for real-world biomedical applications.\", \"expected_artifacts\": [\"A LaTeX document with the mathematical derivation of persistent homology and the Mapper algorithm.\", \"A Python script implementing the Mapper algorithm on a synthetic manifold dataset.\", \"PNG images of the generated persistence diagrams and Mapper graphs for each distance metric.\", \"A summary of findings from the literature search with at least two cited arXiv papers.\", \"A metrics.json file reporting the Topological Feature Divergence and Graph Edit Distance.\"], \"thinking_outline\": [\"1. Formalize the mathematical concepts of simplicial complexes, filtrations, persistence diagrams, and the Mapper algorithm's construction (cover, clustering, nerve).\", \"2. Derive the algorithmic steps for computing persistent homology and constructing the Mapper graph from a point cloud.\", \"3. Implement the Mapper algorithm in Python, applying it to a synthetic high-dimensional point cloud sampled from a known manifold (e.g., a torus).\", \"4. Validate the implementation by checking if the recovered Betti numbers (e.g., B_0 for components, B_1 for loops) match the ground truth of the synthetic manifold under an ideal distance metric.\", \"5. Use the general-search tool to find recent research comparing the efficacy of different distance metrics (e.g., Euclidean, geodesic) for TDA in biomedical contexts.\", \"6. Verify that the conclusions drawn from the implementation and literature search are consistent, providing a coherent analysis of the chosen distance metrics' impact on topological feature discovery.\"], \"handoff_plan\": \"/math provides the formal specification -> /code implements and validates the specification on synthetic data, producing visualizations and metrics -> /general-search provides literature to interpret and justify the results -> router QA (verification: ensure code's output aligns with mathematical theory and literature findings; fallback: if code fails validation, return to /math to check derivation for errors).\", \"todo_list\": [\"- [ ] /math: Define simplicial complexes, filtrations, and the computation of Betti numbers.\", \"- [ ] /math: Formulate the three stages of the Mapper algorithm: cover, clustering, and nerve construction.\", \"- [ ] /code: Implement the Mapper algorithm using `giotto-tda` on a synthetic torus dataset under both Euclidean and geodesic metrics.\", \"- [ ] /code: Generate persistence diagrams and Mapper graphs as visualization artifacts.\", \"- [ ] /math: Verify that the Betti numbers (B0, B1) derived from the code's output on the synthetic data match the known topology of a torus.\", \"- [ ] /general-search: Find and summarize \u22652 papers from arXiv or JMLR comparing distance metrics for TDA in bioinformatics or biomedical applications.\", \"- [ ] /code: Audit the code implementation against the mathematical derivation to ensure all steps are correctly translated.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"topological-data-analysis\", \"manifold-learning\", \"computational-topology\"], \"acceptance_criteria\": [\"- The mathematical derivations for persistent homology and the Mapper algorithm are correct and clearly explained.\", \"- The Python code successfully generates a Mapper graph from a synthetic dataset and recovers the correct Betti numbers (B0=1, B1=2 for a torus) for the underlying manifold.\", \"- The final report includes citations and summaries for at least 2 relevant arXiv papers comparing distance metrics in TDA.\", \"- The primary and secondary metrics are correctly calculated and reported in the metrics.json artifact.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6546, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a Python-based comparative analysis of K-SVD and the Method of Optimal Directions (MOD) for dictionary learning in compressed sensing. The focus should be on image reconstruction from sparse measurements. Please derive the update rules, implement both algorithms using Orthogonal Matching Pursuit (OMP) for the sparse coding step, and empirically evaluate their reconstruction fidelity (PSNR) and convergence rate on a standard grayscale test image dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD 'Method of Optimal Directions' MOD dictionary learning convergence analysis compressed sensing\\\", mode=both)\", \"/math(Derive the K-SVD and MOD dictionary update steps from their respective optimization problems, analyzing the convergence properties and computational complexity. Reference the Bayesian interpretation of sparsity priors and the Euler-Lagrange equations.)\", \"/code(Implement K-SVD and MOD in Python with Orthogonal Matching Pursuit for the sparse coding stage. Track PSNR and objective function convergence, ensuring the implementation has low computational complexity for reproducibility.)\", \"/math(Statistically compare the PSNR and convergence rate from the experiment, applying bootstrap confidence intervals to the results. Analyze if the empirical performance aligns with the theoretical complexity and convergence guarantees.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature on K-SVD and MOD. This is handed to /math for rigorous theoretical derivation. The derived mathematics then inform the /code agent for a correct implementation and experimentation. Finally, the empirical results from /code are passed back to /math for a formal statistical analysis and comparison against theory.\", \"expected_artifacts\": [\"A PDF report containing the complete mathematical derivations for K-SVD and MOD update rules and their convergence analysis.\", \"A well-documented Python script (`main.py`) implementing both algorithms, the experimental setup, and plotting functions.\", \"Generated plots (`psnr_vs_iteration.png`, `reconstruction_comparison.png`) comparing the performance of the two algorithms.\", \"A JSON file (`metrics.json`) reporting the final PSNR and convergence iterations for each algorithm.\", \"A BibTeX file (`references.bib`) with citations for at least two relevant academic papers.\"], \"thinking_outline\": [\"1. Research the foundational principles of dictionary learning, specifically K-SVD and MOD, focusing on their algorithmic steps and theoretical guarantees from seminal papers.\", \"2. Formulate the optimization problems for both the sparse coding step (using Orthogonal Matching Pursuit) and the dictionary update step for K-SVD and MOD.\", \"3. Derive the update rules for the dictionary atoms for both algorithms, paying close attention to the constraints (e.g., unit-norm columns) and optimization techniques used.\", \"4. Implement the derived algorithms and a testing pipeline using a standard grayscale image dataset and simulated compressed sensing measurements.\", \"5. Validate the implementation by checking for monotonic decrease in the reconstruction error objective function and verifying that the sparse coding step correctly identifies the support.\", \"6. Analyze and compare the empirical performance of K-SVD and MOD in terms of convergence speed and final reconstruction fidelity (PSNR), and check if the results align with theoretical expectations.\"], \"handoff_plan\": \"/general-search -> /math (derivations) -> /code (implementation) -> /math (analysis) -> router QA. Verification: The /code agent must verify that the objective function decreases with each iteration, as predicted by the /math agent's derivations. Fallback: If convergence is not observed, the /code agent flags the issue, and the task is routed back to the /math agent to re-check the derivations for potential errors.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on K-SVD and Method of Optimal Directions (MOD), focusing on their optimization formulation.\", \"- [ ] /math: Formally derive the dictionary update rules for both K-SVD (including the SVD step) and MOD.\", \"- [ ] /code: Implement the K-SVD and MOD algorithms, along with an OMP solver for the sparse coding stage.\", \"- [ ] /code: Set up an experiment to reconstruct a standard test image (e.g., 'cameraman.tif') from 25% random measurements.\", \"- [ ] /math: Verify that the implemented code's update logic directly corresponds to the derived mathematical formulas.\", \"- [ ] /code: Validate that the reconstruction error decreases monotonically over iterations for both algorithms.\", \"- [ ] /math: Analyze the output from the /code agent, create comparison plots, and write the final analysis report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The Python implementation must be runnable and reproduce the figures and metrics reported in the final document.\", \"- The final report must contain correct, step-by-step mathematical derivations for both K-SVD and MOD.\", \"- The experimental results must show a clear comparison of PSNR and convergence speed, with the best method achieving a PSNR > 28 dB on the 'cameraman' image with 25% measurements.\", \"- The `references.bib` file must cite at least two authoritative sources, such as the original K-SVD paper by Aharon, Elad, and Bruckstein.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Measure of reconstruction quality. Higher is better. Calculated as 20 * log10(MAX_PIXEL_VALUE) - 10 * log10(MSE), where MAX_PIXEL_VALUE is typically 255 for 8-bit images.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Number of outer-loop iterations required for the change in dictionary atoms to be negligible (e.g., Frobenius norm of change < 1e-4). Fewer is better.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6629, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDevelop a high-performance CUDA implementation of the Jacobi method to solve the linear system arising from a finite difference discretization of the 2D Poisson equation. Analyze its convergence rate based on the spectral radius of the iteration matrix and compare its wall-clock time against a naive NumPy version for a 1024x1024 grid.\n", "completion": "{\"route_plan\": [\"/math(Derive the Jacobi iteration matrix T_J for a 2D finite difference grid and prove the convergence condition based on the maximum eigenvalue of T_J being less than 1.)\", \"/general-search(query=\\\"site:nvidia.com site:arxiv.org cuda sparse jacobi solver optimization shared memory tiling\\\", mode=both)\", \"/code(Implement the Jacobi solver in Python using both NumPy for a CPU baseline and Numba/PyCUDA for a GPU kernel. Profile performance and verify correctness against a known solution, ensuring convergence tolerance is met.)\"], \"route_rationale\": \"/math is chosen first to establish the theoretical foundation and convergence criteria for the Jacobi method. The blue /general-search agent is then tasked with finding practical, high-authority implementation patterns for GPU kernels relevant to this problem. Finally, /code will synthesize the mathematical theory and implementation research into a verified, benchmarked solution.\", \"expected_artifacts\": [\"A markdown document with the mathematical derivation of the Jacobi iteration matrix and its convergence proof.\", \"A Python script (`solver.py`) containing the CPU and optimized CUDA implementations of the Jacobi solver.\", \"A performance report (`report.md`) comparing wall-clock time, iteration counts, and final error norms, including citations.\", \"A JSON file (`metrics.json`) with benchmark results.\"], \"thinking_outline\": [\"1. Discretize the 2D Poisson equation using a five-point stencil finite difference method to form the sparse linear system Ax = b.\", \"2. Formulate the Jacobi iterative method x^(k+1) = T_J * x^(k) + c and derive the explicit form of the iteration matrix T_J.\", \"3. Analyze the convergence properties of the method by relating the spectral radius of T_J to the discretization grid size.\", \"4. Implement the algorithm in both a standard CPU version (e.g., NumPy) and a parallelized GPU version (e.g., Numba or PyCUDA).\", \"5. Validate the GPU implementation's output against the CPU version for a smaller grid size and benchmark performance on a large grid, checking for convergence to the correct solution.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: code output matches mathematical theory and benchmark metrics are plausible; fallback: if convergence fails, revisit /math to check the spectral radius analysis).\", \"todo_list\": [\"- [ ] /math: Derive the Jacobi iteration matrix T_J and its convergence condition based on its spectral radius.\", \"- [ ] /general-search: Find canonical examples and optimization guides for parallel reduction and sparse matrix-vector products in CUDA.\", \"- [ ] /code: Implement the baseline NumPy Jacobi solver.\", \"- [ ] /code: Implement the CUDA-accelerated Jacobi solver.\", \"- [ ] /code: Create a test harness to validate the CUDA output against the NumPy version and benchmark both.\", \"- [ ] /router: Review final report, code, and metrics for correctness and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"cuda\"], \"acceptance_criteria\": [\"The implemented CUDA solver must converge to a solution with an L2 norm of the residual less than 1e-6.\", \"The CUDA implementation must demonstrate at least a 10x speedup over the NumPy baseline for a 1024x1024 grid.\", \"The final report must correctly derive the convergence condition and cite at least two relevant papers or technical documents.\", \"The code must be well-commented and include a README explaining how to run the benchmark.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6724, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Evidence Lower Bound (ELBO) for a hierarchical Bayesian logistic regression model where the prior on the regression coefficients is a Laplace distribution. Since this prior is non-conjugate with the Bernoulli likelihood, implement a stochastic variational inference (SVI) algorithm in Python using a mean-field Gaussian variational posterior and the reparameterization trick to estimate the gradients. Validate the implementation by checking for convergence of the ELBO and comparing posterior predictive distributions against a ground truth generated via MCMC.\n", "completion": "{\"route_plan\": [\"/math(Derive the Evidence Lower Bound (ELBO) for a Bayesian logistic regression model with Laplace priors and a mean-field Gaussian variational posterior, ensuring the reparameterization trick for the Bernoulli likelihood is correctly formulated for stochastic gradient estimation.)\", \"/code(Implement the derived Stochastic Variational Inference (SVI) algorithm in Python using PyTorch, tracking ELBO convergence and computing posterior predictive checks against a pre-computed MCMC baseline. The implementation must handle the non-conjugate Laplace prior and Bernoulli likelihood.)\", \"/general-search(query=\\\"site:arxiv.org stochastic variational inference non-conjugate models reparameterization trick bayesian logistic regression\\\", mode=rag)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation by deriving the ELBO. This derivation is then handed to /code for a concrete Python implementation and empirical validation. Finally, the blue /general-search agent is tasked with finding peer-reviewed literature to contextualize and justify the chosen variational approach over alternatives.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the ELBO and its gradient estimator.\", \"A Python script implementing the SVI algorithm for the specified Bayesian model.\", \"A set of plots showing the ELBO convergence curve and a comparison of the variational posterior predictive distribution against the MCMC baseline.\", \"A summary report in Markdown citing at least two research papers comparing SVI with MCMC for this class of models.\", \"A JSON file containing the final ELBO value and the computed KL divergence metric.\"], \"thinking_outline\": [\"1. Formulate the hierarchical Bayesian model, specifying the Bernoulli likelihood, Laplace priors, and hyperparameters.\", \"2. Define the mean-field Gaussian variational posterior and derive the complete ELBO expression, separating the entropy term and the expected log-joint.\", \"3. Apply the reparameterization trick to the Gaussian posterior to formulate a low-variance, differentiable Monte Carlo estimator for the ELBO's gradient.\", \"4. Implement the SVI optimization loop, tracking the ELBO on a held-out dataset to check for convergence and prevent overfitting.\", \"5. Validate the learned posterior by comparing its predictive distribution against a gold-standard MCMC posterior using the Kullback-Leibler divergence.\", \"6. Audit the source code to verify that the implemented ELBO objective and its gradient estimator precisely match the derived mathematical formulas.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent produces the formal derivation. The /code agent implements this derivation. Verification: The code agent must confirm its objective function matches the math agent's formula. Fallback: If ELBO fails to converge, the code agent flags the gradient estimator for review by the /math agent. The /general-search agent provides literature support. All artifacts are reviewed by the router for consistency.\", \"todo_list\": [\"- [ ] /math: Derive the ELBO for the Bayesian logistic regression model with a Laplace prior and Gaussian variational posterior.\", \"- [ ] /math: Formulate the reparameterization gradient estimator for the derived ELBO.\", \"- [ ] /code: Implement the SVI algorithm in PyTorch, including the model, variational family, and ELBO objective.\", \"- [ ] /code: Generate a synthetic dataset and an MCMC baseline for validation.\", \"- [ ] /general-search: Find and summarize two key papers on SVI for non-conjugate models, citing their arXiv IDs.\", \"- [ ] /code: Verify implementation by plotting ELBO convergence and computing KL divergence against the MCMC baseline.\", \"- [ ] /math: Verify that the implemented ELBO calculation in the code directly corresponds to the final derived mathematical expression.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"bayesian-deep-learning\", \"probabilistic-graphical-models\"], \"acceptance_criteria\": [\"The mathematical derivation of the ELBO must be complete, correct, and self-contained.\", \"The Python implementation must show a monotonically non-decreasing ELBO curve on a synthetic dataset, indicating successful optimization.\", \"The learned variational posterior's predictive distribution must approximate the MCMC baseline, achieving a KL divergence below 0.1.\", \"The final report must cite at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6764, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the formal identification conditions for a causal effect P(Y|do(X)=x) within a semi-Markovian structural causal model (SCM) that includes unobserved confounders. Validate this derivation by implementing a Python simulation using the PC algorithm with a kernel-based conditional independence test for an instrumental variable, assessing the algorithm's AUPRC for edge recovery under varying confounding strength. Finally, gather and cite recent literature on non-parametric causal identification methods applicable to such graphical structures.\n", "completion": "{\"route_plan\": [\"/math(Derive the formal proof for causal effect identifiability using do-calculus rules within a semi-Markovian SCM, incorporating instrumental variable assumptions to handle unobserved confounding and specifying the necessary Bayesian conditional independence tests.)\", \"/code(Implement a Python simulation using the causal-learn library to generate data from the specified SCM. Apply the PC algorithm with a custom CI test for instrumental variables, measuring the edge recovery AUPRC and bootstrap confidence intervals.)\", \"/general-search(query=\\\"site:arxiv.org site:jmlr.org non-parametric causal identification instrumental variable semi-markovian SCM\\\", mode=both)\"], \"route_rationale\": \"The route follows a theory-to-practice-to-context pattern. /math is first to establish the theoretical foundation and formal derivation. /code is second to empirically validate the mathematical theory through simulation, a crucial step for computational proofs. Finally, the blue /general-search agent is used to contextualize the findings within the current academic literature, ensuring the work is relevant and well-supported.\", \"expected_artifacts\": [\"A LaTeX document detailing the mathematical derivation of the causal effect identification.\", \"A Python script (`.py`) implementing the SCM simulation, PC algorithm, and metric calculations.\", \"A JSON file (`results.json`) containing the AUPRC and SHD metrics for different confounding strengths.\", \"A summary report in Markdown (`summary.md`) that integrates the derivation, simulation results with plots, and a literature review with citations.\"], \"thinking_outline\": [\"1. Formalize the semi-Markovian Structural Causal Model (SCM) and the causal estimand P(Y|do(X)=x) using Pearl's notation and directed acyclic graphs (DAGs).\", \"2. Apply the rules of do-calculus, in conjunction with the graphical criteria for instrumental variables, to derive a testable expression for the causal effect from observational and interventional data.\", \"3. Validate the derived identification conditions by constructing a Python simulation where the ground truth causal graph and confounding effects are known.\", \"4. Implement the PC algorithm, augmenting it with a kernel-based conditional independence test suitable for the instrumental variable setting, and apply it to the simulated datasets.\", \"5. Measure the algorithm's performance in recovering the true causal graph, focusing on AUPRC for edge detection. Verify that the empirical results align with the theoretical derivation, particularly as confounding strength increases.\", \"6. Audit the literature search results to identify and summarize at least two recent, high-impact papers on non-parametric or semi-parametric methods for causal identification in complex models.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The /math agent produces a formal derivation. The /code agent ingests this derivation, implementing a simulation that respects its assumptions (verification: code logic must match math formalism; fallback: request clarification from /math on assumptions). The /code agent produces empirical results. The /general-search agent finds supporting literature. The final report integrates all artifacts. Router QA verifies consistency across theory, code, and literature.\", \"todo_list\": [\"- [ ] /math: Formulate the semi-Markovian SCM and derive the identification formula for P(Y|do(X)=x) using do-calculus and IV assumptions.\", \"- [ ] /math: Specify the precise mathematical form of the conditional independence tests required.\", \"- [ ] /code: Implement the data generation process for the SCM with tunable confounding strength.\", \"- [ ] /code: Implement the PC algorithm using `causal-learn`, integrating a custom kernel-based CI test for the IV.\", \"- [ ] /code: Execute the simulation study, calculating AUPRC and SHD metrics with bootstrap confidence intervals.\", \"- [ ] /math: Verify that the Python implementation of the CI test and SCM correctly reflects the assumptions and logic of the mathematical derivation.\", \"- [ ] /general-search: Execute the literature query and retrieve at least two relevant papers from arXiv or JMLR.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"causal-discovery\", \"structural-causal-models\", \"bayesian-networks\"], \"acceptance_criteria\": [\"- The mathematical derivation using do-calculus must be complete, logically sound, and result in an identifiable expression for the causal effect.\", \"- The Python simulation must be reproducible and correctly implement the PC algorithm with the specified IV-based conditional independence test.\", \"- The AUPRC metric must be reported, demonstrating a clear relationship between confounding strength and the algorithm's ability to recover the true graph structure.\", \"- The final report must integrate all components and cite at least 2 relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6778, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm working on modeling incompressible fluid flow and need to implement a Physics-Informed Neural Network (PINN) to solve the 2D steady-state Navier-Stokes equations. Can you derive the full PDE residual loss, including both momentum and continuity equations, and then implement a deep learning model in PyTorch that leverages automatic differentiation for training? The model should be validated on the classic lid-driven cavity benchmark, assessing convergence and the energy spectrum of the resulting velocity field.\n", "completion": "{\"route_plan\": [\"/math(Derive the residual loss function for the 2D steady-state Navier-Stokes equations, expressing momentum and continuity as mean squared errors. Ensure the formulation is suitable for automatic differentiation and incorporates Bayesian priors on viscosity.)\", \"/general-search(query=\\\"site:arxiv.org PINN architecture Navier-Stokes lid-driven cavity convergence analysis\\\", mode=web)\", \"/code(Implement a PyTorch PINN for the lid-driven cavity problem, using the derived loss function with CUDA acceleration. Validate the model's convergence behavior and track the L2 relative error against a known finite-difference solution., using Python)\"], \"route_rationale\": \"The plan starts with /math to establish the rigorous mathematical foundation of the PDE loss. The \\\"blue\\\" /general-search agent then investigates state-of-the-art architectures and training techniques from recent literature on arXiv to inform the implementation. Finally, /code translates the mathematical formulation and research findings into an optimized, verifiable PyTorch implementation.\", \"expected_artifacts\": [\"A LaTeX document detailing the derivation of the Navier-Stokes residual loss function.\", \"A well-documented Python script implementing the PINN solver in PyTorch.\", \"A PDF report summarizing the training process, including plots of the velocity field, pressure field, and convergence metrics.\", \"A `CITATIONS.md` file listing at least two relevant arXiv papers with IDs.\", \"A JSON file containing final evaluation metrics (PDE residual MSE, L2 relative error).\"], \"thinking_outline\": [\"1. Derive the strong form of the 2D steady-state, incompressible Navier-Stokes equations and formulate the corresponding PDE residual loss for a neural network ansatz.\", \"2. Research common PINN architectures and hyperparameter settings for solving fluid dynamics problems like the lid-driven cavity case.\", \"3. Implement the PINN model in PyTorch, ensuring correct application of boundary conditions and use of automatic differentiation to compute all partial derivatives in the loss.\", \"4. Train the model on a grid of collocation points and validate the accuracy of the predicted velocity and pressure fields by checking the PDE residual on a finer, unseen grid.\", \"5. Verify the physical plausibility of the solution by checking for divergence-free velocity fields and comparing streamlines to established numerical results.\", \"6. Analyze the convergence of the training loss and the L2 relative error with respect to a high-fidelity reference solution.\"], \"handoff_plan\": \"/math will produce the LaTeX derivation of the loss function. -> /general-search will use keywords from the math derivation to find relevant PINN architectures and training strategies on arXiv. -> /code will consume both the mathematical derivation and the research findings to build the PyTorch model. -> Final artifacts (code, plots, report) are passed to the router for QA. (Verification: Code agent must cross-validate its loss implementation against the /math derivation; Fallback: If convergence fails, re-query /general-search for alternative loss weighting schemes or network architectures).\", \"todo_list\": [\"- [ ] /math: Derive the complete residual loss for the 2D steady-state Navier-Stokes equations (momentum and continuity).\", \"- [ ] /general-search: Find and summarize at least two recent arXiv papers on PINN architectures for the lid-driven cavity problem.\", \"- [ ] /code: Implement the PINN architecture in PyTorch, including data sampling for boundary and collocation points.\", \"- [ ] /code: Implement the custom loss function based on the math derivation, using automatic differentiation.\", \"- [ ] /code: Train the model and generate visualizations of the velocity/pressure fields and convergence plots.\", \"- [ ] /math: Verify that the gradients computed in the PyTorch loss function align with the analytical partial derivatives from the derivation.\", \"- [ ] /code: Validate the final solution by calculating the PDE residual MSE and L2 relative error on a hold-out dataset.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"computational-fluid-dynamics\"], \"acceptance_criteria\": [\"- The final report must contain a complete mathematical derivation of the Navier-Stokes residual loss.\", \"- The implemented PINN solver must achieve a PDE residual MSE below 1e-4 on a dense grid of test points.\", \"- The L2 relative error of the velocity field, when compared to a known benchmark solution, must be less than 5%.\", \"- The generated code must be well-documented and include a `requirements.txt` file for reproducibility.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6779, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the second-order update rule for Model-Agnostic Meta-Learning (MAML), highlighting the Hessian-vector product's role. Implement MAML in PyTorch for a sinusoidal regression few-shot task, ensuring the implementation supports both first-order and second-order approximations. Finally, analyze the convergence behavior and computational complexity trade-offs between the two approaches, referencing recent Bayesian interpretations of the MAML objective.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org Model-Agnostic Meta-Learning Hessian OR Bayesian MAML convergence analysis\\\", mode=both)\", \"/math(Derive the MAML meta-objective gradient, including the full second-order term involving the Hessian. Analyze conditions for convergence and the stability implications of the inner-loop learning rate, referencing Bayesian priors.)\", \"/code(Implement MAML in PyTorch for sinusoidal regression, including a CUDA-enabled Hessian-vector product for the second-order update. Track the meta-loss convergence and task-specific MSE, ensuring the code is well-tested., using Python)\", \"/math(Verify the correctness of the Hessian-vector product implementation against a numerical approximation. Formulate the computational complexity in FLOPS for both first-order and second-order MAML updates, considering the model's eigenvalue structure.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on MAML's optimization theory. This informs the /math agent's formal derivation of the second-order update. Next, the /code agent implements this complex logic. A final /math step is crucial for verification of the implementation against the derivation and for a formal complexity analysis, ensuring theoretical and practical alignment.\", \"expected_artifacts\": [\"A LaTeX document providing the full mathematical derivation of the second-order MAML update rule.\", \"A Python script implementing MAML for few-shot sinusoidal regression, supporting both first and second-order updates.\", \"A results summary in Markdown or JSON comparing convergence plots, final meta-test loss, and wall-clock timings.\", \"A bibliography citing at least two relevant arXiv papers with their IDs.\", \"A verification log detailing the numerical check of the Hessian-vector product implementation against finite differences.\"], \"thinking_outline\": [\"1. Research the MAML algorithm, focusing on the mathematical formulation of the meta-objective and its gradient with respect to the meta-parameters.\", \"2. Formally derive the full gradient using multivariable calculus and the chain rule, explicitly showing how the Hessian of the inner-loop loss appears.\", \"3. Implement the algorithm in PyTorch, using automatic differentiation tools to efficiently compute the Hessian-vector product without materializing the full Hessian.\", \"4. Validate the implemented second-order update by comparing its output on a test vector with a numerical approximation using finite differences to ensure correctness.\", \"5. Design and run experiments to empirically compare the convergence rate (meta-loss vs. epochs) and computational cost (wall-clock time) of first-order MAML vs. second-order MAML.\", \"6. Audit the final report to ensure the mathematical derivation is consistent with the code's logic and the experimental conclusions are supported by the generated metrics.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The initial search results and papers are handed to /math for derivation. The resulting equations are then passed to /code for implementation. The implemented code is then handed back to /math for numerical verification and complexity analysis. The final package is sent to the router for a consistency check. Fallback: If the Hessian-vector product implementation proves unstable or too slow, the agent should default to the first-order approximation and clearly document this limitation and its performance implications.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on MAML theory, focusing on second-order optimization and Bayesian interpretations.\", \"- [ ] /math: Produce a clean, step-by-step derivation of the second-order MAML gradient.\", \"- [ ] /code: Implement the MAML algorithm, the sinusoidal regression task generator, and the training loop.\", \"- [ ] /code: Instrument the implementation to log meta-loss, task performance, and wall-clock time per epoch.\", \"- [ ] /math: Develop a numerical test case and verify that the code's Hessian-vector product matches a finite-difference approximation.\", \"- [ ] /math: Formulate the theoretical computational complexity (FLOPS) for both update rules.\", \"- [ ] router: Review all artifacts for correctness, consistency between derivation and code, and clarity of the final analysis.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"meta-learning\", \"few-shot-learning\", \"optimization-based-meta-learning\"], \"acceptance_criteria\": [\"The mathematical derivation of the second-order update must be correct, complete, and clearly typeset.\", \"The Python implementation must execute without errors and generate convergence plots comparing first-order and second-order MAML on the sinusoidal regression task.\", \"The final report must correctly analyze the trade-off between meta-test loss (primary metric) and wall-clock time (secondary metric).\", \"The bibliography must cite at least two relevant arXiv pre-prints with valid IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"guidance\": \"The primary metric is the Mean Squared Error on a set of held-out, unseen sinusoidal regression tasks after k-shot adaptation; lower is better. The secondary metric is the time to complete one full pass over the meta-training dataset; lower is better.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6905, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI am preparing a study group session on reinforcement learning and need to create a tutorial on the REINFORCE algorithm. Please derive the policy gradient theorem from first principles and then implement a basic agent to solve the CartPole-v1 environment.\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient theorem, starting from the objective function J(\u03b8) and using the log-derivative trick to find its gradient with respect to the policy parameters \u03b8, ensuring convergence criteria are noted.)\", \"/general-search(query=\\\"site:arxiv.org REINFORCE algorithm simple baseline tutorial\\\", mode=rag, context=\\\"Find a foundational paper on the REINFORCE algorithm to understand its practical implementation details and common baseline subtraction techniques for variance reduction, leveraging this arxiv query.\\\")\", \"/code(Implement the REINFORCE algorithm in Python to solve CartPole-v1. Track episodic reward and use a simple Bayesian network for the policy representation to model uncertainty in control theory.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation by deriving the policy gradient theorem. Then, the blue /general-search agent is tasked with finding a canonical, practical reference for the REINFORCE algorithm to bridge theory and practice. Finally, /code implements the algorithm to provide a concrete, verifiable example of the concepts in action.\", \"expected_artifacts\": [\"A LaTeX document containing the step-by-step derivation of the policy gradient theorem.\", \"A Python script implementing the REINFORCE agent for the CartPole-v1 environment using PyTorch.\", \"A plot (.png) showing the agent's learning curve, specifically the total reward per episode over the course of training.\", \"A brief markdown report summarizing the results and citing at least two authoritative sources with arXiv IDs.\", \"A verification log confirming the agent achieves the CartPole-v1 solved-state criteria.\"], \"thinking_outline\": [\"1. Formulate the reinforcement learning objective function J(\u03b8) as the expected total reward under a stochastic policy \u03c0_\u03b8.\", \"2. Apply the log-derivative trick (\u2207log(f(x)) = \u2207f(x)/f(x)) to the policy gradient \u2207_\u03b8 J(\u03b8) to arrive at an expectation that can be estimated with Monte Carlo samples.\", \"3. Translate the derived mathematical formula into the pseudocode for the REINFORCE (Monte Carlo Policy Gradient) algorithm.\", \"4. Implement the algorithm in Python, focusing on the policy network architecture, action sampling, and the parameter update loop based on collected trajectories.\", \"5. Validate the implementation by training the agent on the CartPole-v1 environment and verifying that the average reward over 100 consecutive episodes surpasses the environment's defined 'solved' threshold.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The formal derivation from /math provides the theoretical background needed to interpret the practical algorithm details found by /general-search. These details then directly inform the /code implementation. The final code and its performance metrics are submitted for verification. Fallback: If the agent fails to learn, revisit the /general-search step to find alternative implementation guidance or baseline techniques.\", \"todo_list\": [\"- [ ] /math: Complete the formal derivation of the policy gradient theorem.\", \"- [ ] /general-search: Identify and summarize a key tutorial or paper on the REINFORCE algorithm.\", \"- [ ] /code: Implement the REINFORCE agent and generate the learning curve plot for CartPole-v1.\", \"- [ ] /verification: Check that the agent's average reward over the final 100 episodes exceeds 195.0.\", \"- [ ] /router-qa: Submit all generated artifacts (derivation, code, plot, report) for final review.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"reinforcement-learning\", \"policy-gradient\", \"control-theory\"], \"acceptance_criteria\": [\"The mathematical derivation of the policy gradient theorem is complete, logically sound, and correctly uses the log-derivative trick.\", \"The Python code for the REINFORCE agent runs without errors and demonstrates a clear positive learning trend on the CartPole-v1 task.\", \"The final trained agent achieves an average reward of at least 195.0 over 100 consecutive episodes.\", \"The final report includes a learning curve plot and properly cites at least two relevant papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/train.jsonl", "index": 6914, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous comparison between K-SVD and the Method of Optimal Directions (MOD) for dictionary learning in a compressed sensing context. Please derive the update rules for both from the sparse coding objective function, then implement them to reconstruct a synthetic biomedical signal (e.g., ECG-like) from sparse measurements. Analyze their convergence speed and the incoherence of the learned dictionaries, referencing foundational literature on recovery guarantees.\n", "completion": "{\"route_plan\": [\"/math(Derive the complete update rules for both K-SVD and MOD by solving the sparse coding Lagrangian, analyzing the subproblems for both the sparse coefficients and the dictionary atoms, and discussing convergence criteria.)\", \"/general-search(query=\\\"site:arxiv.org K-SVD MOD dictionary learning compressed sensing convergence guarantees coherence\\\", mode=both)\", \"/code(Implement K-SVD and MOD in Python, including Orthogonal Matching Pursuit for the sparse coding step. Simulate a compressed sensing scenario for a synthetic biomedical signal and evaluate reconstruction SNR and dictionary coherence.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation of the algorithms. Next, the blue /general-search agent gathers academic context and performance benchmarks to inform the experiment. Finally, /code provides an empirical validation of the theory and a direct comparison of the algorithms' performance on a practical problem.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivations for K-SVD and MOD update rules.\", \"A Python script (`.py`) with a complete, commented implementation of both algorithms and the simulation.\", \"A set of plots (`.png`) comparing the reconstruction SNR vs. iteration and final dictionary coherence for both methods.\", \"A summary report in Markdown (`.md`) analyzing the results, with BibTeX citations for at least two relevant papers.\", \"A verification log (`.txt`) confirming the consistency between the mathematical derivations and the code's core logic.\"], \"thinking_outline\": [\"1. Formulate the general dictionary learning optimization problem, defining the objective function with a sparsity-inducing penalty (e.g., L0 pseudo-norm).\", \"2. For both K-SVD and MOD, derive the iterative update steps by alternately optimizing for the sparse representations and the dictionary atoms.\", \"3. Implement the derived algorithms in Python, using a sparse coding subroutine like Orthogonal Matching Pursuit (OMP).\", \"4. Design and execute a compressed sensing simulation using a random Gaussian measurement matrix and a synthetic sparse signal.\", \"5. Validate that the implemented code's reconstruction error decreases over iterations, consistent with the expected convergence behavior of these algorithms.\", \"6. Verify the final dictionary coherence and signal recovery performance against theoretical bounds or reported results from the literature found via search.\"], \"handoff_plan\": \"/math -> /code: The mathematical derivations from /math directly specify the algorithmic logic to be implemented in /code. /general-search -> /code: Literature provides context for setting simulation parameters (e.g., sparsity levels, measurement rates) and performance baselines. /code -> router QA: The final code, plots, and report are submitted for verification. Verification: Cross-check implementation against derivations and simulation results against cited literature. Fallback: If implementation diverges from theory, flag for /math review and correction.\", \"todo_list\": [\"- [ ] /math: Derive the update rules for the K-SVD and MOD algorithms from the sparse coding objective function.\", \"- [ ] /general-search: Find and summarize at least two seminal papers on K-SVD/MOD for compressed sensing.\", \"- [ ] /code: Implement both K-SVD and MOD, including a sparse approximation subroutine like OMP.\", \"- [ ] /code: Create a simulation to test sparse signal recovery and generate comparison plots for SNR and coherence.\", \"- [ ] /math: Verify that the core update loops in the Python code are a correct translation of the derived mathematical formulas.\", \"- [ ] /code: Audit the final reconstruction SNR from the simulation to ensure it is within a plausible range compared to results cited in the literature.\", \"- [ ] /router: Perform final QA review on all artifacts, ensuring consistency across derivation, code, and report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- Mathematical derivations for both K-SVD and MOD are present and correct.\", \"- The Python implementation successfully runs and generates plots comparing reconstruction SNR and dictionary coherence.\", \"- The final report compares the two algorithms, citing at least two relevant papers and referencing the simulation results.\", \"- The primary metric (reconstruction SNR) for the recovered signal is greater than 20 dB for both methods under the specified simulation settings.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 9, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nImplement a Bayesian optimization loop using a Gaussian Process with a Mat\u00e9rn covariance function. Demonstrate its effectiveness in finding the minimum of a noisy, non-convex 2D function and compare its convergence rate against a random search baseline.\n", "completion": "{\"route_plan\": [\"/math(Derive the closed-form posterior predictive equations for a Gaussian Process regression model using a Mat\u00e9rn covariance function, ensuring the final result is expressed in terms of the kernel matrix and observed noise.)\", \"/general-search(query=\\\"site:arxiv.org gaussian process bayesian optimization expected improvement convergence\\\", mode=web)\", \"/code(Implement the Bayesian optimization loop in Python, using the derived equations. The code must track simple regret over iterations and visualize the convergence against a random search baseline for a 2D benchmark function.)\"], \"route_rationale\": \"The /math tool is used first to establish the theoretical foundation by deriving the core Gaussian Process equations. The blue /general-search agent then finds authoritative references for implementing the acquisition function, which can have numerical stability issues. Finally, /code synthesizes the math and implementation details into a verifiable Python script.\", \"expected_artifacts\": [\"A PDF document (`derivation.pdf`) containing the complete mathematical derivation of the GP posterior predictive distribution.\", \"A well-commented Python script (`bayesian_opt.py`) implementing the Bayesian optimization loop and a random search baseline.\", \"A PNG image (`convergence.png`) plotting the simple regret versus number of iterations for both methods.\", \"A JSON file (`metrics.json`) reporting the final simple regret and wall clock time for both optimizers.\", \"A `citations.bib` file with at least two references on Bayesian Optimization.\"], \"thinking_outline\": [\"1. Formulate the Gaussian Process regression problem, defining the prior over functions via a Mat\u00e9rn kernel, the Gaussian likelihood, and the resulting posterior over functions.\", \"2. Derive the closed-form expressions for the posterior predictive mean and variance for a new, unobserved input point, conditioned on existing observations.\", \"3. Define the Expected Improvement (EI) acquisition function and express its formula in terms of the GP's predictive mean and variance.\", \"4. Implement the complete Bayesian Optimization loop: update GP posterior with new data, find the maximum of the acquisition function to select the next point, query the objective function, and repeat for a fixed budget.\", \"5. Validate the implementation by running it on a known 2D benchmark function (e.g., Branin-Hoo) and verify that the simple regret for the Bayesian optimizer decreases significantly faster than for a random search baseline.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivations from /math provide the core formulas needed for implementation. /general-search provides context on robust numerical methods for the acquisition function, which informs the /code agent. The final code artifact is passed to the router for QA. Verification: The code's convergence plot must match theoretical expectations. Fallback: If verification fails, the /math agent will be asked to re-check the derivations for the acquisition function, and /general-search will be tasked to find alternative acquisition functions.\", \"todo_list\": [\"- [ ] /math: Derive the posterior predictive mean and variance equations for a Gaussian Process with a Mat\u00e9rn covariance function.\", \"- [ ] /general-search: Find reference papers on the numerically stable calculation of the Expected Improvement acquisition function.\", \"- [ ] /code: Implement the Bayesian optimization algorithm in Python, including the GP model, Mat\u00e9rn kernel, and EI acquisition function.\", \"- [ ] /code: Implement a random search baseline for comparison and a test harness using a standard benchmark function.\", \"- [ ] /code: Generate plots comparing the convergence (simple regret vs. iterations) of Bayesian optimization against the random search baseline.\", \"- [ ] /code: Validate that the regret decreases monotonically (on average) and faster for the Bayesian optimizer than for random search.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"bayesian-inference\", \"stochastic-processes\", \"gaussian-processes\"], \"acceptance_criteria\": [\"The provided mathematical derivation for the GP posterior predictive distribution must be complete and correct.\", \"The Python script must execute without errors and successfully run both the Bayesian optimization and random search algorithms for at least 50 iterations.\", \"The final convergence plot must show the simple regret for the Bayesian optimizer is consistently lower than the random search baseline after the first 10 iterations.\", \"The `metrics.json` file must be generated and contain valid numerical values for regret and time for both methods.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 16, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a rigorous analysis of perturbed gradient descent's ability to find second-order stationary points. Please derive the convergence guarantee, focusing on the role of isotropic noise in escaping saddle points characterized by negative Hessian eigenvalues. The analysis should also include an empirical validation using Hessian-vector products on a non-convex matrix factorization problem, demonstrating the theoretical escape dynamics.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'perturbed gradient descent' convergence 'strict-saddle property' analysis\\\", mode=rag)\", \"/math(Derive the convergence rate for perturbed gradient descent, specifically analyzing the Hessian's minimum eigenvalue dynamics and the role of isotropic noise in satisfying the strict-saddle property.)\", \"/code(Implement perturbed GD in Python/JAX for a matrix factorization problem. Track the convergence rate and use CUDA-accelerated Hessian-vector products to monitor the minimum eigenvalue as a diagnostic for escaping saddle points.)\", \"/math(Verify the empirical convergence from the code against the derived theoretical bounds. Use bootstrap methods to establish a confidence interval for the escape time from a saddle point.)\"], \"route_rationale\": \"The blue /general-search agent is needed first to gather the latest theorems on perturbed gradient descent and the strict-saddle property from authoritative sources like arXiv. This literature informs the formal /math derivation of the convergence bounds. The /code tool is then used to implement the algorithm and create an empirical testbed to validate the theory. A final /math call is crucial for a rigorous comparison between the theoretical derivation and the empirical results, ensuring correctness.\", \"expected_artifacts\": [\"- A LaTeX report detailing the formal proof of the convergence guarantee.\", \"- A Python script implementing perturbed gradient descent and the non-convex test problem.\", \"- Plots showing the gradient norm vs. iteration and the minimum Hessian eigenvalue over time near a saddle point.\", \"- A bibliography file with citations for at least two relevant research papers.\", \"- A verification log comparing theoretical predictions to empirical measurements.\"], \"thinking_outline\": [\"1. Conduct a literature review to find canonical papers on perturbed gradient descent and the conditions required to escape saddle points, focusing on the strict-saddle property.\", \"2. Formally derive the high-probability convergence bound, making explicit the dependence on dimensionality, smoothness parameters, and the magnitude of the Hessian's negative eigenvalue.\", \"3. Implement the algorithm and a suitable non-convex test function (e.g., matrix factorization) where saddle points can be easily characterized.\", \"4. Run simulations to collect data on convergence speed and the behavior of the minimum Hessian eigenvalue when the iterate is near a saddle point.\", \"5. Verify that the empirical convergence rate from the simulation matches the theoretical rate derived in step 2 within a reasonable statistical margin.\", \"6. Validate that the code's mechanism for escaping a saddle point (i.e., moving in the direction of the negative eigenvector) is consistent with the theory.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code -> /math (verification) -> router QA. The initial /math agent uses theorems from /general-search. The /code agent implements the algorithm derived by /math. The final /math agent verifies the output from /code against the initial derivation. If verification fails (e.g., empirical rate deviates significantly from theory), the fallback is to return to the /math agent to check for errors in the derivation or assumptions before re-running the /code agent with corrected diagnostics.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on perturbed gradient descent for non-convex optimization.\", \"- [ ] /math: Formally derive the convergence rate and the conditions for escaping saddle points based on the literature.\", \"- [ ] /code: Implement the perturbed gradient descent algorithm and a matrix factorization test case.\", \"- [ ] /code: Add instrumentation to track the gradient norm and estimate the minimum Hessian eigenvalue using power iteration.\", \"- [ ] /math: Verify that the algorithm implemented in the code correctly reflects the mathematical derivation.\", \"- [ ] /code: Generate plots visualizing convergence and the eigenvalue dynamics near a saddle point.\", \"- [ ] /math: Compare the empirical convergence rate from plots against the derived theoretical bound and report the delta.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"stochastic-methods\"], \"acceptance_criteria\": [\"- The final report must contain a complete mathematical proof of the convergence guarantee for perturbed gradient descent.\", \"- The provided code must successfully converge on the test problem, demonstrably escaping a saddle point.\", \"- The empirical results (plots of gradient norm, eigenvalues) must be consistent with the theoretical claims made in the proof.\", \"- The report must cite at least two relevant arXiv papers with full IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculated by fitting a line to the log-log plot of gradient norm vs. number of iterations.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Estimated using the Lanczos algorithm or power iteration when the gradient norm is below a small threshold, indicating proximity to a stationary point.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 80, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to synthesize a proof-program for the theorem `\u2200 (P Q : Prop), P \u2192 (Q \u2192 P)` within a minimal Calculus of Constructions framework. The synthesis should be guided by a transformer-based policy network trained via reinforcement learning, specifically using a Monte Carlo Tree Search (MCTS) to explore the proof space. Please provide the formal specification of the search problem, a Python implementation of the guided MCTS, and a formal verification of the resulting proof term.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org transformer guided monte carlo tree search theorem proving Coq Lean\\\", mode=both)\", \"/math(Formalize the proof synthesis as a Markov Decision Process. Define the state space as partial proof terms in the Calculus of Constructions, the action space as tactic applications, and derive a Bayesian reward shaping function to encourage convergence.)\", \"/code(Implement a prototype MCTS algorithm in Python with a placeholder neural policy. The environment should simulate tactic applications in a minimal type theory kernel. Ensure the implementation includes convergence checks and Bayesian optimization hooks for hyperparameters.)\", \"/math(Formally verify the synthesized proof term by providing a step-by-step type-checking derivation against the rules of the Calculus of Constructions, ensuring it proves the target proposition `\u2200 (P Q : Prop), P \u2192 (Q \u2192 P)`., includes cybersecurity, gradient)\"], \"route_rationale\": \"The blue general-search agent first grounds the task in state-of-the-art literature. The first /math agent then establishes the rigorous formal foundation required for the synthesis problem. The /code agent implements the specified search algorithm. The final /math agent provides the critical formal verification of the artifact produced by the /code agent, ensuring logical correctness.\", \"expected_artifacts\": [\"- A literature review summary with at least two cited arXiv papers on neural theorem proving.\", \"- A LaTeX document detailing the MDP formalization of the proof search problem.\", \"- A Python script (`mcts_prover.py`) containing the MCTS implementation and a simple environment.\", \"- A verification report showing the full type-checking derivation of the synthesized proof term.\"], \"thinking_outline\": [\"1. Research existing frameworks for neural-guided theorem proving (e.g., GPT-f, PACT) to select an appropriate MCTS variant and policy network architecture.\", \"2. Formalize the target theorem and the rules of the Calculus of Constructions as a well-defined search problem (states, actions, transitions, rewards).\", \"3. Implement the search environment, including the logic for applying proof tactics and checking the validity of partial proof states.\", \"4. Execute the guided MCTS to synthesize a complete proof term for the given proposition.\", \"5. Verify the logical correctness of the synthesized term by running it through an independent type-checker or by manually detailing the step-by-step derivation.\", \"6. Audit the search process by comparing the number of nodes expanded by the guided search versus a baseline uninformed search (e.g., breadth-first search) to validate efficiency.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The initial search results from /general-search inform the formal model created by /math. This model is the specification for the /code implementation. The proof term artifact from /code is then passed to the second /math agent for formal verification. The router performs a final QA check. Verification: The final proof derivation from /math must match the structure of the object produced by /code. Fallback: If verification fails, the task is routed back to /code to debug the search algorithm or its neural guide.\", \"todo_list\": [\"- [ ] /general-search: Identify and summarize two key papers on using transformers with MCTS for automated theorem proving.\", \"- [ ] /math: Define the state, action, and reward functions for the proof synthesis MDP.\", \"- [ ] /code: Implement the core MCTS loop and the proof state transition logic in Python.\", \"- [ ] /math: Provide the formal type-checking derivation for the generated proof term.\", \"- [ ] /code: Add unit tests to validate the correctness of individual tactic applications within the proof environment.\", \"- [ ] /math: Verify that the reward structure theoretically encourages finding the shortest proof.\", \"- [ ] /code: Validate that the synthesized proof object can be serialized and re-parsed correctly.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"neural-symbolic\", \"theorem-proving\", \"program-synthesis\"], \"acceptance_criteria\": [\"- The Python script must execute and output a candidate proof term.\", \"- The final verification report must demonstrate, via a formal derivation, that the synthesized term is a valid proof of the target theorem.\", \"- The literature review must cite at least two relevant arXiv papers with their IDs.\", \"- The solution must contain both a code component (synthesis) and a mathematical component (verification).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"1 if a valid, type-checked proof is generated within the compute budget, 0 otherwise.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Total number of MCTS nodes explored before finding the proof. Lower is better.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 90, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the complete Lagrangian for a feed-forward neural network's weight matrix regularized by its spectral norm. Then, implement an optimization algorithm using projected gradient descent to enforce this constraint and analyze its convergence properties compared to a standard L2 penalty.\n", "completion": "{\"route_plan\": [\"/math(Derive the Euler-Lagrange equations for a neural network weight matrix regularized by its spectral norm, and formulate the projected gradient descent update rule for this specific constraint.)\", \"/general-search(query=\\\"site:arxiv.org projected gradient descent spectral norm regularization deep learning\\\", mode=web. Gather foundational arXiv papers discussing the theoretical convergence guarantees and practical implementations.)\", \"/code(Implement the derived optimizer in PyTorch, training a model on a synthetic dataset. Verify the spectral norm constraint is met and compare the training loss convergence against Adam with weight decay., using Python)\"], \"route_rationale\": \"The task requires a multi-step approach: first, /math is used for the formal derivation of the optimization problem and update rules. Next, the blue /general-search agent is tasked with finding foundational literature to ground the theory and inform the implementation. Finally, /code is used to empirically validate the derived method by implementing it and comparing its performance against a known baseline, thus connecting theory to practice.\", \"expected_artifacts\": [\"A PDF document containing the full mathematical derivation of the Lagrangian, KKT conditions, and the projected gradient descent update rule.\", \"A Python script (`.py`) implementing the custom optimizer and the training loop for comparison.\", \"A plot (`.png`) comparing the training loss convergence of the projected gradient descent method versus a baseline (e.g., Adam with L2 regularization).\", \"A `citations.json` file listing at least two relevant academic papers with their arXiv IDs.\", \"A verification report (`verification.md`) confirming that the spectral norm of the weight matrix remained below the specified threshold during training.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem for a neural network layer with a spectral norm constraint on its weight matrix.\", \"2. Derive the Lagrangian for this problem and state the Karush-Kuhn-Tucker (KKT) optimality conditions.\", \"3. Formulate the projected gradient descent update rule. This involves a standard gradient step followed by a projection step, which requires computing the Singular Value Decomposition (SVD) to project the weights onto the spectral norm ball.\", \"4. Implement the algorithm in Python and train a simple model on a synthetic dataset, comparing its training loss convergence against a standard L2-regularized model.\", \"5. Verify that the spectral norm of the weight matrix remains within the constraint throughout training for the custom optimizer and document any violations.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (literature review) -> /code (implementation & verification). The formal equations from /math guide the implementation in /code. The papers found by /general-search provide theoretical context for the /math agent and implementation details for the /code agent. The final results from /code (plots, metrics) are sent to router QA for verification against the theoretical claims from /math. Fallback: If the /code implementation fails to converge or violates constraints, the issue is routed back to /math to check the derivation of the projection operator.\", \"todo_list\": [\"- [ ] /math: Derive the Lagrangian and KKT conditions for the spectral norm constrained optimization problem.\", \"- [ ] /math: Formulate the projected gradient descent update step, detailing the projection operator via SVD.\", \"- [ ] /general-search: Find at least two key papers on spectral norm regularization in deep learning from arXiv.\", \"- [ ] /code: Implement the projected gradient descent optimizer in PyTorch or JAX.\", \"- [ ] /code: Train a simple MLP on a synthetic dataset and plot the convergence of the custom optimizer versus Adam with L2 regularization.\", \"- [ ] /code: Validate that the spectral norm of the learned weights respects the constraint at each training step.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"constrained-optimization\", \"spectral-regularization\", \"deep-learning\"], \"acceptance_criteria\": [\"The mathematical derivation of the projection operator must be correct and complete.\", \"The Python implementation must successfully train a model and enforce the spectral norm constraint to within a tolerance of 1e-5 throughout training.\", \"The final report must include a plot clearly showing the comparative convergence behavior and cite at least two relevant academic papers with valid arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 109, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a Monte Carlo simulation for the surface code, specifically the toric code on a 2L x 2L lattice, under a bit-flip and phase-flip (depolarizing) noise model. The primary goal is to determine the quantum error correction threshold. This requires implementing a minimum-weight perfect matching (MWPM) decoder, possibly by interfacing with an existing library like Blossom V, to process the error syndromes and infer the most likely error chain. The final output should be a plot of the logical error rate versus the physical error probability for various lattice sizes (e.g., L=4, 6, 8), with a clear identification of the error threshold derived from a finite-size scaling analysis.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org toric code minimum weight perfect matching decoder depolarizing noise threshold\\\", mode=rag, includes cybersecurity)\", \"/math(Formalize the stabilizer group algebra for the toric code, derive the syndrome-to-graph reduction for the depolarizing channel, and analyze the eigenvalue spectrum of the logical operators., includes cybersecurity)\", \"/code(Implement a Monte Carlo simulator in Python for the toric code. Use CUDA for parallel error generation and employ a bootstrap method for estimating confidence intervals on logical error rates.)\", \"/math(Derive the finite-size scaling ansatz for the logical error rate P_L(p, L) and specify a nonlinear least-squares fitting procedure to extract the error threshold p_th, establishing a formal confidence interval for the estimate.)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational literature on the toric code and MWPM decoders from authoritative sources like arXiv. This informs the /math agent's task of formalizing the model. The core simulation logic is then implemented by the /code agent. Finally, the /math agent is re-engaged to perform the sophisticated statistical analysis (finite-size scaling) on the simulation output to extract the primary metric.\", \"expected_artifacts\": [\"A Python script implementing the full Monte Carlo simulation for the toric code with an MWPM decoder.\", \"A formal mathematical derivation of the toric code stabilizers, syndrome-to-graph mapping, and the finite-size scaling ansatz.\", \"A final report in PDF format containing plots of logical error rate vs. physical error rate, the finite-size scaling analysis, the calculated error threshold, and a list of cited arXiv papers.\", \"A JSON file containing the raw simulation data and the final computed metrics (error_threshold, confidence_interval).\", \"A verification log detailing the checks performed to validate the simulation logic against the mathematical model.\"], \"thinking_outline\": [\"1. Research the toric code's stabilizer formalism, the properties of the depolarizing noise channel, and the algorithm for reducing syndrome data to a minimum-weight perfect matching problem on a graph.\", \"2. Formulate the precise mathematical model, including the definition of stabilizer operators (star and plaquette), the generation of the syndrome graph from measurement outcomes, and the definition of a logical error.\", \"3. Implement the complete Monte Carlo simulation loop: initialize the lattice, apply random physical errors, measure syndromes, construct the matching graph, run the MWPM decoder, apply corrections, and check for logical errors.\", \"4. Verify the implementation's core components by testing with simple, known error patterns (e.g., single qubit errors, short error chains) and validating that the correct syndrome graph and correction operator are produced.\", \"5. Execute large-scale simulation sweeps across a matrix of lattice sizes (L) and physical error probabilities (p) to gather sufficient statistics for the logical error rate.\", \"6. Audit the collected data and apply the finite-size scaling model to fit the logical error rates, extracting the critical error threshold and comparing it against established literature values (around 10-11%) as a final validation.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (model formalization) -> /code (simulation implementation) -> /math (results analysis) -> router QA. Verification: At the code-to-math handoff, the /math agent must verify that the scaling analysis is appropriate for the data structure produced by the /code agent. Fallback: If the MWPM decoder implementation proves too complex, the /code agent should fallback to using a pre-existing, well-vetted library like PyMatching and document its integration.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers (e.g., by Kitaev, Dennis) on the toric code and MWPM decoding.\", \"- [ ] /math: Write down the explicit matrix representations for the stabilizer and logical operators for a small (L=2) lattice.\", \"- [ ] /code: Implement the core simulation class, including methods for error application, syndrome measurement, and logical operator checking.\", \"- [ ] /code: Integrate an external MWPM solver (e.g., Blossom V via a Python wrapper) to handle the decoding step.\", \"- [ ] /math: Derive the specific data collapse function for the finite-size scaling analysis.\", \"- [ ] /math: Verify that the graph construction logic in the Python code correctly implements the mathematical mapping from syndromes to graph edges.\", \"- [ ] /code: Validate the decoder's output for at least three distinct, non-trivial error configurations by manually calculating the expected correction.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"quantum-computing\", \"error-correction-codes\", \"fault-tolerance\"], \"acceptance_criteria\": [\"- The Python code must be executable and reproduce the logical error rate plots presented in the final report.\", \"- The final report must clearly state the calculated error threshold, and it must be within 15% of the accepted literature value for the toric code (~10.3%).\", \"- The finite-size scaling plot must show a clear crossing point for the logical error rates of different lattice sizes, visually confirming the phase transition.\", \"- All mathematical derivations must be self-contained and consistent with the simulation's implementation details.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculated by finding the crossing point of the logical error rate P_L as a function of physical error rate p for different system sizes L. This is achieved by fitting the data to the scaling ansatz P_L = f((p - p_th)L^(1/\u03bd)) and finding the p_th that minimizes the fit error.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"For a given physical error rate p and lattice size L, this is computed as the fraction of Monte Carlo trials where the decoder fails to return the system to the codespace, i.e., N_failed / N_total_trials.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 125, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a preconditioned conjugate gradient (PCG) solver in CUDA for a large, sparse, symmetric positive-definite (SPD) system Ax=b arising from a finite element discretization of the Poisson equation. Please derive the PCG algorithm, discuss the role of the preconditioner in improving the condition number and convergence rate, and then implement a CUDA kernel for the sparse matrix-vector product (SpMV) using the Compressed Sparse Row (CSR) format. The focus is on analyzing the computational complexity and ensuring numerical stability via eigenvalue analysis of the preconditioned system M\u207b\u00b9A.\n", "completion": "{\"route_plan\": [\"/math(Derive the preconditioned conjugate gradient (PCG) algorithm from the standard CG method. Analyze how an incomplete Cholesky preconditioner affects the eigenvalue spectrum and convergence rate bounds of the preconditioned system M\u207b\u00b9A.)\", \"/general-search(query=\\\"site:arxiv.org 'CUDA SpMV CSR performance' 'incomplete cholesky preconditioner' GPU convergence\\\", mode=both)\", \"/code(Implement the PCG solver in Python using CuPy for CUDA acceleration. The implementation must include an efficient CSR-based SpMV kernel and a Jacobi preconditioner. Profile the SpMV kernel's GFLOPS and verify solver convergence against a known solution.)\"], \"route_rationale\": \"The /math tool is used first to establish the theoretical foundation and algorithm derivation. The blue /general-search agent then gathers state-of-the-art implementation techniques for the performance-critical CUDA SpMV kernel. Finally, the /code tool synthesizes the theory and research into a concrete, verifiable GPU implementation.\", \"expected_artifacts\": [\"A PDF document with the LaTeX derivation of the PCG algorithm and its convergence analysis.\", \"A Python source file (`pcg_solver.py`) containing the CUDA-accelerated PCG implementation.\", \"A plot (`convergence.png`) showing the L2 norm of the residual vs. iteration number.\", \"A summary report (`report.md`) explaining the implementation, presenting performance results, and including all citations.\"], \"thinking_outline\": [\"1. Derive the Preconditioned Conjugate Gradient (PCG) algorithm, starting from the standard CG method and correctly introducing the preconditioning matrix M.\", \"2. Theoretically analyze the role of the preconditioner M by examining the condition number of M\u207b\u00b9A and its effect on the convergence bound.\", \"3. Research efficient CUDA implementations of the key bottleneck: the Sparse Matrix-Vector (SpMV) product for matrices in CSR format.\", \"4. Implement the full PCG solver in Python/CUDA, including the SpMV kernel and a suitable preconditioner (e.g., Jacobi).\", \"5. Validate the implementation's correctness by testing it on a known SPD system and checking that the residual norm decreases at a rate consistent with theory.\", \"6. Verify the performance of the CUDA kernel by profiling its execution time and calculating GFLOPS against a CPU baseline for varying matrix sizes.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the exact algorithm specification for /code. The research from /general-search informs the low-level implementation choices within the CUDA kernel. /code produces the final implementation and performance plots, which are passed to the router for a final QA review. Verification: router checks if the code logic matches the math derivation and if performance meets expectations. Fallback: If CUDA implementation is unsuccessful, /code will implement a CPU version using SciPy sparse routines and focus the report on the algorithmic analysis.\", \"todo_list\": [\"- [ ] /math: Derive the PCG algorithm and analyze preconditioner impact on eigenvalue distribution.\", \"- [ ] /general-search: Find SOTA CUDA SpMV implementations and relevant preconditioner literature from arXiv.\", \"- [ ] /code: Implement the CSR SpMV CUDA kernel using CuPy or a raw CUDA kernel.\", \"- [ ] /code: Implement the main PCG solver loop using the SpMV kernel and a Jacobi preconditioner.\", \"- [ ] /math: Verify that the implemented PCG steps in the code artifact correctly match the derived algorithm.\", \"- [ ] /code: Test the solver on a sample Poisson problem, plot residual convergence, and profile performance.\", \"- [ ] /general-search: Collect and format citations for the final report artifact.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-computing\", \"scientific-computing\"], \"acceptance_criteria\": [\"The mathematical derivation of the PCG algorithm is complete, typeset in LaTeX, and correct.\", \"The CUDA implementation correctly solves a sparse SPD system to a 1e-8 residual tolerance and the `convergence.png` plot confirms monotonic decrease of the residual norm.\", \"The final report cites at least 2 relevant papers with arXiv IDs on GPU-accelerated iterative methods.\", \"The SpMV kernel performance is reported in GFLOPS and demonstrates a significant speedup over a naive sequential CPU implementation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 171, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the coordinate ascent updates for mean-field variational inference in a Bayesian Latent Dirichlet Allocation model, then implement this from scratch. Compare its convergence against a black-box ADVI implementation and justify the trade-offs using recent literature.\n", "completion": "{\"route_plan\": [\"/math(Derive the evidence lower bound (ELBO) and the complete coordinate ascent variational inference (CAVI) update equations for a Bayesian Latent Dirichlet Allocation model, clearly stating all distributional assumptions for the variational family.)\", \"/general-search(query=\\\"site:arxiv.org Automatic Differentiation Variational Inference vs coordinate ascent LDA performance\\\", mode=web)\", \"/code(Implement the derived CAVI updates for LDA in Python using NumPy. Separately, use Pyro to implement an ADVI-based equivalent. Compare their convergence on a synthetic dataset by tracking the ELBO and topic coherence, ensuring CUDA is utilized for acceleration.)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation by deriving the specific update equations for LDA. Next, the blue /general-search agent is tasked with gathering modern context and comparative literature on different VI methods. Finally, /code is used to implement the derived theory, validate it empirically against a modern baseline, and generate quantitative results.\", \"expected_artifacts\": [\"- A PDF document containing the full mathematical derivation of the CAVI updates for LDA.\", \"- A Python script (`lda_vi_comparison.py`) implementing both CAVI and ADVI for LDA and generating comparison plots.\", \"- A summary report (`report.md`) explaining the results, comparing the methods, and citing the discovered literature.\", \"- A JSON file (`metrics.json`) with the final ELBO and topic coherence scores for both methods.\"], \"thinking_outline\": [\"1. Formulate the complete generative process for Latent Dirichlet Allocation (LDA) and define the posterior inference problem.\", \"2. Propose a fully-factorized mean-field variational family and derive the Evidence Lower Bound (ELBO) for the LDA model.\", \"3. Analytically derive the optimal coordinate ascent updates for each variational parameter by taking functional derivatives of the ELBO.\", \"4. Implement the derived CAVI algorithm and an ADVI baseline, then generate a synthetic corpus with known topic structure to test them.\", \"5. Validate the CAVI implementation by checking for monotonic, non-decreasing ELBO at each iteration and comparing learned topics to the ground truth.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: code outputs must match theoretical ELBO convergence properties and reference findings from literature; fallback: if CAVI fails to converge, re-audit math derivation for errors).\", \"todo_list\": [\"- [ ] /math: Derive the complete ELBO and CAVI update equations for LDA.\", \"- [ ] /general-search: Find at least two key papers comparing model-specific VI (like CAVI) with black-box VI (like ADVI).\", \"- [ ] /code: Implement the CAVI algorithm for LDA using the derived updates from the /math agent.\", \"- [ ] /code: Implement an ADVI baseline for LDA using a probabilistic programming framework like Pyro.\", \"- [ ] /code: Validate the CAVI implementation by confirming the ELBO is non-decreasing on each update step on a small, controlled dataset.\", \"- [ ] /code: Generate plots comparing the convergence speed and final ELBO of both methods.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"variational-inference\", \"probabilistic-models\", \"topic-modeling\"], \"acceptance_criteria\": [\"- The mathematical derivation of the CAVI updates must be complete, correct, and match established literature.\", \"- The Python implementation must execute without errors and show a monotonically non-decreasing ELBO for the CAVI algorithm.\", \"- The final report must include a plot comparing the ELBO convergence of CAVI vs. ADVI over iterations/time and cite at least two relevant papers with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 243, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to understand the Alternating Direction Method of Multipliers (ADMM) for solving the LASSO problem. Can you derive the augmented Lagrangian, explain the primal-dual updates, and show how ADMM achieves convergence through a simple Python example?\n", "completion": "{\"route_plan\": [\"/math(Derive the augmented Lagrangian for the LASSO problem and outline the iterative primal-dual update steps for the ADMM algorithm, analyzing the conditions for convergence.)\", \"/code(Implement the ADMM algorithm in Python using NumPy to solve a synthetic LASSO regression problem, a common task in econometrics, and validate the solution against a standard library solver.)\", \"/general-search(query=\\\"site:stanford.edu site:arxiv.org Alternating Direction Method of Multipliers for convex optimization convergence proof\\\", mode=rag)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical underpinnings of ADMM. This theory is then implemented and validated using /code. Finally, the blue /general-search agent is tasked with finding authoritative references to ground the theoretical and practical work in established literature.\", \"expected_artifacts\": [\"A LaTeX document with the full mathematical derivation of the ADMM updates for LASSO.\", \"A documented Python script implementing ADMM for LASSO and comparing its output to `sklearn.linear_model.Lasso`.\", \"A bibliography file with at least two key references on ADMM, including arXiv IDs.\", \"A JSON file containing metrics comparing the convergence speed and final objective value of the custom ADMM implementation against the library solution.\"], \"thinking_outline\": [\"1. Formulate the LASSO problem as a constrained optimization problem and derive its corresponding augmented Lagrangian.\", \"2. Decompose the augmented Lagrangian to derive the individual update rules for the primal variables (x and z) and the dual variable (u) in the ADMM framework.\", \"3. Implement the derived ADMM update steps in a Python script, creating a synthetic dataset for testing.\", \"4. Validate the correctness of the implementation by comparing the final estimated coefficients and objective value against the results from a trusted library like scikit-learn.\", \"5. Analyze the convergence properties by plotting the primal and dual residuals over iterations.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: Cross-check the derived update equations in the /math step against the implemented logic in the /code step. Verify that the Python script's output matches the scikit-learn baseline within a reasonable tolerance.; fallback: If derivation is flawed, re-run /math with corrected constraints. If code fails validation, debug the update steps or data generation.)\", \"todo_list\": [\"- [ ] /math: Derive ADMM update equations for LASSO from the augmented Lagrangian.\", \"- [ ] /code: Implement the ADMM solver in Python and create a validation test case.\", \"- [ ] /general-search: Find and summarize two seminal papers on ADMM theory.\", \"- [ ] verification: Plot the primal/dual residuals to visually confirm convergence.\", \"- [ ] router-qa: Review all artifacts for correctness, clarity, and consistency before final approval.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"convex-optimization\", \"admm\", \"lasso-regression\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ADMM updates for LASSO is complete and correct.\", \"- The Python implementation successfully converges on a synthetic dataset, and its solution is within 1e-4 tolerance of the scikit-learn baseline.\", \"- The final report includes plots showing the decay of primal and dual residuals below a threshold of 1e-5.\", \"- At least two relevant academic papers (e.g., from arXiv) are cited correctly.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 329, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement a mean-field variational inference algorithm for Latent Dirichlet Allocation (LDA) from first principles. Please derive the coordinate ascent update equations for the variational parameters (phi and gamma) and the full expression for the Evidence Lower Bound (ELBO). Then, implement this in Python using NumPy, ensuring your code tracks ELBO convergence and can report document-topic distributions. Validate the mathematical derivation against the implementation.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:jmlr.org site:arxiv.org Latent Dirichlet Allocation mean-field variational inference derivation\\\", mode=web)\", \"/math(Derive the coordinate ascent variational inference update equations for LDA's gamma and phi parameters, and the full Evidence Lower Bound (ELBO) expression, referencing the Digamma function.)\", \"/code(Implement the Bayesian coordinate ascent algorithm for LDA in Python/NumPy. The implementation must track ELBO convergence and handle sparse document-term matrices.)\", \"/math(Verify the implemented ELBO formula against the derived expression, confirming all terms, signs, and the use of the Digamma function match the mathematical derivation., includes cybersecurity, gradient)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather foundational papers on LDA and VI. Then, /math is used for the complex symbolic derivation of the update equations and the ELBO. /code implements this algorithm, and a final /math call is used to cross-verify the code's logic against the formal derivation, ensuring correctness.\", \"expected_artifacts\": [\"- A LaTeX document with the full mathematical derivation of the coordinate ascent updates and the ELBO for LDA.\", \"- A Python script (`lda_vi.py`) containing the implementation of the variational inference algorithm.\", \"- A report (`report.md`) summarizing the convergence plot of the ELBO and showing the top words for inferred topics.\", \"- A list of at least two cited papers (with arXiv IDs) that informed the derivation and implementation.\"], \"thinking_outline\": [\"1. Research the probabilistic graphical model for Latent Dirichlet Allocation and the general form of mean-field variational inference.\", \"2. Derive the specific coordinate ascent update equations for the variational distributions over topics per document (gamma) and words per topic (phi). Also derive the complete ELBO expression.\", \"3. Implement the derived algorithm in Python, focusing on the iterative update loop and the ELBO calculation for monitoring convergence.\", \"4. Verify that the signs and terms in the implemented ELBO calculation exactly match the derived mathematical formula.\", \"5. Run the implementation on a sample text corpus and analyze the resulting topic-word distributions for coherence.\", \"6. Validate the algorithm's convergence by plotting the ELBO over iterations, ensuring it is monotonically increasing.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: code's ELBO calculation must match the math derivation from the second step; fallback: if divergence, re-run /math derivation with stricter constraints and audit the code's implementation of special functions like Digamma).\", \"todo_list\": [\"- [ ] /general-search: Find canonical papers on LDA and mean-field VI, such as Blei et al. (2003).\", \"- [ ] /math: Derive the update equations for variational parameters phi and gamma.\", \"- [ ] /math: Derive the full expression for the Evidence Lower Bound (ELBO).\", \"- [ ] /code: Implement the coordinate ascent variational inference algorithm in Python/NumPy.\", \"- [ ] /code: Add functionality to track and plot the ELBO to monitor convergence.\", \"- [ ] /math: Verify that the code's ELBO computation is identical to the derived mathematical formula.\", \"- [ ] /router: Perform final QA check on all artifacts, ensuring consistency between derivation, code, and report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"topic-modeling\"], \"acceptance_criteria\": [\"- The provided mathematical derivation for the ELBO and update equations must be complete and correct.\", \"- The Python implementation must converge on a sample dataset, demonstrated by a monotonically non-decreasing ELBO plot.\", \"- The final report must present coherent topics (qualitatively assessed) and cite at least two relevant academic papers.\", \"- The ELBO calculation in the code must pass a symbolic verification check against the mathematical derivation.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Calculate the ELBO at each iteration. The value must be monotonically non-decreasing. Report the final converged value.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate the perplexity on a held-out test set. Lower values indicate better generalization. Formula: exp(-sum(log(p(w_d))) / N).\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 360, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nFor the 2D Rastrigin function, formally derive the analytical conditions for all critical points. Use the Hessian's eigenvalue spectrum to classify these points as local minima, maxima, or saddle points, and then implement a Newton-Raphson method in Python to converge to a non-trivial saddle point, verifying the theoretical classification.\n", "completion": "{\"route_plan\": [\"/math(Derive the gradient and Hessian matrix for the 2D Rastrigin function. Solve for all critical points and use the Hessian eigenvalue properties to create a formal classification rule for minima vs. saddle points.)\", \"/general-search(query=\\\"site:arxiv.org saddle point finding newton method rastrigin function analysis\\\", mode=rag)\", \"/code(Implement a Newton-Raphson algorithm in Python to find a saddle point of the 2D Rastrigin function, validating the numerical convergence to a point where the Hessian has mixed-sign eigenvalues.)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation and analytical solutions for critical points. Then, the blue /general-search agent is used to gather literature on suitable numerical methods for saddle-point finding, which informs the practical implementation. Finally, /code is used to implement the algorithm and empirically validate the theoretical findings from the /math agent.\", \"expected_artifacts\": [\"- A LaTeX document detailing the mathematical derivation of the gradient, Hessian, critical points, and their classification.\", \"- A Python script implementing the Newton-Raphson method and the Hessian eigenvalue analysis for verification.\", \"- A convergence plot showing the optimization path towards the saddle point.\", \"- A summary report (.md) synthesizing the findings, including plots and at least two citations from authoritative sources.\", \"- A JSON file containing the final metrics for the located saddle point.\"], \"thinking_outline\": [\"1. Define the 2D Rastrigin function and compute its first and second partial derivatives to find the gradient vector and Hessian matrix.\", \"2. Analytically solve for the set of all critical points where the gradient is zero.\", \"3. Formulate a classification rule for these critical points (local minima, maxima, saddle points) based on the definiteness of the Hessian matrix, determined by its eigenvalues.\", \"4. Implement a numerical method, such as a modified Newton-Raphson algorithm, designed to converge to saddle points.\", \"5. Validate the computationally discovered point by executing the implementation, checking its location against the analytical solution, and verifying that the numerically computed Hessian at that point has one positive and one negative eigenvalue.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The /math agent produces the analytical target (the properties of a saddle point) which is passed to /code. The /general-search agent provides algorithmic context to the /code agent. The /code agent implements the search and produces a candidate point. The final results from /code are then compared against the theoretical derivation from /math for verification. Fallback: If the /code implementation fails to converge, the /general-search agent will be re-queried for alternative saddle-point finding algorithms with better stability properties.\", \"todo_list\": [\"- [ ] /math: Derive the gradient and Hessian of the 2D Rastrigin function.\", \"- [ ] /math: Solve for the grid of critical points and establish the eigenvalue-based classification criteria.\", \"- [ ] /general-search: Find reference papers on Newton-based methods for locating saddle points in non-convex optimization.\", \"- [ ] /code: Implement the Newton-Raphson method to locate a non-origin saddle point.\", \"- [ ] /code: Validate the found point by computing its Hessian and verifying it has one positive and one negative eigenvalue.\", \"- [ ] /router: Perform QA review, confirming the consistency between the mathematical derivation and the computational results.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"numerical-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly identifies the locations and classification of all critical points.\", \"- The Python code successfully converges to a point that is analytically known to be a saddle point.\", \"- The computed eigenvalues of the Hessian at the final point confirm its saddle point nature (one positive, one negative).\", \"- The final report includes all specified artifacts and cites at least two relevant academic papers.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 443, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI want to understand saddle points in non-convex optimization. Can you explain the concept using the function f(x, y) = x^2 - y^2, derive its Hessian matrix, and then write a Python script to visualize the surface and confirm the saddle point's nature through its eigenvalues?\n", "completion": "{\"route_plan\": [\"/math(Derive the gradient and Hessian matrix for f(x,y) = x^2 - y^2, then find the critical point and classify it by calculating the Hessian's eigenvalue spectrum.)\", \"/code(Write a Python script using NumPy and Matplotlib to visualize the function's 3D surface and programmatically verify the saddle point's nature by checking the convergence of numerical eigenvalue calculations.)\"], \"route_rationale\": \"The route starts with /math for the formal analytical derivation of the critical point and its properties via the Hessian matrix. It then proceeds to /code for empirical validation and visualization, which provides an intuitive understanding and a rigorous check of the mathematical result, fulfilling the math_plus_code variant requirement.\", \"expected_artifacts\": [\"A PDF document with the mathematical derivation of the gradient, Hessian, critical point, and eigenvalue analysis.\", \"A Python script `saddle_point_visualization.py` that generates the visualization and computes eigenvalues.\", \"A PNG image `saddle_point_surface.png` showing the 3D surface plot.\", \"A JSON file `metrics.json` containing the analytically derived and numerically computed eigenvalues.\"], \"thinking_outline\": [\"1. Define the function f(x, y) = x^2 - y^2 and state the goal: find and classify its critical points.\", \"2. Use multivariable calculus to compute the gradient \u2207f(x, y) and set it to zero to find the critical point(s).\", \"3. Compute the Hessian matrix H(x, y) for the function.\", \"4. Evaluate the Hessian at the critical point and calculate its eigenvalues to classify the point as a local minimum, maximum, or saddle point.\", \"5. Verify the analytical findings by implementing a Python script to plot the function's surface and programmatically compute the eigenvalues of the Hessian at the critical point, ensuring they match the derivation.\"], \"handoff_plan\": \"/math -> /code -> router QA (verification: code must numerically compute eigenvalues that match the math derivation; fallback: if a mismatch occurs, /math re-checks the derivation and /code audits its numerical implementation for errors).\", \"todo_list\": [\"- [ ] /math: Derive the gradient, Hessian, and classify the critical point of f(x, y) = x^2 - y^2 using its eigenvalues (verify results)\", \"- [ ] /code: Implement a Python script to visualize the 3D surface and contour plot of the function.\", \"- [ ] /code: Add functionality to the script to numerically compute the Hessian's eigenvalues at the critical point.\", \"- [ ] Verification: Compare the eigenvalues from the /math derivation with the numerical output from the /code script.\", \"- [ ] Final QA: Package the derivation, code, plot, and metrics for router QA approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"non-convex-optimization\", \"saddle-point\", \"hessian-matrix\"], \"acceptance_criteria\": [\"- The mathematical derivation must correctly identify the critical point at (0,0) and its corresponding eigenvalues.\", \"- The Python script must generate a clear 3D plot that visually represents a saddle point at the origin.\", \"- The eigenvalues computed by the Python script must numerically match the analytically derived eigenvalues to within a tolerance of 1e-9.\", \"- All expected artifacts (derivation PDF, Python script, PNG plot, metrics JSON) must be delivered.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 497, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the Karush-Kuhn-Tucker (KKT) conditions for training a simple MLP classifier with an L1 penalty and a hard constraint on the maximum eigenvalue of the weight matrix's covariance, then implement a projected gradient descent algorithm in PyTorch to solve it and demonstrate its convergence properties on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/math(Formulate the Lagrangian for an MLP with an L1 penalty and a spectral norm constraint. Derive the complete Karush-Kuhn-Tucker (KKT) conditions to characterize the optimal weight matrix's eigenvalue distribution.)\", \"/general-search(query=\\\"site:arxiv.org projected gradient descent spectral norm constraint OR eigenvalue projection algorithm\\\", mode=rag)\", \"/code(Implement projected gradient descent in PyTorch for the derived KKT system. Numerically validate the constraints and plot the algorithm's convergence rate, ensuring the solution path respects the theoretical optimality conditions., using Python)\"], \"route_rationale\": \"The route starts with /math to establish the rigorous theoretical foundation (the optimization problem and its optimality conditions). Then, the blue /general-search agent is tasked with finding practical algorithmic solutions for the specific projection step required by the math, which is a non-standard operation. Finally, /code is used to translate the mathematical theory and algorithmic research into a working implementation, perform numerical validation, and generate the final results.\", \"expected_artifacts\": [\"A LaTeX document containing the full derivation of the Lagrangian and Karush-Kuhn-Tucker conditions.\", \"A Python script (using PyTorch) that implements the projected gradient descent solver for the constrained problem.\", \"A markdown report (`report.md`) visualizing the convergence of the loss function and the satisfaction of the eigenvalue constraint over training iterations.\", \"A JSON file (`metrics.json`) reporting the final loss, accuracy, and constraint violation.\", \"A bibliography file citing at least two relevant academic sources for the projection algorithm.\"], \"thinking_outline\": [\"1. Define the complete optimization problem: specify the objective function (e.g., cross-entropy loss plus L1 regularization) and the inequality constraint on the maximum eigenvalue of W^T * W.\", \"2. Construct the Lagrangian function for this problem by introducing a Lagrange multiplier for the eigenvalue constraint.\", \"3. Derive the full set of Karush-Kuhn-Tucker (KKT) conditions: stationarity (subgradient form), primal feasibility, dual feasibility, and complementary slackness.\", \"4. Design a projected gradient descent algorithm where the gradient step minimizes the objective and the projection step (informed by the search) enforces the eigenvalue constraint.\", \"5. Validate the implementation by running it on a synthetic dataset and checking that the final solution approximately satisfies the derived KKT conditions, particularly the stationarity and complementary slackness conditions.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA (verification: /code outputs are audited against the KKT conditions from /math; fallback: if the solver in /code fails to converge or satisfy constraints, the problem is routed back to /general-search to find more robust projection algorithms).\", \"todo_list\": [\"- [ ] /math: Derive the complete KKT conditions for the specified constrained optimization problem.\", \"- [ ] /general-search: Find and summarize established algorithms for projecting a matrix onto the set of matrices with a bounded spectral norm.\", \"- [ ] /code: Implement the projected gradient descent (PGD) algorithm in PyTorch, including the projection step.\", \"- [ ] /code: Generate plots showing loss convergence and the largest eigenvalue of the weight matrix's covariance over iterations.\", \"- [ ] /code: Validate numerically that the final trained weights and Lagrange multiplier satisfy the KKT stationarity condition within a reasonable tolerance.\", \"- [ ] router: Perform a final QA review of the mathematical derivation's correctness, the code's implementation, and the coherence of the final report.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"intermediate\", \"tags\": [\"lagrangian-multipliers\", \"karush-kuhn-tucker\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"The mathematical derivation of the KKT conditions must be complete, correctly handling the non-differentiability of the L1 norm.\", \"The Python implementation must converge on a synthetic dataset, with the final weight matrix satisfying the maximum eigenvalue constraint within a numerical tolerance of 1e-5.\", \"The final report must include at least one plot clearly demonstrating the algorithm's convergence and another plot showing the eigenvalue constraint is satisfied throughout training.\", \"The submission correctly cites at least two relevant academic sources (e.g., from arXiv) for the projection method.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 571, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project on neural network optimization and want to understand how the eigenvalues of the Hessian matrix describe the curvature of the loss landscape. Can you explain this concept and demonstrate it for a simple regression model?\n", "completion": "{\"route_plan\": [\"/math(Derive the Hessian matrix for a simple linear regression model's MSE loss function and explain how its eigenvalue relates to the loss surface's curvature.)\", \"/general-search(query=\\\"site:arxiv.org Hessian eigenvalues loss landscape introductory tutorial\\\", mode=both)\", \"/code(Implement a Python script using NumPy/JAX to compute and visualize the loss landscape and Hessian eigenvalues for a 2D linear regression problem, highlighting the relationship to optimization convergence.)\"], \"route_rationale\": \"The route begins with /math to establish the formal mathematical foundation. The blue /general-search agent then gathers authoritative context and citations to ground the explanation. Finally, /code provides a concrete, visual implementation to solidify the user's understanding.\", \"expected_artifacts\": [\"A formal mathematical derivation of the Hessian matrix for MSE loss in a markdown file.\", \"A Python script that generates a visualization of the loss landscape, its principal curvatures, and the computed eigenvalues.\", \"A summary report integrating the derivation, visualization, and explanation, including at least two citations.\", \"A verification log confirming that the numerically computed eigenvalues align with the visualized landscape curvature.\"], \"thinking_outline\": [\"1. Define the Mean Squared Error (MSE) loss for a simple linear regression model with two parameters (weights).\", \"2. Analytically derive the gradient and the Hessian matrix of this loss function with respect to the model weights.\", \"3. Explain the theoretical connection between a symmetric matrix's eigenvalues/eigenvectors and the principal axes of a quadratic form, which locally approximates the loss function.\", \"4. Implement code to compute the Hessian for a sample dataset and perform an eigen-decomposition.\", \"5. Create a 2D contour plot of the loss surface and overlay the eigenvectors scaled by their corresponding eigenvalues to visually validate that they align with the principal curvatures.\", \"6. Summarize the findings, explicitly linking larger eigenvalues to steeper curvature and its implications for gradient-based optimization.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivation from /math provides the formulas needed for the /code agent. The /general-search agent provides papers that the /code agent can reference for context and that will be cited in the final report. All artifacts are passed to the router for final review. (verification: code output of eigenvalues matches the visualized curvature of the loss surface; fallback: if discrepancy, re-run /math with specific weight values to debug the analytical derivation).\", \"todo_list\": [\"- [ ] /math: Derive the Hessian for the linear regression MSE loss function.\", \"- [ ] /general-search: Find 2+ foundational papers or tutorials on Hessian analysis in deep learning.\", \"- [ ] /code: Implement the visualization of the loss landscape and its eigenvectors.\", \"- [ ] Verification: Manually check if the calculated eigenvalues in the code correspond to the curvature shown in the plot.\", \"- [ ] Router QA: Consolidate all artifacts into a final report and approve for delivery.\"], \"difficulty\": \"introductory\", \"tags\": [\"linear-algebra\", \"eigen-decomposition\", \"spectral-analysis\"], \"acceptance_criteria\": [\"The mathematical derivation of the Hessian must be correct and clearly explained.\", \"The Python code must run without errors and produce a contour plot where eigenvectors correctly align with the principal axes of the loss landscape.\", \"The final report must clearly explain the link between the Hessian's eigenvalues and loss curvature, citing at least two authoritative sources.\", \"The primary and secondary metrics must meet their specified targets.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 586, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the optimality conditions using the Karush-Kuhn-Tucker (KKT) framework for a neural network regularized with both an L1-norm sparsity penalty and a Frobenius norm constraint on its weight matrices. Then, implement a projected gradient descent algorithm in PyTorch to train this network, ensuring the Frobenius norm constraint is satisfied at each step, and validate the convergence properties against the theoretical KKT conditions.\n", "completion": "{\"route_plan\": [\"/math(Derive the Lagrangian and full Karush-Kuhn-Tucker conditions for a neural network loss function with an L1 sparsity penalty and a Frobenius norm constraint on weight matrices, analyzing the problem's energy landscape.)\", \"/code(Implement a projected gradient descent solver in PyTorch to train a multilayer perceptron under the derived Frobenius norm constraint and L1 penalty. Validate the projection step's correctness and track convergence of the Lagrangian. Enable CUDA support., using Python)\", \"/general-search(query=\\\"site:arxiv.org projected gradient descent frobenius norm neural network sparsity KKT conditions convergence analysis\\\", mode=rag)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation by deriving the KKT conditions. This is followed by /code to create a practical implementation and empirically validate the theory. Finally, the blue /general-search agent is used to find authoritative literature on arXiv to contextualize the method and compare it with state-of-the-art approaches.\", \"expected_artifacts\": [\"A formal mathematical derivation of the KKT conditions for the constrained optimization problem.\", \"A Python script implementing the projected gradient descent algorithm in PyTorch.\", \"A report summarizing the convergence analysis, comparing empirical results with theoretical expectations.\", \"A bibliography with at least two cited arXiv preprints on constrained optimization in deep learning.\", \"Verification logs showing the Frobenius norm constraint is satisfied after each projection step.\"], \"thinking_outline\": [\"1. Formulate the constrained optimization problem by defining the objective function (e.g., cross-entropy loss + L1 regularization) and the inequality constraint (Frobenius norm <= C).\", \"2. Construct the Lagrangian for the problem and derive the complete Karush-Kuhn-Tucker (KKT) conditions: stationarity, primal feasibility, dual feasibility, and complementary slackness.\", \"3. Design the projected gradient descent algorithm, deriving the projection operator that maps an arbitrary weight matrix onto the Frobenius norm ball.\", \"4. Implement the algorithm in Python/PyTorch and train a model on a synthetic dataset. During training, log the objective function, constraint violation, and the gradient of the Lagrangian.\", \"5. Validate the implementation by numerically checking if the KKT conditions hold at the converged solution. For instance, verify that the gradient of the Lagrangian with respect to the weights is close to zero.\", \"6. Audit the results by searching for relevant literature to contextualize the method and compare its convergence properties to other constrained optimization techniques like ADMM.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math directly informs the projection operator implemented in /code. Verification: The code's final converged state is passed back to be checked against the KKT conditions derived by /math. Fallback: If the KKT residuals are high, the derivation from /math is re-audited for correctness before re-running the code. /general-search provides citations for the final report. Router QA performs the final consistency check.\", \"todo_list\": [\"- [ ] /math: Formulate the optimization problem and derive the Lagrangian and KKT conditions.\", \"- [ ] /math: Derive the closed-form solution for the projection onto the Frobenius norm ball.\", \"- [ ] /code: Implement the projected gradient descent algorithm in PyTorch.\", \"- [ ] /code: Add logging to track loss, gradient norms, and constraint satisfaction at each iteration.\", \"- [ ] /math: Verify that the implemented projection operator in the code correctly matches the mathematical derivation.\", \"- [ ] /code: Validate that the converged solution from the code numerically satisfies the KKT conditions derived by /math.\", \"- [ ] /general-search: Find and summarize at least two relevant arXiv papers on constrained optimization for neural networks.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"constrained-optimization\", \"lagrangian-methods\", \"deep-learning-theory\"], \"acceptance_criteria\": [\"The mathematical derivation of the KKT conditions must be complete and formally correct.\", \"The Python implementation must converge on a synthetic dataset, and the Frobenius norm constraint must be strictly satisfied (within 1e-6 tolerance) after every update step.\", \"The final report must include a plot showing the convergence of the objective function and a numerical check demonstrating that the KKT stationarity condition holds at the solution.\", \"At least two relevant research papers from arXiv are cited and summarized.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 624, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm new to optimization theory and want to understand the challenges of non-convex landscapes. Can you help me analyze the function f(x, y) = x^3 - 3x + y^2, find its critical points, and use the Hessian eigenvalue test to classify them as local minima, maxima, or saddle points?\n", "completion": "{\"route_plan\": [\"/math(Derive the gradient and Hessian matrix for f(x, y) = x^3 - 3x + y^2. Solve for the critical points where the gradient is zero and analyze the Hessian eigenvalue properties to ensure convergence tests are applicable.)\", \"/code(Implement the function, its gradient, and Hessian in Python with NumPy. Write a script to find the critical points numerically and classify them based on the Hessian's eigenvalues. Visualize the 3D surface plot with Matplotlib, marking each classified point.)\", \"/general-search(query=\\\"site:mit.edu OR site:stanford.edu introductory non-convex optimization saddle point Hessian analysis\\\", mode=rag)\"], \"route_rationale\": \"The plan starts with /math to establish the analytical solution for the critical points and their classification via the Hessian test. This analytical ground truth is then used by /code to implement a numerical solution and create visualizations. Finally, the blue /general-search agent is tasked with finding high-quality educational material to provide broader context and reference for the introductory concepts.\", \"expected_artifacts\": [\"A markdown file with the mathematical derivation of the gradient, Hessian, critical points, and their classification based on Hessian eigenvalues.\", \"A Python script (`saddle_point_analysis.py`) that numerically finds and classifies the critical points and generates a 3D visualization.\", \"A PNG image (`function_surface.png`) showing the 3D plot of the function with annotated critical points.\", \"A summary report (`references.md`) citing at least two authoritative web resources or papers on second-order derivative tests for optimization.\", \"A verification log (`verification.log`) confirming that the numerically calculated points match the analytical derivations.\"], \"thinking_outline\": [\"1. Analytically compute the first partial derivatives (the gradient) of f(x, y) and set them to zero to solve for the coordinates of the critical points.\", \"2. Compute the second partial derivatives to form the Hessian matrix of the function.\", \"3. For each critical point, evaluate the Hessian matrix and compute its eigenvalues.\", \"4. Classify each point: positive definite Hessian (local minimum), negative definite (local maximum), or indefinite (saddle point).\", \"5. Verify that the Python implementation correctly identifies the same critical points and classifications as the analytical derivation.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: The numerical results from /code must exactly match the analytical solutions from /math for the locations and types of all critical points. fallback: If there's a mismatch, /code must debug its numerical root-finding or eigenvalue calculation, potentially increasing precision.)\", \"todo_list\": [\"- [ ] /math: Derive the analytical solution for critical points and their classification.\", \"- [ ] /code: Implement the numerical analysis and visualization in Python.\", \"- [ ] /general-search: Find and summarize two high-quality references on the topic.\", \"- [ ] Verification: Cross-check the results from the /math and /code steps.\", \"- [ ] Router QA: Submit all artifacts for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"non-convex-optimization\", \"saddle-point\", \"hessian-analysis\"], \"acceptance_criteria\": [\"- The analytical derivation correctly identifies all critical points of f(x, y) = x^3 - 3x + y^2.\", \"- The Python script produces a 3D plot that clearly visualizes the function's surface and correctly labels the saddle point and the local minimum.\", \"- The classification of each critical point (e.g., local min, saddle) in the code's output matches the classification derived from the Hessian eigenvalue test in the math derivation.\", \"- The summary includes at least two citations from university course notes or reputable online educational resources.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 631, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm starting a project in biomedical signal processing and need to understand the Alternating Direction Method of Multipliers (ADMM). Can you please derive the ADMM updates for a standard LASSO problem, explain its convergence guarantees, and provide a basic implementation?\n", "completion": "{\"route_plan\": [\"/math(Derive the ADMM update steps for the LASSO problem, starting from the augmented Lagrangian formulation. Detail the primal-dual updates and the role of the penalty parameter, ensuring the convergence conditions are stated.)\", \"/general-search(query=\\\"site:stanford.edu/~boyd OR site:arxiv.org distributed optimization admm lasso convergence proof\\\", mode=web)\", \"/code(Implement the derived ADMM updates in Python with NumPy for a synthetic LASSO problem. Validate the implementation's convergence and solution accuracy against the scikit-learn baseline.)\"], \"route_rationale\": \"/math is used first to establish the theoretical foundation and derive the core equations. The 'blue' /general-search agent is then tasked with finding authoritative literature to support the convergence claims. Finally, /code implements the derived algorithm and validates it empirically.\", \"expected_artifacts\": [\"A LaTeX-formatted PDF containing the step-by-step mathematical derivation of the ADMM updates for LASSO.\", \"A Python script (`admm_lasso.py`) implementing the solver.\", \"A summary document citing at least two foundational papers on ADMM, explaining the convergence guarantees.\", \"A convergence plot showing the primal residual norm decreasing over iterations.\", \"A JSON file with final metrics comparing the ADMM solution to the scikit-learn baseline.\"], \"thinking_outline\": [\"1. Formulate the standard LASSO problem and rewrite it in the ADMM consensus form f(x) + g(z) subject to x - z = 0.\", \"2. Construct the augmented Lagrangian for this formulation, including the quadratic penalty term.\", \"3. Derive the iterative update steps for the primal variables (x, z) and the dual variable (y) by applying block coordinate descent.\", \"4. Implement the derived updates in a Python function, creating a simple solver.\", \"5. Validate the solver's output and convergence on a synthetic dataset by checking its solution against the one from `sklearn.linear_model.Lasso`.\"], \"handoff_plan\": \"/math (derivation) -> /general-search (theory verification) -> /code (implementation & empirical validation) -> router QA. The math agent hands off the exact update equations to the code agent. The search agent provides the theoretical justification for convergence. Verification: The code agent's solution must match the baseline from a trusted library. Fallback: If the code fails to converge, revisit the /math agent's derivation for potential errors.\", \"todo_list\": [\"- [ ] /math: Derive the complete ADMM update equations for the LASSO objective function.\", \"- [ ] /general-search: Find and summarize two key papers on ADMM convergence, noting the required assumptions.\", \"- [ ] /code: Implement the derived updates in a Python script.\", \"- [ ] /code: Add a validation routine comparing the solver's output against `sklearn.linear_model.Lasso` on a synthetic dataset.\", \"- [ ] Verification: Confirm that the mean squared error between the custom solver and the baseline is below a small threshold (e.g., 1e-6).\", \"- [ ] Router QA: Package all artifacts (derivation PDF, code, plot, citations, metrics) for final approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"convex-optimization\", \"admm\", \"lasso-regression\"], \"acceptance_criteria\": [\"- The mathematical derivation must be complete, correct, and clearly explain each step from the Lagrangian to the final update rules.\", \"- The Python implementation must converge on a test problem and produce coefficient estimates that are within 1e-5 MSE of the scikit-learn LASSO solver's results.\", \"- The final report must cite at least two authoritative papers on ADMM (e.g., from Boyd et al.) with correct arXiv IDs or DOIs.\", \"- The convergence plot must show a monotonically decreasing primal residual.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 633, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a Bayesian Neural Stochastic Differential Equation (SDE) model for continuous-time generative modeling of financial time-series data. Please provide the full mathematical derivation of the variational evidence lower bound (ELBO) for a general It\u00f4 process driven by a neural network, including the application of Girsanov's theorem. Then, implement this in JAX using the reparameterization trick for the SDE solution path and demonstrate its training convergence on a synthetic dataset.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org bayesian neural stochastic differential equations variational inference girsanov theorem\\\", mode=both)\", \"/math(Derive the evidence lower bound (ELBO) for a latent SDE model, applying It\u00f4 calculus and Girsanov's theorem for the change of measure to formulate the variational objective for Bayesian inference.)\", \"/code(Implement the Bayesian Neural SDE in JAX with Diffrax, including an SDE solver, the derived ELBO loss function, and a training loop demonstrating convergence on a synthetic dataset., using Python)\", \"/math(Verify the analytical gradient of the drift component against a numerical approximation to check the correctness of the backpropagation-through-solver implementation., includes cybersecurity)\"], \"route_rationale\": \"The plan starts with the blue general-search agent to gather state-of-the-art literature on Bayesian Neural SDEs. This informs the /math agent's formal derivation of the complex variational objective. The /code agent then implements this theory. A final /math step is crucial for verification, ensuring the implemented gradients, which are non-trivial, align with mathematical theory, preventing subtle bugs.\", \"expected_artifacts\": [\"- A PDF document containing the full mathematical derivation of the SDE ELBO.\", \"- A Python script (`main.py`) implementing the JAX model and training logic.\", \"- A plot (`convergence.png`) showing the training loss (negative ELBO) over epochs.\", \"- A `report.md` file summarizing the results and including citations for at least two papers.\", \"- A `verification_log.txt` file detailing the gradient check results from the final math step.\"], \"thinking_outline\": [\"1. Research existing literature on Neural SDEs, variational autoencoders, and the application of Girsanov's theorem in this context.\", \"2. Formally derive the continuous-time evidence lower bound (ELBO) for a latent SDE model, clearly showing how the prior, posterior, and likelihood terms are formulated.\", \"3. Implement the SDE model architecture, a numerical solver (e.g., Euler-Maruyama or a higher-order method like SOSRI), and the variational loss function in JAX.\", \"4. Validate the numerical SDE solver by checking its strong and weak convergence properties on a known analytical SDE, such as the Ornstein-Uhlenbeck process.\", \"5. Train the model on a synthetic dataset and generate sample trajectories to qualitatively assess generative performance.\", \"6. Audit the implemented ELBO loss and its gradients against the formal derivation to verify correctness and numerical stability.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The /general-search agent provides foundational papers to /math. The /math agent produces a formal derivation (LaTeX/PDF) that serves as a specification for the /code agent. The /code agent's implementation is then handed back to the /math agent for a gradient verification step. Fallback: If the gradient check fails, the /math agent will flag the specific discrepancy and return the task to the /code agent for debugging.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers on Bayesian Neural SDEs from arXiv.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the SDE ELBO.\", \"- [ ] /code: Implement the neural network components and the SDE solver.\", \"- [ ] /code: Implement the variational objective function and the main training loop.\", \"- [ ] /math: Verify the analytical gradients of the implementation match numerical estimates.\", \"- [ ] /code: Execute the training run, generate convergence plots and sample trajectories.\", \"- [ ] /code: Consolidate all artifacts (code, plots, report, derivation) into a final deliverable.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-differential-equations\", \"bayesian-inference\", \"generative-modeling\"], \"acceptance_criteria\": [\"- The provided derivation for the ELBO correctly applies It\u00f4 calculus and Girsanov's theorem.\", \"- The JAX implementation runs without errors, and the training loss shows a clear decreasing trend.\", \"- Generated time-series samples from the trained model are qualitatively distinct from random noise.\", \"- The final report cites at least two relevant papers with their arXiv IDs.\", \"- The gradient verification log shows a relative error below 1e-4.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the average Evidence Lower Bound on a held-out test set after training. Higher values indicate better model fit.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"For a simple SDE with a known solution (e.g., Ornstein-Uhlenbeck), measure the Mean Absolute Error between the numerical solver's output and the analytical solution over a fixed time horizon.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 651, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need a comprehensive analysis of dictionary learning for sparse signal representation. Please derive the K-SVD algorithm from first principles, detailing the optimization problem and the singular value decomposition update step. Subsequently, implement K-SVD and Orthogonal Matching Pursuit (OMP) in Python to recover sparsely generated signals from underdetermined measurements, and evaluate the reconstruction quality as a function of dictionary coherence and signal sparsity level k.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org K-SVD dictionary learning convergence analysis OMP complexity\\\", mode=both)\", \"/math(Derive the K-SVD dictionary atom update rule by minimizing the Frobenius norm of the residual error, showing how SVD provides the optimal rank-1 approximation and relating it to eigenvalue properties., includes numpy)\", \"/code(Implement K-SVD and OMP in Python using NumPy. Validate correctness by tracking reconstruction error convergence and ensuring sparse code complexity remains bounded, avoiding any Bayesian frameworks for this deterministic approach.)\", \"/math(Analyze the mutual coherence of the learned dictionary and formally connect it to the theoretical recovery guarantees of OMP, considering the confidence interval of the reconstruction error.)\"], \"route_rationale\": \"The plan starts with literature review via the 'blue' /general-search agent to ground the work in established theory. The first /math agent is for the core theoretical derivation of the K-SVD algorithm. /code is then used to translate this theory into a practical, verifiable implementation. A final /math agent analyzes the output of the simulation from a theoretical standpoint, connecting empirical results back to the mathematical guarantees found in the literature.\", \"expected_artifacts\": [\"- A formal mathematical derivation of the K-SVD algorithm's update steps in LaTeX.\", \"- A Python script implementing K-SVD and OMP for sparse signal recovery.\", \"- Plots visualizing reconstruction MSE vs. iteration and the learned dictionary atoms.\", \"- A summary report in Markdown that integrates the derivation, experimental results, and citations of at least two relevant papers.\", \"- A JSON file containing the final reconstruction MSE and dictionary mutual coherence metrics.\"], \"thinking_outline\": [\"1. Research the foundational papers on K-SVD and Orthogonal Matching Pursuit (OMP) to understand the problem formulation and algorithmic details.\", \"2. Formulate the dictionary learning optimization problem as minimizing the reconstruction error under a sparsity constraint, and formally derive the two stages of K-SVD: sparse coding via OMP and the dictionary atom update via SVD.\", \"3. Implement the complete K-SVD learning loop and the OMP algorithm in Python, including a function to generate synthetic sparse signals and a measurement matrix.\", \"4. Validate the implementation by creating a test case with a known dictionary and verifying that the algorithm recovers it, while also checking that the reconstruction error is monotonically non-increasing over iterations.\", \"5. Conduct experiments to measure the signal reconstruction MSE as a function of dictionary size and signal sparsity, generating plots to visualize these relationships.\", \"6. Audit the learned dictionary by calculating its mutual coherence and discuss how this empirical value relates to the theoretical conditions for exact recovery with OMP.\"], \"handoff_plan\": \"/general-search (literature review) -> /math (derivation) -> /code (implementation) -> /math (analysis) -> router QA. Verification at each step: /math will validate its derivation against sources from /general-search. /code will implement the exact formulation from /math, with unit tests for OMP. The final /math analysis will use the dictionary generated by /code. Fallback: If the derivation is flawed, /math must re-consult /general-search. If the code's error does not decrease, it's returned to /code for debugging against the derivation.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on K-SVD (e.g., Aharon et al.) and OMP recovery guarantees.\", \"- [ ] /math: Produce a step-by-step derivation of the K-SVD dictionary update procedure.\", \"- [ ] /code: Implement the OMP algorithm for sparse approximation.\", \"- [ ] /code: Implement the full K-SVD algorithm, using the OMP implementation for the sparse coding stage.\", \"- [ ] /code: Generate synthetic data and run experiments to test the implementation.\", \"- [ ] /math: Verify that the reconstruction error curve from the /code output behaves as theoretically expected (monotonically non-increasing).\", \"- [ ] /math: Analyze the mutual coherence of the final dictionary produced by /code.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"sparse-coding\", \"dictionary-learning\", \"compressed-sensing\"], \"acceptance_criteria\": [\"- The provided Python code must successfully run and generate a dictionary that reduces reconstruction error over iterations.\", \"- The mathematical derivation must be complete and logically sound, correctly employing SVD for the rank-1 approximation step.\", \"- The final report must accurately connect the theoretical derivation to the empirical results, including plots and metrics.\", \"- The project must cite at least two relevant academic papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Compute the average squared difference between the original and reconstructed signals: (1/N) * ||X - D*A||_F^2, where N is the number of signals, X is the original signal matrix, D is the learned dictionary, and A is the sparse coefficient matrix.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Calculate max_{i!=j} |d_i^T * d_j| for all pairs of distinct, normalized columns (atoms) d_i, d_j of the learned dictionary D. A value closer to 0 is better.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 661, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm investigating the optimization landscape of high-dimensional non-convex functions, common in deep learning. Specifically, I need a rigorous analysis of second-order stationary points. Please derive the conditions for a point to be a 'strict saddle', analyze the local dynamics of perturbed gradient descent in its vicinity by examining the Hessian eigenvalue spectrum, and then validate this theoretical escape behavior numerically. How does the strict saddle property provide guarantees for global convergence of first-order methods?\n", "completion": "{\"route_plan\": [\"/math(Derive the second-order optimality conditions for a strict saddle point, analyzing the Hessian matrix's eigenvalue spectrum and its implications for local dynamics under perturbation.)\", \"/code(Implement perturbed gradient descent in Python to visualize escape trajectories from a monkey saddle point, analyzing the convergence behavior and its parallels to risk landscapes in quantitative finance.)\", \"/general-search(query=\\\"site:arxiv.org 'strict saddle' 'global convergence' non-convex optimization deep learning\\\", mode=web. Find foundational papers linking the strict saddle property to guaranteed convergence, potentially connecting to diffusion model theory.)\"], \"route_rationale\": \"The route initiates with /math to establish the core theoretical framework of strict saddles and Hessian analysis. This theory is then passed to /code for empirical validation and visualization of the escape dynamics. Finally, the blue /general-search agent is tasked with contextualizing these findings within the broader academic landscape, providing citations to support the claims about global convergence.\", \"expected_artifacts\": [\"A formal mathematical derivation of strict saddle conditions and the local dynamics of perturbed gradient descent, delivered as a LaTeX-formatted PDF.\", \"A Python script (JAX/NumPy) implementing perturbed gradient descent on a non-convex function, generating a 3D plot of the function landscape and the algorithm's escape trajectory.\", \"A summary report in Markdown that connects the derivation to the simulation results and cites at least two relevant research papers.\", \"A JSON file containing the primary and secondary metrics from the simulation run.\", \"A verification log detailing the checks performed between the mathematical theory and the code implementation.\"], \"thinking_outline\": [\"1. Formally define a second-order stationary point in non-convex optimization and differentiate between local minima, maxima, and saddle points using the Hessian matrix.\", \"2. Derive the 'strict saddle' property, emphasizing the requirement for the Hessian to have at least one strictly negative eigenvalue, and analyze the geometry of the local landscape.\", \"3. Theoretically model how isotropic noise added to gradient updates allows the algorithm to escape the saddle along the direction of negative curvature corresponding to the negative eigenvector.\", \"4. Implement a numerical simulation for a canonical non-convex function (e.g., f(x,y) = x^3 - 3xy^2) to visualize the landscape and the escape trajectory of perturbed gradient descent.\", \"5. Validate that the numerical escape trajectory aligns with the direction predicted by the principal eigenvector of the Hessian at the saddle point by computing their cosine similarity.\", \"6. Audit the cited literature to confirm that the strict saddle property is a common feature in deep learning loss landscapes and is sufficient for first-order methods to achieve global convergence to a local minimum.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math defines the Hessian conditions and expected escape vector, which are inputs for the /code agent's implementation and validation logic. The numerical results from /code are used to confirm the math. /general-search provides literature to frame the final report. Router QA performs a final consistency check. Fallback: If the code's validation metric (trajectory alignment) is low, the task is returned to /math to check the derivation for the specific function used.\", \"todo_list\": [\"- [ ] /math: Derive the Hessian eigenvalue conditions for a strict saddle point.\", \"- [ ] /math: Analyze the local update rule for perturbed gradient descent near the saddle to predict the escape direction.\", \"- [ ] /code: Implement the test function (e.g., monkey saddle) and its gradient/Hessian.\", \"- [ ] /code: Implement perturbed gradient descent and log the trajectory when initiated near the saddle.\", \"- [ ] /general-search: Find and summarize at least two key papers (e.g., by Jin, Ge, Jordan) on the strict saddle property and convergence guarantees.\", \"- [ ] /math: Verify that the Hessian implementation in the code correctly matches the analytical derivation.\", \"- [ ] /code: Validate that the observed escape trajectory has a high cosine similarity (>0.95) with the dominant negative eigenvector of the Hessian at the saddle.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"non-convex-optimization\", \"saddle-point-analysis\", \"hessian-methods\"], \"acceptance_criteria\": [\"- The mathematical derivation correctly identifies that a strict saddle requires a Hessian with at least one negative eigenvalue.\", \"- The Python simulation must produce a visualization clearly showing the algorithm's trajectory escaping the saddle point (0,0) and converging to a minimum.\", \"- The secondary metric `trajectory_alignment_cosine_similarity` must be greater than 0.95, confirming the escape path aligns with the theory.\", \"- The final report must cite and accurately summarize at least two relevant arXiv papers with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 750, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nHow can I use a neural network to solve a simple 1D heat equation? I need the basic theory behind Physics-Informed Neural Networks (PINNs) and a Python example demonstrating how the PDE residual loss term enforces the physics.\n", "completion": "{\"route_plan\": [\"/math(Formulate the loss function for a Physics-Informed Neural Network (PINN) designed to solve the 1D heat equation, clearly defining the data, boundary, and PDE residual terms based on the governing Euler-Lagrange equation.)\", \"/code(Implement a simple PINN in Python using PyTorch to solve the 1D heat equation. Validate the model by plotting the solution and checking convergence of the PDE residual loss to near-zero.)\", \"/general-search(query=\\\"site:arxiv.org physics-informed neural networks introductory heat equation tutorial\\\", mode=web)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation of the PINN loss function for the heat equation. This theory is then implemented and validated using /code to provide a practical demonstration. Finally, the blue /general-search agent is used to find authoritative academic papers from arXiv, grounding the derived theory and implementation in established literature.\", \"expected_artifacts\": [\"A mathematical derivation of the PINN loss function for the 1D heat equation.\", \"A Python script (pinn_heat_equation.py) implementing the PINN solver.\", \"A plot visualizing the predicted solution against the analytical solution.\", \"A summary of at least two relevant arXiv papers on introductory PINNs.\", \"A verification report confirming the PDE residual loss converges below a threshold.\"], \"thinking_outline\": [\"1. Define the 1D heat equation, including initial and boundary conditions.\", \"2. Formulate the composite loss function for a PINN, breaking it down into the mean squared error for initial/boundary data and the PDE residual.\", \"3. Implement a neural network in Python (PyTorch) and use automatic differentiation to compute the derivatives required for the PDE residual.\", \"4. Train the network and validate the solution by plotting the predicted temperature distribution and verifying that the final PDE residual loss is acceptably low (e.g., < 1e-4).\", \"5. Synthesize findings from foundational PINN papers to confirm the methodology.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: The Python code's loss function must exactly match the mathematical derivation's terms. The final plot must show a physically plausible solution. Fallback: If code validation fails, re-run /math to check the derivation for errors in boundary condition formulation.)\", \"todo_list\": [\"- [ ] /math: Derive the complete loss function for the 1D heat equation PINN.\", \"- [ ] /code: Implement the PINN solver in PyTorch, including model, loss, and training loop.\", \"- [ ] /code: Generate a plot comparing the PINN's output to the known analytical solution.\", \"- [ ] /general-search: Find and summarize two foundational papers on PINNs from arXiv.\", \"- [ ] Verification: Check that the PDE residual loss in the code converges to a value below 1e-4.\", \"- [ ] Router QA: Submit all artifacts for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"neural-networks\"], \"acceptance_criteria\": [\"The mathematical derivation of the loss function must be correct and complete.\", \"The Python script must run without errors and produce a solution where the mean absolute error against the analytical solution is less than 0.05.\", \"The PDE residual loss must decrease during training and reach a final value below 1e-4.\", \"The final report must cite at least two relevant arXiv papers with correct IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 775, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the variational lower bound for the mutual information between the input and the latent representation in a masked autoencoder's self-attention head, framing it through the Information Bottleneck principle. Can you then implement a minimal JAX model to empirically validate this bound's tightness under varying masking ratios and analyze its convergence properties with respect to the latent space's eigenvalue distribution?\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org 'information bottleneck' self-attention 'masked autoencoder' variational inference\\\", mode=web)\", \"/math(Derive the variational Bayesian lower bound for the mutual information objective, ensuring the formulation is tractable for convergence analysis and referencing foundational work from the arxiv search.)\", \"/code(Implement the masked autoencoder and derived loss in JAX; track the latent space eigenvalue distribution and reconstruction error to ensure the model avoids trivial solutions on a benchmark like MNIST., using Python)\", \"/math(Analyze the empirical convergence data and bound tightness from the JAX model, performing a statistical check against theoretical predictions and establishing a confidence interval for the key metrics.)\"], \"route_rationale\": \"The 'blue' /general-search agent first gathers seminal papers to ground the theory. Next, /math formalizes the derivation based on this research. Then, /code provides an empirical testbed to validate the derived theory. Finally, /math returns to analyze the experimental results and verify alignment between theory and practice.\", \"expected_artifacts\": [\"A LaTeX document containing the full mathematical derivation of the variational Information Bottleneck bound for self-attention.\", \"A commented JAX/Flax implementation of the masked autoencoder model and the training loop.\", \"A final report in PDF format analyzing the results, including plots of bound tightness versus masking ratio, and citing at least two relevant research papers.\", \"A JSON file with the logged metrics, such as the variational bound components and reconstruction error, for each experimental run.\", \"A verification script output demonstrating that the analytical gradients match numerical approximations.\"], \"thinking_outline\": [\"1. Formalize the problem by defining the relevant random variables (input, mask, latent representation, reconstruction) in the context of a masked autoencoder and the Information Bottleneck (IB) framework.\", \"2. From the foundational literature, derive a tractable variational lower bound (ELBO) for the IB objective, I(X;Z) - \u03b2I(Z;Y), where Z is the latent representation from the self-attention mechanism.\", \"3. Implement this ELBO as a loss function in a JAX-based masked autoencoder, ensuring the architecture is minimal but sufficient to test the hypothesis on a dataset like MNIST.\", \"4. Validate the implementation by performing a gradient check, comparing the analytical gradients used in the optimizer with numerical gradients computed via finite differences.\", \"5. Execute a series of experiments, varying the masking ratio and the \u03b2 parameter, to generate data on the bound's tightness and the model's convergence behavior.\", \"6. Verify that the empirical results align with the theoretical predictions, analyzing how the latent space's eigenvalue distribution changes and how this correlates with the IB objective.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The initial math derivation is handed to the coder. The code's output (metrics.json) is handed back to the math agent for analysis. Verification: The final analysis must explicitly check if the empirical convergence rates from /code match the theoretical properties derived by /math. Fallback: If the derivation from /math is intractable or the model from /code fails to converge, the router will re-engage /general-search to find alternative IB formulations.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on the Information Bottleneck principle as applied to self-attention or deep variational models.\", \"- [ ] /math: Produce a complete, step-by-step derivation of the variational lower bound for the specified IB objective.\", \"- [ ] /code: Implement the masked autoencoder architecture and the derived loss function in JAX.\", \"- [ ] /code: Develop and execute an experimental script to train the model across a range of masking ratios, logging all specified metrics to a JSON file.\", \"- [ ] /math: Verify the correctness and logical flow of the mathematical derivation produced in the first /math step.\", \"- [ ] /code: Verify the gradient implementation in the JAX code using a numerical finite-difference check.\", \"- [ ] /math: Analyze the logged metrics from the coding step, generate plots, and write a conclusion connecting the empirical results to the initial theory.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"self-supervised-learning\", \"attention-theory\", \"information-bottleneck\"], \"acceptance_criteria\": [\"- The submitted mathematical derivation must be self-contained, with all steps clearly justified.\", \"- The JAX code must be executable and include a passing gradient verification test.\", \"- The final report must contain plots illustrating the trade-off between compression and prediction as a function of the masking ratio, and it must cite at least 2 relevant arXiv preprints with their IDs.\", \"- The reported metrics must quantitatively demonstrate that the model learned a non-trivial representation (i.e., reconstruction MSE is significantly below a random baseline).\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"Computed as the Kullback-Leibler divergence term in the final ELBO, E_q[log q(z|x) - log p(z)]. Lower values indicate a tighter bound. Track its mean value after convergence.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"Computed as the slope of the log-log plot of the sorted eigenvalues of the latent representation's covariance matrix. A steeper slope suggests a more compressed, lower-dimensional representation.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/validation.jsonl", "index": 809, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nHow can I use the concept of Shannon entropy from information theory to quantify the unpredictability of a generative AI's output and potentially detect simple adversarial attacks on image classifiers?\n", "completion": "{\"route_plan\": [\"/math(Derive the formula for Shannon entropy for a discrete probability distribution, and explain how it applies to the output of a softmax layer in a neural network, treating each class prediction as a Bernoulli trial outcome.)\", \"/code(Write a Python script using PyTorch to calculate the Shannon entropy of a pre-trained image classifier's predictions on both a clean image and a simple FGSM-based adversarial example, demonstrating the entropy difference.)\", \"/general-search(query=\\\"site:arxiv.org shannon entropy adversarial attack detection generative model uncertainty\\\", mode=rag)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation of Shannon entropy. This theory is then implemented and validated in a practical AI safety scenario using /code. Finally, the blue /general-search agent is used to find authoritative references on arXiv, grounding the approach in existing research.\", \"expected_artifacts\": [\"A markdown file with the mathematical derivation of Shannon entropy.\", \"A Python script implementing entropy calculation for classifier outputs on clean and adversarial inputs.\", \"A results summary in a JSON file comparing entropy values and prediction confidence.\", \"A list of at least 2 relevant arXiv paper citations with IDs in a .bib file.\", \"Verification output log confirming the Python code runs and produces the expected entropy shift.\"], \"thinking_outline\": [\"1. Formally define Shannon entropy for a discrete random variable and derive its mathematical formula from first principles (e.g., additivity, continuity).\", \"2. Explain how the output vector of a softmax activation function in a classifier can be interpreted as a probability distribution.\", \"3. Hypothesize why an adversarial attack might alter the entropy of this distribution (e.g., by making the model less certain and pushing the distribution towards uniform).\", \"4. Implement the entropy calculation in Python and apply it to a real classifier's output for a clean image.\", \"5. Generate a simple adversarial example (e.g., using FGSM) and validate that the entropy of the classifier's prediction increases, confirming the hypothesis.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA (verification: Python script must correctly implement the derived entropy formula and show a higher entropy for the adversarial example; fallback: if entropy decreases or stays the same, re-run /code with a stronger attack or different image).\", \"todo_list\": [\"- [ ] /math: Derive the Shannon entropy formula and explain its application to softmax outputs.\", \"- [ ] /code: Implement the entropy calculation in Python for a classifier on a clean and an adversarial image.\", \"- [ ] /general-search: Find at least two key papers on using entropy for adversarial detection.\", \"- [ ] /verification: Run the Python script and confirm that the entropy for the adversarial example is higher than for the clean one.\", \"- [ ] /router-qa: Review all artifacts for correctness, clarity, and consistency before final approval.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"introductory\", \"tags\": [\"information-theory\", \"adversarial-attacks\", \"ai-safety\"], \"acceptance_criteria\": [\"The mathematical derivation of Shannon entropy must be correct and clearly explained.\", \"The Python code must execute without errors and demonstrate a measurable increase in prediction entropy for the adversarial input compared to the clean input.\", \"At least two relevant research papers from arXiv must be cited with their IDs.\", \"The final report must correctly connect the theoretical concept, the code's output, and the findings from the cited literature.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/test.jsonl", "index": 18, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nHow can I implement a simple Physics-Informed Neural Network (PINN) to solve the 1D Burgers' equation? I need to understand the theoretical basis of using the strong form of the PDE in the loss function and see a working Python example.\n", "completion": "{\"route_plan\": [\"/math(Derive the composite loss function for a Physics-Informed Neural Network (PINN) designed to solve the 1D Burgers' equation, treating the PDE residual as a soft constraint analogous to an Euler-Lagrange variational principle.)\", \"/code(Implement a PINN in Python using PyTorch to solve the 1D Burgers' equation, visualizing the solution and tracking the loss components to check for convergence.)\", \"/general-search(query=\\\"site:arxiv.org physics-informed neural networks Burgers' equation tutorial\\\", mode=both)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation for the PINN loss function. This theory is then implemented and tested using /code. Finally, the blue /general-search agent is used to find authoritative references to justify the methodology and compare results against established work.\", \"expected_artifacts\": [\"A formal mathematical derivation of the PINN loss function for the 1D Burgers' equation.\", \"A documented Python script implementing the PINN solution using PyTorch.\", \"A plot comparing the PINN's predicted solution u(x,t) against the exact solution.\", \"A metrics report in JSON format detailing final MSE and loss components.\", \"A bibliography with at least two cited arXiv papers.\"], \"thinking_outline\": [\"1. Formulate the 1D Burgers' equation, defining the problem domain, initial conditions (e.g., a sine wave), and Dirichlet boundary conditions.\", \"2. Derive the PINN composite loss function, showing how it combines the Mean Squared Error (MSE) from the boundary/initial data and the MSE of the PDE residual evaluated at collocation points.\", \"3. Implement a feedforward neural network in Python to approximate the solution u(x,t), using automatic differentiation to compute the partial derivatives required for the PDE residual.\", \"4. Train the network by minimizing the composite loss function, and visualize the evolution of the solution over time.\", \"5. Validate the final trained model by computing the L2 error against the known analytical solution and verifying that the PDE residual is minimized across the domain.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math provides the exact loss function formula for the /code agent to implement. The /code agent produces a solution plot and metrics, which are then contextualized by canonical examples found by /general-search. The complete package is submitted for review. Verification: The implemented loss in code must match the derivation. Fallback: If the code fails to converge, re-evaluate the neural network architecture or optimizer settings based on recommendations from the searched literature.\", \"todo_list\": [\"- [ ] /math: Derive the complete PINN loss for the 1D Burgers' equation, detailing the data and physics residual terms.\", \"- [ ] /code: Write a Python script to implement and train the PINN, ensuring automatic differentiation is correctly applied to the network output.\", \"- [ ] /code: Generate a plot comparing the PINN's predicted solution against the ground truth for a given time point.\", \"- [ ] /general-search: Find at least two key papers or tutorials on PINNs for Burgers' equation to contextualize the approach.\", \"- [ ] /verification: Check that the implemented loss function in the Python code exactly matches the mathematical derivation from the /math step.\", \"- [ ] /router QA: Submit all artifacts for final approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"physics-informed-ml\", \"deep-learning\", \"differential-equations\"], \"acceptance_criteria\": [\"- The mathematical derivation of the loss function must be correct and clearly distinguish between data loss and physics loss.\", \"- The Python code must execute successfully and produce a solution plot that qualitatively shows the characteristic shock wave formation of the Burgers' equation.\", \"- The final report must cite at least two relevant arXiv papers with their IDs.\", \"- The primary metric (MSE vs ground truth) must be below 1e-3.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/test.jsonl", "index": 93, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nCan you help me understand the REINFORCE algorithm for policy gradients? I want to see the mathematical derivation for the policy gradient theorem and then implement it in Python for the CartPole environment, including a simple value function baseline to reduce variance.\n", "completion": "{\"route_plan\": [\"/math(Derive the policy gradient theorem using the likelihood ratio trick, starting from the objective function of maximizing expected return for a Bernoulli action distribution.)\", \"/code(Implement the REINFORCE algorithm with a simple value function baseline in Python for the CartPole-v1 environment, and plot the reward convergence curve.)\", \"/general-search(query=\\\"site:arxiv.org REINFORCE algorithm variance reduction policy gradient baseline\\\", mode=rag)\"], \"route_rationale\": \"The route starts with /math to establish the theoretical foundation of the policy gradient theorem. This is followed by /code to translate the theory into a practical, working implementation and validate the learning dynamics. Finally, the blue /general-search agent is used to find foundational literature that justifies the use of a baseline for variance reduction, providing academic context.\", \"expected_artifacts\": [\"A markdown file with the step-by-step mathematical derivation of the policy gradient theorem.\", \"A Python script implementing the REINFORCE algorithm with a baseline for CartPole-v1.\", \"A plot showing the episodic rewards over training, demonstrating learning convergence.\", \"A summary report explaining the role of the baseline in variance reduction, with citations.\", \"A JSON file containing the final evaluation metrics.\"], \"thinking_outline\": [\"1. Formulate the reinforcement learning objective function as the expected total reward.\", \"2. Use the log-derivative trick (likelihood ratio) to derive the policy gradient theorem, showing how to estimate the gradient without knowing the environment dynamics.\", \"3. Introduce a state-dependent baseline to the policy gradient update rule to reduce the variance of the gradient estimator.\", \"4. Implement the REINFORCE algorithm in Python, including the policy network and the baseline network, and train it on the CartPole environment.\", \"5. Verify that the implementation leads to performance improvement by plotting the moving average of episodic rewards and checking for a clear upward trend.\"], \"handoff_plan\": \"/math -> /code -> /general-search -> router QA. The mathematical derivation from /math provides the exact gradient formula needed for the /code implementation. The working code provides practical context for the /general-search agent to find relevant papers on performance improvements like baselines. The router performs a final verification to ensure the code's gradient calculation matches the math derivation and is supported by the cited literature. Fallback: if convergence fails in /code, revisit the /math derivation for potential errors.\", \"todo_list\": [\"- [ ] /math: Derive the policy gradient theorem, clearly showing the application of the log-derivative trick.\", \"- [ ] /code: Implement the REINFORCE algorithm with a value function baseline in Python for CartPole-v1.\", \"- [ ] /general-search: Find and summarize at least two foundational papers on policy gradients and variance reduction techniques.\", \"- [ ] /code: Generate a plot of episodic rewards versus training episodes.\", \"- [ ] Verification: Check that the implemented gradient update in the code correctly matches the derived mathematical formula.\", \"- [ ] Router QA: Submit all artifacts for final review and approval.\"], \"difficulty\": \"introductory\", \"tags\": [\"policy-gradient\", \"reinforcement-learning\", \"variance-reduction\"], \"acceptance_criteria\": [\"The mathematical derivation of the policy gradient theorem is correct and self-contained.\", \"The Python implementation successfully trains an agent that solves CartPole-v1 (achieves an average reward of >= 195 over 100 consecutive episodes).\", \"The final report correctly explains the purpose of a baseline and cites at least two relevant academic papers with arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/test.jsonl", "index": 142, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the full expression for the Hessian of the cross-entropy loss for a fully-connected layer employing weight normalization (Salimans & Kingma, 2016). Analyze how this reparameterization affects the spectral norm and the principal eigenvalue (\u03bb_max) of the Hessian, and formally connect these spectral properties to optimization convergence rates, referencing recent literature from arXiv.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org \\\\\\\"weight normalization\\\\\\\" Hessian spectral norm \\\\\\\"convergence rate\\\\\\\" OR eigenvalue\\\", mode=both)\", \"/math(Derive the Hessian matrix of the loss with respect to the pre-normalized weights `v`. Apply tensor calculus and the chain rule, analyzing the structure's impact on its largest eigenvalue and relating it to the model's geometry.)\", \"/code(Implement a PyTorch model with a weight-normalized layer. Use `torch.autograd.functional.hessian` combined with Power Iteration to empirically estimate the maximum eigenvalue (\u03bb_max). Validate convergence speed against a standard layer., using Python)\", \"/math(Synthesize the theoretical derivation and the empirical eigenvalue results from the code. Formulate a rigorous argument connecting the reduced spectral norm of the Hessian to improved convergence guarantees and validate the code's empirical results against theoretical bounds.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to gather foundational literature from arXiv as requested. This informs the core theoretical work in /math, which derives the Hessian. The /code agent then provides empirical verification of the theoretical findings by numerically estimating the key spectral metric (\u03bb_max). Finally, /math synthesizes both theoretical and empirical results into a conclusive proof, including a verification step.\", \"expected_artifacts\": [\"A LaTeX document (.pdf) containing the full mathematical derivation of the Hessian for the weight-normalized layer.\", \"A Python script (.py) implementing the network and the Power Iteration method to estimate the Hessian's largest eigenvalue.\", \"A Markdown report (.md) summarizing the findings, embedding plots of eigenvalue evolution, and connecting the theory to empirical convergence results, including citations.\", \"A JSON file (`metrics.json`) logging the largest eigenvalue and training loss per epoch.\", \"Verification notes confirming the consistency between the analytical derivation and numerical estimates.\"], \"thinking_outline\": [\"1. Formulate the weight normalization reparameterization (w = g * v / ||v||) and the cross-entropy loss function for a single layer.\", \"2. Apply matrix calculus, specifically the multivariate chain rule, to derive the gradient and then the Hessian of the loss with respect to the pre-normalized weight vector `v` and scale `g`.\", \"3. Implement a numerical routine in Python to compute the Hessian and estimate its largest eigenvalue. Verify that the empirical results for a test case align with the derived analytical form.\", \"4. Analyze the structure of the derived Hessian to prove how weight normalization projects out gradient components, effectively damping updates and improving the landscape's conditioning (reducing \u03bb_max).\", \"5. Validate the final proof by cross-referencing with established results on optimization and implicit regularization from the literature search, ensuring the argument is sound.\", \"6. Audit the final report to ensure the theoretical claims are directly supported by the empirical data generated by the code.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA (verification: code's empirical eigenvalue estimation must align with the analytical bounds from the /math derivation; fallback: if discrepancy > 5%, re-run /math to check derivation for errors.)\", \"todo_list\": [\"- [ ] /general-search: Find and summarize at least two key papers from arXiv on weight normalization and Hessian analysis.\", \"- [ ] /math: Complete the formal derivation of the Hessian, clearly showing each step of the chain rule application.\", \"- [ ] /code: Implement the PyTorch model and the Power Iteration algorithm for eigenvalue estimation.\", \"- [ ] /code: Generate plots showing the evolution of the maximum eigenvalue and training loss for both normalized and unnormalized layers.\", \"- [ ] /math: Verify that the code's output for the largest eigenvalue is numerically consistent with the derived Hessian structure.\", \"- [ ] /math: Finalize the proof connecting the spectral analysis to the observed convergence improvements.\", \"- [ ] /router: Perform final QA review, ensuring all artifacts (proof, code, report, citations) are consistent and meet acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"spectral-analysis\", \"matrix-calculus\", \"optimization-theory\"], \"acceptance_criteria\": [\"The derived Hessian expression in the LaTeX document must be mathematically correct and fully expanded.\", \"The Python script must execute without errors and generate plots clearly showing a lower maximum Hessian eigenvalue for the weight-normalized layer compared to the baseline.\", \"The final report must cite at least 2 relevant arXiv papers with IDs and cogently link the theoretical derivation to the empirical results.\", \"The primary metric (max_eigenvalue_Hessian) must be reported and show a statistically significant reduction for the normalized model.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_guidance\": \"The largest eigenvalue (\u03bb_max) of the loss function's Hessian. Compute using the Power Iteration method on the Hessian matrix, which is obtained via `torch.autograd.functional.hessian`.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_guidance\": \"The number of training epochs required to reach a target cross-entropy loss threshold (e.g., < 0.05). A lower value is better.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_guidance", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/test.jsonl", "index": 148, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nDerive the strong-form residual loss for a Physics-Informed Neural Network (PINN) solving the 2D steady, incompressible Navier-Stokes equations. Implement this using a Fourier feature network in JAX to mitigate spectral bias, validating convergence on the lid-driven cavity problem by tracking the energy spectrum decay and comparing against high-fidelity simulation data.\n", "completion": "{\"route_plan\": [\"/math(Derive the multi-component loss function for the 2D Navier-Stokes PINN, including PDE residual, incompressibility constraint, and boundary conditions. Analyze the theoretical impact of Fourier features on the neural tangent kernel's eigenvalue decay to justify its use for improving convergence.)\", \"/code(Implement the Navier-Stokes PINN in JAX, incorporating Fourier feature mappings. Train on the lid-driven cavity benchmark, logging PDE residual loss, L2 error against a reference solution, and computing the kinetic energy spectrum to verify physical plausibility., using Python)\", \"/general-search(query=\\\"site:arxiv.org physics-informed neural network Navier-Stokes fourier features spectral bias convergence\\\", mode=rag)\"], \"route_rationale\": \"/math is used first to establish the theoretical foundation and derive the specific loss equations. /code follows to translate the mathematical model into a working implementation and generate empirical results. Finally, the blue /general-search agent is used to find authoritative references from arXiv that support the theoretical choices and contextualize the empirical findings, fulfilling the tri-route reasoning pattern.\", \"expected_artifacts\": [\"- A LaTeX document detailing the derivation of the Navier-Stokes PINN loss function.\", \"- A commented Python script (JAX) implementing the PINN, including the training loop and data generation for the lid-driven cavity problem.\", \"- A summary report (.md) containing plots of the loss curves, velocity field visualizations, and a comparison of the predicted vs. reference energy spectrum.\", \"- A bibliography with at least two cited arXiv preprints on PINNs for fluid dynamics or spectral bias.\", \"- A JSON file containing the final computed metrics (L2 error, mean residual loss).\"], \"thinking_outline\": [\"1. Formulate the strong form of the 2D steady, incompressible Navier-Stokes equations and derive the corresponding PDE residual loss, including terms for momentum in x and y, and the continuity (incompressibility) equation.\", \"2. Define the complete loss function by adding terms for the Dirichlet boundary conditions on the cavity walls (u=v=0 on three walls, u=1, v=0 on the lid).\", \"3. Implement the PINN architecture in Python, ensuring the input coordinates are first passed through a Fourier feature mapping layer before entering the main MLP.\", \"4. Validate the implementation by performing a gradient check on the loss function and ensuring the network output correctly satisfies the boundary conditions at initialization.\", \"5. Train the model and evaluate its performance by calculating the L2 relative error against a known high-fidelity solution and plotting the kinetic energy spectrum to check for physical consistency.\", \"6. Audit the results against findings from peer-reviewed literature to confirm that the observed convergence behavior and accuracy are in line with state-of-the-art methods.\"], \"handoff_plan\": \"/math (loss derivation) -> /code (implementation & training). The /code agent must verify its implementation against the equations from /math. /code (results) -> /general-search (literature context). The final report must synthesize the code's empirical results with the theoretical claims in the literature. Fallback: If the /code agent observes poor convergence or high residual loss (>1e-2), it must halt and request a review of the loss weighting scheme from the /math agent.\", \"todo_list\": [\"- [ ] /math: Derive the complete loss function for the 2D Navier-Stokes equations, including all residual and boundary terms.\", \"- [ ] /math: Provide a brief theoretical justification for using Fourier feature networks to combat spectral bias in PINNs.\", \"- [ ] /code: Implement the PINN architecture with Fourier features and the custom loss function in JAX.\", \"- [ ] /code: Train the model on the lid-driven cavity problem and generate all required plots and metric data.\", \"- [ ] /math: Verify that the Python code's loss implementation correctly matches the derived mathematical formula.\", \"- [ ] /general-search: Find and summarize at least two relevant arXiv papers on PINNs for fluid dynamics, focusing on convergence and spectral methods.\", \"- [ ] /code: Verify that the final L2 relative error is below the target threshold of 5%.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"physics-informed-ml\", \"pde-solvers\", \"fourier-feature-networks\"], \"acceptance_criteria\": [\"- The final L2 relative error of the velocity field must be less than 5% compared to the reference solution.\", \"- The submitted report must include a clear mathematical derivation of the loss function, consistent with the code implementation.\", \"- The energy spectrum plot must show a physically plausible decay rate for the given Reynolds number.\", \"- The report must cite at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/test.jsonl", "index": 162, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to implement and rigorously evaluate Coordinate Ascent Variational Inference (CAVI) for a Bayesian Gaussian Mixture Model with Normal-Wishart priors. Please derive the full update equations for the variational distributions over component responsibilities, means, and precision matrices. Then, implement this in JAX and compare its ELBO convergence against a modern black-box VI technique, like path-derivative-based methods, on a high-dimensional synthetic dataset.\n", "completion": "{\"route_plan\": [\"/math(Derive the closed-form CAVI update equations for a Bayesian Gaussian Mixture Model, ensuring the resulting Evidence Lower Bound (ELBO) formulation is correct to monitor convergence.)\", \"/general-search(query=\\\"site:arxiv.org black-box variational inference bayesian mixture models reparameterization trick\\\", mode=rag)\", \"/code(Implement both the derived CAVI algorithm and the selected black-box VI method in Python, comparing their ELBO convergence and final parameter accuracy on a synthetic dataset, reporting bootstrap confidence intervals on the metrics.)\"], \"route_rationale\": \"The plan starts with /math to establish the theoretical foundation and specific update equations for the CAVI algorithm, which is essential before any implementation. Next, the blue /general-search agent is tasked with finding a suitable modern black-box VI method to serve as a strong baseline for comparison. Finally, /code implements both the derived theory and the researched baseline to conduct an empirical comparison and validation.\", \"expected_artifacts\": [\"A LaTeX document detailing the full mathematical derivation of the CAVI update equations and the ELBO.\", \"A Python script implementing both the CAVI and black-box VI algorithms, runnable on a synthetic dataset.\", \"A summary report in Markdown or PDF format, including plots of ELBO convergence versus iteration/time for both methods.\", \"A JSON file containing the final metrics, including final ELBO, convergence time, and parameter estimation error.\", \"A bibliography file citing at least two relevant academic papers with their arXiv IDs.\"], \"thinking_outline\": [\"1. Formulate the Bayesian Gaussian Mixture Model, specifying the likelihood and the conjugate Normal-Wishart priors for the model parameters.\", \"2. Define the mean-field variational family of distributions and write down the expression for the Evidence Lower Bound (ELBO).\", \"3. Derive the CAVI update equations for each variational factor by taking the expectation of the log joint probability with respect to all other factors.\", \"4. Verify the correctness of the derived update equations by checking for consistency with the principles of exponential families and conjugate priors.\", \"5. Implement both the CAVI algorithm and a black-box VI alternative (e.g., using Pyro or TFP's libraries), ensuring the ELBO is correctly computed and monitored for both.\", \"6. Validate the entire pipeline by comparing the performance of the two methods on a synthetic dataset with known ground-truth parameters, analyzing both convergence speed and the quality of the posterior approximation.\"], \"handoff_plan\": \"/math -> /general-search -> /code -> router QA. The mathematical derivations from /math are passed to /code for implementation. /general-search provides candidate baseline algorithms for /code to implement. The /code agent must verify its ELBO implementation against the formula from /math. Fallback: If the CAVI updates from /math are not yielding a non-decreasing ELBO in the /code implementation, the task is returned to /math to audit the derivations for potential errors.\", \"todo_list\": [\"- [ ] /math: Derive the CAVI update equations for the responsibilities, means, and precision matrices in the BGM model.\", \"- [ ] /math: Provide the final mathematical expression for the ELBO to be used for convergence tracking.\", \"- [ ] /general-search: Identify and summarize two recent black-box variational inference techniques suitable for this comparison.\", \"- [ ] /code: Implement the CAVI algorithm based on the provided mathematical derivations.\", \"- [ ] /code: Implement the chosen black-box VI baseline using a library like Pyro or TensorFlow Probability.\", \"- [ ] /code: Verify that the implemented CAVI ELBO calculation is non-decreasing on a simple test case and matches the derivation.\", \"- [ ] /code: Generate plots comparing the ELBO convergence curves and a table of final performance metrics.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"probabilistic-graphical-models\", \"bayesian-methods\"], \"acceptance_criteria\": [\"The provided LaTeX derivation of the CAVI update equations must be mathematically correct and fully detailed.\", \"The Python implementation of CAVI must show a monotonically non-decreasing ELBO curve during execution.\", \"The final report must contain a comparative plot showing the ELBO convergence of both CAVI and the black-box method over iterations.\", \"The recovered cluster parameters from the synthetic data must have a Mean Squared Error below a threshold of 0.1 relative to the ground truth.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["math_first", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/test.jsonl", "index": 237, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to develop a variational inference procedure for a hierarchical Bayesian model in bioinformatics. The posterior is non-Gaussian and complex. I want to use a more expressive approximating family than mean-field, specifically a planar normalizing flow. Please derive the evidence lower bound (ELBO) for this setup, detail the reparameterization trick for the flow, and implement it in PyTorch to check for convergence and posterior quality.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org variational inference normalizing flows 'planar flow' Rezende Mohamed\\\", mode=web)\", \"/math(Derive the full Evidence Lower Bound (ELBO) for a Bayesian hierarchical model where the variational posterior is parameterized by a multi-layer planar normalizing flow. The derivation must explicitly handle the log-determinant of the Jacobian and the Kullback-Leibler divergence term.)\", \"/code(Implement the amortized variational inference scheme in PyTorch for the specified Bayesian bioinformatics model. The implementation must use a planar flow, track ELBO convergence, and include posterior predictive checks for model validation., using Python)\", \"/math(Verify the analytical gradient of the ELBO derived previously by comparing it to a numerical gradient computed via finite differences on the Python implementation for a single data point. Audit the convergence criteria for stability.)\"], \"route_rationale\": \"The route starts with the blue general-search agent to gather foundational literature on normalizing flows. This informs the /math agent's formal derivation of the model's objective function (ELBO). The /code agent then implements this mathematical specification. A final /math call provides a critical verification step, ensuring the theoretical derivation and practical implementation are consistent before final analysis.\", \"expected_artifacts\": [\"- A LaTeX/PDF document containing the full mathematical derivation of the ELBO for a planar normalizing flow.\", \"- A Python script (`.py`) implementing the variational inference procedure.\", \"- A summary report in Markdown format, including plots of the ELBO convergence and results from posterior predictive checks.\", \"- A `citations.bib` file with at least two relevant papers on normalizing flows and variational inference.\", \"- A verification log showing the comparison between analytical and numerical gradients.\"], \"thinking_outline\": [\"1. Formalize the target Bayesian hierarchical model and its posterior distribution, identifying the sources of complexity that motivate using normalizing flows.\", \"2. Use search to find the original papers on normalizing flows (e.g., by Rezende & Mohamed) to understand the mechanics of planar flows, including the transformation and the calculation of the Jacobian determinant.\", \"3. Derive the ELBO for the model, carefully applying the change of variables formula to account for the normalizing flow and detailing how the reparameterization trick enables low-variance gradient estimation.\", \"4. Implement the model, the variational distribution (planar flow), and the stochastic optimization loop in PyTorch, ensuring metrics like the ELBO are logged correctly.\", \"5. Validate the implementation's correctness by implementing a numerical gradient checker and comparing its output to the analytical gradients produced by PyTorch's autograd for a small, fixed input.\", \"6. Evaluate the quality of the learned posterior approximation by analyzing the ELBO convergence curve and performing posterior predictive checks to see if the model can generate realistic data.\"], \"handoff_plan\": \"/general-search -> /math (derivation) -> /code (implementation) -> /math (verification) -> router QA. The /math agent uses papers from /general-search as a theoretical basis. The /code agent implements the exact formulas from the /math agent's derivation. The second /math call verifies the /code output against the theory. Verification failure at the last step triggers a loop back to the first /math agent to check the derivation. Fallback: If planar flows prove unstable, the /code agent will revert to a simpler radial flow and notify the router.\", \"todo_list\": [\"- [ ] /general-search: Find and summarize seminal papers on normalizing flows for variational inference (e.g., Rezende & Mohamed, 2015; Kingma et al., 2016).\", \"- [ ] /math: Provide a step-by-step derivation of the ELBO, including the log-prob of the base distribution and the log-determinant Jacobian term.\", \"- [ ] /code: Implement the Bayesian hierarchical model and the planar normalizing flow variational family in PyTorch.\", \"- [ ] /code: Write the training loop using the Adam optimizer to maximize the ELBO via stochastic gradient ascent.\", \"- [ ] /math: Write the mathematical formula for the numerical gradient using the finite difference method.\", \"- [ ] /code: Implement the gradient checker to compare the analytical gradient from autograd with the numerical one.\", \"- [ ] /code: Generate and plot the ELBO convergence curve and posterior predictive check visualizations.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"variational-inference\", \"normalizing-flows\", \"bayesian-deep-learning\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO is complete, self-contained, and mathematically sound.\", \"- The Python implementation runs without errors, and the reported ELBO shows a clear trend of convergence during training.\", \"- The verification step confirms that the analytical and numerical gradients match within a small tolerance (e.g., 1e-4).\", \"- The final report correctly interprets the convergence plots and cites at least two relevant academic papers with their arXiv IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"Computed as E_q[log p(x,z) - log q(z|x)]. Track its value per training epoch. A monotonically increasing curve that flattens out indicates successful optimization.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"Calculated from MCMC diagnostics on the resulting posterior samples if applicable, or visual inspection of posterior predictive distributions. Higher ESS or realistic predictive samples indicate a better approximation.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_computation", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/test.jsonl", "index": 371, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI'm trying to model high-frequency financial time series and need to properly quantify posterior uncertainty. Can you help me derive and implement a Bayesian Neural Stochastic Differential Equation (SDE)? Specifically, I want to use variational inference with a recognition network to approximate the posterior path measure. Please detail the derivation of the ELBO using Girsanov's theorem for the change of measure and the adjoint sensitivity method for efficient gradient computation. Also, implement this in JAX, showing how to use an Euler-Maruyama solver and evaluate the model's calibration using CRPS and NLL metrics.\n", "completion": "{\"route_plan\": [\"/general-search(query=\\\"site:arxiv.org neural stochastic differential equations bayesian variational inference Girsanov theorem adjoint sensitivity method\\\", mode=both)\", \"/math(Derive the Evidence Lower Bound for the Neural SDE posterior, applying Girsanov's theorem to handle the Radon-Nikodym derivative between the prior and variational posterior SDEs, focusing on convergence properties.)\", \"/code(Implement the Bayesian Neural SDE in JAX using an Euler-Maruyama solver. The implementation must include the variational objective derived by /math, track CRPS metrics, and handle the score-matching diffusion dynamics., using Python)\", \"/math(Verify the correctness of the coded objective function by deriving the analytical gradients for a simplified drift network component and establishing the convergence criteria for the Bayesian model.)\"], \"route_rationale\": \"The plan starts with the blue /general-search agent to find authoritative arXiv papers on Neural SDEs and variational inference. /math then uses these sources to construct the core mathematical derivation. /code translates this complex derivation into a JAX implementation. A final /math step is included for verification, ensuring the implemented gradients match the formal derivation, which is critical for such complex models.\", \"expected_artifacts\": [\"- A literature review summary citing at least two foundational arXiv papers on Neural SDEs.\", \"- A LaTeX document containing the full mathematical derivation of the variational inference objective.\", \"- A Python script (`neural_sde.py`) with a JAX/Equinox implementation of the model and training loop.\", \"- A verification report comparing analytical and auto-differentiated gradients for a model sub-component.\", \"- A JSON file (`metrics.json`) reporting the final CRPS and NLL scores on a test dataset.\"], \"thinking_outline\": [\"1. Establish the theoretical foundation by surveying literature on Neural SDEs, particularly focusing on variational inference approaches using the adjoint sensitivity method and Girsanov's theorem.\", \"2. Formally derive the Evidence Lower Bound (ELBO) for a continuous-time latent variable model defined by an SDE, explicitly showing the application of Girsanov's theorem for the change of measure.\", \"3. Implement the Neural SDE model in JAX, including a numerical SDE solver (e.g., Euler-Maruyama), the derived variational objective function, and data loaders for a synthetic time-series task.\", \"4. Verify the implementation's correctness by performing a gradient check, comparing analytical gradients derived in the math step with auto-differentiated gradients from the code for a key parameter block.\", \"5. Train the model and evaluate its ability to quantify predictive uncertainty using appropriate metrics like Continuous Ranked Probability Score (CRPS) and Negative Log-Likelihood (NLL).\", \"6. Validate the final results by analyzing the posterior predictive distributions to ensure the model produces well-calibrated and physically plausible uncertainty estimates.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The first /math agent consumes arXiv papers to produce a derivation. /code implements this derivation. The second /math agent verifies the implementation by comparing its analytical gradients against the code's autodiff output. Verification: Gradient mismatch < 1e-5. Fallback: If mismatch is large, return to the first /math agent to re-audit the derivation for errors.\", \"todo_list\": [\"- [ ] /general-search: Find seminal papers on Bayesian Neural SDEs and the adjoint method.\", \"- [ ] /math: Derive the ELBO using Girsanov's theorem, referencing the search results.\", \"- [ ] /code: Implement the Neural SDE, solver, and variational objective in JAX.\", \"- [ ] /math: Derive the analytical gradients for the drift network's final layer weights.\", \"- [ ] /code: Implement a numerical gradient check comparing the analytical and autodiff gradients.\", \"- [ ] /code: Train the model on a synthetic dataset and generate evaluation metrics.\", \"- [ ] /router: Perform final QA review, ensuring derivation, code, and metrics are consistent.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"stochastic-differential-equations\", \"bayesian-deep-learning\", \"variational-inference\"], \"acceptance_criteria\": [\"- The mathematical derivation of the ELBO is complete, self-contained, and correctly applies Girsanov's theorem.\", \"- The JAX implementation trains successfully and achieves a CRPS score below 0.1 on the specified synthetic dataset.\", \"- The verification report shows a relative error between analytical and numerical gradients of less than 1e-5.\", \"- The submission includes citations for at least two relevant arXiv preprints with their IDs.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"primary_computation\": \"Compute as the integral of the squared difference between the model's predictive cumulative distribution function (CDF) and the empirical CDF of the ground truth. Average over the test set.\", \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"], \"secondary_computation\": \"Calculate the average negative log probability density of the test data points under the model's predictive distribution.\"}}", "categories": ["four_step", "math_backstop", "math_multi_pass", "metrics_computation", "non_canonical_route"]}
{"source": "../../Milestone-3/router-agent-scripts/data/vertex_tuning/test.jsonl", "index": 391, "prompt": "You are the Router Agent coordinating Math, Code, and General-Search specialists.\nGiven the user query, respond with a JSON object describing the full routing plan.\n\nRequirements:\n- STRICT JSON with double quotes and no extra commentary.\n- Keys: route_plan, route_rationale, expected_artifacts, thinking_outline,\n  handoff_plan, todo_list, difficulty, tags, acceptance_criteria, metrics.\n- route_plan must be an ordered list of tool invocations.\n- Use the original phrasing when possible.\n\nUser query:\nI need to solve a large, sparse, symmetric positive-definite linear system arising from a finite element discretization of a Poisson equation. Develop the theory for the Preconditioned Conjugate Gradient (PCG) method using an Incomplete Cholesky (IC0) preconditioner. Then, implement this solver in Python using CUDA via CuPy, focusing on optimizing the sparse matrix-vector product (SpMV) and the preconditioner application. Analyze the method's convergence rate and computational complexity, specifically how the preconditioner affects the eigenvalue clustering of the system matrix.\n", "completion": "{\"route_plan\": [\"/math(Derive the Preconditioned Conjugate Gradient (PCG) algorithm for a sparse symmetric positive-definite system. Analyze the theoretical impact of an Incomplete Cholesky (IC0) preconditioner on the system's eigenvalue distribution and expected CUDA convergence rate.)\", \"/general-search(query=\\\"site:arxiv.org efficient CUDA incomplete cholesky factorization sparse conjugate gradient\\\", mode=rag)\", \"/code(Implement the PCG solver with an IC0 preconditioner using Python with CuPy for CUDA acceleration. Profile the SpMV and preconditioner solve steps, ensuring correctness against a known solution and tracking convergence residuals.)\", \"/math(Validate the empirical convergence rate observed in the CUDA implementation against the theoretical bounds derived from the eigenvalue analysis. Quantify the improvement in condition number and justify any discrepancies.)\"], \"route_rationale\": \"/math first establishes the theoretical foundation for PCG and IC0 preconditioning. The blue /general-search agent then gathers state-of-the-art implementation techniques from arXiv to inform the practical coding phase. /code translates the theory into a high-performance CUDA implementation, focusing on profiling and correctness. The final /math call provides a critical verification step, linking empirical results back to the initial theoretical analysis.\", \"expected_artifacts\": [\"A LaTeX document with the full mathematical derivation of the PCG algorithm and the theoretical analysis of the IC0 preconditioner's effect on eigenvalues.\", \"A commented Python script implementing the GPU-accelerated PCG-IC0 solver using CuPy, including a test case with a sparse matrix.\", \"A performance report comparing the convergence of standard CG vs. PCG, including plots of residual norms vs. iteration and a table of execution times.\", \"A summary of findings from the literature search, with at least two cited arXiv papers on GPU-accelerated iterative solvers.\", \"A final verification report confirming the alignment between theoretical convergence rates and empirical results.\"], \"thinking_outline\": [\"1. Formulate the Preconditioned Conjugate Gradient (PCG) algorithm mathematically, detailing each step: initialization, matrix-vector products, dot products, and updates.\", \"2. Theoretically analyze the role of a preconditioner, specifically the Incomplete Cholesky (IC0) factorization, in improving the condition number of the system matrix A by analyzing the eigenvalues of M\u207b\u00b9A.\", \"3. Research and synthesize best practices for implementing sparse linear algebra operations (SpMV, triangular solves) on GPUs using CUDA, leveraging findings from authoritative sources.\", \"4. Implement the PCG-IC0 algorithm in Python with CuPy, ensuring numerical stability and correctness by testing against a small, well-conditioned problem with a known solution.\", \"5. Validate the performance of the implementation on a larger sparse system, plotting the convergence history (residual norm vs. iteration) and comparing it to the unpreconditioned CG method.\", \"6. Audit the empirical convergence results against the theoretical predictions from step 2, explaining any discrepancies and concluding on the effectiveness of the IC0 preconditioner.\"], \"handoff_plan\": \"/general-search -> /math -> /code -> /math -> router QA. The search agent provides literature to the math agent for theoretical grounding. The math agent's derivation serves as a spec for the code agent. The code agent produces an implementation and empirical results, which are handed back to the math agent for final verification. Fallback: If verification fails, the code agent will re-audit its preconditioner implementation for precision issues before final router QA.\", \"todo_list\": [\"- [ ] /math: Derive the PCG algorithm and analyze the spectral properties of the IC0-preconditioned system matrix.\", \"- [ ] /general-search: Find >=2 recent arXiv papers on high-performance CUDA implementations of sparse iterative solvers like PCG with IC0.\", \"- [ ] /code: Implement the PCG-IC0 solver in Python/CuPy for a sparse matrix in CSR format.\", \"- [ ] /code: Profile the CUDA kernel execution times for SpMV and the forward/backward substitution steps of the preconditioner.\", \"- [ ] /math: Verify the correctness of the code's output against a reference solution from a library like SciPy.sparse.linalg.\", \"- [ ] /math: Validate that the empirical convergence plot from the code aligns with the theoretical rate predicted by the condition number analysis.\", \"- [ ] /router: Review all generated artifacts for consistency, correctness, and adherence to acceptance criteria.\", \"- [ ] router QA: consolidate tool outputs, verify citations, and approve the response.\"], \"difficulty\": \"advanced\", \"tags\": [\"numerical-linear-algebra\", \"gpu-acceleration\", \"krylov-subspace-methods\"], \"acceptance_criteria\": [\"The mathematical derivation of PCG and the analysis of the IC0 preconditioner must be complete and correct.\", \"The Python/CuPy implementation must successfully solve a sparse linear system and demonstrate faster convergence (fewer iterations) than the standard Conjugate Gradient method.\", \"The final report must include convergence plots (residual vs. iteration) and a performance table, and cite at least two relevant arXiv papers with IDs.\", \"The verification step must explicitly show the comparison between the theoretical and observed convergence rates.\"], \"metrics\": {\"primary\": [\"Define at least one domain-specific performance metric with formulas and acceptance thresholds.\"], \"secondary\": [\"Report runtime/memory usage and summarize failure cases discovered during evaluation.\"]}}", "categories": ["four_step", "math_backstop", "math_first", "math_multi_pass", "non_canonical_route"]}
